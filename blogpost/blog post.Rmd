---
title: "How popular is a new technique?"
author: "Xiaotian Wang, Ying Chen"
date: "11/9/2020"
output: 
  html_document:
    code_folding: hide
  
---

### Background and data
How long would it take for a new technique to be popular? Do people notice the cutting-edge techniques once itâ€™s published? Are the techniques applied to different areas of study? In this project, we try to answer the questions. 

Full data can be found [here](http://s2-public-api-prod.us-west-2.elasticbeanstalk.com/corpus/).

We would like to explore if some certain journals publish papers related to a technique more often than others. We use the semantic scholar dataset, which provides detailed information on research papers. The huge dataset gives us a chance to explore publishing pattern. we use LASSO as key word, journal names as variable of interest. We bring in title, abstract, year published, and journal name of each paper. Below is the r codes to get data. Every record in each raw data file is searched. If the record contains 'LASSO' (not case sensitive), the information we need will be stored in a separate spreadsheet. We put all .csv spreadsheet in the data folder.  

```{r,message = FALSE}
library(data.table)
library(tidyverse)
library(tidytext)
library(Matrix)
library(vsp)
```


```{r, eval=FALSE,message=FALSE}
library(tidyverse)
source("code/getData.R")

includeLine = function(x) {
  if(nchar(x$paperAbstract) == 0) return(F) 
  grepl("lasso", x$paperAbstract,ignore.case = TRUE)
}

processLine = function(x) tibble(title = x$title, 
                                 abstract = x$paperAbstract,
                                 year = x$year,
                                 journal = x$journalName)
outputPath = "LASSO"

processDataFiles(includeLine, processLine, outputPath)

dat = pullDataFiles(outputPath)
```

We read the data line by line, and decide whether to process the line by the includeLine function. 
For getData function, it can be found here[].


### Data Exploration
Now we have all research papers that mentioned 'LASSO' in abstract. Let's take a look on them.

```{r,warning=FALSE,message=FALSE}
rm(list = ls())
filenames = list.files("data/LASSO")
data = data.frame()
for(name in filenames)
{
  temp = read.csv(paste("data/LASSO/",name,sep=''))
  data = rbind(data,temp)
}
# Delete the date with missing journal
data<-data[data$journal!="",]

str(data)
```
There are 9944 observations after removing papers with no journal name.   
```{r}
freq<-data %>% group_by(journal) %>% mutate(Frequency=n())
freq %>% arrange(desc(Frequency))  %>% distinct(journal, Frequency) %>% 
  filter(Frequency>50)

```
Journals or archives with most papers that mentioned LASSO are ArXiv (290), asXiv: Methodology (160), arXiv: Statistics Theory (154), PLoS ONE (126) and bioRxiv (82). Among the 4,281 journals, there are 262 journals/archives with more than 5 papers that mention LASSO.  

Next, instead of viewing each abstract as a whole object, we get words out of the text. By doing so, we are able to see if these papers have anything in common. 


### Factors
Based on the paper-word pair, we create a matrix with paper ID as row and each word as column. Then, we try the vsp package to see if there are any factors standing out.
```{r}
text_df <- tibble(paper = 1:nrow(data), abstract = data$abstract)
tt  = text_df %>% unnest_tokens(word, abstract)
A = cast_sparse(tt, paper, word)
str(A)
dim(A)
cs = colSums(A)

fa = vsp(A, rank = 4)
fa
abstract = text_df$abstract

```

We do the clustering on the paper-word matrix with rank 3, and see what will happen.

```{r}
topPapers = 5

topDoc = fa$Z %>% 
  apply(2,
        function(x) which(rank(-x, ties.method = "random") <= topPapers)
  )
for(j in 1:ncol(topDoc)){
  paste("topic", j, "\n \n") %>% cat
  data$title[topDoc[,j]] %>% print
  paste("\n \n \n") %>% cat
}
```

Looking at the top five papers in each clusters, we can see that the titles of the papers are gathered by the languages.

For the first cluster, they are mostly English paper, and for the 2nd and 3rd, they are French and German.

```{r}
plot(fa$d)
```

```{r}
plot_varimax_z_pairs(fa, 1:3)
```


### Analysis on English-only papers
From the above result, papers are clustered together by different languages. Then, what if we focus only on one language? As most papers are in English, we repeat the similar analysis among papers in English.

We can use the new includeLine function
```{r,eval=FALSE}
includeLine = function(x) {
  if(nchar(x$paperAbstract) == 0) return(F) 
  grepl("lasso", x$paperAbstract,ignore.case = TRUE)&grepl("regression", x$paperAbstract,ignore.case = TRUE)
}
```
The new includeLine rule becomes we process the line only when it includes the keywords LASSO and the commenly used English word regression.
```{r}
filenames = list.files("data/LASSOreg")
data = data.frame()
for(name in filenames)
{
  temp = read.csv(paste("data/LASSOreg/",name,sep=''))
  data = rbind(data,temp)
}

str(data)

# Delete the date with missing journal
# data<-data[data$journal!="",]


text_df <- tibble(paper = 1:nrow(data), abstract = data$abstract)
tt  = text_df %>% unnest_tokens(word, abstract)
A = cast_sparse(tt, paper, word)
str(A)
dim(A)
# hist(rowSums(A))
cs = colSums(A)
hist(log(cs[cs>1]))


library(vsp)
fa = vsp(A, rank = 4)

abstract = text_df$abstract


topPapers = 5
# just run the next code chunk...

topDoc = fa$Z %>% 
  apply(2,
        function(x) which(rank(-x, ties.method = "random") <= topPapers)
  )
for(j in 1:ncol(topDoc)){
  paste("topic", j, "\n \n") %>% cat
  data$title[topDoc[,j]] %>% print
  paste("\n \n \n") %>% cat
}
plot(fa$d)
plot_varimax_z_pairs(fa, 1:4)

bff(fa$Z,A,6) %>% print
```
