title,abstract,year,journal
Research on regional differences and influencing factors of green technology innovation efficiency of China's high-tech industry,"Abstract Through the K-means clustering analysis, it divides the regions of China into four clusters according to the differences in high-tech industry development level between 2008 and 2016. Considering â€environmental pollutionâ€ and â€innovation failureâ€, an improved SBM-DEA efficiency measurement model was constructed to measure the green technology innovation efficiency of Chinaâ€™s high-tech industry clusters. Lasso regression was used to screen out the factors affecting the green technology innovation efficiency of high-tech industry in each cluster area. On this basis, quantile regression method is used to study the influence degree and regional differences of various influencing factors on green innovation efficiency of high-tech industry at different quantile. Meanwhile, DEA-tobit model is used for robustness test. The research shows that in each cluster area, the factors that significantly affect the green innovation efficiency of high-tech industry are different, and the degree of influence of each factor on the innovation efficiency at different quantile is also different. Combining the empirical results with the reality of high-tech industries in various regions, the corresponding policy recommendations are put forward.",2020,J. Comput. Appl. Math.
Explaining the Stars: Weighted Multiple-Instance Learning for Aspect-Based Sentiment Analysis,"This paper introduces a model of multiple-instance learning applied to the prediction of aspect ratings or judgments of specific properties of an item from user-contributed texts such as product reviews. Each variable-length text is represented by several independent feature vectors; one word vector per sentence or paragraph. For learning from texts with known aspect ratings, the model performs multiple-instance regression (MIR) and assigns importance weights to each of the sentences or paragraphs of a text, uncovering their contribution to the aspect ratings. Next, the model is used to predict aspect ratings in previously unseen texts, demonstrating interpretability and explanatory power for its predictions. We evaluate the model on seven multi-aspect sentiment analysis data sets, improving over four MIR baselines and two strong bag-of-words linear models, namely SVR and Lasso, by more than 10% relative in terms of MSE.",2014,
Apoplastic tracer studies in the leaves of a seagrass. II. Pathway into leaf veins,"The apoplastic tracers Cellufluor and lanthanum nitrate were used at the light and electron microscope levels, respectively, to determine whether an apoplastic pathway exists for solutes from the water column into leaf veins of the seagrass Thalassodendron ciliatum (Forssk.) Den Hartog. Tracers penetrated walls of epidermal and mesophyll cells of leaves within 4 h of exposure, but entered the apoplast of veins only 12â€“24 h later, their path being impeded by barriers in the walls of bundle sheath cells around the veins. The passage of lanthanum was blocked by the compound middle lamella between contiguous sheath cells and impeded by suberin lamellae in all bundle sheath walls, except in pit fields. In these regions, suberin lamellae appear to be microporous. Cellufluor probably crossed bundle sheath walls via the same pathway. The results suggest that bundle sheath cells may slow down the entry of certain solutes similar to these tracers into the veins.",1989,Aquatic Botany
Integrative multi-view regression: Bridging group-sparse and low-rank models.,"Multi-view data have been routinely collected in various fields of science and engineering. A general problem is to study the predictive association between multivariate responses and multi-view predictor sets, all of which can be of high dimensionality. It is likely that only a few views are relevant to prediction, and the predictors within each relevant view contribute to the prediction collectively rather than sparsely. We cast this new problem under the familiar multivariate regression framework and propose an integrative reduced-rank regression (iRRR), where each view has its own low-rank coefficient matrix. As such, latent features are extracted from each view in a supervised fashion. For model estimation, we develop a convex composite nuclear norm penalization approach, which admits an efficient algorithm via alternating direction method of multipliers. Extensions to non-Gaussian and incomplete data are discussed. Theoretically, we derive non-asymptotic oracle bounds of iRRR under a restricted eigenvalue condition. Our results recover oracle bounds of several special cases of iRRR including Lasso, group Lasso, and nuclear norm penalized regression. Therefore, iRRR seamlessly bridges group-sparse and low-rank methods and can achieve substantially faster convergence rate under realistic settings of multi-view learning. Simulation studies and an application in the Longitudinal Studies of Aging further showcase the efficacy of the proposed methods.",2018,Biometrics
Comparative study of two carnivorous fish (Parupeneus forsskali and Thalassoma klunzingeri) from the Red Sea: Hemato-biochemical parameters and cellular characterization.,"Although, the Red sea is highly rich with fish fauna but still the information known is so limited for the researchers especially about the fish physiology baselines. So, in the present study we investigated the heamto-biochemical parameters and cell characterization of two fish having the same feeding habitats. Fish specimens of Red Sea goatfish (Parupeneus forsskali) and Klunzinger's wrasse (Thalassoma klunzingeri) were captured from Hurghada, Egypt. Haematological and biochemical analysis as well as blood cells characterization were performed. The morphological and cytochemical aspects of peripheral blood cells of the two species were studied by light microscopy. Thalassoma klunzingeri showed lower Hct and Hb values and RBCs count. Fusiform to spindle shape thrombocytes were found only in the blood of Parupeneus forsskali while spiked thrombocytes were found only in the blood of Thalassoma klunzingeri. This investigation may be helpful as a tool to monitor the health status of the two species and will be used as biomarkers for clinical pathology.",2020,Tissue & cell
"Carbon-isotope, petrological and floral record in coals: Implication for Bajocian (Middle Jurassic) climate change","Abstract Much of our existing knowledge of Middle Jurassic paleoclimate is based on well-dated marine isotopic records that show fluctuations between warm (greenhouse effected) and cool climates. In contrast, much less is known from contemporaneous terrestrial deposits that are often difficult to correlate stratigraphically with marine successions, and are typically considered as showing regional rather than global signals. In this paper we show that the carbon-isotopic, petrological and floral record through 20 m thick coals in the Northern Qaidam Basin (NW, China) represents a relatively comprehensive northern hemisphere Bajocian (Middle Jurassic) record for terrestrial climatic change. Î´13Ccoal values for 88 samples from the Yuqia area in Northern Qaidam Basin range fromâ€“25.3â€° to â€“22.5â€°, indicating that C3 plants were the main coal-forming vegetation in the region during the Middle Jurassic. This isotopic variation follows fluctuations in the composition of ferns and gymnosperms, where higher ratios correspond to more negative Î´13C values. The similar carbon isotope values of gymnosperms and coal from the Yuqia area indicate that the coal comprises a high proportion of organic material derived from gymnosperm taxa, which is consistent with the very high abundance of diterpenoids in coal and especially pimarane and abietane that are produced primarily by gymnosperms. Results from the coal matrix provide an opportunity to record paleoclimate changes, showing several striking and regular coal-forming cycles with distinct long- and short-term variations. Trends of Î´13Ccoal values changes coupled with the evolution of coal-forming plants may record a gradual increase in paleo-CO2 (pCO2) concentration, water-table level changes and the decrease in abundance of Classopollis conifer pollen through Bajocian. This result is in accordance with the published marine carbonate records for this time, with correlation enabled through matching carbon isotope curves from the terrestrial succession at Yuqia and marine records. The similarity of the terrestrial and marine geochemical and floral record is an indication that the observed paleoclimate signal is a global phenomenon. Furthermore, the high-frequency fluctuation of Î´13Ccoal values, along with the coal petrologic variations may record short-term changes of environmental factors (e.g. temperature or humidity/precipitation), especially during intervals when palaeobotanical composition has not fluctuated intensely.",2020,International Journal of Coal Geology
Association of MRI-derived radiomic biomarker with disease-free survival in patients with early-stage cervical cancer,"Pre-treatment survival prediction plays a key role in many diseases. We aimed to determine the prognostic value of pre-treatment Magnetic Resonance Imaging (MRI) based radiomic score for disease-free survival (DFS) in patients with early-stage (IB-IIA) cervical cancer. Methods: A total of 248 patients with early-stage cervical cancer underwent radical hysterectomy were included from two institutions between January 1, 2011 and December 31, 2017, whose MR imaging data, clinicopathological data and DFS data were collected. Patients data were randomly divided into the training cohort (n = 166) and the validation cohort (n=82). Radiomic features were extracted from the pre-treatment T2-weighted (T2w) and contrast-enhanced T1-weighted (CET1w) MR imagings for each patient. Least absolute shrinkage and selection operator (LASSO) regression and Cox proportional hazard model were applied to construct radiomic score (Rad-score). According to the cutoff of Rad-score, patients were divided into low- and high- risk groups. Pearson's correlation and Kaplan-Meier analysis were used to evaluate the association of Rad-score with DFS. A combined model incorporating Rad-score, lymph node metastasis (LNM) and lymphovascular space invasion (LVI) by multivariate Cox proportional hazard model was constructed to estimate DFS individually. Results: Higher Rad-scores were significantly associated with worse DFS in the training and validation cohorts (P<0.001 and P=0.011, respectively). The Rad-score demonstrated better prognostic performance in estimating DFS (C-index, 0.753; 95% CI: 0.696-0.805) than the clinicopathological features (C-index, 0.632; 95% CI: 0.567-0.700). However, the combined model showed no significant improvement (C-index, 0.714; 95%CI: 0.642-0.784). Conclusion: The results demonstrated that MRI-derived Rad-score can be used as a prognostic biomarker for patients with early-stage (IB-IIA) cervical cancer, which can facilitate clinical decision-making.",2020,Theranostics
Our Lady of Guadalupe: An Ambiguous Symbol,"Documentos guadalupanos: Un estudio sobre las fuentes de informacion tempranas en torno a las mariofanias en el Tepeyac. By Xavier Noguez. (Mexico City: El Colegio Mexiquense, A.C., Fondo de Cultura Economica. 1993. Pp. 280; appendixes 11; illustrations 27.) The Image of Guadalupe. By Jody Brant Smith. Second and revised edition. (Macon, Georgia: Mercer University Press in association with Gracewing/Fowler Wright Books Ltd. 1994. Pp. xvii, 132; appendices 8; illustrations 15. Paperback.) Our Lady of Guadalupe: Faith and Empowerment among Mexican-American Women. By Jeanette Rodriguez. (Austin: University of Texas Press. 1994. Pp. xxxvi, 227; appendices 8; illustrations 4; tables 9. $35.00 clothbound; $13.95 paperback.) The devotion to Our Lady of Guadalupe of Mexico, based on the story of the Virgin's appearance to a native neophyte named Juan Diego at the hill of Tepeyac in December 1531, has an enduring fascination. The Virgin Mary was said to have directed Juan Diego to go to the bishop of Mexico, Juan de Zumarraga, with a request to have him build a church on the place where she appeared. There, she promised, she would be the natives' mother, comfort them in their sorrows, and hear their prayers, tears, and entreaties. As proof of this, she had Juan Diego collect Bowers from the hill at a time when they were not in season and take them in his cloak (tilma) to Zumarraga. When Juan Diego unfolded his tilma, the Virgin's image was imprinted on it. In the past two centuries Guadalupe has become central to Mexican religion and nationality and today is one of the most powerful religious/national symbols in the world. Though the traditional date of the apparitions is 1531, there is no incontrovertible evidence for them before 1648. In that year the Oratorian priest Miguel Sanchez first made the story known in his book Imagen de la Virgen Maria. Sanchez's thesis was that the mariophany was an affirmation of the special position and divine election of the rriollos of New Spain. Six months after Sanchez's work appeared, Luis Lasso de la Vega, the vicar of Guadalupe, published the Huei tlamahuicoltica (1649), an extended treatment in Nahuatl (Aztec) of the apparitions, the shrine, and the miracles worked there. The description of the apparitions, known by its opening words as the Nican mopohua, has come to be regarded in many quarters as the authentic version, perhaps dating back to the very time of the apparitions. Its authorship is often attributed to the famed Nahua scholar and governor, Antonio Valeriano. The story and meaning of Guadalupe has fascinated and continues to fascinate scholars in a variety of disciplines. Historians, ethnologists, anthropologists, linguists, and theologians have mined it for a variety of cultural, political, and religious interpretations. The result has been strong, sometimes acrimonious debate. As Xavier Noguez remarks, ""el tema parece inacabable"" (the subject seems endless). The three books reviewed here reflect this ongoing fascination, and each approaches the controversial devotion from a different point of view. Noguez's Documentos guadalupanos, an outgrowth of his doctoral dissertation at Tulane University, is a study of the earliest sources of the Guadalupan account. The author has set certain limits to this study, ""the more basic and useful objectives"" (p. 13), that is, a detailed examination of the written documentation principally during the first two centuries of Spanish rule. The documents include both originals and those that have survived only in copies. In the latter case Noguez has chosen only those of some certainty and has excluded such hearsay evidence as the account that, according to some testimonies, Zumarraga supposedly wrote. He also excludes Zumarraga's letter to Fernando Cortes, published by Mariano Cuevas and dated by him December 24, 1531, which has no real connection with the Guadalupe phenomenon. Noguez shows how the various documents add to our knowledge of this phenomenon in an incremental way. â€¦",1995,The Catholic Historical Review
Preface for the Special Issue on Computational Techniques in Theoretical and Applied Statistics,"Recent technological developments and the exponential increase in computational power provide researchers in statistics with endless possibilities for developing methodologies and applying new techniques. This special issue, entitled â€œComputational Techniques in Theoretical and Applied Statisticsâ€, highlights some topics and specific problems, combining theoretical results, applications and computational techniques. The works presented in this special issue cover different topics of research, as for example: Distribution Theory, Nonparametric Inference, Shrinkage/Penalized Estimation Strategies, Actuarial Sciences, Survival Analysis, and etc. In what follows we present a brief review of the various works published in this special issue. In Survival Analysis, Balakrishnan and Feng introduced a ConwayMaxwellPoisson Cure rate model under a proportional odds assumption for lifetime of susceptibles, with the baseline either the Weibull or the logistic distribution. A computationally efficient EM algorithm is presented, followed by an extensive Monte Carlo simulation study to illustrate the performance of this flexible model. In the topic of Distribution Theory, Marques developed near-exact approximations for the linear combination of independent Logistic random variables which are based on the difference of two independent Generalized Integer Gamma distributions. The practical implementation of these distributions is only possible using software with high precision. In Bekker and Ferreira, the paper starts with a description of the setting. Bivariate gamma type distributions, within the elliptical framework are proposed, with the bivariate Nakagami distribution that is well known in the communications systems domain to describe the signal behaviour of fading channel, as a special case. Interesting observations follow from the calculated values of the outage probability and Laplace expressions. This work has a significant impact on the assumption of the underlying normality that is pervasive in the current literature in the communications systems field. In Nunes et al., theoretical results about limiting distributions of asymptotically linear statistics are demonstrated and applied through the use of computational simulations. In Wu et al., the authors considered the problem in Actuarial Sciences of modelling aggregate claims with a dependence structure. The authors combine theoretical results for the aggregate claim distributions with computational algorithms for its implementation and present numerical studies for a specific case. In the topic of Shrinkage/Penalized Estimation Strategies, Saleh and Norouzirad, proposed a closed form modified LASSO in multiple regression and compared its performance with shrinkage estimators. More precisely, using the principle of marginal distribution theory, shrinkage and penalized estimation strategies are developed for a non-orthogonal design matrix. Hence, a closed form modified LASSO estimator is proposed for the non-orthogonal design. In an extensive simulation study, the performance of shrinkage estimators is compared with the ridge regression and modified LASSO estimators. The computational aspect of this study is on finding the upper bound of risk functions and theoretical comparisons. In Emami and Kiani, shrinkage differenced based linear unified (Liu) estimators are proposed in semiparametric linear models, when multicollinearity is present. Exact characteristics of the estimators are derived and the region of optimality of each estimator is explored. These attempts formed the computational framework of this study, in the field of shrinkage estimation with multicollinearity problem. In Kazemi et al., a procedure is developed for ultra-high dimensional additive models, to identify nonzero and linear components. Hence a sure independence screening procedure based on the distance correlation between predictors and marginal distribution function of the response variable are proposed. Afterwards, penalized estimation techniques are used to identify nonzero and linear components, simultaneously. In this piece of work, complex computational aspects are revealed by proposing the procedures in ultra-high dimensions and algorithms used for",2018,"Statistics, Optimization and Information Computing"
Lasso-Based Tag Expansion and Tag-Boosted Collaborative Filtering,"With the increasing popularity of the social tagging systems, tags can be effectively utilized to enhance Collaborative Filtering (CF) algorithms. Tags not only reflect users' preference, but also are a cue to describe the semantics of items. This paper formulates the problem of collaborative filtering as random walks over the user-item-tag tripartite graph. In order to alleviate the sparsity of tags, a lasso logistic regression model is conducted to accomplish tag expansion, i.e., adding relevant tags and removing irrelevant tags for each item. Experimental results on MovieLens dataset demonstrate the superiority of the proposed algorithms over several existing CF algorithms in terms of ranking performance measure F1 and Macro DOA.",2010,
"Discussion of ""Least angle regression"" by Efron et al","Algorithms for simultaneous shrinkage and selection in regression and classification provide attractive solutions to knotty old statistical challenges. Nevertheless, as far as we can tell, Tibshirani's Lasso algorithm has had little impact on statistical practice. Two particular reasons for this may be the relative inefficiency of the original Lasso algorithm and the relative complexity of more recent Lasso algorithms [e.g., Osborne, Presnell and Turlach (2000)]. Efron, Hastie, Johnstone and Tibshirani have provided an efficient, simple algorithm for the Lasso as well as algorithms for stagewise regression and the new least angle regression. As such this paper is an important contribution to statistical computing. 1. Predictive performance. The authors say little about predictive performance issues. In our work, however, the relative out-of-sample predictive performance of LARS, Lasso and Forward Stagewise (and variants thereof) takes center stage. Interesting connections exist between boosting and stage-wise algorithms so predictive comparisons with boosting are also of interest. The authors present a simple C p statistic for LARS. In practice, a cross-validation (CV) type approach for selecting the degree of shrinkage, while computationally more expensive, may lead to better predictions. We considered this using the LARS software. Here we report results for the authors' diabetes data, the Boston housing data and the Servo data from the UCI Machine Learning Repository. Specifically, we held out 10% of the data and chose the shrinkage level using either C p or nine-fold CV using 90% of the data. Then we estimated mean square error (MSE) on the 10% hold-out sample. Table 1 shows the results for main-effects models. Table 1 exhibits two particular characteristics. First, as expected, Stage-wise, LARS and Lasso perform similarly. Second, C p performs as well as cross-validation; if this holds up more generally, larger-scale applications will want to use C p to select the degree of shrinkage.",2004,arXiv: Statistics Theory
Percutaneous closure of post-traumatic and congenital muscular ventricular septal defects with the Amplatzer Muscular VSD Occluder.,"BACKGROUND
Muscular ventricular septal defects (VSD) are an important and difficult surgical problem. In the last few years a new alternative has emerged - possibility of VSD closure using percutaneous approach.


AIM
To present our experience in percutaneous closure of congenital muscular and one posttraumatic VSD.


METHODS
We treated 10 patients - 7 children (age 0.8-7 years) and 2 adults (43 and 46 years) with congenital VSD, and one 18-year-old patient with posttraumatic VSD (knife stab). All the patients had a large haemodynamic shunt (Qp:Qs 1.9) and in all cases percutaneous closure attempt with an Amplatzer Muscular VSD Occluder (MVSDO) implant was undertaken. Five of 6 children with multiple muscular VSDs had in infancy previous pulmonary artery banding and one patient had complex heart disease: transposition of great arteries (dTGA), pulmonary stenosis (PS) and perimembranous VSD. All procedures were performed using the standard technique.


RESULTS
Eleven procedures were performed in 10 patients (one child had 2 attempts). Seven procedures were successful. In all cases a considerable reduction in flow or complete closure was achived. In one case, despite multiple attempts, VSD caniulation was ineffective and the procedure was abandoned. The patient had oblique VSD - morphology confirmed was later by the operating surgeon. The reason for the other 3 failures was early embolisation to the left ventricle and aorta. This complication was noted in 2 adult patients - one with congenital and one with post-traumatic VSD. In both cases the interventricular septum was thick (10 and 11 mm) and implants were removed with a bioptome or vascular lasso. Another embolisation occurred in a child with TGA - in this case the cardiac surgeon removed the implant from the aortic arch during Rastelli operation.


CONCLUSION
Our experience acquired during muscular VSD closure with MVSDO indicates that the method is useful in children with isolated defects. Adult patients and children with a complex form of congenital defects should have morphology of MVSDO carefully evaluated and width of the interventricular septum measured to avoid potential implant embolisation.",2008,Kardiologia polska
Assessment of performance of services in a tertiary care Neuropsychiatric Institute using Pabon Lasso Model,"Background: The objective of the study to assess optimal utilisation of hospital facilities &Â  evaluate their performance in a tertiary care hospital using Pabon Lasso Model using indicators- bed turnover (BTO), bed occupancy rate (BOR) and average length of stay (ALS). Aims and Objective: To assess optimal utilisation of hospital facilities various wards catering to psychiatric, neurological and neurosurgery services in a tertiary care hospital using Pabon Lasso model and to identify strategies for more efficient use of the existing health service resources. Materials and Methods: This cross-sectional descriptive study was carried out in 2015 at the Institute of Human Behaviour and Allied Sciences, Delhi, India. This study involved various wards catering to psychiatric, neurological and neurosurgery facilities in the institute. Their performance was evaluated over 8 year period (2007- 2014) using three performance indicators (BTO, BOR & ALS) to assess optimal utilisation of hospital facilities. Results: Psychiatry department was initially located in quadrant IV in 2007 & shifted to quadrant III in 2014 which suggests departmentâ€™s good quantitative performance with small proportion of unused beds. Similarly Neurosurgery department was in quadrant I at its inception in 2010, but shifted to quadrant III in 2014. However, Neurology department was located in quadrant III initially (2007), but shifted to quadrant II indicating either excess bed supply or less need for utilisation. Conclusion: Pabon Lasso model can be used by hospital management for evaluating the performance of health services in cost effective manner. Asian Journal of Medical Sciences Vol.7(5) 2016 69-74",2016,Asian Journal of Medical Sciences
Cultivation-dependent and cultivation-independent characterization of hydrocarbon-degrading bacteria in Guaymas Basin sediments,"Marine hydrocarbon-degrading bacteria perform a fundamental role in the biodegradation of crude oil and its petrochemical derivatives in coastal and open ocean environments. However, there is a paucity of knowledge on the diversity and function of these organisms in deep-sea sediment. Here we used stable-isotope probing (SIP), a valuable tool to link the phylogeny and function of targeted microbial groups, to investigate polycyclic aromatic hydrocarbon (PAH)-degrading bacteria under aerobic conditions in sediments from Guaymas Basin with uniformly labeled [(13)C]-phenanthrene (PHE). The dominant sequences in clone libraries constructed from (13)C-enriched bacterial DNA (from PHE enrichments) were identified to belong to the genus Cycloclasticus. We used quantitative PCR primers targeting the 16S rRNA gene of the SIP-identified Cycloclasticus to determine their abundance in sediment incubations amended with unlabeled PHE and showed substantial increases in gene abundance during the experiments. We also isolated a strain, BG-2, representing the SIP-identified Cycloclasticus sequence (99.9% 16S rRNA gene sequence identity), and used this strain to provide direct evidence of PHE degradation and mineralization. In addition, we isolated Halomonas, Thalassospira, and Lutibacterium sp. with demonstrable PHE-degrading capacity from Guaymas Basin sediment. This study demonstrates the value of coupling SIP with cultivation methods to identify and expand on the known diversity of PAH-degrading bacteria in the deep-sea.",2015,Frontiers in Microbiology
Leave-one-out prediction intervals in linear regression models with many variables,"We study prediction intervals based on leave-one-out residuals in a linear regression model where the number of explanatory variables can be large compared to sample size. We establish uniform asymptotic validity (conditional on the training sample) of the proposed interval under minimal assumptions on the unknown error distribution and the high dimensional design. Our intervals are generic in the sense that they are valid for a large class of linear predictors used to obtain a point forecast, such as robust M-estimators, James-Stein type estimators and penalized estimators like the LASSO. These results show that despite the serious problems of resampling procedures for inference on the unknown parameters, leave-one-out methods can be successfully applied to obtain reliable predictive inference even in high dimensions.",2016,arXiv: Statistics Theory
Molecular structure of human KATP in complex with ATP and ADP,"In many excitable cells, KATP channels respond to intracellular adenosine nucleotides: ATP inhibits while ADP activates. We present two structures of the human pancreatic KATP channel, containing the ABC transporter SUR1 and the inward-rectifier K+ channel Kir6.2, in the presence of Mg2+ and nucleotides. These structures, referred to as quatrefoil and propeller forms, were determined by single-particle cryo-EM at 3.9 Ã… and 5.6 Ã…, respectively. In both forms, ATP occupies the inhibitory site in Kir6.2. The nucleotide-binding domains of SUR1 are dimerized with Mg2+-ATP in the degenerate site and Mg2+-ADP in the consensus site. A lasso extension forms an interface between SUR1 and Kir6.2 adjacent to the ATP site in the propeller form and is disrupted in the quatrefoil form. These structures support the role of SUR1 as an ADP sensor and highlight the lasso extension as a key regulatory element in ADP's ability to override ATP inhibition.",2017,eLife
Regularized Extreme Learning Machine Ensemble Using Bagging for Tropical Cyclone Tracks Prediction,"This paper aims to improve the prediction accuracy of Tropical Cyclone Tracks (TCTs) over the South China Sea (SCS) and its coastal regions with 24 h lead time. The model proposed in this paper is a regularized extreme learning machine (ELM) ensemble using bagging. A new method is proposed in this paper to solve lasso and elastic net problem in ELM, which turns the original problem into familiar quadratic programming (QP) problem. The forecast error of TCTs data set is the distance between real position and forecast position. Compared with the stepwise regression method widely used in TCTs, 16.49 km accuracy improvement is obtained by our model. Results show that the regularized ELM ensemble using bagging has a better generalization capactity on TCTs data set.",2018,
Genome Sequence of the Electrogenic Petroleum-Degrading Thalassospira sp. Strain HJ,"We present the draft genome of the petroleum-degrading Thalassospira sp. strain HJ, isolated from tidal marine sediment. Knowledge of this genomic information will inform studies on electrogenesis and means to degrade environmental organic contaminants, including compounds found in petroleum.",2015,Genome Announcements
InnovaciÃ³n en el uso de las microalgas en termalismo,"espanolIntroduccion y objetivos: Desde hace algunos anos las microalgas se vienen utilizando en distintos ambitos como nutricion, cosmetica, farmacologia, etc. Su aprovechamiento como recurso terapeutico puede tener un importante futuro en la cura marina y la terapia termal, debido a las diferentes propiedades que poseen. Ademas, con el incremento del interes por la utilizacion de productos naturales en la sociedad, el uso de las microalgas puede llegar a ser un area importante de desarrollo en los centros termales y de talasoterapia. Las microalgas marinas son una fuente de vitaminas, pigmentos, proteinas y otras sustancias de interes para el cuidado de la piel de manera que podrian utilizarse en el tratamiento de algunas dermatosis como la psoriasis y el acne, y tambien en otras alteraciones esteticas, como la celulitis, piel seca, etc. Para su uso en termalismo es importante la utilizacion de productos de calidad contrastada y caracterizados desde el punto de vista fisico-quimico, asi como de su aplicacion. El objetivo de este trabajo es comparar las propiedades termofisicas y la evaluacion de parametros cutaneos (como la hidratacion, la elasticidad y la fatiga) de diferentes peloides: con microalgas y sin microalgas. Metodos: El material es un peloide elaborado a partir de una mezcla de arcilla y agua; a una parte de la mezcla se la ha adicionado microalgas a diferentes proporciones. Para la caracterizacion termofisica se ha determinado la densidad, el calor especifico y la conductividad termica, comparando los peloides con y sin microalgas. Posteriormente se han realizado pruebas de biometrologia cutanea, analizando el grado de hidratacion, la elasticidad y la fatiga. Resultados y conclusiones: El estudio de las propiedades termofisicas y la evaluacion de la aplicacion del peloide sobre la piel han mostrado que la mezcla de peloide con microalgas posee un mejor comportamiento termico que el peloide sin microalga, y que mejoran asimismo los parametros biofisicos de la piel, en especial cuando la hidratacion y la elasticidad son mas bajos, que es cuando se obtienen los porcentajes de mejoria mayores. EnglishIn recent years microalgae have been used in various fields such as nutrition, cosmetics, pharmacology, etc. Its use as a therapeutic resource can have an important future in marine cure and thermal therapy, due to their valuable properties. Furthermore, with the increasing of the interest for products in society, the use of microalgae can become an important area of development in the thermal and thalassotherapy centers. Marine microalgae are an important source of vitamins, pigments, proteins and other substances that could be suitable for skin care so that they could be used in the treatment of some dermatological diseases such as psoriasis and acne, and also for cellulite, dry skin, etc. In the field of Hydrotherapy it is important to use products that have been proven and characterized from the point of view of physico-chemical quality and application. The aim of this study is to compare the thermophysical properties and evaluation of skin parameters (such as hydration, elasticity and fatigue) of different peloids made of microalgae and sea water.",2014,
Isolation and characterization of a marine bacterium Thalassomonas sp. SL-5 producing Î²-agarase,"A novel agar-degrading bacterium SL-5 was isolated from seashore of Homigot at Kyung-Buk province, and cultured in marine broth 2216 media. The bacterium SL-5 was identified as Thalassomonas genus by 16S rDNA sequencing with 96% identity. Growth rate was faster at than at and agarase was produced as growth-related. The optimum pH of the enzyme activity was 7.0 and the optimum temperature for the reaction was . Although the enzyme had no thermostability, the enzyme activity was remained over 80% at . The enzyme hydrolyzed neoagarohexaose to yield neoagarobiose as the main product, indicating that the enzyme is . Thus, the enzyme would be useful for the industrial production of neoagarobiose.",2007,Journal of Life Science
Regularized Linear Regression: A Precise Analysis of the Estimation Error,"Non-smooth regularized convex optimization procedures have emerged as a powerful tool to recover structured signals (sparse, low-rank, etc.) from (possibly compressed) noisy linear measurements. We focus on the problem of linear regression and consider a general class of optimization methods that minimize a loss function measuring the misfit of the model to the observations with an added structured-inducing regularization term. Celebrated instances include the LASSO, GroupLASSO, Least-Absolute Deviations method, etc.. We develop a quite general framework for how to determine precise prediction performance guaranties (e.g. mean-square-error) of such methods for the case of Gaussian measurement ensemble. The machinery builds upon Gordonâ€™s Gaussian min-max theorem under additional convexity assumptions that arise in many practical applications. This theorem associates with a primary optimization (PO) problem a simplified auxiliary optimization (AO) problem from which we can tightly infer properties of the original (PO), such as the optimal cost, the norm of the optimal solution, etc. Our theory applies to general loss functions and regularization and provides guidelines on how to optimally tune the regularizer coefficient when certain structural properties (such as sparsity level, rank, etc.) are known.",2015,
Three new minute nematode species of the superfamily Monhysteroidea from Arctic Abyss,"Three new nematode species of the superfamily Monhysteroidea were found in the Molloy Deep (Fram Strait, Arctic Ocean) at depths of > 5000 m. Amphimonhystrella bullacauda sp. n. (Xyalidae) is similar to A. unita Lorenzen 1977 but differs from it by having longer cephalic setae (1.5â€“3.5 versus 1.0 Âµm), lacking cervical setae, head set off from the body, singular anterior testis, distally pointed spicules, larger protuberant terminal tail widening. The generic diagnosis of Amphimonhystrella Timm 1961 is amended, and a dichotomous key for identification of the three Amphimonhystrella species is provided. Thalassomonhystera oxycephalata sp. n. (Monhysteridae) is distinguished from all valid Thalassomonhystera species by a number of morphological characters including narrowed cephalic end, wide amphid, short tubular weakly sclerotised stoma, esophagus gradually widening posteriorly, lacking evident renette cell, short arcuate spicules and gubernaculum with small dorsocaudal apophysis, tail with slender cylindrical portion distinctly set off. Thalassomonhystera molloyensis sp. n. is related to T. bathislandica Riemann 1995 described also from the north-eastern Atlantic abyssal but differs by having: 1) smaller body, male length 392â€“460 Âµm versus 572â€“684 Âµm, female length 376â€“472 Âµm versus 615â€“633 Âµm; 2) sclerotised lips of vulva; 3) presense of two ventral papillae or pairs of papillae, first pair in preanal position and second pair in postanal position just behind the middle of the tail. It is noteworthy that in both species the male genital tract has a variable position in relation to the intestine which is a deviation from the normal monhysterid morphology.",2005,Zootaxa
Electrophysiology: catheter ablation of atrial fibrillation.,"This article will review some of our new understanding about the physiology and treatment of atrial fibrillation. Atrial fibrillation involves a very rapidly firing atrial focus and is usually maintained through multiple re-entrant circuits. The electrical triggers for atrial fibrillation are basically rotors that initiate and perpetuate the arrhythmia. These can be focal anatomic triggers, or they can be caused by variations in autonomic tone, by acute or chronic changes in atrial wall tension, or (as a special case) by cardiac surgery (Fig. 1). 
 
 
 
Fig. 1 Essential components of atrial fibrillation. 
 
 
 
The focal triggers, in the great majority of cases (approx. 85%), are located in the pulmonary veins. In the remaining 15% of cases, these triggers lie in other areas, such as the posterior left atrium, the coronary sinus, around the crista terminalis, the ligament of Marshall, the superior vena cava, or the inferior vena cava. Histologic examination of the junction of pulmonary veins and the left atrium demonstrates muscle sleeves from the left atrium that extend into the pulmonary veins, and these are the specific anatomic areas where we can actually map the rapid-firing triggers. 
 
The substrates upon which atrial fibrillation is superimposed include myocardial fibrosis and inflammation of some degree, caused by cardiac or noncardiac disease; this condition results in the perpetuation and maintenance of the arrhythmia. Usually, atria in patients with atrial fibrillation are not structurally normal; abnormalities may take the form of chamber dilatation, wall thinning or thickening, or fibrosis in the muscle. Atrial fibrillation, when first detected, can be paroxysmal and self- terminating, or it can be persistent and not self-terminating. Over time, paroxysmal atrial fibrillation can evolve into permanent atrial fibrillation (Fig. 2). 
 
 
 
Fig. 2 Atrial fibrillation, when first detected, can be paroxysmal and self-terminating, or it can be persistent and not self-terminating. Over time, paroxysmal atrial fibrillation can evolve into permanent atrial fibrillation. 
 
 
 
The techniques that have been used to ablate atrial fibrillation have changed dramatically over the last 7 or 8 years. Initially, the approach involved focal ablation within the pulmonary vein. This method is now obsolete, for the simple reason that there was a 3% to 5% risk of pulmonary vein stenosisâ€”a disastrous complication. Pulmonary vein isolation, which involves segmental ostial ablation, is a more recent method that attempts to electrically isolate pulmonary veins from the left atrium. It has a success rate of about 70% in paroxysmal atrial fibrillation; however, this drops to 25% in chronic atrial fibrillation. 
 
The goals of pulmonary vein isolation are to eliminate premature depolarizations that can trigger atrial fibrillation and to eliminate bursts of tachycardia that contribute to its perpetuation. One tool that some (although not all) operators have advocated is intracardiac ultrasonography, both to guide the transseptal approach and to locate the ostium of each pulmonary vein. In addition, during the ablation, intracardiac echo enables the operator to determine whether he or she is really outside the pulmonary vein and to adjust the degree of energy given. 
 
The first type of catheter that was used in these segmental ablation procedures was the LASSO Circular Mapping Catheter (Biosense Webster, a Johnson & Johnson company; Diamond Bar, Calif). These were 20-pole catheters, which were placed at the ostia of the pulmonary veins through a transseptal sheath; a 2nd transseptal puncture/catheter was then used to do the ablation. With this approach, all of the muscular sleeves between the pulmonary vein and the left atrium are individually disconnected. 
 
Another technique involves a constellation basket catheter. The general approach is the same: the catheter is placed in a pulmonary vein, ablation is performed, and this is repeated in the other pulmonary veins. 
 
What are the results? Pulmonary vein isolation combined with linear lesions (in the mitral isthmus or in the roof of the left atrium) has a success rate of about 80%. Circumferential ablation (Fig. 3) around the right and left pulmonary veins combined with the lesions in the mitral isthmus and a posterior left atrial line also yields a success rate of about 80% to 85%. And, finally, left atrial, electrogram-guided, selective-site ablation, which tries to avoid the posterior left atrium, also has an approximate success rate of 80%. 
 
 
 
Fig. 3 Left atrial circumferential ablation. 
 
 
 
Another way of guiding ablation is with 3-dimensional mapping systems such as the CARTOâ„¢ EP Navigation System (Biosense Webster) and the EnSite NavXâ„¢ Navigation & Visualization Technology (St. Jude Medical; St. Paul, Minn). These new techniques use digital images to make a 3-D reconstruction of a virtual endoscopic view from inside the pulmonary vein, which can be placed side by side with the live images. Unfortunately, if the volume status of the patient changes significantly, there can be some discrepancy between the images. Furthermore, as you do more ablation, there is a tendency towards edema, which also distorts the anatomy in comparison with the older reference images. 
 
What are the appropriate clinical endpoints for these ablation procedures? How do you know when you are done? Is successful pulmonary vein isolation an endpoint? Is voltage reduction? Is non-inducibility of atrial fibrillation? All of the above? Interestingly, some reports have suggested that if patients are non-inducible at the end of the ablation, the success rate rises to about 87%. If they are inducible and no more ablation is done, the success rate drops to 67%. However, if more ablation is done to make these patients non-inducible, the success rate returns to 87%. 
 
What about complications? In a series of 800 patients from the University of Michigan, investigators documented a 1% incidence of tamponade and an 18% incidence of left atrial flutter, which subsequently resolved in about two thirds of the patients. The major disastrous complication that everyone worries about is the development of an atrial-esophageal fistula. Fortunately, this is seen very rarely (there are about 20 cases reported worldwide). The problem is that the esophagus can shift significantly during ablation and come into the treatment field on the posterior wall adjacent to the pulmonary veins. There are groups in the United States that are systematically doing posterior left atrial ablation, but a number of others in this country have shied away from this approach because of potential complications. 
 
What does the future hold? There are a number of technological advances looming, including merged CARTO images, which include 3-D imaging of the pulmonary veins by computed tomography or magnetic resonance imaging. Another possibility includes magnetically guided catheters that have much better contact with the wall. 
 
In conclusion, there has been significant progress in recent years in our understanding of the mechanisms of atrial fibrillation and the opportunities for catheter-based therapies for atrial fibrillation.",2006,Texas Heart Institute journal
TÃ¬palo: A Tool for Automatic Typing of DBpedia Entities,"In this paper we demonstrate the potentiality of Tipalo, a tool for automatically typing DBpedia entities. Tipalo identifies the most appropriate types for an entity in DBpedia by interpreting its definition extracted from its corresponding Wikipedia abstract. Tipalo relies on FRED, a tool for ontology learning from natural language text, and on a set of graph-pattern-based heuristics which work on the output returned by FRED in order to select the most appropriate types for a DBpedia entity. The tool returns a RDF graph composed of rdf:type, rdfs:subClassOf, owl:sameAs, and owl:equivalentTo statements providing typing information about the entity. Additionally the types are aligned to two lists of top-level concepts, i.e., Wordnet supersenses and a subset of DOLCE Ultra Lite classes. Tipalo is available as a Web-based tool and exposes its API as HTTP REST services.",2013,
Properties and iterative methods for the lasso and its variants,"The lasso of Tibshirani (1996) is a least-squares problem regularized by the â„“1 norm. Due to the sparseness promoting property of the â„“1 norm, the lasso has been received much attention in recent years. In this paper some basic properties of the lasso and two variants of it are exploited. Moreover, the proximal method and its variants such as the relaxed proximal algorithm and a dual method for solving the lasso by iterative algorithms are presented.",2014,"Chinese Annals of Mathematics, Series B"
"IdentificaciÃ³n de nematodos fitoparÃ¡sitos asociados al cultivo de rosa (rosa sp), en el sector lasso provincia de cotopaxi","he present research work was carried out with the objective of identifying the different species of phytoparasitic nematodes that are associated with the cultivation of rose (Rosa sp), in the Lasso sector, Cotopaxi province, Saquimalag neighborhood through the respective soil sampling in four varieties of rose cultivars. 
For the investigation, samples were collected and taken to the laboratory to be analyzed by extraction. The well-known method of the Baerman funnel and the Centrifuging method, root damage was analyzed as physiopathies by extracting the plant in its entirety, the pure complete root sample was collected. The morphometry of the specimens was prepared semi-permanent microscopic sheets and were observed under a phase contrast microscope (leica DM1000) and finally the identification of the species, the adult state of the nematode was taken into account to analyze body length, stylet, head and tail, gender identification according to the type of esophagus, tail size of the nematode and type of stiletto. 
In the research, no experimental design was applied since the culture was established, the laboratory study was carried out, taking into account the factors under study of four varieties of rose cultivars and as treatments two flower farms, for the data analysis was used statistical programs and related to mathematical and percentage calculations",2018,
Estimation and Testing Under Sparsity: Ã‰cole d'Ã‰tÃ© de ProbabilitÃ©s de Saint-Flour XLV â€“ 2015,"Taking the Lasso method as its starting point, this book describes the main ingredients needed to study general loss functions and sparsity-inducing regularizers. It also provides a semi-parametric approach to establishing confidence intervals and tests. Sparsity-inducing methods have proven to be very useful in the analysis of high-dimensional data. Examples include the Lasso and group Lasso methods, and the least squares method with other norm-penalties, such as the nuclear norm. The illustrations provided include generalized linear models, density estimation, matrix completion and sparse principal components. Each chapter ends with a problem section. The book can be used as a textbook for a graduate or PhD course.",2016,
Detection of circular telomeric DNA without 2D gel electrophoresis.,"The end of linear chromosomes forms a lasso-like structure called the t-loop. Such t-loops resemble a DNA recombination intermediate, where the single-stranded 3' overhang is arrested in a stretch of duplex DNA. Presumably, such a t-loop can also be deleted via a recombination process. This would result in the occurrence of circular extrachromosomal telomeric DNA (t-circles), which are known to be abundantly present in immortal cells engaging the recombination-based alternative lengthening of telomeres pathway (ALT pathway). Little is known about the basic mechanism of telomeric recombination in these cells and what ultimately causes the generation of such t-circles. Current standard procedures for detecting these molecules involve 2D gel electrophoresis or electron microscopy. However, both methods are labor intense and sophisticated to perform. Here, we present a simpler, faster, and equally sensitive method for detecting t-circles. Our approach is a telomere restriction fragment assay that involves the enzymatic preservation of circular DNA with Klenow enzyme followed by Bal31 degradation of the remaining linear DNA molecules. We show that with this approach t-circles can be detected in ALT cell lines, whereas no t-circles are present in telomerase-positive cell lines. We consider our approach a valid method in which t-circle generation is the experimental readout.",2008,DNA and cell biology
Thresholded Basis Pursuit: Support Recovery for Sparse and Approximately Sparse Signals,"In this paper we present a linear programming solution for support recovery. Support recovery involves the estimation of sign pattern of a sparse signal from a set of randomly projected noisy measurements. Our solution of the problem amounts to solving min â€–Zâ€–1 s.t. Y = GZ, and quantizing/thresholding the resulting solution Z. We show that this scheme is guaranteed to perfectly reconstruct a discrete signal or control the element-wise reconstruction error for a continuous signal for specific values of sparsity. We show that the sign pattern of X can be recovered with SNR = O(log n) and m = O(k log n/k) measurements, where k is the sparsity level and satisfies 0 < k â‰¤ Î±n, where, Î± is some non-zero constant. Our proof technique is based on perturbation of the noiseless l1 problem. Consequently, the maximum achievable sparsity level in the noisy problem is comparable to that of the noiseless problem. Our result offers a sharp characterization in that neither the SNR nor the sparsity ratio can be significantly improved. In contrast previous results based on LASSO and MAX-Correlation techniques either assume significantly larger SNR or sub-linear sparsity. Our results has implications for approximately sparse problems. We show that the k largest coefficients of a non-sparse signal X can be recovered from m = O(k log n/k) random projections for certain classes of signals.",2008,
Machine learning: how much does it improve the prediction of unplanned hospital admissions?,"IntroductionRisk prediction models can be used to inform decision-making in clinical settings. With large and detailed electronic medical record data, machine learning may improve predictions. The objective of this work is to determine the feasibility and accuracy of machine learning versus logistic regression to predict unplanned hospital admissions. 
Objectives and ApproachData from primary care electronic medical records for community-dwelling adults in Alberta, Canada available from the Canadian Primary Care Sentinel Surveillance Network will be linked to acute care administrative health data held by Alberta Health Services. Two regression methods (forward stepwise logistic, LASSO logistic) will be compared with three machine learning methods (classification tree, random forest, gradient boosted trees). Prior primary and acute care use will be used to predict three outcomes: â‰¥1 unplanned admission within 1 year, â‰¥1 unplanned admission within 90 days, and â‰¥1 unplanned admission within 1 year due to an ambulatory care sensitive condition. 
ResultsThe results of this work in progress will be presented at the conference. 41,142 patients will have their primary and acute care data linked. We anticipate that the machine learning methods will improve predictive performance but will be more challenging for clinicians and patients to understand, including why a given patient is predicted to be at higher risk. The primary comparison of machine learning and regression methods will be based on positive predictive values corresponding to the top 5% predicted risk threshold, and estimated via 10-fold cross-validation. 
Conclusion/ImplicationsThis project aims to help researchers decide which statistical methods to use for risk prediction models. When considering machine learning methods the best approach may be to try multiple methods, compare their predictive accuracy and interpretability, and then choose a final method.",2018,International Journal for Population Data Science
GoodRelations: An Ontology for Describing Products and Services Offers on the Web,"A promising application domain for Semantic Web technology is the annotation of products and services offerings on the Web so that consumers and enterprises can search for suitable suppliers using products and services ontologies. While there has been substantial progress in developing ontologies for typesof products and services, namely eClassOWL, this alone does not provide the representational means required for e-commerce on the Semantic Web. Particularly missing is an ontology that allows describing the relationships between (1) Web resources, (2) offerings made by means of those Web resources, (3) legal entities, (4) prices, (5) terms and conditions, and the aforementioned ontologies for products and services (6). For example, we must be able to say that a particular Web site describes an offer to sell cell phones of a certain make and model at a certain price, that a piano house offers maintenance for pianos that weigh less than 150 kg, or that a car rental company leases out cars of a certain make and model from a set of branches across the country. In this paper, we analyze the complexity of product description on the Semantic Web and define the GoodRelations ontology that covers the representational needs of typical business scenarios for commodity products and services.",2008,
Oracle Inequalities and Optimal Inference under Group Sparsity,"We consider the problem of estimating a sparse linear regression vector s* under a gaussian noise model, for the purpose of both prediction and model selection. We assume that prior knowledge is available on the sparsity pattern, namely the set of variables is partitioned into prescribed groups, only few of which are relevant in the estimation process. This group sparsity assumption suggests us to consider the Group Lasso method as a means to estimate s*. We establish oracle inequalities for the prediction and l2 estimation errors of this estimator. These bounds hold under a restricted eigenvalue condition on the design matrix. Under a stronger coherence condition, we derive bounds for the estimation error for mixed (2,p)-norms with 1=p=8. When p=8, this result implies that a threshold version of the Group Lasso estimator selects the sparsity pattern of s* with high probability. Next, we prove that the rate of convergence of our upper bounds is optimal in a minimax sense, up to a logarithmic factor, for all estimators over a class of group sparse vectors. Furthermore, we establish lower bounds for the prediction and l2 estimation errors of the usual Lasso estimator. Using this result, we demonstrate that the Group Lasso can achieve an improvement in the prediction and estimation properties as compared to the Lasso.",2010,Annals of Statistics
"Carotid artery stenting: ""good news"" or ""bad news"" for post-procedural cognitive function?","eluting coronary stents. N Engl J Med 2007;356:998â€“1008. [14] Nordmann AJ, Briel M, Bucher HC. Mortality in randomized controlled trials comparing drug-eluting vs. bare metal stents in coronary artery disease: a metaanalysis. Eur Heart J 2006;27:2784â€“814. [15] Lagerqvist B, James SK, Stenestrand U, Lindback J, Nilsson T, Wallentin L. Long-term outcomes with drug-eluting stents versus bare-metal stents in Sweden. N Engl J Med 2007;356:1009â€“19. [16] Kukreja N, Onuma Y, Serruys PW. Xience V everolimus-eluting coronary stent. Expert Rev Med Devices 2009;6:219â€“29. [17] deWaha A, Dibra A, Byrne RA, Ndrepepa G, et al. Everolimus-eluting versus sirolimuseluting stents: a meta-analysis of randomized trials. Circ Cardiovasc Interv 2011;4(4):371â€“7. [18] Cassese S, Piccolo R, Galasso G, De Rosa R, Piscione F. Twelve-month clinical outcomes of everolimus-eluting stent as compared to paclitaxeland sirolimuseluting stent in patients undergoing percutaneous coronary interventions. A meta-analysis of randomized clinical trials. Int J Cardiol 2011;150:84â€“9. [19] Takagi H, Umemoto T. An updated meta-analysis of randomized controlled trials of everolimusversus paclitaxel-eluting stents. Int J Cardiol 2011;151(3):354â€“5. [20] Serruys PW, Silber S, Garg S, et al. Comparison of zotarolimus-eluting and everolimuseluting coronary stents. N Engl J Med 2010;363(2):136â€“46. [21] Stefanini GG, Serruys PW, Silber S, et al. The impact of patient and lesion complexity on clinical and angiographic outcomes after revascularization with zotarolimusand everolimus-eluting stents: a substudy of the RESOLUTE All Comers Trial (a randomized comparison of a zotarolimus-eluting stent with an everolimus-eluting stent for percutaneous coronary intervention). J Am Coll Cardiol 2011;57(22):2221â€“32. [22] Herrador JA, Fernandez JC, Guzman M, Aragon V. Comparison of zotarolimusversus everolimus-eluting stents in the treatment of coronary bifurcation lesions. Catheter Cardiovasc Interv 2011;78(7):1086â€“92. [23] Coats AJS, Shewan LG. Statement on authorship and publishing ethics in the International Journal of Cardiology. Int J Cardiol 2011;153:239â€“40.",2012,International journal of cardiology
Network-based analysis of prostate cancer cell lines reveals novel marker gene candidates associated with radioresistance and patient relapse,"Radiation therapy is an important and effective treatment option for prostate cancer, but high-risk patients are prone to relapse due to radioresistance of cancer cells. Molecular mechanisms that contribute to radioresistance are not fully understood. Novel computational strategies are needed to identify radioresistance driver genes from hundreds of gene copy number alterations. We developed a network-based approach based on lasso regression in combination with network propagation for the analysis of prostate cancer cell lines with acquired radioresistance to identify clinically relevant marker genes associated with radioresistance in prostate cancer patients. We analyzed established radioresistant cell lines of the prostate cancer cell lines DU145 and LNCaP and compared their gene copy number and expression profiles to their radiosensitive parental cells. We found that radioresistant DU145 showed much more gene copy number alterations than LNCaP and their gene expression profiles were highly cell line specific. We learned a genome-wide prostate cancer-specific gene regulatory network and quantified impacts of differentially expressed genes with directly underlying copy number alterations on known radioresistance marker genes. This revealed several potential driver candidates involved in the regulation of cancer-relevant processes. Importantly, we found that ten driver candidates from DU145 (ADAMTS9, AKR1B10, CXXC5, FST, FOXL1, GRPR, ITGA2, SOX17, STARD4, VGF) and four from LNCaP (FHL5, LYPLAL1, PAK7, TDRD6) were able to distinguish irradiated prostate cancer patients into early and late relapse groups. Moreover, in-depth in vitro validations for VGF (Neurosecretory protein VGF) showed that siRNA-mediated gene silencing increased the radiosensitivity of DU145 and LNCaP cells. Our computational approach enabled to predict novel radioresistance driver gene candidates. Additional preclinical and clinical studies are required to further validate the role of VGF and other candidate genes as potential biomarkers for the prediction of radiotherapy responses and as potential targets for radiosensitization of prostate cancer.",2019,PLoS Computational Biology
A data-driven approach to modeling physical fatigue in the workplace using wearable sensors.,"Wearable sensors are currently being used to manage fatigue in professional athletics, transportation and mining industries. In manufacturing, physical fatigue is a challenging ergonomic/safety ""issue"" since it lowers productivity and increases the incidence of accidents. Therefore, physical fatigue must be managed. There are two main goals for this study. First, we examine the use of wearable sensors to detect physical fatigue occurrence in simulated manufacturing tasks. The second goal is to estimate the physical fatigue level over time. In order to achieve these goals, sensory data were recorded for eight healthy participants. Penalized logistic and multiple linear regression models were used for physical fatigue detection and level estimation, respectively. Important features from the five sensors locations were selected using Least Absolute Shrinkage and Selection Operator (LASSO), a popular variable selection methodology. The results show that the LASSO model performed well for both physical fatigue detection and modeling. The modeling approach is not participant and/or workload regime specific and thus can be adopted for other applications.",2017,Applied ergonomics
Environmental factors analysis and comparison affecting software reliability in development of multi-release software,"Abstract As the application of the principles of agile and lean software development, software multiple release becomes very common in the modern society. Short iteration and short release cycle have driven the significant changes of the development process of multi-release software product, compared with single release software product. Thus, it is time to conduct a new study investigating the impact level of environmental factors on affecting software reliability in the development of multi-release software to provide a sound and concise guidance to software practitioners and researchers. Statistical learning methods, like principle component analysis, stepwise backward elimination, lasso regression, multiple linear regression, and Tukey method, are applied in this study. Comparisons regarding significant environmental factors during the whole development process, principle components, significant environmental factors in each development phase and significance level of each development phase between the development of single release software and multi-release software are also discussed.",2017,J. Syst. Softw.
Jurassic Palynofloral Provinces of Tarim Basin,"Jurassic strata are widely distributed in Tarim Basin and rich in fossil spores and pollen grains. According to the present data, two Jurassic palynofloras occupy the South Tarim and the North Tarim palynofloral provinces. For each palynoflora an assemblage sequence is proposed and described. The southern flora is composed of five assemblages (in ascending order): 1. Cyathidites-Classopollis Assemblage; 2. Disacciatrileti-Cyathidites-Classopollis-Quadraeculina Assemblage; 3. Disacciatrileti-Cyathidites-Classopollis-Neoraistrackia Assemblage; 4. Disacciatrileti-Cyathidites-Classopollis Assemblage; and 5. Cyathidites-Classopollis-Disacciatrileti Assemblage. The northern flora includes assemblages: 1. Disacciatrileti-Cyathidites Assemblage; 2. Cyathidites-Cibotiumspora-Disacciatrileti Assemblage; 3. Cyathidites-Neoraistrickia-Disacciatrileti Assemblage; and 4. Cyathidites-Classopollis Assemblage. There are clear differences in the Jurassic palynological floras between South Tarim and North Tarim provinces. A preliminary study shows that the sporopollen fossils from South Tarim are close to those of South China, the sporopollen assemblages from the North Tarim are similar to those of North China. According to Vakhrameev (1988), South China Jurassic Floral Province is an important part of the East Asia Province in the Europe-China Region. The flora of North China falls into the Siberia Region. Thus, the boundary between the South and North Tarim provinces should be shifted northward to central Tarim, extending in northwest-southeast direction. The above boundary separated the Tarim Sporopollen Province into two different sporopollen subprovinces, namely the South Tarim Palynofloras province and North Tarim Palynofloras province (Fig.1).",2002,Journal of stratigraphy
Machine Learning Techniques Applied to US Army and Navy Data,"We apply machine learning techniques to the synthetic data (Stevens and Anderson-Cook, 2017a), which is univariate data with a binary response of passing or failing for complex munitions generated to match age and usage rate, found in United States Department of Defense complex systems (the Army and Navy). Instead of the generalized linear model (GLM) with probit link function used in Stevens and Anderson-Cook (2017a), we propose applying machine learning techniques to predict the binary response of passing or failing for the Army and Navy data. We propose two methods of machine learning techniques to find the best models for the Army and Navy. The first method is the artificial neural networks (ANN), which simulates biological neural networks to make predictions through transforming artificial neurons (variables) by particular learning rule(s). The second method is the penalized logistic regression. It adds penalty to the coefficients of the logistic general linear regression. We use three penalized methods: lasso, ridge, and elastic net. We compare the predictive accuracy among the ANN, three types of penalized regression, GLM with logit link, and GLM with probit link. We use two criteria to measure the predictive accuracy: Root-meansquare error (RMSE) and the area under Receiver Operating Characteristic (ROC) curve (AUC). Both show that machine learning techniques give better predictions compared to the standard GLMs.",2019,International Journal of Productivity and Quality Management
Breath and darkness: realism and representation in astronomical illustration and cinematographic special effects,"Despite their inevitable lacunae and elisions, extant visual forms within Science Fiction constitute a mode of representation that allows us to understand humanityâ€™s engagement with the cosmos as an Other to the normative or habitual conventions of representation found in that which, following Christian Davies, might be thought of as â€˜small historyâ€™. 
 
Drawing on both filmic tropes of human space flight and interplanetary travel since Klushantsevâ€™s Road to the Stars (1957) as well as Apollo documentation, this paper examines and considers the concept of work both as historic act (Marx), and as something necessitated by conditions of solitude or isolation (Levinas). Reviewing representations found in films such as Christopher Nolanâ€™s Interstellar (2014) and Sebastian Corderoâ€™s Europa Report (2014), the paper proposes representations of astronautical work as a foundational element of a possible â€˜ecriture cosmiqueâ€™ in light of the problematics of the arche fossil described by Quentin Meillassoux.",2019,
Modeling High Dimensional Multilevel Data using the Lasso Estimator: A Simulation Study,"In some situations, researchers are faced with high dimensional data, where the number of variables in the dataset is large, and the sample size is relatively small. In such cases standard statistical methods do not perform well, making model parameter estimation potentially problematic. In order to deal with such high dimensional data, statisticians have developed estimators, such as the lasso, that are specially designed to provide model parameter estimates for such scenarios. Recently, this work has been extended to the context of high dimensional multilevel, or mixed effects data in which individuals at level-1 are nested within clusters at level-2. Such data structures are extremely common in the social sciences, particularly education and sociology. The goal of this simulation study was to assess a multilevel extension of the lasso estimator in high dimensional multilevel data case, and to compare this approach with the standard restricted maximum likelihood estimator typically used to fit multilevel models. Results of the study demonstrated that the multilevel lasso yielded better control of the Type I error rate and better parameter coverage than did REML, when level-1 and level-2 sample sizes were small, and there were many predictor variables. Implications of these results are discussed.Mathematics Subject Classification: 62P99Keywords: high dimensional data; multilevel and mixed effects models; Lasso Estimator",2018,Journal of Statistical and Econometric Methods
Regularized Regression Versus the High-Dimensional Propensity Score for Confounding Adjustment in Secondary Database Analyses.,"Selection and measurement of confounders is critical for successful adjustment in nonrandomized studies. Although the principles behind confounder selection are now well established, variable selection for confounder adjustment remains a difficult problem in practice, particularly in secondary analyses of databases. We present a simulation study that compares the high-dimensional propensity score algorithm for variable selection with approaches that utilize direct adjustment for all potential confounders via regularized regression, including ridge regression and lasso regression. Simulations were based on 2 previously published pharmacoepidemiologic cohorts and used the plasmode simulation framework to create realistic simulated data sets with thousands of potential confounders. Performance of methods was evaluated with respect to bias and mean squared error of the estimated effects of a binary treatment. Simulation scenarios varied the true underlying outcome model, treatment effect, prevalence of exposure and outcome, and presence of unmeasured confounding. Across scenarios, high-dimensional propensity score approaches generally performed better than regularized regression approaches. However, including the variables selected by lasso regression in a regular propensity score model also performed well and may provide a promising alternative variable selection method.",2015,American journal of epidemiology
Principled estimation of regression discontinuity designs with covariates: a machine learning approach,"The regression discontinuity design (RDD) has become the ""gold standard"" for causal inference with observational data. Local average treatment effects (LATE) for RDDs are often estimated using local linear regressions with pre-treatment covariates typically added to increase the efficiency of treatment effect estimates, but their inclusion can have large impacts on LATE point estimates and standard errors, particularly in small samples. In this paper, I propose a principled, efficiency-maximizing approach for covariate adjustment of LATE in RDDs. This approach allows researchers to combine context-specific, substantive insights with automated model selection via a novel adaptive lasso algorithm. When combined with currently existing robust estimation methods, this approach improves the efficiency of LATE RDD with pre-treatment covariates. The approach will be implemented in a forthcoming R package, AdaptiveRDD which can be used to estimate and compare treatment effects generated by this approach with extant approaches.",2019,arXiv: Applications
Wideband sound reproduction in a 2D multi-zone system using a combined two-stage Lasso-LS algorithm,"This paper addresses the provision of personal soundfields (zones) to multiple listeners in a space for which there are several fixed, virtual sources. For such multizone systems, optimization of speaker positions and weightings is important to reduce the number of active speakers. In this paper a two stage pressure matching optimization is proposed for wideband sound sources (such as speech signals). In the first stage, the least-absolute shrinkage and selection operator (Lasso) is used to select the speakers' positions for all sources and frequencies. A second stage then optimizes reproduction using all selected speakers on the basis of regularized least-squares (LS) or Lasso algorithm. The performance of these new, two-stage approaches is investigated for different speaker numbers, frequency range and reproduction angles. Results show that a limited arc of speakers using a two-stage optimization can give up to 38dB improvement in zone normalized squared error over a single-stage LS optimization.",2012,2012 IEEE 7th Sensor Array and Multichannel Signal Processing Workshop (SAM)
Adaptive testing of SNP-brain functional connectivity association via a modular network analysis,"Due to its high dimensionality and high noise levels, analysis of a large brain functional network may not be powerful and easy to interpret; instead, decomposition of a large network into smaller subcomponents called modules may be more promising as suggested by some empirical evidence. For example, alteration of brain modularity is observed in patients suffering from various types of brain malfunctions. Although several methods exist for estimating brain functional networks, such as the sample correlation matrix or graphical lasso for a sparse precision matrix, it is still difficult to extract modules from such network estimates. Motivated by these considerations, we adapt a weighted gene co-expression network analysis (WGCNA) framework to resting-state fMRI (rs-fMRI) data to identify modular structures in brain functional networks. Modular structures are identified by using topological overlap matrix (TOM) elements in hierarchical clustering. We propose applying a new adaptive test built on the proportional odds model (POM) that can be applied to a high-dimensional setting, where the number of variables (p) can exceed the sample size (n) in addition to the usual p < n setting. We applied our proposed methods to the ADNI data to test for associations between a genetic variant and either the whole brain functional network or its various subcomponents using various connectivity measures. We uncovered several modules based on the control cohort, and some of them were marginally associated with the APOE4 variant and several other SNPs; however, due to the small sample size of the ADNI data, larger studies are needed.",2017,Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing
Overlapping-Group-Lasso-Based ISAR Imagery via Alternating Direction Method of Multipliers,"In this paper, we propose a novel method called overlapping group Lasso to solve inverse synthetic aperture radar (ISAR) imaging problem. Unlike the traditional least absolute shrinkage and selection operator (Lasso) model, overlapping group Lasso is based on the $\ell_{1}/\ell_{2}$ mixed-norm and take advantage of the prior knowledge of the continuity structures of the scatters. Besides, we present a generic optimization approach, the alternating direction method of multipliers (ADMM) method, for dealing with overlapping group Lasso that including structured-sparsity penalties and the predefined weight for group. ADMM is a simple but powerful algorithm that blending the benefits of augmented Lagrangian and dual decomposition method. Therefore, it makes the proposed algorithm faster and more robust. Experimental results of simulated data and Yak-42 real data verify the feasibility of ADMM achieves sparse and structural feature enhancement via the overlapping group Lasso. The comparison of the results of overlapping group Lasso and Lasso shows: the new developed model has the good ability of denoising and structural feature enhancement.",2019,2019 6th Asia-Pacific Conference on Synthetic Aperture Radar (APSAR)
An ensemble learning algorithm based on Lasso selection,"Ensemble learning, especially selective ensemble learning is now becoming more and more popular in the field of machine learning. This paper introduces a new ensemble algorithm, named Lasso-Bagging Trees ensemble algorithm. This algorithm is in order to improve the whole learning ability, which is a combination of tree predictors and this method chooses and ensembles trees based on the shrinkage estimation of lasso technology. Compared with a series of other learning algorithms, it demonstrates better generalization ability and higher efficiency.",2010,2010 IEEE International Conference on Intelligent Computing and Intelligent Systems
Prediction of genomic breeding values for reproductive traits in Nellore heifers.,"The objective of this study was to assess the accuracy of genomic predictions for female reproductive traits in Nellore cattle. A total of 1853 genotyped cows and 305,348 SNPs were used for genomic selection analyses. GBLUP, BAYESCÏ€, and IBLASSO were applied to estimate SNP effects. The pseudo-phenotypes used as dependent variables were: observed phenotype (PHEN), adjusted phenotype (CPHEN), estimated breeding value (EBV), and deregressed estimated breeding value (DEBV). Predictive abilities were assessed by the average correlation between CPHEN and genomic estimated breeding value (GEBV) and by the average correlation between DEBV and GEBV in the validation population. Regression coefficients of pseudo-phenotypes on GEBV in the validation population were indicators of prediction bias in GEBV. BAYESCÏ€ showed higher predictive ability to estimate SNP effects and GEBV for all traits.",2019,Theriogenology
[Prolassomucosectomy according to Longo. Intermediate results].,"We present results obtained in our first series of 25 patient treated by prolassomucosectomy for haemorrhoidal prolapse with a follow up of 18-36 months. Control of the disease and functional results proved to be optimal. After small early haemorrhages from the suture line, we started adding stitch sutures with haemostatic intent to all three vascular pedicles. Early or late additional complication were not observed.",2004,Annali italiani di chirurgia
Introduction to Statistical Process Control,Introduction Quality and the Early History of Quality Improvement Quality Management Statistical Process Control Organization of the Book Basic Statistical Concepts and Methods Introduction Population and Population Distribution Important Continuous Distributions Important Discrete Distributions Data and Data Description Tabular and Graphical Methods for Describing Data Parametric Statistical Inferences Nonparametric Statistical Inferences Univariate Shewhart Charts and Process Capability Introduction Shewhart Charts for Numerical Variables Shewhart Charts for Categorical Variables Process Capability Analysis Some Discussions Univariate CUSUM Charts Introduction Monitoring the Mean of a Normal Process Monitoring the Variance of a Normal Process CUSUM Charts for Distributions in Exponential Family Self-Starting and Adaptive CUSUM Charts Some Theory for Computing ARL Values Some Discussions Univariate EWMA Charts Introduction Monitoring the Mean of a Normal Process Monitoring the Variance of a Normal Process Self-Starting and Adaptive EWMA Charts Some Discussions Univariate Control Charts by Change-Point Detection Introduction Univariate Change-Point Detection Control Charts by Change-Point Detection Some Discussions Multivariate Statistical Process Control Introduction Multivariate Shewhart Charts Multivariate CUSUM Charts Multivariate EWMA Charts Multivariate Control Charts by Change-Point Detection Multivariate Control Charts by LASSO Some Discussions Univariate Nonparametric Process Control Introduction Rank-Based Nonparametric Control Charts Nonparametric SPC by Categorical Data Analysis Some Discussions Multivariate Nonparametric Process Control Introduction Rank-Based Multivariate Nonparametric Control Charts Multivariate Nonparametric SPC by Log-Linear Modeling Some Discussions Profile Monitoring Introduction Parametric Profile Monitoring Nonparametric Profile Monitoring Some Discussions Appendix A: R Functions for SPC Appendix B: Datasets Used in the Book Bibliography Index Exercises appear at the end of each chapter.,2013,
Risikoverhalten und Alkoholkonsum im Jugendalter,"ZusammenfassungAls Entwicklungsphase des Experimentierens mit neuen Rollen und Verhaltensmustern ist die Adoleszenz fÃ¼r Risikoverhaltensweisen wie den exzessiven Konsum von Alkohol prÃ¤destiniert. Das Trinken von Alkohol dient der Demonstration von Autonomie und trÃ¤gt zur Ãœberwindung sozialer Hemmungen, der Sicherung eines hohen Status in der Peergroup und zur Spannungsreduktion bei. Im weiteren Entwicklungsverlauf werden problematische Konsummuster zumeist eingestellt; bei einer Minderheit kommt es allerdings zu einer gesundheitsgefÃ¤hrdenden langfristigen AbhÃ¤ngigkeitsentwicklung. BezÃ¼glich der Entstehung des Alkoholmissbrauchs werden eine Vielzahl internaler und externaler EinflussgrÃ¶ÃŸen sowie entsprechende Interaktionen diskutiert. Im vorliegenden Beitrag wird zunÃ¤chst auf die normative Entwicklung im Jugendalter und die entwicklungspsychologische Bedeutung von Risikoverhaltensweisen eingegangen. Im Anschluss werden Risikofaktoren fÃ¼r das Zustandekommen von Alkoholmissbrauch und die Verlaufsmuster des Alkoholkonsums erÃ¶rtert sowie alkoholassoziierte Risiken und SchÃ¤den dargestellt.AbstractThe developmental phase of adolescence is one of experimenting with risk behaviour such as alcohol consumption. This may serve to demonstrate autonomy, to contribute to overcoming social inhibition as well as securing a high status within the peer group. In the process of adolescent development problematic consumer behaviour involving alcohol is usually given up; a minority however develops a health endangering long-term alcohol dependence. With respect to excessive consumption multiple internal and external influencing factors â€“ interacting in complex ways â€“ are under discussion. This contribution first addresses normative development and the influence of risk behaviour and then discusses risk factors contributing to the development of alcohol abuse and patterns of abuse as well as specific health risks.",2010,Monatsschrift Kinderheilkunde
Spectral Resolution Enhancement of Hyperspectral Images via Sparse Representations,"High-spectral resolution imaging provides critical insights into important computer vision tasks such as classification, tracking, and remote sensing. Modern Snapshot Spectral Imaging (SSI)systems directly acquire the entire 3D data-cube through the intelligent combination of spectral filters and detector elements. Partially because of the dramatic reduction in acquisition time, SSI systems exhibit limited spectral resolution, for example, by associating each pixel with a single spectral band in Spectrally Resolvable Detector Arrays. In this paper, we propose a novel machine learning technique aiming to enhance the spectral resolution of imaging systems by exploiting the mathematical framework of Sparse Representations (SR). Our formal approach proposes a systematic way to estimate a high-spectral resolution pixel from a measured low-spectral resolution version by appropriately identifying a sparse representation that can directly generate the highspectral resolution output. We enforce the sparsity constraint by learning a joint space coding dictionary from multiple low and high spectral resolution training data and we demonstrate that one can successfully reconstruct high-spectral resolution images from limited spectral resolution measurements. Introduction Over the last decade, the demand for designing imaging systems able to reveal the physical properties of the objects in a scene of interest, has grown tremendously. To that end, Hyperspectral Imaging has emerged as a powerful technology, able to capture and process a huge amount of data, including the spatial and spectral variations of an input scene. This type of data is crucial for multiple applications, such as remote sensing, precision agriculture, food processing, medical and biological research, etc. Despite the important merits of hyperspectral imaging systems in structure identification and remote sensing, HSI acquisition and processing also comes with multiple functional constraints. Slow acquisition time, limited spectral and spatial resolution, low dynamic range, and restricted field of view, are just a few of the limitations that hyperspectral sensors exhibit, and which require further investigation. The rapid evolution of the spectrally resolvable detector array systems that directly acquire the entire 3D data-cube through a clever combination of spectral filters and detector elements, has created an enormous excitement in the hyperspectral imaging community [1â€“5]. Although these systems acquire the entire spatial and spectral information directly from a single snapshot image, they unfortunately cause a reduction in spectral resolution by associating each detector/pixel with a single spectral band. The spectral resolution is a critical parameter for both visualization and subsequent procedures such as unmixing [6, 7], classification [8â€“10], and understanding of the variations of an input scene over time. Traditional hyperspectral resolution enhancement approaches focus mostly on the spatial resolution of HSI systems. In the remote sensing community, conventional techniques generate the high-spatial resolution scene by fusing a low spatial resolution hyperspectral image with a high spatial resolution panchromatic image, a procedure known as pan-sharpening [11]. Another class of techniques utilizes spatio-spectral relations to improve spatial resolution [12, 13]. Furthermore, over the past decade multiple techniques have been proposed that seek to enhance the spatial resolution of multispectral imagery by exploiting the Sparse Representations framework [13, 14]. More specifically, the authors in [14] propose a sparsity-based approach based on the assumption that corresponding pairs of high and low spatial resolution pixel curves share the same sparse codes with respect to appropriate resolution dictionaries. In order to improve the quality of their reconstruction, a maximum a priory algorithm is utilized. Contrary to the spatial resolution enhancement problem, in the spectral domain only a handful of techniques have been reported. The authors in [16] propose a spectral super-resolution approach applied on a generalization of the Coded Aperture Snapshot Spectral Imaging (CASSI) instrument, increasing simultaneously both the spectral and the spatial dimensions of hyperspectral scenes. Additionally, in [17] another spectral resolution enhancement is demonstrated, where the authors consider geographically co-located multispectral and hyperspectral oceanic water-color images and they enhance the limited multispectral measurements utilizing a sparsity based approach. First, they use a spectral mixing formulation and they define the measured spectrum for each pixel as the sum of the weighted material spectra. The desired high-spectral resolution spectra is expressed as a linear combination between a blurring matrix and the measured spectra. This problem is solved via a sparse-based technique. Our proposed algorithm aims to enhance the spectral dimension, i.e., the number of acquired spectral bands, providing richer and more thorough descriptions of a scene of interest. Instead of introducing hardware solutions for spectral-resolution enhancement, such as modifying the optics or the hyperspectral sensor characteristics, the proposed scheme adheres to a signal learning paradigm, offering convenient post-acquisition processing, able to extract a richer spectral information from a limited number of spectral bands. The proposed spectral super-resolution method is formulated as an inverse imaging algorithm that recovers highspectral information from low-spectral resolution data acquired Figure 1: Block diagram of the proposed scheme: Our algorithm takes as input a hypercube acquired with a limited number of spectral bands and constructs an estimate of the full spectrum of the scene. by the spectral detectors, by capitalizing on the Sparse Representations (SR) and the joint dictionary learning frameworks [18,19], effectively encoding the relationships between high and low spectral resolution â€œhyper-pixelsâ€. Spectral Super-Resolution Using Sparsity This work proposes a novel scheme for synthesizing a highspectral resolution hypercube from a limited number of acquired spectral bands. More specifically, given a low-spectral resolution hyperspectral scene acquired with M spectral bands, our goal is to estimate the extended spectrum composed of N spectral bands, where N > M. In order to achieve this, we employ the Sparse Representations framework [18], which states that linear combinations between high-frequency signals can be accurately recovered from their corresponding low-frequency linear representations. The notion of sparsity has revolutionized modern signal processing and machine learning, and has lead to very impressive results in a variety of imaging problems, including superresolution, de-nighting etc. [20, 21]. Instead of observing directly the high-spectral resolution components, we work with double over-complete dictionaries, Dh for the high-spectral, and D` for the low-spectral resolution scenes. The sparse code of the low-spectral resolution part in terms of D`, will be combined with the high spectral resolution dictionary to generate the desired high-spectral resolution component. Formally, given a low-spectral resolution input hypercube S`, â€hyper-pixelsâ€ s` âˆˆ RM are extracted and mapped to the lowspectral resolution dictionary matrix D` âˆˆRMÃ—P containing P examples. Subsequently, we seek to identify the sparse code vector wâˆˆRP, with respect to the corresponding low-spectral resolution dictionary matrix. Recovery of the sparse code w is achieved by solving the following minimization problem: min w ||w||0 subject to ||s`âˆ’D`w||2 < Îµ, (1) where Îµ stands for the acceptable approximation error which is related to the added noise. This problem can be solved by a greedy strategy such as the Orthogonal Matching Pursuit algorithm [22]. Alternatively, one can replace the non-zero counting `0 pseudonorm by its convex surrogate `1-norm: ||w||1 = âˆ‘i |wi|, and solve the corresponding problem given by: min w ||s`âˆ’D`w||2 +Î» ||w||1, (2) where Î» is a regularization parameter, a formulation known as the LASSO problem [23]. By considering the joint training of the low and high spectral resolution dictionaries, the objective is to identify the sparse code vector that can produce both the low and the high spectral resolution representations. Consequently, assuming that such an optimal sparse code w? is found by solving Eq. 2, we recover the high spectral resolution â€hyper-pixelâ€ sh, by projecting w? to the high-spectral resolution dictionary, Dh, according to: sh = Dhw (3) The two main challenges for the proposed spectral resolution enhancement scheme is the sufficient sparsity measure for the sparse vector w, and the proper construction of the dictionary matrices D`, and Dh which will allow the sparsification of both low and higher spectral resolution data. In the following, an efficient scheme for multiple feature space dictionary learning is provided. Coupled Dictionary Construction Consider a set composed of high and low spectral resolution hypercubes. We assume that these scenes are realized by the same statistical process under different spectral resolution conditions, and as such, they share approximately the same sparse code with respect to their corresponding dictionaries, Dh and D`. A straightforward strategy to create these dictionaries is to randomly sample multiple correspondent â€œhyper-pixelsâ€ extracted from corresponding high and low spectral resolution training sets and use this random selection as the sparsifying dictionary. However, such a strategy is not able to guarantee that the same sparse code can be utilized among the two different representations. To overcome this limitation, we propose learning a compact dictionary from such pairs of high and low-spectral resolution data-cubes. Given a large set of training â€œhyper-pixelsâ€ extracted from multipl",2016,
Unravelling the effect of flow regime on macroinvertebrates and benthic algae in regulated versus unregulated streams,"Ecohydrology. 2018;11:e1996. https://doi.org/10.1002/eco.1996 Abstract Variability in riverine flow regimes is important for aquatic biodiversity. However, across the globe, management of water resources has altered natural flow dynamics. We explored relationships between flow regime (calculated from 3 years' daily averaged discharge), and water chemistry, benthic algae, as well as macroinvertebrate datasets from 64 sites across Germany and Norway. To deal with multicollinearity while maintaining interpretability, we performed principal component (PC) analyses for each dataset in each country and selected the metric with the highest absolute loading on each PC to represent that PC. We then used L1â€regularized (lasso) regression to link differences in water chemistry and hydrology to differences in ecology and compared this approach with the more popular bestâ€subsets ordinary least squares (OLS) regression. The results obtained using lasso regression were broadly comparable to those produced by bestâ€subsets OLS, but the lasso approach â€œrejectedâ€ more models than the bestâ€subsets approach. When lasso identified a plausible model, it was the same or similar to the best model found by bestâ€subsets OLS. The lasso method was more â€œdiscerningâ€, that is, it identified a smaller number of potentially interesting models, whereas bestâ€subsets regression seemed to find â€œtoo manyâ€ relationships. We identified two response variables that were potentially affected by regulation: (a) River regulation may lead to higher cyanobacterial abundance, possibly via a less variable flow regime; (b) reduced flow variability may lead to a reduced proportion of grazers and scrapers, possibly indicating a shift towards an increased importance of heterotrophic energy sources in ecosystems with less variable flows.",2018,Ecohydrology
Crystal structure and electrochemical behaviors of Pt/mischmetal film electrodes,"The Ml(La-rich mischmetal) films with a thin Pt layer on the substrate of chemically coarsen ITO glassor silicon slices were prepared by magnetic sputtering technique. The crystal structure and surface morphology ofthe films were investigated by X-ray diffraction(XRD) analysis and atomic force microscopy(AFM), respectively.The electrochemical hydridation/dehydridation behaviors of the films in KOH solution were studied by using cyclicvoltammagraph and electrochemical impedance spectrum(EIS) as well. The AFM results show that the Pt cover lay-er on the M1 films is of island structure with a grain of 150 - 200 nm in size. The presence of a thin Pt layer can pro-vide sufficient high electrocatalytic activity for the electrochemical charge-transfer reaction. The electrochemical re-duction and oxidation reaction occur on the Pt layer, and the diffusion of H into the Ml film is the rate-controlledstep. The Pt coatings also act as protective layers, preventing oxidation and/or poisoning of the underlying Ml filmsin air.",2003,
Development and validation of an ultrasound-based nomogram to improve the diagnostic accuracy for malignant thyroid nodules,"ObjectivesThe aim of this study was to develop an ultrasound-based nomogram to improve the diagnostic accuracy of the identification of malignant thyroid nodules.MethodsA total of 1675 histologically proven thyroid nodules (1169 benign, 506 malignant) were included in this study. The nodules were grouped into the training dataset (n = 700), internal validation dataset (n = 479), or external validation dataset (n = 496). The grayscale ultrasound features included the nodule size, shape, aspect ratio, echogenicity, margins, and calcification pattern. We applied least absolute shrinkage and selection operator (lasso) regression to select the strongest features for the nomogram. Nomogram discrimination (area under the receiver operating characteristic curve, AUC) and calibration were assessed. The nomogram was subjected to bootstrapping validation (1000 bootstrap resamples) to calculate a mean AUC and 95% confidence interval (CI).ResultsThe nomogram showed good discrimination in the training dataset, with an AUC of 0.936 (95% CI: 0.918â€“0.953) and good calibration. Application of the nomogram to the internal validation dataset also resulted in good discrimination (AUC: 0.935; 95% CI, 0.915â€“0.954) and good calibration. The model tested in an external validation dataset demonstrated a lower AUC of 0.782 (95% CI: 0.776â€“0.789).ConclusionsThis ultrasound-based nomogram can be used to quantify the probability of malignant thyroid nodules.Key Pointsâ€¢ Ultrasound examination is helpful in the differential diagnosis of malignant and benign thyroid nodules.â€¢ However, ultrasound accuracy relies heavily on examiner experience.â€¢ A less subjective diagnostic model is desired, and the developed nomogram for thyroid nodules showed good discrimination and good calibration.",2018,European Radiology
Stability Feature Selection using Cluster Representative LASSO,"Variable selection in high dimensional regression problems with strongly correlated variables or with near 
 
linear dependence among few variables remains one of the most important issues. We propose to cluster the 
 
variables first and then do stability feature selection using Lasso for cluster representatives. The first step 
 
involves generation of groups based on some criterion and the second step mainly performs group selection 
 
with controlling the number of false positives. Thus, our primary emphasis is on controlling type-I error for 
 
group variable selection in high-dimensional regression setting. We illustrate the method using simulated and 
 
pseudo-real data, and we show that the proposed method finds an optimal and consistent solution.",2016,
Stable Feature Selection with Minimal Independent Dominating Sets,"In this paper, we focus on stable selection of relevant features. The main contribution is a novel framework for selecting most informative features which can preserve the linear combination property of the original feature space. We propose a novel formulation of this problem as selection of a minimal independent dominating set (MIDS). MIDS of a feature graph is a smallest subset such that no two of its nodes are connected and all other nodes are connected to at least one node in it. In this way, the diversity and coverage of the original feature space can be preserved.
 Furthermore, the proposed MIDS framework complements standard feature selection algorithms like SVM-RFE, stability lasso and ensemble SVM RFE. When these algorithms are applied to feature subsets selected by MIDS as opposed to all the input features, they select more stable features and achieve better prediction accuracy, as our experimental results clearly demonstrate.",2013,
Effects of Caulerpa racemosa var. cylindracea on prey availability: an experimental approach to predation of amphipods by Thalassoma pavo (Labridae),"Alien plant species, such as Caulerpa racemosa var. cylindracea, that invade Mediterranean marine vegetated habitats can affect habitat structure. In turn, changes in habitat structure may affect the associated invertebrate assemblages, either through changes in habitat selection or as a result of altered predation efficiency. In order to test for effects of changes in habitat structure resulting from colonization by C. racemosa on prey availability for predators, the importance of amphipods as a trophic resource in natural vegetated habitat was first assessed, and later experiments were undertaken to assess the effects of the alien alga on predation by Thalassoma pavo of two dominant amphipods: Elasmopus brasiliensis (Gammaridea) and Caprella dilatata (Caprellidea). Laboratory experiments were conducted in separate aquaria with five vegetation habitat types: Halopteris scoparia, Jania rubens, C. racemosa without detritus, C. racemosa with detritus, Cymodocea nodosa, together with controls. The vegetation was first defaunated, and then 30 amphipods were introduced to each aquarium and exposed to a single Thalassoma pavo individual for 1Â h, after which the fishâ€™s gut contents were examined. Consumption (per fish per hour) of caprellids (11.7Â Â±Â 1.4) was higher overall than that of gammarids (8.7Â Â±Â 1.5) and likely reflects different microhabitat use by amphipods, which affects susceptibility to predators. Consumption of amphipods also varied by habitat type. The highest predation rate was found in the C. nodosa habitat (12.7Â Â±Â 2.19) and the lowest in the C. racemosa habitats with detritus (4.1Â Â±Â 1.78) and without detritus (5.2Â Â±Â 0.55), which did not differ. The pattern of predation across habitats, however, was similar for both caprellid and gammarid amphipods, indicating a more general effect of habitat on amphipod predation. Our findings showed that invasive species such as C. racemosa can decrease feeding by predators such as T. pavo. Changes in predatorâ€“prey interactions could have consequences for food web support in the Mediterranean.",2010,Hydrobiologia
"Process for the preparation of liquid products and, optionally, gaseous from gaseous reactants.","Procedure used for liquid products and, optionally, gaseous products from gaseous reactants, and of introducing (14) these reagents in a bed (40) of composite particles suspended in a liquid reactor to effect the reaction simultaneously with its rise through the bed, which is the product, the reagents and any gaseous product and helps to maintain the suspension. The liquid product is, with the suspension liquid, the liquid phase of the bed. Any gaseous product and reagents that have not experienced any reaction rise from the bed to form a gas phase (44). The suspension descends into the bed through the downcomers (22) located respectively in a gas phase 844). The suspension descends into the bed through the downcomers (22, 32) located respectively at a first and a second zone (20, 30) of l bed, allowing particulassolidas redistribute within the bed. The second downcomer zone away from the first zone in a vertical direction. Is removed (16) any gaseous product and reactants which have not undergone any reaction from the gas phase (44) and withdrawn (18) the liquid phase of the bed. It is preferably carried out a Fischer-Tropsch, introducing the gaseous reactants as a synthesis gas stream containing mainly carbon monoxide and hydrogen.",1998,
Evolutionary multitasking fuzzy cognitive map learning,"Abstract In real-world applications, there exist multiple fuzzy cognitive maps (FCMs) learning tasks with similar attributes that have to be optimized simultaneously, however, all existing algorithms were designed to learn single FCM without considering the valuable patterns that can share with each other. For the purpose of making use of similar structure patterns among different tasks, we introduce the evolutionary multitasking framework to learn different FCMs at one time by taking each FCM learning problem as a task. Most proposed evolutionary-based algorithms learn FCMs from time series by minimizing data error which evaluates the difference between generated response sequences and available response sequences, which did not take the sparsity of the weight matrix into consideration. To learn large-scale FCMs for each task, in this paper we adopt a decomposition strategy based multiobjective optimization algorithm considering both the measure error and sparsity of FCMs. Moreover, the memetic algorithm and LASSO initialization operator are incorporated into the multitasking framework to improve the performance and accelerate the convergence. Through the whole process, we find that multitasking optimization can not only learn various FCMs in a population but also improve the accuracy of similar tasks by taking the advantage of gene transfer for similar patterns. Extensive experiments on two-task FCM learning problems with varying number of nodes, densities and activation functions and the application for the problem of reconstructing gene regulatory networks have been conducted to illustrate that the proposal can learn large-scale FCMs with low errors in a fast convergence speed.",2020,Knowl. Based Syst.
Existence of Solutions of Inequalities Involving Quasimonotone Functions~1,"A large amount of nonlinear boundary value problems may be treated with the help of the theory of monotone operators. Two of the basic results of this theory are the Debrunner-Flor monotone extention theorem and the Hartman-Stampacchia theorem on variational inequalities. In 1983, Lassonde extended and improved the Debrunner and Flor's theorem and Hartman and Stampacchia's theorem in a convex space under a relaxation of the compactness assumption.It is our object in this paper to establish Lassonde type theorems under an assumption greatly weaker than semimonotonicity.",2005,Journal of Yantai University
Modeling double time-scale travel time processes with application to assessing the resilience of transportation systems,"Abstract This paper proposes a double time-scale model to capture the day-to-day evolution along with the within-day variability of travel time. The proposed model can be used to evaluate short-term to long-term effects of new transport policies and construction of critical infrastructures, and to analyze the resilience of road networks under disruptions. The within-day travel time variability is modeled using the functional data analysis, in which the effects of road traffic congestion and noise of traffic data are considered explicitly. The within-day process is then regarded as the local volatility (or the noise process) to drive the day-to-day process while the latter is described by a modified geometric Brownian motion (GBM). Then, the day-to-day travel time process is obtained by the statistics of the modified GBM. The model probabilistically captures the evolution of day-to-day and within-day travel time processes analytically. Moreover, an efficient method based on the cross-entropy method is developed for calibrating the model parameters. A lasso-like regularization is employed to guarantee a small bias between the model estimations and the measurement counterparts. Finally, two empirical studies are carried out to validate the proposed model at different scales with different traffic scenarios, i.e., a case study on the Guangzhou Airport Expressway (link to path scale) under traffic accident conditions and a case study in New York City (city-scale) to analyze the network resilience under Hurricane Sandy. Both case studies adopted probe vehicle data but with different configurations (fine versus coarse, small versus big data). The empirical studies confirm that the proposed model can accommodate the inherent variability in traffic conditions and data meanwhile being computationally tractable. The numerical results illustrate the applicability of the proposed model as a powerful tool in practice for analyzing the short-term and long-term impacts of disruptions and systematic changes in the performance of road networks.",2019,Transportation research procedia
Hierarchical Bayesian LASSO for a negative binomial regression,"ABSTRACT Numerous researches have been carried out to explain the relationship between the count data y and numbers of covariates x through a generalized linear model (GLM). This paper proposes a hierarchical Bayesian least absolute shrinkage and selection operator (LASSO) solution using six different prior models to the negative binomial regression. Latent variables Z have been introduced to simplify the GLM to a standard linear regression model. The proposed models regard two conjugate zero-mean Normal priors for the regression parameters and three independent priors for the variance: the Exponential, Inverse-Gamma and Scaled Inverse- distributions. Different types of priors result in different amounts of shrinkage. A Metropolisâ€“Hastings-within-Gibbs algorithm is used to compute the posterior distribution of the parameters of interest through a data augmentation process. Based on the posterior samples, an original double likelihood ratio test statistic have been proposed to choose the most relevant covariates and shrink the insignificant coefficients to zero. Numerical experiments on a real-life data set prove that Bayesian LASSO methods achieved significantly better predictive accuracy and robustness than the classical maximum likelihood estimation and the standard Bayesian inference.",2016,Journal of Statistical Computation and Simulation
Supplementary materials: Iterative Smoothing Proximal Gradient for Regression with Structured Sparsity,"Learning predictive models from high-dimensional brain imaging opens up for possibilities to decode cognitive states or diagnosis/prognosis of a clinical condition/evolution. 
Structured sparsity is a promising approach which alleviates the risk of overfitting while forcing the solution to adhere to some domain-specific constraints and thus offering new means to identify biomarkers. 
We consider the problem of optimizing the sum of a smooth convex loss and a non-smooth convex penalty, whose proximal operator is known, and a wide range of complex, non-smooth convex structured penalties such as \eg total variation, or overlapping group lasso. 
%We propose a generic optimization framework that can combine any smooth convex loss function with: (i) penalties whose proximal operator is known and (ii) with a large range of complex, non-smooth convex structured penalties such as total variation, or overlapping group lasso. 
Although many solvers have already been proposed, their practical use in the context of high-dimensional imaging data ($\geq10^5$ features) remains an open issue. 
We propose to smooth the structured penalty, since this approach allows a generic framework in which a large range of non-smooth convex structured penalties can be minimized without computing their proximal operators. 
This is beneficial since the proximal operators are either not known or expensive to compute. 
The problem can be minimized with an accelerated proximal gradient method, taking advantage of the potential (non-smoothed) sparsity-inducing penalties. 
As a first contribution we propose an expression of the duality gap to control the convergence of the global non-smooth problem. 
This expression is applicable to a large range of structured penalties. 
However, smoothing methods have many limitations that the proposed solver aims to overcome. 
Therefore, as a second contribution, we propose a continuation algorithm, called \textit{CONESTA}, that dynamically generates a decreasing sequence of smoothing parameters in order to maintain the optimal convergence speed towards any globally desired precision. 
At each continuation step, the aforementioned duality gap provides the current error and thus the next smaller prescribed precision. 
Given this precision, we propose a expression to calculate the optimal smoothing parameter, that minimizes the number of iterations to reach such precision. 
We demonstrate that CONESTA achieves an improved convergence rate compared to classical (without continuation) proximal gradient smoothing. 
Moreover, experiments conducted on both simulated and high-dimensional neuroimaging (MRI) data, exhibit that CONESTA significantly outperforms the excessive gap method, ADMM, classical proximal gradient smoothing and inexact FISTA in terms of convergence speed and/or precision of the solution.",2016,
Measles and pregnancy. 16 case reports from Burkina Faso,Measles remains in an endemic-epidemic state in developing countries affecting especially children under 2 years old. Cases of adolescent and adult measles are rare. 16 cases of measles are reported among pregnant women seen during a recent epidemic in Bobo Dioulasso Burkina Faso. The women were of mean age 20.6 years with a range of 14-27 years. Their mean gravidity and parity were 2.1 and 1.1 respectively. There were 15 cases of conjunctivitis 10 cases of hyperthermia and 16 cases of cutaneous rash. 6 cases of laryngitis and 3 pneumopathies were seen as maternal complications. No maternal death was observed and all patients were HIV seronegative. 2 abortions 3 stillbirths 1 preterm delivery and 2 full-term births were recorded. 8 women with ongoing pregnancies were lost to follow-up after their discharge from the hospital. No case of congenital or neonatal measles was observed among the 3 living infants. While measles associated with pregnancy is rare such an association can threaten pregnancy over the short term. Systematic prevention is therefore called for among pregnant exposed women and their newborns through immunotherapy as well as in children through vaccination.,1997,Journal De Gynecologie Obstetrique Et Biologie De La Reproduction
The Swedish model in times of crisis: decline or resilience?,"In the mid-2000s, my colleague Harald Niklasson and I published an article, â€˜The Swedish Model in Turbulent Timesâ€™, which retraced the main developments of the Swedish model from its inception in the early 1950s up to the early 2000s (Anxo and Niklasson 2006). Our main conclusion was that, at the turn of the century, the Swedish model appeared more in line with the core components of the original Swedish model1 than during the decades 1970â€“1980, which constituted, in our views, a clear deviation. During the 1990s, the economic policy modifi cations towards more restrictive and anti-infl ationary macroeconomic policies, the reorientation of active labour market policies towards supply-oriented measures and the structural reforms undertaken in wage formation, tax and social protection systems suggested a revival of the Swedish model. After a period of turbulence related to the early 1990s economic crisis, the Swedish economy underwent particularly favourable economic development. Up to the current global recession, unemployment oscillated between 5 and 6 per cent, infl ation was curbed and current account and public fi nances were restored.",2015,
Action elements of emotional body expressions for flying robots,"This paper describes emotional body expressions by flying robots for displaying emotions. Subjective evaluation experiments were performed to clarify the relation between movement parameters and subjective feeling of the expressions. Emotional body expressions were designed by eight people. The expressions were evaluated by another group of twelve people, who answered a questionnaire. The elements of emotional body expressions were investigated using the Lasso method. The results indicate movements that are most effective for displaying a particular emotion to a human. Such expressions are expected to enhance communication between the flight robots and humans.",2016,2016 11th ACM/IEEE International Conference on Human-Robot Interaction (HRI)
Predictive Abilities of Machine Learning Techniques May Be Limited by Dataset Characteristics: Insights From the UNOS Database.,"BACKGROUND
Traditional statistical approaches to prediction of outcomes have drawbacks when applied to large clinical databases. It is hypothesized that machine learning methodologies might overcome these limitations by considering higher-dimensional and nonlinear relationships among patient variables.


METHODS AND RESULTS
The Unified Network for Organ Sharing (UNOS) database was queried from 1987 to 2014 for adult patients undergoing cardiac transplantation. The dataset was divided into 3 time periods corresponding to major allocation adjustments and based on geographic regions. For our outcome of 1-year survival, we used the standard statistical methods logistic regression, ridge regression, and regressions with LASSO (least absolute shrinkage and selection operator) and compared them with the machine learning methodologies neural networks, naÃ¯ve-Bayes, tree-augmented naÃ¯ve-Bayes, support vector machines, random forest, and stochastic gradient boosting. Receiver operating characteristic curves and C-statistics were calculated for each model. C-Statistics were used for comparison of discriminatory capacity across models in the validation sample. After identifying 56,477 patients, the major univariate predictors of 1-year survival after heart transplantation were consistent with earlier reports and included age, renal function, body mass index, liver function tests, and hemodynamics. Advanced analytic models demonstrated similarly modest discrimination capabilities compared with traditional models (C-statistic â‰¤0.66, all). The neural network model demonstrated the highest C-statistic (0.66) but this was only slightly superior to the simple logistic regression, ridge regression, and regression with LASSO models (C-statisticâ€¯=â€¯0.65, all). Discrimination did not vary significantly across the 3 historically important time periods.


CONCLUSIONS
The use of advanced analytic algorithms did not improve prediction of 1-year survival from heart transplant compared with more traditional prediction models. The prognostic abilities of machine learning techniques may be limited by quality of the clinical dataset.",2019,Journal of cardiac failure
Prescription-drug-related risk in driving: comparing conventional and lasso shrinkage logistic regressions.,"BACKGROUND
Large data sets with many variables provide particular challenges when constructing analytic models. Lasso-related methods provide a useful tool, although one that remains unfamiliar to most epidemiologists.


METHODS
We illustrate the application of lasso methods in an analysis of the impact of prescribed drugs on the risk of a road traffic crash, using a large French nationwide database (PLoS Med 2010;7:e1000366). In the original case-control study, the authors analyzed each exposure separately. We use the lasso method, which can simultaneously perform estimation and variable selection in a single model. We compare point estimates and confidence intervals using (1) a separate logistic regression model for each drug with a Bonferroni correction and (2) lasso shrinkage logistic regression analysis.


RESULTS
Shrinkage regression had little effect on (bias corrected) point estimates, but led to less conservative results, noticeably for drugs with moderate levels of exposure. Carbamates, carboxamide derivative and fatty acid derivative antiepileptics, drugs used in opioid dependence, and mineral supplements of potassium showed stronger associations.


CONCLUSION
Lasso is a relevant method in the analysis of databases with large number of exposures and can be recommended as an alternative to conventional strategies.",2012,Epidemiology
Identification of 13 blood-based gene expression signatures to accurately distinguish tuberculosis from other pulmonary diseases and healthy controls.,"Tuberculosis (TB), caused by infection with mycobacterium tuberculosis, is still a major threat to human health worldwide. Current diagnostic methods encounter some limitations, such as sample collection problem or unsatisfied sensitivity and specificity issue. Moreover, it is hard to identify TB from some of other lung diseases without invasive biopsy. In this paper, the logistic models with three representative regularization approaches including Lasso (the most popular regularization method), and L1/2 (the method that inclines to achieve more sparse solution than Lasso) and Elastic Net (the method that encourages a grouping effect of genes in the results) adopted together to select the common gene signatures in microarray data of peripheral blood cells. As the result, 13 common gene signatures were selected, and sequentially the classifier based on them is constructed by the SVM approach, which can accurately distinguish tuberculosis from other pulmonary diseases and healthy controls. In the test and validation datasets of the blood gene expression profiles, the generated classification model achieved 91.86% sensitivity and 93.48% specificity averagely. Its sensitivity is improved 6%, but only 26% gene signatures used compared to recent research results. These 13 gene signatures selected by our methods can be used as the basis of a blood-based test for the detection of TB from other pulmonary diseases and healthy controls.",2015,Bio-medical materials and engineering
Optimization of Polygalacturonase Production from a Newly Isolated Thalassospira frigidphilosprofundus to Use in Pectin Hydrolysis: Statistical Approach,"The present study deals with the production of cold active polygalacturonase (PGase) by submerged fermentation using Thalassospira frigidphilosprofundus, a novel species isolated from deep waters of Bay of Bengal. Nonlinear models were applied to optimize the medium components for enhanced production of PGase. Taguchi orthogonal array design was adopted to evaluate the factors influencing the yield of PGase, followed by the central composite design (CCD) of response surface methodology (RSM) to identify the optimum concentrations of the key factors responsible for PGase production. Data obtained from the above mentioned statistical experimental design was used for final optimization study by linking the artificial neural network and genetic algorithm (ANN-GA). Using ANN-GA hybrid model, the maximum PGase activity (32.54 U/mL) was achieved at the optimized concentrations of medium components. In a comparison between the optimal output of RSM and ANN-GA hybrid, the latter favored the production of PGase. In addition, the study also focused on the determination of factors responsible for pectin hydrolysis by crude pectinase extracted from T. frigidphilosprofundus through the central composite design. Results indicated 80% degradation of pectin in banana fiber at 20 Â°C in 120 min, suggesting the scope of cold active PGase usage in the treatment of raw banana fibers.",2013,BioMed Research International
New Evidence of Marine Fauna Tropicalization off the Southwestern Iberian Peninsula,"2 Department of Environmental Science, Policy, and Management, Mulford Hall, University of 9 California, Berkeley, Berkeley, CA 94720, USA; pmorais@ualg.pt (P.M.) 10 * Correspondence: pmorais@berkeley.edu 11 12 13 14 Abstract: Climate change and the overall increase of seawater temperature is causing a poleward 15 shift in species distribution, which includes a phenomenon described as tropicalization of temperate 16 regions. This work aims at reporting the first records of four species off the southwestern Iberian 17 Peninsula, namely oceanic puffer Lagocephalus lagocephalus Linnaeus, 1758, Madeira rockfish 18 Scorpaena maderensis Valenciennes, 1833, ornate wrasse Thalassoma pavo Linnaeus, 1758, and bearded 19 fireworm Hermodice carunculata Pallas, 1766. These last three species, along with other occurrences of 20 aquatic fauna and flora along the Portuguese coast, reveal an ongoing process of poleward expansion 21 of several species for which a comprehensive survey along the entire Iberian Peninsula is urgent. The 22 putative origins of these subtropical and tropical species off continental Portugal are discussed, as 23 well as the urgent need of public awareness due to potential health risks resulting from the toxicity 24 of two of the four species reported in this paper. 25 26",2019,
"Food of Cobia, Rachycentron canadum, from the Northcentral Gulf of Mexico","The stomach contents of 403 cobia, Rachycentron canadwn, caught in the northcentral Gulf of Mexico recreational fishery from April through October of 1987-1990 were examined. Cobia ranged from 373-1,530 mm in fork length. Of the 403 stomachs, 287 (71.2%) contained at least one identifiable prey taxon. Crustaceans, consisting primarily of portunid crabs, were the predominant food. Crustaceans occurred in 79.1% of the stomachs and comprised 77.6% of the total number of identifiable prey. The second most important prey categoy was fish which was dominated by hardhead catfish, Arius felis, and eels. Fish occurred in 58.5% of the stomachs but only accounted for 20.3% of the total number of prey. The importance of fish as prey increased withincreasing size(length)ofcobia, with thelargest sizeclassofcobia(1,150-1,530mmFL)showingthehighest percent frequency occurrence of fish prey (84.4%). There were no significant differences between the diets of male and female cobia. Species composition of the diet indicated that cobia examined in this study were generalist carnivores in their feeding habits and fed primarily on benthic/epibenthic crustaceans and fishes. However, the Occurrence of pelagic prey provided evidence of diversity in the foraging behavior of cobia. Feeding in cobia indicated their dependence upon prey availability rather than upon a few specific food organisms.",1996,Gulf and Caribbean Research
Intra- and interspecific variation in wood density and fine-scale spatial distribution of stand-level wood density in a northern Thai tropical montane forest.,"Tropicaltreewooddensityisoftenrelatedtootherspecies-specificfunctionaltraits,e.g.size,growthrateand mortality. We would therefore expect significant associations within tropical forests between the spatial distributions of stand-level wood density and micro-environments when interspecific variation in wood density is larger than intraspecific variation and when habitat-based species assembly is important in the forest. In this study, we used wood cores collected from 515 trees of 72 species in a 15-ha plot in northern Thailand to analyse intra- and interspecific variationinwooddensityandthespatialassociationofstand-levelwooddensity.Intraspecificvariationwaslowerthan interspecific variation (20% vs. 80% of the total variation), indicating that species-specific differences in wood density, rather than phenotypic plasticity, are the major source of variation in wood density at the study site. Wood density of individual species was significantly negatively related to maximum diameter, growth rate of sapling diameter and mortalityofsaplings.Stand-levelmeanwooddensitywassignificantlynegativelyrelatedtoelevation,slopeconvexity, saplinggrowthrateandsaplingmortality,andpositivelyrelatedtoslopeinclination.East-facingslopeshadsignificantly lower stand-level mean wood densities than west-facing slopes. We hypothesized that ridges and east-facing slopes in the study forest experience strong and frequent wind disturbance, and that this severe impact may lead to faster stand turnover, creating conditions that favour fast-growing species with low wood density.",2009,Journal of Tropical Ecology
Sea cucumber intestinal alkaline protease producing strains and Applications,"The present invention provides an isolated sea cucumber intestinal alkaline protease producing strain (Thalassobacillus sp.), Was October 18, 2013 at Accession No. 3, 1 Microorganisms located North Star Road, Chaoyang District, Beijing Academy of Chinese Academy of Sciences the Culture Collection of China Committee of general Microbiology Center, under the accession number CGMCC No.8366. The strain of Bacillus, which is characterized in more humid colonies, yellowish, and central raised circular edges neat. The present invention discloses the major component of the fermentation medium for enzyme production, concentration and culture process. The present invention is applied to sea cucumber processing waste fermentation enzyme preparation of biologically active polypeptide, it has good application prospect. CGMCC No.8366 20131018",2014,
Application of lÏ norm regularization methods for modelling biological systems,"In systems biology, molecular interactions are typically modelled using white-box differential equations based on mass action kinetics. Unfortunately, problems with dimensionality can arise when the number of molecular species in the system becomes very large, which make the transparent modelling and behavior simulation extremely difficult or computationally too expensive. As an alternative, data-driven identification of molecular interaction pathways using a black-box approach has recently been investigated. One of the main objectives in building black-box models, which in many cases are linear-in-the-parameters ones, is to produce a sparse model to effectively represent the system behavior. A popular approach is to select model terms one by one from a pool of candidates (basis functions), and an information criterion is then used to stop the selection process. The advantage is the computational efficiency, the disadvantage is that the derived model is not necessarily sparse. Alternative approach is to introduce into the normal loss function a penalty term on the parameters, leading to improved sparseness and generalization performance of the derived model. Moreover, there is a positive probability that the model structure can be accurately picked up among a wide range of possibilities. Generally speaking, there are three lÏ norm regularization methods, including the Lasso (Ï = 1), Ridge (Ï = 2) and Bridge (0 â‰ª Ï â‰ª 1). In particular, Lasso has been introduced into computational biology in recent years. This paper investigates the effectiveness of the three (lÏ) regularization methods on the model identification of the MAPK signal transduction pathway, and simulation results are compared and analyzed.",2009,2009 International Conference on Machine Learning and Cybernetics
Parametric Programming Approach for Powerful Lasso Selective Inference without Conditioning on Signs,"In the past few years, Selective Inference (SI) has been actively studied for inference on the features of linear models that are adaptively selected by feature selection methods. A seminal work is proposed by Lee et al. [22] in the case of the Lasso. The basic idea of SI is to make inference conditional on the selection event. The authors in [22] proposed a tractable way to conduct inference conditional on the selected features and their signs. Unfortunately, additionally conditioning on the signs leads to low statistical power because of over-conditioning. To improve the power, a current available possible solution is to remove the conditioning on signs by considering the union of an exponentially large number of all possible sign vectors, which leads to an unrealistically large amount of computational cost unless the number of selected features is sufficiently small. To address this problem, we propose an efficient method to characterize the selection event without conditioning on signs by using parametric programming. The main idea is to compute the continuum path of Lasso solutions in the direction of a test statistic, and identify the subset of data space corresponding to the feature selection event by following the solution path. We conduct several experiments to demonstrate the effectiveness and efficiency of our proposed method.",2020,ArXiv
A Four-Pseudogene Classifier Identified by Machine Learning Serves as a Novel Prognostic Marker for Survival of Osteosarcoma,"Osteosarcoma is a common malignancy with high mortality and poor prognosis due to lack of predictive markers. Increasing evidence has demonstrated that pseudogenes, a type of non-coding gene, play an important role in tumorigenesis. The aim of this study was to identify a prognostic pseudogene signature of osteosarcoma by machine learning. A sample of 94 osteosarcoma patients' RNA-Seq data with clinical follow-up information was involved in the study. The survival-related pseudogenes were screened and related signature model was constructed by cox-regression analysis (univariate, lasso, and multivariate). The predictive value of the signature was further validated in different subgroups. The putative biological functions were determined by co-expression analysis. In total, 125 survival-related pseudogenes were identified and a four-pseudogene (RPL11-551L14.1, HR: 0.65 (95% CI: 0.44-0.95); RPL7AP28, HR: 0.32 (95% CI: 0.14-0.76); RP4-706A16.3, HR: 1.89 (95% CI: 1.35-2.65); RP11-326A19.5, HR: 0.52(95% CI: 0.37-0.74)) signature effectively distinguished the high- and low-risk patients, and predicted prognosis with high sensitivity and specificity (AUC: 0.878). Furthermore, the signature was applicable to patients of different genders, ages, and metastatic status. Co-expression analysis revealed the four pseudogenes are involved in regulating malignant phenotype, immune, and DNA/RNA editing. This four-pseudogene signature is not only a promising predictor of prognosis and survival, but also a potential marker for monitoring therapeutic schedule. Therefore, our findings may have potential clinical significance.",2019,Genes
Bacteria associated with sabellids (Polychaeta: Annelida) as a novel source of surface active compounds.,"A total of 69 bacteria were isolated from crude oil enrichments of the polychaetes Megalomma claparedei, Sabella spallanzanii and Branchiomma luctuosum, and screened for biosurfactant (BS) production by conventional methods. Potential BS-producers (30 isolates) were primarily selected due to the production of both interesting spots on thin layer chromatography (TLC) plates and highly stable emulsions (Eâ‚‚â‚„ â‰¥ 50%). Only few strains grew on cetyltrimethylammonium bromide and blood agar plates, indicating the probable production of anionic surfactants. The 16S rRNA gene sequencing revealed that selected isolates mainly belonged to the CFB group of Bacteroidetes, followed by Gammaproteobacteria and Alphaproteobacteria. A number of BS-producers belonged to genera (i.e., Cellulophaga, Cobetia, Cohaesibacter, Idiomarina, Pseudovibrio and Thalassospira) that have been never reported as able to produce BSs, even if they have been previously detected in hydrocarbon-enriched samples. Our results suggest that filter-feeding Polychaetes could represent a novel and yet unexplored source of biosurfactant-producing bacteria.",2013,Marine pollution bulletin
Single-trial Connectivity Estimation through the Least Absolute Shrinkage and Selection Operator,"Methods based on the use of multivariate autoregressive models (MVAR) have proved to be an accurate tool for the estimation of functional links between the activity originated in different brain regions. A well-established method for the parameters estimation is the Ordinary Least Square (OLS) approach, followed by an assessment procedure that can be performed by means of Asymptotic Statistic (AS). However, the performances of both procedures are strongly influenced by the number of data samples available, thus limiting the conditions in which brain connectivity can be estimated. The aim of this paper is to introduce and test a regression method based on Least Absolute Shrinkage and Selection Operator (LASSO) to broaden the estimation of brain connectivity to those conditions in which current methods fail due to the limited data points available. We tested the performances of the LASSO regression in a simulation study under different levels of data points available, in comparison with a classical approach based on OLS and AS. Then, the two methods were applied to real electroencephalographic (EEG) signals, recorded during a motor imagery task. The simulation study and the application to real EEG data both indicated that LASSO regression provides better performances than the currently used methodologies for the estimation of brain connectivity when few data points are available. This work paves the way to the estimation and assessment of connectivity patterns with limited data amount and in on-line settings.",2019,2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)
Accuracy of genomic prediction using RR-BLUP and Bayesian LASSO,"We compared the accuracies of two genomic-selection prediction methods as affected by marker density and quantitative trait locus (QTL) number. Methods used to derive genomic estimated breeding values (GEBV) were random regression best linear unbiased prediction (RRâ€“BLUP) and a Bayesian LASSO (Least Absolute Shrinkage and Selection Operator). In this study the genome comprised four chromosomes of 250 cM each. Also considering the number of markers 1000, 2000 and 5000 and the number of QTLs 4, 10, 20 and 40 and heritability of 5, 10 and 25 percent were compared.. In all scenarios Bayesian LASSO was more accurate than RR-BLUP, also increasing the number of QTLs, the evaluation accuracy decreases slightly which this reduction is greater in the lower heritability. The correlation between true breeding value and the genomic estimated breeding value in target generations applying RR-BLUP and Bayesian LASSO decreased from 0.918 to 0.807 and 0.933 to 0.847 respectively.",2013,European Journal of Experimental Biology
Bayesian quantile regression using the skew exponential power distribution,"Traditonal Bayesian quantile regression relies on the Aymmetric Laplace Distribution (ALD) due primarily due to its satisfactory empirical and theoretical perforances. However, the ALD dispalys medium tails and it is not suitable for data characterized by strong deviations from the Gaussian hypotesis. In this paper we propose and extension of the ALD Bayesian quantile regression framework to account for fat tails using the Skew Exponential PowerÂ distribution (SEP). Linear and Additive model (AM) with penalized splines are used to show the felxibility of the SEP in the Bayesian quantile regression context. Lasso priors are used to acount for the problem of shrinking parameters when the parameters space becames wide. We propose a new adaptive Metropolis-Hastings algorithm in the linear model, and an adaptive Metropolis withing Gibbs one in the AM framework. Empirical evidence of the statistical properties of the model is provided through several examples based on both simulated and real data sets.",2018,Comput. Stat. Data Anal.
Estimation and HAC-based Inference for Machine Learning Time Series Regressions,"Time series regression analysis in econometrics typically involves a framework relying on a set of mixing conditions to establish consistency and asymptotic normality of parameter estimates and HAC-type estimators of the residual long-run variances to conduct proper inference. This article introduces structured machine learning regressions for high-dimensional time series data using the aforementioned commonly used setting. To recognize the time series data structures we rely on the sparse-group LASSO estimator. We derive a new Fuk-Nagaev inequality for a class of $\tau$-dependent processes with heavier than Gaussian tails, nesting $\alpha$-mixing processes as a special case, and establish estimation, prediction, and inferential properties, including convergence rates of the HAC estimator for the long-run variance based on LASSO residuals. An empirical application to nowcasting US GDP growth indicates that the estimator performs favorably compared to other alternatives and that the text data can be a useful addition to more traditional numerical data.",2019,arXiv: Econometrics
Image segmentation by adaptive nonconvex local and global subspace representation,Abstract. We formulate image segmentation as subspace clustering of image feature vectors. We propose a subspace representation model using a nonconvex extension of trace Lasso and a nonconvex approximation of rank function to regularize the subspace representation. The proposed model can adaptively capture the local and the global structures of the subspace representation so that the subspace representation can reveal the real subspace structure of the data and obtains excellent clustering performance. Experimental results show that the proposed model is better than the previous models in clustering and natural image segmentation.,2016,Journal of Electronic Imaging
"â€˜Lassoingâ€™ a phylogenetic tree I: basic properties, shellings, and covers","A classical result, fundamental to evolutionary biology, states that an edge-weighted tree T with leaf set X, positive edge weights, and no vertices of degree 2 can be uniquely reconstructed from the leaf-to-leaf distances between any two elements of X. In biology, X corresponds to a set of taxa (e.g. extant species), the tree T describes their phylogenetic relationships, the edges correspond to earlier species evolving for a time until splitting in two or more species by some speciation/bifurcation event, and their length corresponds to the genetic change accumulating over that time in such a species. In this paper, we investigate which subsets of $${\binom{X}{2}}$$ suffice to determine (â€˜lassoâ€™) the tree T from the leaf-to-leaf distances induced by that tree. The question is particularly topical since reliable estimates of genetic distanceâ€”even (if not in particular) by modern mass-sequencing methodsâ€”are, in general, available only for certain combinations of taxa.",2012,Journal of Mathematical Biology
What influences motivation in Physical Education? A multilevel approach for identifying climate determinants of achievement motivation,"AbstractThe present research tested the longitudinal and hierarchical influences of students' climate perception on the development of achievement motives in Physical Education (PE). Students from Switzerland (N = 919; 45 classes; 50.1% female, age: M = 13.2, SD = 0.6) responded to the questionnaire. Perceived climate was measured using the German LASSO scales (Von Saldern & Littig, 1987), namely teacher care, classmate cooperativeness and satisfaction with teaching. To assess sport specific achievement motives (Hope of Success, HS; Fear of Failure, FF), we used a validated German scale from Elbe, Wenhold, and Muller (2005). Multilevel analysis revealed a link between perceived climate on change of students' motivation in PE. The investigation also identified factors determining motivation decline caused by the classroom environment and teachers. Moreover, results showed significant gender effects on both motives and a significant impact of individual teacher care on the HS. This was also found for individual and aggregated satisfaction with teaching. The latter was significant for FF on both levels. Interestingly, teacher care showed inhibitory effects on both achievement motives. These findings suggest that students in PE may have unique behaviour which requires a different teaching approach than in normal classroom. This describes a specific learning environment in PE classes. Results are discussed based on students' unique needs and gender effects.Keywords: achievement motivation, longitudinal study, multilevel analysis, physical education, climate perceptionsIntroductionA learning environment in a typical classroom is characterized by active interactions between learner and instructor or between learner and other learners. In contrast to distance learning environments and normal classrooms, Physical Education (PE) offers a whole range of opportunities for intensive social interactions that first need to be organized (Hascher, 2004; Telama & Polvi, 2007). Therefore, learning in PE classes is always about controlling these social interactions and entailed emotions. Awareness of interaction patterns in the classroom can help PE teachers to manage the classroom and reach the particular important curricular goal of enhancing motivation (Roberts, Treasure, & Conroy, 2007; Vallerand, 2007).School climate as social learning environment is an important aspect of student experience and a particularly powerful predictor of motivational factors (Hamre & Pianta, 2010; Weigand & Burton, 2002; Wang, Haertel, & Walber, 1993). A comprehensive understanding of students' behavior on learning patterns has to include the structur and processes of classroom interactions. Furthermore, motivation is a strong determinant of achievement in the classroom (Anderman & Anderman, 2013). This makes PE a fascinating research field for achievement motivation (Heckhausen, 1971). Therefore, it is important to determine which environmental factors influence motives in school classes in general and in PE. In order to obtain findings about mechanisms the motivation of students in general as well as in PE classes needs to be promoted. Yet, climate in PE class has special characteristics: PE is characterized by content that is closely related to the real world and to the leisure time of children and youth. Moreover, PE classes are marked by the occurrence of intense emotional moments. These moments make PE an authentic and fascinating research field. Unfortunately, there are limited empirical findings on this topic in PE, whereas there is some good evidence on it in other subjects (Hamre & Pianta, 2010). Thus, this study describes the specific characteristics of PE and examines the effect of climate perception (Von Saldern & Littig, 1987) on the change of students' achievement motivation (Atkinson, 1957) for the first time.Achievement motivationThere is a long history of motivational research grounded in achievement motive theory in Anglo-American studies (McClelland, Atkinson, Clark, & Lowell, 1953; Spence, 1989) as well as in the German speaking countries (Brunstein & Heckhausen, 2006). â€¦",2015,Psychological test and assessment modeling
A Pliable Lasso,"We propose a generalization of the lasso that allows the model coefficients to vary as a function of a general set of modifying variables. These modifiers might be variables such as gender, age or time. The paradigm is quite general, with each lasso coefficient modified by a sparse linear function of the modifying variables $Z$. The model is estimated in a hierarchical fashion to control the degrees of freedom and avoid overfitting. The modifying variables may be observed, observed only in the training set, or unobserved overall. There are connections of our proposal to varying coefficient models and high-dimensional interaction models. We present a computationally efficient algorithm for its optimization, with exact screening rules to facilitate application to large numbers of predictors. The method is illustrated on a number of different simulated and real examples.",2017,arXiv: Methodology
Covariate Selection for the Semiparametric Additive Risk Model,"This paper considers covariate selection for the additive hazards model. This model is particularly simple to study theoretically and its practical implementation has several major advantages to the similar methodology for the proportional hazards model. One complication compared with the proportional model is, however, that there is no simple likelihood to work with. We here study a least squares criterion with desirable properties and show how this criterion can be interpreted as a prediction error. Given this criterion, we define ridge and Lasso estimators as well as an adaptive Lasso and study their large sample properties for the situation where the number of covariates ""p"" is smaller than the number of observations. We also show that the adaptive Lasso has the oracle property. In many practical situations, it is more relevant to tackle the situation with large ""p"" compared with the number of observations. We do this by studying the properties of the so-called Dantzig selector in the setting of the additive risk model. Specifically, we establish a bound on how close the solution is to a true sparse signal in the case where the number of covariates is large. In a simulation study, we also compare the Dantzig and adaptive Lasso for a moderate to small number of covariates. The methods are applied to a breast cancer data set with gene expression recordings and to the primary biliary cirrhosis clinical data. Copyright (c) 2009 Board of the Foundation of the Scandinavian Journal of Statistics.",2009,Scandinavian Journal of Statistics
Sharp Threshold Detection Based on Sup-norm Error rates in High-dimensional Models,"We propose a new estimator, the thresholded scaled Lasso, in high-dimensional threshold regressions. First, we establish an upper bound on the lâˆž estimation error of the scaled Lasso estimator of Lee, Seo, and Shin. This is a nontrivial task as the literature on high-dimensional models has focused almost exclusively on l1 and l2 estimation errors. We show that this sup-norm bound can be used to distinguish between zero and nonzero coefficients at a much finer scale than would have been possible using classical oracle inequalities. Thus, our sup-norm bound is tailored to consistent variable selection via thresholding. Our simulations show that thresholding the scaled Lasso yields substantial improvements in terms of variable selection. Finally, we use our estimator to shed further empirical light on the long-running debate on the relationship between the level of debt (public and private) and GDP growth. Supplementary materials for this article are available online.",2015,Journal of Business & Economic Statistics
Epidemiology of traumatic injuries from earthquakes.,"Morethan500,000earthquakesaredocumentedeachyear.Although the vast majority are too small or too remotelylocated to be felt by humans, approximately 3,000 areperceptible by human populations, of which seven to 11result in signiï¬cant loss of life (1, 2). Over the last 30 years,a yearly average of 21 earthquakes were reported, disastersdeï¬nedaseventsresultinginmorethan10deaths,morethan100peopleaffected,arequestforinternationalassistance,ora declaration of a state of emergency (3). This average hasincreased to more than 30 in the last 5 years. In addition toloss of life, earthquakes cause considerably more nonfataltraumatic injuries and long-term damage to transportation,communication, and ï¬nancial infrastructures; yet, onlyrecentlyhastherebeenarecognizedneedtoroutinelycollectdata on these less severe effects.Inthepast25years,over530,000deathshavebeenreportedfrom earthquakes, with death tolls from major earthquakesranging from fewer than ï¬ve to more than 240,000 (1, 3).Table 1 shows characteristics of 32 selected earthquakes thatoccurred from 1985 to 2003. These earthquakes show sub-stantial variabilityinthenumberofdeathsandinmagnitude.Epidemiologicmethodsfordescribingcausalassociationsarea promising approach to account for this variability and toidentify potential avenues for preparedness and mitigation.Earthquakes are not randomly distributed but are con-centrated in regions where tectonic plates that compose theearthâ€™s surface coincide (4â€“7). Populations located aboveplate activity are at greatest risk of earthquake-related mor-bidity and mortality, such as communities along the Paciï¬cRim (e.g., the western edge of North and South America),along island chains (e.g., Japan and the Aleutians), andboundaries between certain continents (e.g., along theHimalayas to central Asia to the Caucasus Mountains andto the Mediterranean Sea) (4, 7).Populationscontinuetogrowinmanyoftheseseismicallyactive regions, particularly urban communities along thewestern United States and in Japan, China, South America,and India (6, 8, 9). By 2030, it is anticipated that 5 billionpeople, about 60 percent of the worldâ€™s population, willoccupyurbanareas.Thefastestratesofgrowthareprojectedin the less developed regions of the world (9). Earthquakesthat strike urban centers have the potential to cause sub-stantialdamage and death giventhegreater concentration ofpeople, modern construction materials and building tech-niques, and complex transportation, communication, com-mercial, and residential infrastructures (10). However, ruralpopulationsatriskfacedifferentchallenges,suchasisolationfromrespondersandthepotentialforsubstandardhousesandbuildings. Hence, we face multiple challenges as publichealth professionals to prevent and reduce earthquake-relatedmorbidityandmortalityin thisincreasingly complexenvironment.Todesignprogramstoprepareforearthquakesand mitigatetheir effects, we must achievea comprehensiveunderstanding of the risks for earthquake-related injuries.In this paper, we highlight ï¬ndings from and methodsutilized in various population-based epidemiologic studiesidentiï¬edthroughanextensiveliteraturesearchofpublished",2005,Epidemiologic reviews
Long head biceps tenodesis with a knotless cinch suture anchor: a biomechanical analysis.,"PURPOSE
The purpose of this study was to evaluate the initial fixation strength of 3 techniques of arthroscopic tenodesis of the long head of the biceps (LHB).


METHODS
Eighteen human cadaveric shoulders were randomly assigned to one of 3 simulated arthroscopic biceps tenodesis techniques-simple suture (SS), Krakow stitch (KS), or lasso loop (LL)-combined with a knotless fixation implant (3.5-mm Piton Anchor; Tornier, Minneapolis, MN). Biomechanical parameters were evaluated by cyclic loading and load to failure.


RESULTS
The mean failure load (PÂ = .007) was 158.3 Â± 32.2 N, 109.8 Â± 41.1 N, and 46.6 Â± 3.8 N for the KS, SS, and LL techniques, respectively. Mean stiffness was greater (statistically significant) in the KS (21.4 Â± 3.0 N/mm) and SS (20.7 Â± 7.9 N/mm) treatment groups compared with the LL group (4.5 Â± 1.5 N/mm) (PÂ = .011).


CONCLUSIONS
Biceps tenodesis performed with a more secure tendon suturing technique, such as the Krakow technique, provides superior ultimate and fatigue strength and thus may be more secure in clinical application and yieldÂ better clinical results. The mechanical properties of the LL technique were especially poor in comparison.


CLINICAL RELEVANCE
Although more complex suturing techniques for arthroscopic biceps tenodesis can be technically challenging, more secure tendon fixation may improve clinical outcomes.",2015,Arthroscopy : the journal of arthroscopic & related surgery : official publication of the Arthroscopy Association of North America and the International Arthroscopy Association
Outcomes and predictors of response in steroid-refractory acute graft-versus-host disease: single-center results from a cohort of 203 patients.,"The prognosis of steroid-refractory acute graft-versus-host disease (aGVHD) is poor and predictors of response and survival are unclear. In an exploratory analysis of 203 steroid-refractory aGVHD patients with prospectively collected GVHD data who received antithymocyte globulin, etanercept, or mycophenolate mofetil (MMF) as second-line treatment, we determined the predictors of day 28 response, 2-year overall survival (OS), and 2-year non-relapse mortality (NRM). To minimize the risk of finding false positive results, we used lasso regression, aggressively eliminating variables that are unlikely to be associated with outcome. Day 28 response to second-line therapy was 38% (complete response [CR] 23%), with a 2-year OS of 25% and a 2-year NRM of 62%. Factors associated with response were GVHD prophylaxis, organ involvement, and initial aGVHD to steroid-refractory aGVHD interval. Specifically, compared with cyclosporine (CsA)/MMF as GVHD prophylaxis, the odds ratio (OR) for calcineurin inhibitor/methotrexate was 0.8 and for CsA/prednisone was 0.6. The OR for aGVHD to steroid-refractory aGVHD interval â‰¥14 vs. <14 days was 1.3. The ORs for skin only involvement and gut or liver only involvement when compared with multi-organ involvement were 1.4 and 1.2, respectively. The only variable associated with worse survival was age, with a hazard ratio (HR) per decade of 1.04 for overall mortality. Similarly, age was the only variable associated with NRM (HR 1.02 per decade). When compared with CR, no response at day 28 increased the risk of death (HR: 2.4, 95% confidence interval: 1.5-3.7). In conclusion, using an underutilized statistical technique in the field of transplantation, we identified predictors of response and survival in steroid-refractory aGVHD. Our results highlight the importance of developing novel treatment strategies as current treatments yield poor outcomes.",2019,Biology of blood and marrow transplantation : journal of the American Society for Blood and Marrow Transplantation
Group-penalized feature selection and robust twin SVM classification via second-order cone programming,"Selecting the relevant factors in a particular domain is of utmost interest in the machine learning community. This paper concerns the feature selection process for twin support vector machine (TWSVM), a powerful classification method that constructs two nonparallel hyperplanes in order to define a classification rule. Besides the Euclidean norm, our proposal includes a second regularizer that aims at eliminating variables in both twin hyperplanes in a synchronized fashion. The baseline classifier is a twin SVM implementation based on second-order cone programming, which confers robustness to the approach and leads to potentially better predictive performance compared to the standard TWSVM formulation. The proposal is studied empirically and compared with well-known feature selection methods using microarray datasets, on which it succeeds at finding low-dimensional solutions with highest average performance among all the other methods studied in this work. HighlightsNovel feature selection approach for twin SVM based on second-order cone programming.Extension of the Group Lasso penalty for coordinated variable selection.A single optimization problem is used to solve both twin subproblems.Superior performance is achieved in experiments on high-dimensional datasets.",2017,Neurocomputing
Vitamin D-binding protein ( rs 4588 ) T / T genotype is associated with anteroseptal myocardial infarction in coronary artery disease patients,"Department of Medical Rehabilitation, Medical Faculty, University of Rijeka, Rijeka, Croatia; Division of Cardiology, Hospital for Medical Rehabilitation of the Heart and Lung Diseases and Rheumatism â€œThalassotherapia-Opatijaâ€, Opatija, Croatia; Department of Biotechnology, Centre for High-throughput Technologies, University of Rijeka, Rijeka, Croatia Contributions: (I) Conception and design: S KraljeviÄ‡ PaveliÄ‡, V PerÅ¡iÄ‡; (II) Administrative support: V PerÅ¡iÄ‡, S KraljeviÄ‡ PaveliÄ‡, D RaljeviÄ‡; (III) Provision of study materials or patients: V PerÅ¡iÄ‡, D RaljeviÄ‡, R MiÅ¡kulin; (IV) Collection and assembly of data: All authors; (V) Data analysis and interpretation: All authors; (VI) Manuscript writing: All authors; (VII) Final approval of manuscript: All authors. These authors contributed equally to this work. Correspondence to: Prof. Sandra KraljeviÄ‡ PaveliÄ‡. Department of Biotechnology, Centre for High-throughput Technologies, University of Rijeka, Radmile MatejÄiÄ‡ 2, 51000 Rijeka, Croatia. Email: sandrakp@biotech.uniri.hr.",2019,
"Data-driven, voxel-based analysis of brain PET images: Application of PCA and LASSO methods to visualize and quantify patterns of neurodegeneration","Spatial patterns of radiotracer binding in positron emission tomography (PET) images may convey information related to the disease topology. However, this information is not captured by the standard PET image analysis that quantifies the mean radiotracer uptake within a region of interest (ROI). On the other hand, spatial analyses that use more advanced radiomic features may be difficult to interpret. Here we propose an alternative data-driven, voxel-based approach to spatial pattern analysis in brain PET, which can be easily interpreted. We apply principal component analysis (PCA) to identify voxel covariance patterns, and optimally combine several patterns using the least absolute shrinkage and selection operator (LASSO). The resulting models predict clinical disease metrics from raw voxel values, allowing for inclusion of clinical covariates. The analysis is performed on high-resolution PET images from healthy controls and subjects affected by Parkinson's disease (PD), acquired with a pre-synaptic and a post-synaptic dopaminergic PET tracer. We demonstrate that PCA identifies robust and tracer-specific binding patterns in sub-cortical brain structures; the patterns evolve as a function of disease progression. Principal component LASSO (PC-LASSO) models of clinical disease metrics achieve higher predictive accuracy compared to the mean tracer binding ratio (BR) alone: the cross-validated test mean squared error of adjusted disease duration (motor impairment score) was 16.3 Â± 0.17 years2 (9.7 Â± 0.15) with mean BR, versus 14.4 Â± 0.18 years2 (8.9 Â± 0.16) with PC-LASSO. We interpret the best-performing PC-LASSO models in the spatial sense and discuss them with reference to the PD pathology and somatotopic organization of the striatum. PC-LASSO is thus shown to be a useful method to analyze clinically-relevant tracer binding patterns, and to construct interpretable, imaging-based predictive models of clinical metrics.",2018,PLoS ONE
"After the Crisis: Reform, Recovery, and Growth in Europe","After the Crisis reassesses the twin projects of structural reform and European integration in the wake of the Great Recession and the European Sovereign Debt Crisis. The introduction compares the pre-crises debate to the current situation, and highlights a number of ways in which both reform and further integration may have become more difficult. Chapter 1 surveys the state of the structural-reform agenda, its successes, failures, and priorities for further action. The second chapter focuses on the fiscal-policy response to the crisis and advocates a greater balance between supply-side reforms and demand-side management. The third chapter focuses on the asymmetric shocks across economies in the monetary union, and discusses institutional mechanisms to reduce their frequency and impact. Chapter 4 examines the cyclical behavior of output and financial indicators, as well as the counter-cyclical role of macro-financial policies, both at the national and the European level. The fifth chapter studies changes in Europeans' attitudes, showing how the recent crises have eroded public confidence in European institutions. The sixth chapter tackles the demographic challenges facing Europe, and particularly the way that demographic change may impact the reform agenda. Chapter 7 highlights the under-appreciated extent to which 'Europe', taken as a whole, is characterized by a substantial amount of inequality and geographical income clustering, and the challenge this poses for further integration. Contributors to this volume - Francesco Caselli, London School of Economics Mario Centeno, Banco de Portugal and Lisbon University Antonio Fatas, INSEAD Carlo A. Favero, Universita Bocconi, IGIER, and CEPR Jeffry A. Frieden, Harvard University Vincenzo Galasso, Universita Bocconi, IGIER, and CEPR Paul De Grauwe, London School of Economics Yuemei Ji, University College London Philip R. Lane, Trinity College Dublin and CEPR Alvaro Novo, Banco de Portugal Andre Sapir, Universite libre de Bruxelles Carlos da Silva Costa, Governor of Banco de Portugal Jose Tavares, Nova School of Business and Economics",2016,
Abstract 3580: Hotspot mutation and fusion transcript detection from the same non-small lung adenocarcinoma sample,"Proceedings: AACR Annual Meeting 2014; April 5-9, 2014; San Diego, CA

The presence of certain chromosomal rearrangements and the subsequent fusion gene derived from translocations has been implicated in a number of cancers. Hundreds of translocations have been described in the literature recently but the need to efficiently detect and further characterize these chromosomal translocations is growing exponentially. The two main methods to identify and monitor translocations, fluorescent in situ hybridization (FISH) and comparative genomic hybridization (CGH) are challenging, labor intensive, the information obtained is limited, and sensitivity is rather low. Common sample types for these analyses are biopsies or small tumors, which are very limited in material making the downstream measurement of more than one analyte rather difficult; obtaining another biopsy, using a different section or splitting the sample can raise issues of tumor heterogeneity. The ability to study mutation status as well as measuring fusion transcript expression from the same sample is powerful because you're maximizing the information obtained from a single precious sample and eliminating any sample to sample variation. Here we describe the efficient isolation of two valuable analytes, RNA and DNA, from the same starting sample without splitting, followed by versatile and informative downstream analysis. This methodology was applied to FFPE and degraded samples as well as fresh tissues and cells. DNA and RNA were recovered from the same non-small lung adenocarcinoma sample and both mutation analysis, as well as fusion transcript detection was performed using the Ion Torrent PGMâ„¢ platform on the same Ion 318â„¢ chip. Using 10ng of DNA and 10ng of RNA input, we applied the Ion AmpliSeqâ„¢ Colon and Lung Cancer panel to analyze over 500 COSMIC mutations in 22 genes and the Ion AmpliSeqâ„¢ RNA Lung Fusion panel to detect 40 different fusion transcripts.

Citation Format: Angie Cheng, Varun Bagai, Joey Cienfuegos, Natalie Hernandez, Mu Li, Jeff Schageman, Richard Fekete, Rosella Petraroli, Alexander Vlassov, Susan Magdaleno. Hotspot mutation and fusion transcript detection from the same non-small lung adenocarcinoma sample. [abstract]. In: Proceedings of the 105th Annual Meeting of the American Association for Cancer Research; 2014 Apr 5-9; San Diego, CA. Philadelphia (PA): AACR; Cancer Res 2014;74(19 Suppl):Abstract nr 3580. doi:10.1158/1538-7445.AM2014-3580",2014,Cancer Research
Performance of genomic prediction within and across generations in maritime pine,"BackgroundGenomic selection (GS) is a promising approach for decreasing breeding cycle length in forest trees. Assessment of progeny performance and of the prediction accuracy of GS models over generations is therefore a key issue.ResultsA reference population of maritime pine (Pinus pinaster) with an estimated effective inbreeding population size (status number) of 25 was first selected with simulated data. This reference population (nâ€‰=â€‰818) covered three generations (G0, G1 and G2) and was genotyped with 4436 single-nucleotide polymorphism (SNP) markers. We evaluated the effects on prediction accuracy of both the relatedness between the calibration and validation sets and validation on the basis of progeny performance. Pedigree-based (best linear unbiased prediction, ABLUP) and marker-based (genomic BLUP and Bayesian LASSO) models were used to predict breeding values for three different traits: circumference, height and stem straightness. On average, the ABLUP model outperformed genomic prediction models, with a maximum difference in prediction accuracies of 0.12, depending on the trait and the validation method. A mean difference in prediction accuracy of 0.17 was found between validation methods differing in terms of relatedness. Including the progenitors in the calibration set reduced this difference in prediction accuracy to 0.03. When only genotypes from the G0 and G1 generations were used in the calibration set and genotypes from G2 were used in the validation set (progeny validation), prediction accuracies ranged from 0.70 to 0.85.ConclusionsThis study suggests that the training of prediction models on parental populations can predict the genetic merit of the progeny with high accuracy: an encouraging result for the implementation of GS in the maritime pine breeding program.",2016,BMC Genomics
FMRI group studies of brain connectivity via a group robust Lasso,"Inferring effective brain connectivity from neuroimaging data such as functional Magnetic Resonance Imaging (fMRI) has been attracting increasing interest due to its critical role in understanding brain functioning. Incorporating sparsity into connectivity modeling to make models more biologically realistic and performing group analysis to deal with inter-subject variability are still challenges associated with fMRI brain connectivity modeling. To address the above two crucial challenges, the attractive computational and theoretical properties of the least absolute shrinkage and selection operator (LASSO) in sparse linear regression provide a suitable starting point. We propose a group robust LASSO (grpRLASSO) model by combining advantages of the popular group-LASSO and our recently developed robust-LASSO. Here group analysis is formulated as a grouped variable selection procedure. Superior performance of the proposed grpRLASSO in terms of group selection and robustness is demonstrated by simulations with large noise variance. The grpRLASSO is also applied to a real fMRI data set for brain connectivity study in Parkinson's disease, resulting in biologically plausible networks.",2010,2010 IEEE International Conference on Image Processing
"Monitoring adult sorghum shoot fly, Atherigona soccata Rondani (Diptera: Muscidae), and related species in Burkina Faso.","Abstract Fish meal was used as attractant in four trap types for assessing the relative abundance and species composition of sorghum shoot flies, which are major pests in the wetter southern zones of Burkina Faso. Trapping was carried out in 1988 and 1989 during the rainy season in Boboâ€Dioulasso. Three trap models were effective in catching Atherigona soccata: (1) water trap, (2) Multiâ€Pher and (3) ICRISAT (International Crops Research Institute for Semiâ€Arid Tropics) traps. Multiâ€Pher and water traps were the most efficient. The advantages and disadvantages of each model are discussed. Identification of male flies demonstrated the presence of 34 species of the subgenus Atherigona and two species of the subgenus Acritochaeta with Atherigona soccata, A. occidentalis Deeming and A. tomentigera van Emden being predominant. Thirteen species were new records to Burkina Faso: A. aberrans Malloch, A. africana Deeming, A. fililoba Deeming, A. gabonensis Deeming, A. gilvifolia van Emden, A. griseiventris van Emde...",1991,International Journal of Pest Management
