title,abstract,year,journal
RÃ©flexions sur la SystÃ©matique et la GenÃ©se Des Bassins de SÃ©dimentation,"Publisher Summary This chapter illustrates the differences among recent basinsâ€”in the morphological senseâ€”and paleobasinsâ€”in the geological sense. A classification of the latter is presented, in which new elements are introduced: (1) the longitudinal development, (2) the genesis, (3) the kinematics, (4) the age, and (5) the concept of spatial and geographical unity. It is considered in what ways certain types of paleobasins can develop, and how they can change into other types. It is also considered whether the alpine basins represent a prototype and whether one should not reckon with the possibility of a different evolution in the course of geological times. As to the genesis, a distinction is made between thalassogenesis (origin of the ocean) and anguieogenesis (origin of the basin). The basin can be a part of the ocean, generally situated on the continental border where the maximum sedimentation takes place. Certain paleobasins can originate by deformation and subsidence of old peneplains that became continental shelves. In the chapter a tentative comparison is made between deposition in the recent depressions and in the paleobasins.",1964,Developments in sedimentology
"Mutagenicity Testing of Herbicides, Fungicides and Insecticides","The insecticides: summithion, lannate, carbicron, thiodan and kelthane, the fungicides: diathane M-45 and aladrin and the herbicides: ramrod, lasso, round up and grammoxone induce acute mitostatic effects of Vicia faba root meristematic cells. Pycnotic nuclei and premature chromosome condensation are also evident. Also carbicron, kelthane, lasso and ramrod induce anaphase bridges accompanied by relatively scarce micronuclei. The results suggest possible genetic damage by these chemicals.",1981,Cytologia
A methodology for qualitative archaeometallurgical fieldwork using a handheld X-ray fluorescence spectrometer,"AbstractRecent work aimed at provenancing metal slag from Sagalassos, south-west Turkey, as part of a study investigating the Roman iron industry in the area. Although previously samples of the slag material had been exported from the country for the purposes of analysis, a method of analysing the materials in-situ was required. It was decided that the best technique for achieving â€˜in-the-fieldâ€™ results would be handheld X-ray fluorescence spectrometry (HH-XRF). A series of laboratory based tests were first performed in order to determine the ideal working parameters for the HH-XRF and the best method for preparing the samples. The results indicated that different slag (i.e. Ti-rich/poor) could clearly be distinguished amongst the powdered samples.A total of 45 metal slag were analysed in the field in order to see whether the slag could be qualitatively characterised based on provenance. The results of the field study indicated two principle groups (a high Ti â€“ Zr group and a low Ti â€“ Zr group). The Ca an...",2015,
Machine Learning-Based Temperature Prediction for Runtime Thermal Management Across System Components,"Elevated temperatures limit the peak performance of systems because of frequent interventions by thermal throttling. Non-uniform thermal states across system nodes also cause performance variation within seemingly equivalent nodes leading to significant degradation of overall performance. In this paper we present a framework for creating a lightweight thermal prediction system suitable for run-time management decisions. We pursue two avenues to explore optimized lightweight thermal predictors. First, we use feature selection algorithms to improve the performance of previously designed machine learning methods. Second, we develop alternative methods using neural network and linear regression-based methods to perform a comprehensive comparative study of prediction methods. We show that our optimized models achieve improved performance with better prediction accuracy and lower overhead as compared with the Gaussian process model proposed previously. Specifically we present a reduced version of the Gaussian process model, a neural networkâ€“based model, and a linear regressionâ€“based model. Using the optimization methods, we are able to reduce the average prediction errors in the Gaussian process from <inline-formula><tex-math notation=""LaTeX""> $4.2^\circ$</tex-math><alternatives><inline-graphic xlink:href=""ogrencimemik-ieq1-2732951.gif""/></alternatives> </inline-formula>C to <inline-formula><tex-math notation=""LaTeX"">$2.9^\circ$</tex-math><alternatives> <inline-graphic xlink:href=""ogrencimemik-ieq2-2732951.gif""/></alternatives></inline-formula>C. We also show that the newly developed models using neural network and Lasso linear regression have average prediction errors of <inline-formula><tex-math notation=""LaTeX"">$2.9^\circ$</tex-math><alternatives> <inline-graphic xlink:href=""ogrencimemik-ieq3-2732951.gif""/></alternatives></inline-formula>C and <inline-formula> <tex-math notation=""LaTeX"">$3.8^\circ$</tex-math><alternatives> <inline-graphic xlink:href=""ogrencimemik-ieq4-2732951.gif""/></alternatives></inline-formula>C respectively. The prediction overheads are 0.22, 0.097, and 0.026 ms per prediction for reduced Gaussian process, neural network, and Lasso linear regression models, respectively, compared with 0.57 ms per prediction for the previous Gaussian process model. We have implemented our proposed thermal prediction models on a two-node system configuration to help identify the optimal task placement. The task placement identified by the models reduces the average system temperature by up to <inline-formula><tex-math notation=""LaTeX"">$11.9^\circ$</tex-math><alternatives> <inline-graphic xlink:href=""ogrencimemik-ieq5-2732951.gif""/></alternatives></inline-formula>C without any performance degradation. Furthermore, these models respectively achieve 75, 82.5, and 74.17 percent success rates in correctly pointing to those task placements with better thermal response, compared with 72.5 percent success for the original model in achieving the same objective. Finally, we extended our analysis to a 16-node system and we were able to train models and execute them in real time to guide task migration and achieve on average 17 percent reduction in the overall system cooling power.",2018,IEEE Transactions on Parallel and Distributed Systems
Structural studies of salvage enzymes in nucleotide biosynthesis,"There are two routes to produce deoxyribonucleoside triphosphates (dNTPs) precursors for DNA synthesis, the de novo and the salvage pathways. Deoxyribonucleoside kinases (dNKs) perform the initial phosphorylation of deoxyribonucleosides (dNs). Furthermore, they can act as activators for several medically important nucleoside analogs (NAs) for treatment against cancer or viral infections. Several disorders are characterized by mutations in enzymes involved in the nucleotide biosynthesis, such as Lesch-Nyhan disease that is linked to hypoxanthine guanine phosphoribosyltransferase (HPRT). In this thesis, the structures of human thymidine kinase 1 (TK1), a mycoplasmic deoxyadenosine kinase (Mm-dAK), and phosphoribosyltransferase domain containing 1 (PRTFDC1) are presented. Furthermore, a structural investigation of Drosophila melanogaster dNK (Dm-dNK) N64D mutant was carried out. The obtained structural information reveals the basis for substrate specificity for TKs and the bacterial dAKs. The TK1 revealed a structure different from other known dNK structures, containing an Î±/Î² domain similar to the RecA-F1ATPase family, and a lasso-like domain stabilized by a structural zinc. The Mm-dAK structure was similar to its human counterparts, but with some alterations in the proximity of the active site. Furthermore several residues important for substrate specificity were identified. The crystal structure of PRTFDC1 was structurally very similar to its homolog HPRT. PRTFDC1 was classified as having an unknown function and with structural and biochemical data we showed that PRTFDC1 has some phosphoribosyltransferase activity. A changed behavior of the Dm-dNK N64D mutant was previously observed. This mutant displayed an increased sensitivity towards NAs and a decreased feedback inhibition. Complexes with substrate and feedback inhibitor provided an explanation for the changed behavior of the mutant. The structural data presented here, provide a foundation for substrate specificity for dNKs. The information of differences between human and bacterial enzymes will be of importance for the design of new anti-bacterial agents. Mutational studies to improve desired properties of an enzyme are an important issue in suicide gene/chemotherapy. Although we have found a potential function of PRTFDC1 there are still a lot of questions concerning its biological role to answer.",2007,
"Calcineurin Associated with the lnositol 1,4,5=Trisphosphate Receptor-FKBPI 2 Complex Modulates Ca*+ Flux","Siraj M. Ali, * Gabriele V. Ronnett,? and Solomon H. Snyder**5 *Department of Neuroscience tDepartment of Neurology $Department of Pharmacology and Molecular Sciences SDepartment of Psychiatry and Behavioral Sciences Johns Hopkins University School of Medicine Baltimore, Maryland 21205 Summary The immunosuppressant drug FK506 binds to the im- munophilin protein FKBPl2 and inhibits its prolyl iso- meraseactivity. Immunosuppresiveactions, however, are mediated via an FK506-FKBP12 inhibition of the Ca*+-activated phosphatase calcineurin. Physiologic cellular roles for FKBPl2 have remained unclear. FKBPl2 is physically associated with the RyR and IP3R Ca*+ channels in the absence of FK506, with added FK506 disrupting these complexes. Dissociation of FKBPl2 results in alteration of channel Ca% conduc- tance in both cases. We now report that calcineurin is physiologically associated with the IP3R-FKBP12 and RyR-FKBP12 receptor complexes and that this inter- action can be disrupted by FK506 or rapamycin. Cal- cineurin anchored to the IP3R via FKBP12 regulates the phosphorylation status of the receptor, resulting in a dynamic Ca*+ -sensitive regulation of IP3-mediated Ca2+ flux. Introduction The immunophilin proteins mediate the clinical effects of the immunosuppressant drugs cyclosporin A, FK506, and rapamycin. Cyclophilins, thefirstclassof immunophilinsto be identified, bind to cyclosporin A, acyclic undecapeptide (Handschumacher et al., 1984; for review see Walsh et al., 1992). FK506 and rapamycin, structurally related mac- rolides, instead bind to the FK506-binding protein (FKBP) class of immunophilins. Although there are a number of membersoftheimmunophilinfamily, immunosuppressant actions appear largely attributable to cyclophilin A and B and FKBP12 (Bram et al., 1993). The cyclophilins and FKBPs do not share amino acid sequence similarity, but both display peptidylprolyl-cis-trans isomerase (rota- mase) activity thought to be associated with protein folding (Schmid, 1993). Furthermore, immunophilin rotamase ac- tivity is inhibited by their respective immunosuppressant ligands. lmmunosuppressant actions, however, do not ap- pear to derive from inhibition of this isomerase activity. Rather, the drug-immunophilin complex of cyclosporin A-cyclophilin or FK506-FKBP12 binds to the protein phosphatase calcineurin to inhibit its activity, thereby aug- menting levels of phosphorylated calcineurin substrates such as the transcription factor NFAT (for nuclear of activated T cells) (Liu et al., 1991; Oâ€™Keefe et al., 1992; Clipstone and Crabtree, 1992; for review of calcineurin, see Klee et al., 1988). In its phosphorylated state, NFAT cannot translocate to the nucleus, where it regulates ex- pression of genes critical for T cell activation such as in- terleukin-2 (IL-2). Rapamycin acts differently than cyclo- sporin A and FK506, as it blocks the actions of IL-2 rather than its synthesis (Bierer et al., 1990). Rapamycin binds with high affinity to FKBP12 and inhibits its rotamase activ- ity, but the rapamycin-FKBP12 complex does not interact with calcineurin. Rather, this immunosuppressant-immu- nophilin complex binds to a recently identified target pro- tein designated RAFT (for rapamycin and FKBP12 target) or FRAP (for FKBP-rapamycin-associated protein) (Sa- batini et al., 1994; Brown et al., 1994). Marks and colleagues have observed an association of FKBP12 with the ryanodine receptor (RyR) skeletal muscle in the absence of FK506 (Jayaraman et al., 1992; Timerman et al., 1993; Brillantes et al., 1994). RyR is a tetrameric Ca*+ channel that mediates Caâ€™+-induced Ca2+ release in muscle, brain, and other tissues (Takeshima et al., 1989; McPherson and Campbell, 1993). We have demonstrated that the inositol 1,4,5-trisphosphate (IP,) re- ceptor (IPBR), a structurally and functionally related tetra- merit Ca2+ channel that shares up to 40% homology RyR in some regions, is also a direct target of FKBP12 (Cameron et al., 1995; for review see Furuichi and Miko- shiba, 1995; Snyder and Sabatini, 1995). Whereas FK506 stimulates the binding of FKBP12 to calcineurin, FK506 dissociates FKBP12 from RyR and IP$R. When FKBP12 is â€œstrippedâ€ from the RyR-FKBPlP or IP3R- FKBP12 complex, the Ca2+ channels of these two proteins become â€œleaky.â€ Accordingly, net accumulation of Caâ€™+ into RyR-or IPsR-gated stores is diminished, and releasing agents such as caffeine or IPa cause increased Ca*+ flux at lower concentrations. It was assumed that the isomerase activity of FKBP12 was the means by which this protein modulates the RyR and IP3R. However, recent evidence obtained from mutant FKBP12 proteins lacking isomerase activity suggests that this is not the case (Timerman et al., 1995). Accordingly, we have attempted investigate alternative means by which FKBP12 modulates IP,R- and RyR-mediated Ca*+ flux. IP3R can be phosphorylated at three distinct sites by protein kinase C (PKC), Caz+/calmodulin~dependent pro- tein kinase II (CaMKII), and cyclic AMP-dependent protein kinase (PKA), respectively (Ferris et al., 1991). Whether phosphorylation and dephosphorylation physiologically regulate IP,R function has not been established. The asso- ciation of FKBP12 with IP3R suggested a potential func- tional link of calcineurin and IP3R. We now demonstrate a physical association of calcineurin with the IP,R-FKBP12 and RyR-FKBP12 complexes that is potently disrupted by treatment with FK506 or rapamycin. We also show that complexed calcineurin modulates the phosphorylation status and Caâ€™+ flux properties of IP3R.",1995,
A Differentiable Alternative to the Lasso Penalty,"Regularized regression has become very popular nowadays, particularly on high-dimensional problems where the addition of a penalty term to the log-likelihood allows inference where traditional methods fail. A number of penalties have been proposed in the literature, such as lasso, SCAD, ridge and elastic net to name a few. Despite their advantages and remarkable performance in rather extreme settings, where $p \gg n$, all these penalties, with the exception of ridge, are non-differentiable at zero. This can be a limitation in certain cases, such as computational efficiency of parameter estimation in non-linear models or derivation of estimators of the degrees of freedom for model selection criteria. With this paper, we provide the scientific community with a differentiable penalty, which can be used in any situation, but particularly where differentiability plays a key role. We show some desirable features of this function and prove theoretical properties of the resulting estimators within a regularized regression context. A simulation study and the analysis of a real dataset show overall a good performance under different scenarios. The method is implemented in the R package DLASSO freely available from CRAN.",2016,arXiv: Methodology
Penalized Cox models and Frailty,A very general mechanism for penalized regression has been added to the coxph function in S Plus A user written S Plus function can be supplied that gives addi tional term s to the partial likelihood along with the rst and second derivatives of those terms The variance and degrees of freedom for the extended model are then computed as outlined in Gray Several other arguments control optional aspects of the iteration This setup allows for general shrinkage methods including ridge regression the lasso smoothing splines and other techniques There is an interesting connection between penalized regression and random e ects or frailty models It happens that the gamma frailty model can be repre sented exactly as a penalized regression and a Gaussian frailty can be represented approximately Thus we can t these models as well using the generalized program,1998,
The Incidence of Venous Thromboembolism (VTE) in 892 Allogeneic Hematopoietic Cell Transplant (allo-HCT) Recipients ( A single institution study comparison of VTE incidence with sirolimus versus non-sirolimus-based GVHD prophylaxis),"s / Biol Blood Marrow Transplant 21 (2015) S266eS321 S297 multivariate analysis using the LASSO approach to logistic regression analysis. Results: Of 134 allo-HSCTs, 29 (21.6%) patients experienced CMV viremia. Among these patients, median age was 51 years (range 27-67), with 48 episodes of viremia. Nine (31%) viremic patients developed CMV disease. CMV disease occurred at a median of 124 days post HSCT (range 61-322). Patients with CMV disease had a median of 2 viremic episodes before disease (range 1-4). Disease occurred at a median of 33 days from the start of the last viremic episode, and 75 days from the start of the first episode of viremia. On univariate analysis, factors associated with progression to CMV disease were: steroid-refractory acute GVHD (60 vs. 20%, p1â„40.028); number of episodes of viremia >1x103 copies/mL (mean 2.4 vs 1.1, p1â„40.016); longer duration of viremia (mean 38 vs. 22 days, p1â„40.01); higher peak viral load (mean 4.49x105 vs. 8.31x103 copies/mL, p 1x103 copies/mL, a longer duration of viremia, and to have failed first-line pre-emptive ganciclovir therapy. This study, while limited, suggests that these risk factors may be predictive of CMV disease. Larger, prospective studies are needed to confirm these risk factors, some of which may be amenable to more aggressive anti-viral therapy.",2015,Biology of Blood and Marrow Transplantation
Learning to Share: simultaneous parameter tying and Sparsification in Deep Learning,"Deep neural networks (DNNs) usually contain millions, maybe billions, of parameters/weights, making both storage and computation very expensive. This has motivated a large body of work to reduce the complexity of the neural network by using sparsity-inducing regularizers. Another well-known approach for controlling the complexity of DNNs is parameter sharing/tying, where certain sets of weights are forced to share a common value. Some forms of weight sharing are hard-wired to express certain in- variances, with a notable example being the shift-invariance of convolutional layers. However, there may be other groups of weights that may be tied together during the learning process, thus further re- ducing the complexity of the network. In this paper, we adopt a recently proposed sparsity-inducing regularizer, named GrOWL (group ordered weighted l1), which encourages sparsity and, simulta- neously, learns which groups of parameters should share a common value. GrOWL has been proven effective in linear regression, being able to identify and cope with strongly correlated covariates. Unlike standard sparsity-inducing regularizers (e.g., l1 a.k.a. Lasso), GrOWL not only eliminates unimportant neurons by setting all the corresponding weights to zero, but also explicitly identifies strongly correlated neurons by tying the corresponding weights to a common value. This ability of GrOWL motivates the following two-stage procedure: (i) use GrOWL regularization in the training process to simultaneously identify significant neurons and groups of parameter that should be tied together; (ii) retrain the network, enforcing the structure that was unveiled in the previous phase, i.e., keeping only the significant neurons and enforcing the learned tying structure. We evaluate the proposed approach on several benchmark datasets, showing that it can dramatically compress the network with slight or even no loss on generalization performance.",2018,
Artificial intelligence-guided tissue analysis combined with immune infiltrate assessment predicts stage III colon cancer outcomes in PETACC08 study,"OBJECTIVE
Diagnostic tests, such as Immunoscore, predict prognosis in patients with colon cancer. However, additional prognostic markers could be detected on pathological slides using artificial intelligence tools.


DESIGN
We have developed a software to detect colon tumour, healthy mucosa, stroma and immune cells on CD3 and CD8 stained slides. The lymphocyte density and surface area were quantified automatically in the tumour core (TC) and invasive margin (IM). Using a LASSO algorithm, DGMate (DiGital tuMor pArameTErs), we detected digital parameters within the tumour cells related to patient outcomes.


RESULTS
Within the dataset of 1018 patients, we observed that a poorer relapse-free survival (RFS) was associated with high IM stromal area (HR 5.65; 95%â€‰CI 2.34 to 13.67; p<0.0001) and high DGMate (HR 2.72; 95% CI 1.92 to 3.85; p<0.001). Higher CD3+ TC, CD3+ IM and CD8+ TC densities were significantly associated with a longer RFS. Analysis of variance showed that CD3+ TC yielded a similar prognostic value to the classical CD3/CD8 Immunoscore (p=0.44). A combination of the IM stromal area, DGMate and CD3, designated 'DGMuneS', outperformed Immunoscore when used in estimating patients' prognosis (C-index=0.601 vs 0.578, p=0.04) and was independently associated with patient outcomes following Cox multivariate analysis. A predictive nomogram based on DGMuneS and clinical variables identified a group of patients with less than 10% relapse risk and another group with a 50% relapse risk.


CONCLUSION
These findings suggest that artificial intelligence can potentially improve patient care by assisting pathologists in better defining stage III colon cancer patients' prognosis.",2019,Gut
Temporal causal modeling with graphical granger methods,"The need for mining causality, beyond mere statistical correlations, for real world problems has been recognized widely. Many of these applications naturally involve temporal data, which raises the challenge of how best to leverage the temporal information for causal modeling. Recently graphical modeling with the concept of ""Granger causality"", based on the intuition that a cause helps predict its effects in the future, has gained attention in many domains involving time series data analysis. With the surge of interest in model selection methodologies for regression, such as the Lasso, as practical alternatives to solving structural learning of graphical models, the question arises whether and how to combine these two notions into a practically viable approach for temporal causal modeling. In this paper, we examine a host of related algorithms that, loosely speaking, fall under the category of graphical Granger methods, and characterize their relative performance from multiple viewpoints. Our experiments show, for instance, that the Lasso algorithm exhibits consistent gain over the canonical pairwise graphical Granger method. We also characterize conditions under which these variants of graphical Granger methods perform well in comparison to other benchmark methods. Finally, we apply these methods to a real world data set involving key performance indicators of corporations, and present some concrete results.",2007,
[The influence of different types of bathes for general health on the efficiency of combined spa and resort treatment of coronary heart disease].,"The spa and resort treatment of coronary heart disease was given to 106 patients (34 men and 72 women) aged from 45 to 69 years presenting with angina of effort It consisted of basic therapy in combination with a rational diet, remedial physical exercises, thalassotherapy, underwater shower massage, low-mineralized Chvizhense water. The patients were allocated to three groups depending on the type of prescribed baths for general health, viz. radon baths (group 1), iodine-bromine baths (group 2), and hydrogen sulfide baths (group 3). The evaluation of the efficacy of different therapeutic modalities has demonstrated that the differential application of combined treatment with the inclusion of mineral waters of the Sochi health resort and underwater shower massage shower massage produces beneficial effect on the patients presenting with coronary heart disease.",2011,"Voprosy kurortologii, fizioterapii, i lechebnoi fizicheskoi kultury"
"Thalassomonas agariperforans sp. nov., an agarolytic bacterium isolated from marine sand.","A Gram-staining-negative, motile, agarolytic bacterium, designated M-M1(T), was isolated from marine sand obtained from Geoje Island, South Sea, Korea, and its taxonomic position was investigated using a polyphasic taxonomic approach. Strain M-M1(T) grew optimally at pH 7.0-8.0, at 30 Â°C and in the presence of 2 % (w/v) NaCl. It did not grow in the presence of >7 % (w/v) NaCl. Phylogenetic analysis based on 16S rRNA gene sequences showed that strain M-M1(T) fell within the clade comprising members of the genus Thalassomonas, clustering with Thalassomonas agarivorans TMA1(T), Thalassomonas loyana CBMAI 722(T) and Thalassomonas ganghwensis JC2041(T), with which it exhibited 16S rRNA gene sequence similarity values of 96.4, 96.0 and 94.9 % respectively. Strain M-M1(T) exhibited 94.7-95.2 % 16S rRNA gene sequence similarity to the other species of the genus Thalassomonas. Strain M-M1(T) contained Q-8 as the predominant ubiquinone and C(16 : 1)Ï‰7c and/or iso-C(15 : 0) 2-OH, C(16 : 0) and C(18 : 1)Ï‰7c as the major fatty acids. The DNA G+C content was 44.2 mol%. Strain M-M1(T) could be differentiated from phylogenetically related species of the genus Thalassomonas by differences in some phenotypic properties. On the basis of the phenotypic, chemotaxonomic and phylogenetic data, strain M-M1(T) is considered to represent a novel species of the genus Thalassomonas, for which the name Thalassomonas agariperforans sp. nov. is proposed. The type strain is M-M1(T) (= KCTC 23343(T) = CCUG 60020(T)).",2011,International journal of systematic and evolutionary microbiology
Joint and Long Short-Term Memory Regression of Clinical Scores for Alzheimerâ€™s Disease Using Longitudinal Data,"Alzheimerâ€™s disease (AD), the most common type of the dementia, is a progressive neurodegenerative disease that mainly affects elderly. It causes a high financial burden for patients and their families. For effective treatment of AD, it is important to identify the AD progression of clinical disease over time. As the cognitive scores can effectively indicate the disease status, the prediction of the scores using the longitudinal magnetic resonance imaging (MRI) data is highly desirable. In this paper, we propose a joint learning and clinical scores prediction method for AD diagnosis via longitudinal MRI data. Specifically, we devise a novel feature selection method that consists of a temporally constrained group LASSO model and the correntropy. The baseline MRI data is used to jointly select the most discriminative features. Then, we use the stacked long short-term memory (SLSTM) to effectively capture useful information in the input sequence to predict the clinical scores of future time points. Extensive experiments on the Alzheimerâ€™s disease Neuroimaging Initiative (ADNI) database are conducted to demonstrate the effectiveness of the proposed model. Our model can accurately describe the relationship between MRI data and scores, and thus it can be effective in predicting longitudinal scores.",2019,2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)
Dal culto alla cura. Il corpo in Nietzsche tra eugenetica ed etopoiesi [From Cult to Care. The body in Nietzsche between Eugenics and Constructionof the Self],"This article, From cult to care. The body in Nietzsche between eugenics and construction of the self, points out some pivotal aspects about Nietzscheâ€™s reflection on the body. Zarathustra says: â€œHuman being is something that must be overcomeâ€: but what role does the body play in this overcoming? How can this transvaluation of the human being be accomplished? And what does Zarathustra mean when he says Ãœber-mensch? This essay aims to demonstrate why the body is conceived by Nietzsche as a psychophysical whole and as a plurality of impulses. Than it will be shown how Nietzschean notion of the body can neither be reduced to biological aspects, nor can the enhancement of physical faculties and performance be considered a â€œcultâ€ in Nietzscheâ€™s thought. Through the analysis of the genealogy of values, the Platonic tradition, and the theme of illness, the article highlights how both materialistic ontology and an eugenic vision of corporeity are not compatible with a detailed reading of Nietzscheâ€™s reflection. The final goal of the essay is to propose an alternative vision of corporeity, where the body is understood as a privileged way to self-conquest and to self-care, as well as to experience the art of living and the aesthetics of existence. 1. OLTREUOMO E SUPERUOMO. Lâ€™Â«INDICIBILE COMPLICAZIONEÂ» DEL CORPO Â«In veritÃ , che cosa possa un corpo, nessuno fin qui lâ€™ha determinatoÂ»: dallâ€™imponente architettura della sua Ethica ordine geometrico demonstrata Spinoza sembra alludere con questo scolio ad una delle questioni fondamentali poste oggi dalla riflessione transumanista: di cosa puÃ² essere capace un corpo? Quali sono le sue potenzialitÃ ? Sino a che punto esso puÃ² implementare la sua forza ed estendere la sua sfera psico-fisica? Polemico nei confronti delle visioni distopico-apocalittiche di un certo postmodernismo â€“ che da prospettive diverse concorda sul potenziale subdolo, nichilistico, illusorio e 1 B. Spinoza, Etica, tr. it. di S. Giametta, Bollati Boringhieri, Torino 2002, (ed. orig. 1677), p. 99. DAL CULTO ALLA CURA. IL CORPO IN NIETZSCHE... 140 alienante della tecnica â€“ il transumanesimo contemporaneo rielabora di contro alcune istanze di fondo del neopositivismo a partire dalla fiducia nel progresso tecno-scientifico. Al versante postmoderno, che stigmatizza il â€œcattivo sognoâ€ e le inquietanti fosforescenze della tecnocrazia (il lato oscuro di una tecnica che traghetta lâ€™uomo verso un destino di atrofizzazione e abbrutimento), il transumanesimo saluta invece favorevolmente la tecnologia come occasione di liberazione e potenziamento dei limiti intrinseci dellâ€™umano. Il presente contributo non intende tuttavia sondare le implicazioni bioetiche legate alla possibilitÃ  di trascendere i confini della natura umana attraverso la bioingegneria, la nanotecnologia, la genetica, la robotica, lâ€™informatica, quanto piuttosto fare luce sulla riflessione che investe il decisivo problema del corpo nel pensiero di Nietzsche, inteso come Leib (corpo vivente-vissuto) e non come KÃ¶rper (corpo anatomico oggetto dellâ€™indagine scientifica). Paradossalmente considerato come punto di 2 Si pensi, tra lâ€™amplissima bibliografia a G. Anders, Lâ€™uomo Ã¨ antiquato, vol. I: Considerazioni sullâ€™anima nellâ€™epoca della seconda rivoluzione industriale, tr. it. di L. Dallapiccola, Bollati Boringhieri, Torino 2003 (ed. orig 1956); G. Debord, La societÃ  dello spettacolo, tr. it. di P. Salvadori, SugarCo, Milano 1990 (ed. orig. 1967); J. Baudrillard, Lo scambio simbolico e la morte, tr. it. di G. Mancuso, Feltrinelli, Milano 1984 (ed. orig. 1976). 3 Per le opere di Nietzsche Ã¨ stata utilizzata lâ€™edizione italiana condotta sul testo critico originale stabilito da G. Colli e M. Montinari: Opere di Friedrich Nietzsche, Adelphi, Milano, 1964-. (dâ€™ora in poi: OFN). Le sigle che compaiono nelle note corrispondono ai seguenti scritti: EH: Ecce homo; FW: Die frÃ¶hliche Wissenschaft, La gaia scienza; GD: GÃ¶tzenDÃ¤mmerung, Crepuscolo degli idoli; GM, Zur Genealogie der Moral, Genealogia della morale; JGB: Jenseits von Gut und BÃ¶se, Al di lÃ  del bene e del male; M: MorgenrÃ¶the, Aurora; Za: Also sprach Zarathustra, CosÃ¬ parlÃ² Zarathustra. Le opere vengono riportate con il titolo, abbreviato nelle sigle succitate, seguito dal numero dellâ€™aforisma o dal titolo di sezione e dal numero di pagina. Per i frammenti postumi Ã¨ stata utilizzata la sigla NF: Nachgelassene Fragmente, seguita dallâ€™anno, dal numero di pagina, dal numero di frammenti di appartenenza e, tra parentesi quadre, dal numero del frammento specifico. 4 Tra i piÃ¹ autorevoli protagonisti del dibattito transumanista internazionale, Riccardo Campa si richiama in modo esplicito e diffuso al pensiero di Nietzsche, rileggendo lâ€™esortazione del filosofo Â«devi diventare quello che tu seiÂ» (Cfr. FW, a cura di F. Masini e M. Montinari, in OFN V/II, 1965, Â§ 270, p. 158, EH, tr. it. di R. Calasso, in OFN VI/III, sottotitolo: Come si diventa ciÃ² che si Ã¨), nei termini di uno sviluppo dellâ€™uomo oltre se stesso sulla base delle metamorfosi tecnoscientifiche. Per una critica radicale alle istanze reazionarie dellâ€™umanesimo e dellâ€™antropocentrismo, intesi come orizzonti intrascendibili della specieuomo, cfr. R. Campa, Scienza e superuomo nel pensiero di Friedrich Nietzsche. Per una genealogia del transumanesimo, Â«Letteratura-TradizioneÂ», n. 41, 2007, pp. 30-55; Id., Mutare o perire. La sfida del transumanesimo, Sestante Edizioni, Bergamo 2010; Id., Una spirale ascendente. Origine e sviluppo della visione escatologica transumanista, Â«Pedagogia e vitaÂ», n. 2, 2017, pp. 27-40. Un vivo dibattito focalizzato sulla possibilitÃ  di annoverare o meno Nietzsche tra i precursori del transumanesimo coinvolge attualmente gli interpreti di area anglosassone: cfr. S. L. Sorgner, Nietzsche, the Overhuman, and Transhumanism, Â«Journal of Evolution & TechnologyÂ», n. 1, 2009, pp. 29-42. Il presente articolo Ã¨ stato successivamente discusso in una piÃ¹ ampia collettanea sul tema: cfr. Y. Tuncel (a cura di), Nietzsche and ALBERTO GIACOMELLI 141 riferimento tanto dal postmodernismo quanto dal transumanesimo, Nietzsche da un lato incarna in effetti il grande profeta delle crisi contemporanee, del nichilismo che corrode veritÃ  e radici, nonchÃ© il maestro dellâ€™eterno ritorno come pensiero abissale che frantuma le luci del progresso tecnico e irride la tronfia marcia ascensiva dello spirito, dallâ€™altro Ã¨ fautore di un nichilismo liberatorio, di un atteggiamento esistenziale â€œpositivoâ€ che traduce lo spaesamento e la â€œperdita del centroâ€ in un mare aperto a nuove possibilitÃ  sperimentali e a inedite chances di mutamento. Â«Finalmente possiamo sciogliere le vele alle naviÂ», replica Nietzsche alla notizia che Â«il vecchio Dio Ã¨ mortoÂ»: ma tra i rassicuranti punti dâ€™approdo che ci lasciamo alle spalle vi Ã¨ anche la nozione stessa di â€œuomoâ€, che rappresenta per il filosofo Â«qualche cosa che deve essere superatoÂ» (Der Mensch ist Etwas, das Ã¼berwunden werden soll). Che ruolo gioca il corpo in questo superamento? In che termini si compie questa destinale trasvalutazione dellâ€™umano? In vista di cosa lâ€™uomo deve oltrepassare se stesso? La risposta di Zarathustra va evidentemente nella direzione dellâ€™Ãœber-mensch, dellâ€™oltre-uomo: ciÃ² che si cercherÃ  di mostrare Ã¨ come il corpo, inteso da Nietzsche nei termini di globalitÃ  psico-fisica, non legittimi in alcun modo una lettura â€œintensivaâ€ di tale nozione, nella prospettiva del potenziamento delle facoltÃ  e delle performance fisiche. Rispetto alla traduzione italiana â€œsuper-uomoâ€, che evoca immediatamente scenari di eroismo muscolare ovvero inquietanti modelli antropologico-razziali di esseri â€œmeglio riuscitiâ€ dellâ€™uomo quale Ã¨ oggi, lâ€™espressione â€œoltre-uomoâ€ rinvia in effetti alla capacitÃ  di andare oltre (Ã¼ber) se stessi, oltre il proprio compiacimento, oltre lâ€™adorazione della propria immagine e dunque al di lÃ  (jenseits) del culto del corpo e del suo potenziamento. Opporre al â€œsuperuomoâ€ in quanto forma biologica potenziata e piÃ¹ completa da un punto di vista razziale, lâ€™â€œoltreuomoâ€ in quanto forma di esistenza piÃ¹ elevata rispetto allâ€™uomo moderno, non significa evidentemente avvallare lâ€™opzione ascetica della mortificazione della carne, dellâ€™annullamento del sÃ© nella contemplazione pura, della rimozione e della repressione dei propri istinti vitali. Allâ€™esaltazione del superomismo come culto della fisicitÃ , della giovinezza e del vigore, cosÃ¬ come alla pratica ascetica come macerazione e olocausto di sÃ©, si cercherÃ  di proporre lâ€™alternativa via ermeneutica di unâ€™estetica dellâ€™esistenza in cui la corporeitÃ  viene plasmata e governata al fine di raggiungere un armonico quanto dinamico equilibrio delle pulsioni, una padronanza del proprio sÃ© psico-fisico. Transhumanism. Precursor or Enemy?, Cambridge Scholars Publishing, Newcastle 2017. 5 Cfr. FW, a cura di F. Masini e M. Montinari, in OFN V/II, 1965, Â§ 343, p. 205. 6 Za, tr. it. di M. Montinari, in OFN VI/I, 1968, Prologo, Â§ 3, p. 6. 7 Se un lavoro monografico organico sulla relazione tra estetica ed etica e su una lettura della corporeitÃ  in Nietzsche alla luce delle istanze dellâ€™askesis come pratica, esercizio e formazioDAL CULTO ALLA CURA. IL CORPO IN NIETZSCHE... 142 In quanto â€œponteâ€, metaxá»³ tra la bestia e lâ€™oltreuomo, lâ€™uomo Ã¨ insieme transizione dallâ€™animalitÃ  allâ€™umanitÃ  a venire (Ãœbergang) e tramonto in quanto premessa di una nuova nascita (Untergang). GiacchÃ© il coraggio del tramonto va riferito alle proprie presunzioni, alla propria hybris, e in generale a qualsiasi oggetto di adorazione, il corpo, cosÃ¬ come Nietzsche lo intende, non puÃ² legittimare opzioni di edonismo cultuale (o addirittura culturista), nÃ© la nozione di Ãœbermensch puÃ² preconizzare immediatamente orizzonti ottimistici di osmosi bio-tecnologica e di potenziamento post-umano per mezzo della scienza. Irriducibile a qualsiasi forma di materialismo positivista ovvero a istanza meramente biologico-fisiologica, il corpo rappresenta semmai per Nietzsche una pluralitÃ  di forze problemat",2018,
CaractÃ©ristiques des ictÃ¨res Ã  bilirubine conjuguÃ©e chez lâ€™adulte dans un CHU au Burkina Faso,"RÃ©sumÃ©Dans un contexte de ressources limitÃ©es, les ictÃ¨res Ã  bilirubine conjuguÃ©e posent le problÃ¨me de leur diagnostic Ã©tiologique et de leur prise en charge thÃ©rapeutique.ObjectifDÃ©crire les caractÃ©ristiques Ã©tiologiques et Ã©volutives des ictÃ¨res Ã  bilirubine conjuguÃ©e chez les adultes admis au CHU de Bobo-Dioulasso.MÃ©thodesIl sâ€™agissait dâ€™une Ã©tude transversale couvrant la pÃ©riode du 1er fÃ©vrier 2009 au 31 janvier 2010. Ont Ã©tÃ© inclus tous les patients hospitalisÃ©s dans le dÃ©partement de mÃ©decine prÃ©sentant un ictÃ¨re avec un taux sÃ©rique de bilirubine supÃ©rieur Ã  50 Î¼mol/L et Ã  prÃ©dominance conjuguÃ©e, consentant et ayant pu rÃ©aliser les examens paracliniques prescrits.RÃ©sultatsSur 163 patients prÃ©sentant un ictÃ¨re, 42 correspondaient au profil dâ€™inclusion des cas (25,7 %). Leur Ã¢ge moyen Ã©tait de 42,1 Â± 12,8 ans avec un sex-ratio de 1,6. Le dÃ©lai moyen de consultation depuis le dÃ©but des symptÃ´mes Ã©tait de 20,6 Â± 15,2 jours. Le cancer primitif du foie (CPF) Ã©tait la 1re Ã©tiologie (28,6 %) suivi Ã  part Ã©gale de la cirrhose (11,9 %) et des hÃ©patites virales (11,9 %). Le diagnostic Ã©tait indÃ©terminÃ© dans 5 cas, reprÃ©sentant 1/10e des patients. La durÃ©e moyenne dâ€™hospitalisation Ã©tait de 10,8 Â± 5,3 jours. Lâ€™Ã©volution a Ã©tÃ© marquÃ©e en hospitalisation par le dÃ©cÃ¨s de 11 patients (26,2 %) dont 3 cas de CPF et 3 cas de cirrhose.ConclusionLes hÃ©patites virales, la cirrhose et le CPF dominent encore les Ã©tiologies des ictÃ¨res cholestatiques chez lâ€™adulte au Burkina. Leur diagnostic et leur devenir restent marquÃ©s par lâ€™insuffisance du plateau technique et les ressources limitÃ©es des patients.AbstractIn poor resources countries, conjugated bilirubin jaundice raises aetiologyc and therapeutic issues.ObjectiveTo describe etiologies and evolution of conjugated bilirubin jaundice in adults in Bobo-Dioulasso teaching hospitalMethodsThis was a cross-sectional study covering 1 February 2009 to January 31, 2010. All patients hospitalized in the medicine ward with jaundice with serum bilirubin levels above 50 micromol/L and predominantly conjugated and who achieved the required diagnostic tests have been included.ResultsOf 163 patients with jaundice, 42 matched the inclusion criterias (25.7%). Their average age was 42.1Â±12.8 years with a sex-ratio of 1.6. The mean time from the beginning of symptoms to hospitalization was 20.6Â±15.2 days. The primary liver cancer was the first etiology (28.6%) followed in equal by cirrhosis (11.9%) and viral hepatitis (11.9%). The diagnosis was uncertain in five cases, representing 1/10th of cases. The average hospital stay was 10.8Â±5.3 days. The evolution was marked by the death in hospital of 11 cases (26.2%) including 3 cases of primary liver cancer and 3 cases of cirrhosis.ConclusionViral hepatitis, cirrhosis and primary liver cancer still dominate the etiologies of obstructive jaundice in adults in Burkina. Their diagnosis and their pronosis are still faced limited technical resources in hospital and limited resources of patients.",2013,Journal Africain d'HÃ©pato-GastroentÃ©rologie
Meta-Path Graphical Lasso for Learning Heterogeneous Connectivities,"Sparse inverse covariance estimation has attracted lots of research interests since it can recover the structure of the underlying Gaussian graphical model. This is a useful tool to demonstrate the connections among objects (nodes). Previous works on sparse inverse covariance estimation mainly focus on learning one single type of connections from the observed activities with a lasso, group lasso or tree-structure penalty. However, in many real-world applications, the observed activities on the nodes can be related to multiple types of connections. In this paper, we consider the problem of learning heterogeneous connectivities from the observed activities by incorporating meta paths extracted from a heterogeneous information network (HIN), an information network with multiple types of nodes and links, into the conventional graphical lasso framework. We aim at extracting the strongest type of relation between any pairs of entities and ignoring other minor relations. Specially, we introduce two novel kinds of constraints: meta path constraints and exclusive constraints, which ensure the unique type of relation among a pair of objects. This problem is highly challenging due to the non-convex optimization. We proposed a method based upon the alternating direction method of multipliers (ADMM) to efficiently solve the problem. The conducted experiments on both synthetic and real-world datasets illustrate the effectiveness of the proposed method.",2017,
Linear and Conic Programming Estimators in High-Dimensional Errors-in-variables Models,"We consider the linear regression model with observation error in the design. In this setting, we allow the number of covariates to be much larger than the sample size. Several new estimation methods have been recently introduced for this model. Indeed, the standard Lasso estimator or Dantzig selector turn out to become unreliable when only noisy regressors are available, which is quite common in practice. We show in this work that under suitable sparsity assumptions, the procedure introduced in Rosenbaum and Tsybakov (2013) is almost optimal in a minimax sense and, despite non-convexities, can be efficiently computed by a single linear programming problem. Furthermore, we provide an estimator attaining the minimax efficiency bound. This estimator is written as a second order cone programming minimisation problem which can be solved numerically in polynomial time.",2014,arXiv: Statistics Theory
Supercooling in Floral Buds of `Danka' Black and `Red Lake' Red Currants,"Differential thermal analyses (DTA) and freeze viability tests were conducted to investigate the biophysics of freezing in floral buds of â€˜Dankaâ€™ black (Ribes nigrutn L.) and â€˜Red Lakeâ€™ red currants [Ribe.s sativum (Rchb.) Syrne] sampled from Nov. 1989 through Mar. 1990. Scanning electron microscopy was also used to determine the relationship between floral morphology and the freezing characteristics of the buds. Floral buds had multiple abrupt low-temperature exotherms (LTEs) and one or two broad LTEs in DTA tests. Abrupt LTEs from DTA were associated with apparent injury to the inflorescence in viability tests. The number of LTEs did not correspond to the number of racemes or flowers per bud, indicating that several flowers froze simultaneously. DTA experiments conducted in Dec. 1990 revealed that the broad exotherm detected between â€“ 14 and â€“ 20C in â€˜Dankaâ€™ samples resulted from freezing of supercooled water in the outer nonliving region of the periderm of cane tissue attached to the bud. DTA has been used to study freezing in floral buds of several fruit genera. In DTA experiments, floral buds exhibit a nonlethal high-temperature exotherm (HTE), representing extracellular ice formation (Burke et al., 1976; Quamme, 1974). The HTE is followed by one or more LTEs associated with intracellular freezing of supercooled water in the floral prlmordia. In buds containing a racemose in florescente, such as Rubus, LTEs correspond to freezing injury of individual flowers (Warmund et al., 198$) or to the entire floral region (Kraut et al., 1986; Warmund et al., 1990). Previous research on Ribes spp. has focused on fall acclimation (Lobanov and Shcherbinin, 1990) or spring frost tolerance of R. nigrum after budbreak (Dale, 1981; Dale and Heiberg, 1984; Keep et al., 1983; Mather et al., 1980). The purpose of our study was to investigate supercooling in floral buds of â€˜Dankaâ€™ black and â€˜Red Lakeâ€™ red currants and to ascertain the relationship between floral morphology and freezing characteristics. Materials and Methods â€˜Dankaâ€™ black and â€˜Red Lakeâ€™ red currant samples were collected from 21and 6-year-old plants, respectively, growing at the Horticultural Research Station at Excelsior, Minn. One-yearold wood was obtained at 0.5 to 1 m above the soil surface on 13 Nov. 1989, 17 Jan. 1990, and 19 Mar. 1990 for DTA, viability tests, and scanning electron microscopy (SEM). On 10 Dec. 1990, â€˜Dankaâ€™ samples were also obtained for DTA. All plant material was placed in sealed plastic bags containing moist paper towels, packed on ice, and shipped by overnight mail to the Univ. of Missouri where the freezing tests were conducted. Samples were stored at 2C for 24 h before being tested. DTA. The DTA system used in this study has three sample chambers and a reference chamber, each containing a Yellow Received for publication 11 Mar. 1991. Contribution from the Missouri Agricultural Experiment Station, Journal Series no. 11323. We thank D. Bedford and J. Luby for providing plant material. Technical assistance by Glen Davis is acknowledged. The cost of publishing this paper was defrayed in part by the payment of page charges. Under postal regulations, this paper therefore must be hereby marked advertisement solely to indicate this fact. lAssociate Professor, Dept. of Horticulture. â€˜Associate Professor, School of Natural Resources. 3Research Horticulturist. 1030 Springs Instrument (Yellow Springs Instrument Co., Yellow Springs, Ohio) Series 400 thermistor probe connected to a signal conditioning network (George, 1982). The analog outputs from the signal conditioning network were converted to digital format and stored on floppy disks using an Analog Device AD363 data acquisition system (Analog Devices, Norwood, Mass. ) interfaced with a Southwest Technical Products (Southwest Technical Products Corp., San Antonio, Texas) 6809 microcomputer. Data were plotted on a Houston Instruments (Houston Instruments, Austin, Texas) 2000 X-Y plotter. The DTA system can resolve freezing events that produce differential temperatures of â‰ˆ 0.001C. Intact buds were removed with a small section of adjacent stem tissue from the middle portion of l-year-old canes for all dates, except â€˜Red Lakeâ€™ buds tested on 19 Nov. On this date, bud scales and leaves surrounding the inflorescence were removed before being frozen. Each sample was placed in an aluminum foil cup and attached to a thermistor. An empty aluminum foil cup was attached to the reference sensor to balance the heat capacity of the reference and sample chambers. The temperature was lowered rapidly to 0C and cooling was initiated at 3C/h when the bud temperature was within 2C of the reference temperature. Data were collected at 40-sec intervals until the sample temperature reached â€“ 40C. After the DTA test was completed, samples were removed from the chamber and placed in vials containing formalinâ€“acetic acidâ€“alcohol (FAA) fixative. Samples were mailed to the Appalachian Fruit Research Station, Kearneysville, W.Va., where they were dehydrated in a graded ethanol series, dissected, and examined under a stereoscope. Additional nonfrozen control samples were also fixed in FAA and prepared for SEM in Kearneysville. Dehydrated buds were critical-point-dried with CO2, sputter-coated with gold palladium alloy, and examined on a Cambridge StereoScan 120 electron microscope (Cambridge Instruments, Cambridge, England) operated at 8 kV. In Dec. 1990, additional DTA experiments were conducted on the following tissues: 1) intact sections of canes in which the floral bud had been removed; 2) cane samples of pith and Abbreviations: DTA, differential thermal analysis; HTE, high-temperature exotherm; LTE, low-temperature exotherm; SEM, scanning electron microscopy. J. Amer. Soc. Hort. Sci. 116(6):1030-1034. 1991.",1991,Journal of the American Society for Horticultural Science
Graphical Lasso Quadratic Discriminant Function for Character Recognition,"The quadratic discriminant function (QDF) derived from the multivariate Gaussian distribution is effective for classification in many pattern recognition tasks. In particular, a variant of QDF, called MQDF, has achieved great success and is widely recognized as the state-of-the-art method in character recognition. However, when the number of training samples is small, covariance estimation involved in QDF will usually be ill-posed, and it leads to the loss of the classification accuracy. To attack this problem, in this paper, we engage the graphical lasso method to estimate the covariance and propose a new classification method called the Graphical Lasso Quadratic Discriminant Function (GLQDF). By exploiting a coordinate descent procedure for the lasso, GLQDF can estimate the covariance matrix (and its inverse) more precisely. Experimental results demonstrate that the proposed method can perform better than the competitive methods on two artificial and six real data sets (including both benchmark digit and Chinese character data).",2011,
Field and Shape Reconstruction in Fluid Dynamics,"Inverse problems are concerned with the reconstruction of quantities from remote measurements. Inverse fluid flow problems are important for many applications, for example for determining the state of the atmosphere from measurements on the planets surface and further remote sensing techniques. Here, we investigate the reconstruction of some fluid flow and shape reconstruction for inclusions within the flow from boundary measurements. As a model problem we consider the Oseen equation, which is obtained by linearizing the Navier-Stokes equations.In a first step we develop a point source method for the reconstruction of flow field from remote measurements. In contrast to field reconstructions in acoustics or electromagnetics, here we need a proper setup of the scheme as the fundamental solution of the Oseen equation is not symmetric or anti-symmetric in its arguments; moreover the null-spaces of the integral operators under consideration are no longer trivial, such that the corresponding convergence analysis of the point source method is particularly difficult.Further we extend our study to develop methods to test for analytic extensibility in fluid dynamics for the inverse fluid flow problems. We study and analyze three different approaches for the analytical continuation, the range test, the no-response test and a convergence test. We prove the convergence of these methods when applied to the Oseen equation. In particular, we exhibit a new approach to show convergence of the no-response test. A strong relationship between the convergence test and the no response test is shown.A numerical demonstration of the point source method and the convergence test is presented to exhibit the feasibility of these methods. To carry out the reconstructions we employ either domain sampling or the LASSO scheme is used for the reconstruction of flow field and the shape of unknown obstacles.",2011,
Purisimeno Chumash Prehistory: Maritime Adaptations Along the Southern California Coast,"The Vandenberg Project isfirmly established in the annals of California archaeology as a sigÂ­ nificant investigation that set a standard for coastÂ­ al archaeological research. This book is a very readable synthesis of more than two decades of research by Michael Glassow at Vandenberg Air Force Base, located on the south-central coast of California. The project was initiated in the early 1970s in response to plans by the U. S. Air Force to construct facilities for the space shuttleâ€”a techno pipedream of the 1970s that has since beÂ­ come a commonplace reality. Most of the fieldÂ­ work and analysis was carried out by the nowÂ­ defiinct Office of Public Archaeology, a cultural resource management (CRM) facility started by Glassow in the Department of Anthropology at the University of California, Santa Barbara. As represented by the Vandenberg work, this facility produced quality research in a CRM context at a difficult juncture in the history of California arÂ­ chaeology, as a newly emerging ecological paraÂ­ digm was supplying the discipline with research questions that were at once fascinating and diffiÂ­ cult to operationalize. At die same time, governÂ­ ment agencies and researchers were trying to figÂ­ ure out how to work effectively with newly passed preservation laws. Many large-scale CRM projects of the 1970s are famous mostly for how little they accomplished. The Vandenberg project, however, is not one of these.",1996,
GeneRank-based partly adaptive group-penalised multinomial regression for microarray classification,"This paper proposes a partly adaptive group-penalised multinomial regression for gene selection. Weights with biological significance are constructed by combing the gene expression information with gene ontology network via GeneRank. By introducing the weights into group lasso penalty, the partly adaptive group-penalised multinomial regression is proposed. Two algorithms for fitting the proposed model are presented on the base of blockwise descent. Experimental results on gene expression data of yeast diauxic shift demonstrate that the proposed method can select the stable genes and achieve the better classification performance.",2016,
â€œIt's not what you expectâ€: teaching irony to third graders,"ConclusionsThe purpose of the above analysis is to emphasize the crucial relationship between the teaching and theoretical research done at the university level and the teaching of literary criticism to children. The gaps between what is done in university and school classoroms can be bridged and should be bridged more often. An adult understanding of the nature and function of literary irony is crucial to an understanding of children's stories and is the first step towards the effective presentation of ironic stories to young readers. When I first taught Swift's â€œA Modest Proposalâ€ to university freshmen, I was surprised to find that several were shocked at what they thought to be that essay's sadistic message: they had read Swift literally and not ironically. Should my third grade students become university students, they are not likely to make that mistake. As Kenneth Burke stated (in a quotation used as one of Wayne Booth's epigraphs) â€œWe cannot use language maturely until we are spontaneously at home in ironyâ€ (p. xxvi). But preparing future university students is not the main goal. Rather, I want the school children I work with to be able to perceive the ironies inherent in the stories they consume and, as a result of their perceptions, to arrive at fuller understanding and, therefore, fuller enjoyment of those stories.",1982,Children's Literature in Education
Self-expanding metallic stents and self-expanding plastic stents in the palliation of malignant oesophageal dysphagia.,"Endoluminal stenting has revolutionised the practice of gastrointestinal endoscopy for many years. What started as rigid, inflexible, plastic stents have now evolved into flexible, easy-to-deploy self-expanding stents with a myriad of choices, including covered, partially covered and uncovered types. Many of these also come along with special features including anti-reflux, anti-migration and lasso for stent adjustment and retrieval. Numerous papers with meta-analyses and systemic reviews have without doubt confirmed the efficacy, safety and cost effectiveness of endoluminal stenting in the palliation of malignant obstruction of the oesophagus, stomach, duodenum, colon and the biliary tree. This paper will focus on the use of self-expanding plastic stents (SEPS) as well as self-expanding metallic stents (SEMS) in the palliation of malignant oesophageal obstruction.",2014,Annals of palliative medicine
Flexible discrete space models of animal movement,"Movement drives the spread of infectious disease, gene flow, and other critical ecological processes. To study these processes we need models for movement that capture complex behavior that changes over time and space in response to biotic and abiotic factors. Penalized likelihood approaches, such as penalized semiparametric spline expansions and LASSO regression, allow inference on complex models without overfitting. Continuous-time Markov chains (CTMCs) have been recently introduced as a flexible discrete-space model for animal movement. Modeling with CTMCs involves discretizing an animal's path to the resolution of a raster grid. The resulting stochastic process model can easily incorporate environmental and other covariates, represented as raster layers, that affect directional bias and overall movement rate. We introduce a weighted likelihood approach that allows for modeling movement using CTMCs, with path uncertainty due to missing data modeled by imputing continuous-time paths between telemetry locations. The framework we introduce allows for inference on CTMC movement models using existing software for fitting Poisson regression models, including penalized versions of Poisson regression. The result is a flexible, powerful, and accessible framework for modeling a wide range of animal movement behavior.",2016,arXiv: Applications
Fast projections onto mixed-norm balls with applications,"Joint sparsity offers powerful structural cues for feature selection, especially for variables that are expected to demonstrate a â€œgroupedâ€ behavior. Such behavior is commonly modeled via group-lasso, multitask lasso, and related methods where feature selection is effected via mixed-norms. Several mixed-norm based sparse models have received substantial attention, and for some cases efficient algorithms are also available. Surprisingly, several constrained sparse models seem to be lacking scalable algorithms. We address this deficiency by presenting batch and online (stochastic-gradient) optimization methods, both of which rely on efficient projections onto mixed-norm balls. We illustrate our methods by applying them to the multitask lasso. We conclude by mentioning some open problems.",2012,Data Mining and Knowledge Discovery
Study of negative and positive superhumps in ER Ursae Majoris,"We carried out the photometric observations of the SU UMa-type dwarf nova ER UMa during 2011 and 2012, which showed the existence of persistent negative superhumps even during the superoutburst. We performed two-dimensional period analysis of its light curves by using a method called ""least absolute shrinkage and selection operator"" (Lasso) and ""phase dispersion minimization"" (PDM) analysis, and we found that the period of negative superhumps systematically changed between a superoutburst and the next superoutburst. The trend of the period change can beinterpreted as reflecting the change of the disk radius. This change of the disk radius is in good agreement with the predicted change of the disk radius by the thermal-tidal instability (TTI) model. The normal outbursts within a supercycle showed a general trend that the rising rate to maximum becomes slower as the next superoutburst approaches. The change can be interpreted as the consequence of the increased gas-stream flow onto the inner region of the disk as the result of the tilted disk. Some of the superoutbursts were found to be triggered by a precursor normal outburst when the positive superhumps appeared to develop. The positive and negative superhumps co-existed during the superoutburst. The positive superhumps were prominent only during four or five days after the supermaximum, while the signal of the negative superhumps became strong after the middle phase of the superoutburst plateau. A simple combination of the positive and negative superhumps was found to be insufficient in reproducing the complex profile variation. We were able to detect the developing phase of positive superhumps (stage A superhumps) for the first time in ER UMa-type dwarf novae. Using the period of stage A superhumps, we obtained a mass ratio of 0.100(15), which indicates that ER UMa is on the ordinary evolutional track of CVs.",2014,Publications of the Astronomical Society of Japan
Penalized logistic regression with low prevalence exposures beyond high dimensional settings,"Estimating and selecting risk factors with extremely low prevalences of exposure for a binary outcome is a challenge because classical standard techniques, markedly logistic regression, often fail to provide meaningful results in such settings. While penalized regression methods are widely used in high-dimensional settings, we were able to show their usefulness in low-dimensional settings as well. Specifically, we demonstrate that Firth correction, ridge, the lasso and boosting all improve the estimation for low-prevalence risk factors. While the methods themselves are well-established, comparison studies are needed to assess their potential benefits in this context. This is done here using the dataset of a large unmatched case-control study from France (2005-2008) about the relationship between prescription medicines and road traffic accidents and an accompanying simulation study. Results show that the estimation of risk factors with prevalences below 0.1% can be drastically improved by using Firth correction and boosting in particular, especially for ultra-low prevalences. When a moderate number of low prevalence exposures is available, we recommend the use of penalized techniques.",2019,PLoS ONE
Influence of Viruses on Enzymes in Monkey Kidney Cell Cultures.,"Galasso, G. J. (University of North Carolina School of Medicine, Chapel Hill), and D. G. Sharp. Relative plaque-forming, cell-infecting, and interfering qualities of vaccinia virus. J. Bacteriol. 88:433â€“439. 1964.â€”The growth of vaccinia virus in slant cultures of L cells inoculated with different multiplicities of counted particles suggests a higher incidence of cell infection than can be accounted for by the number of plaque-forming units. From cultures containing antiserum or heated virus to limit the passage of progeny to uninfected cells, the data clearly indicate the ability of all the particles to infect cells even though the plaque titer is only one-tenth of this number. Analogous experiments show that an average of two heat-inactivated (56 C, 45 min) particles induce interference in L cells. There is nothing yet to show whether the few plaque-forming particles are different from the majority or whether they are just statistically fortunate in the complex process of plaque formation.",1964,The Journal of pathology and bacteriology
Regularized Joint Estimation of Related VAR Models via Group Lasso,"In a number of applications, one has access to high-dimensional time series data on several related subjects. Natural example comes from medical experiments: brain fMRI time series data for various groups of patients, be it controls or individuals with a specific mental disorder. In this work, we discuss the problem of their regularized joint estimation, introduced via Vector Autoregressive modeling (VAR) and leveraging a group lasso penalty on top of regular lasso, so as to increase statistical efficiency of estimates by borrowing strength across the subjects. We develop a modeling framework that allows for both group level and subject-specific effects for related subjects, using group lasso to estimate the former. Besides a simulation study, we also use our approach to tackle some of the known issues of effective connectivity estimation for resting-state fMRI data. In particular, a group-level descriptive analysis is conducted for brain inter-regional temporal effects of ADHD and control patients from ADHD-200 Global Competition, and the findings are compared to those in neuroscience literature.",2018,
Herbicidal Control of Weeds in Maize,"Field trials were conducted on statistical basis at the Maize Breeding Station, Amberpet both in Kharif and Rabi seasons during 1971â€“72. Herbicides like Tafazine, Atra-taf, Atrataf + Sutan and Lasso were used at different doses.",1974,Indian Journal of Weed science
Ultrasound Current Source Density Imaging of the Cardiac Activation Wave Using a Clinical Cardiac Catheter,"Ultrasound current source density imaging (UCSDI), based on the acoustoelectric (AE) effect, is a noninvasive method for mapping electrical current in 4-D (space + time). This technique potentially overcomes limitations with conventional electrical mapping procedures typically used during treatment of sustained arrhythmias. However, the weak AE signal associated with the electrocardiogram is a major challenge for advancing this technology. In this study, we examined the effects of the electrode configuration and ultrasound frequency on the magnitude of the AE signal and quality of UCSDI using a rabbit Langendorff heart preparation. The AE signal was much stronger at 0.5 MHz (2.99 Î¼V/MPa) than 1.0 MHz (0.42 Î¼V/MPa). Also, a clinical lasso catheter placed on the epicardium exhibited excellent sensitivity without penetrating the tissue. We also present, for the first time, 3-D cardiac activation maps of the live rabbit heart using only one pair of recording electrodes. Activation maps were used to calculate the cardiac conduction velocity for atrial (1.31 m/s) and apical (0.67 m/s) pacing. This study demonstrated that UCSDI is potentially capable of realtime 3-D cardiac activation wave mapping, which would greatly facilitate ablation procedures for treatment of arrhythmias.",2015,IEEE Transactions on Biomedical Engineering
Stability Selection,"Estimation of structure, such as in variable selection, graphical modelling or cluster analysis, is notoriously difficult, especially for high dimensional data. We introduce stability selection. It is based on subsampling in combination with (high dimensional) selection algorithms. As such, the method is extremely general and has a very wide range of applicability. Stability selection provides finite sample control for some error rates of false discoveries and hence a transparent principle to choose a proper amount of regularization for structure estimation. Variable selection and structure estimation improve markedly for a range of selection methods if stability selection is applied. We prove for the randomized lasso that stability selection will be variable selection consistent even if the necessary conditions for consistency of the original lasso method are violated. We demonstrate stability selection for variable selection and Gaussian graphical modelling, using real and simulated data.",2008,
Large-scale nonconvex stochastic optimization by Doubly Stochastic Successive Convex approximation,"We consider supervised learning problems over training sets in which both the number of training examples and the dimension of the feature vectors are large. We focus on the case where the loss function defining the quality of the parameter we wish to estimate may be non-convex, but also has a convex regularization. We propose a Doubly Stochastic Successive Convex approximation scheme (DSSC) able to handle non-convex regularized expected risk minimization. The method operates by decomposing the decision variable into blocks and operating on random subsets of blocks at each step. The algorithm belongs to the family of successive convex approximation methods since we replace the original non-convex stochastic objective by a strongly convex sample surrogate function, and solve the resulting convex program, for each randomly selected block in parallel. The method operates on subsets of features (block coordinate methods) and training examples (stochastic approximation) at each step. In contrast to many stochastic convex methods whose almost sure behavior is not guaranteed in non-convex settings, DSSC attains almost sure convergence to a stationary solution of the problem. Numerical experiments on a non-convex variant of a lasso regression problem show that DSSC performs favorably in this setting.",2017,"2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
Placement Predict: A Review of Engineering Graduate Placement Statistics in India,"Prediction of graduate jobs is a key problem in the analysis of employability which can be addressed with data-driven strategies. Aspiring Minds dataset describes aptly the attributes of a freshly graduate engineering student, making it one of the most suitable datasets for prediction of graduate placements as well as for building machine learning models for analysis of employability. By implementing various machine learning repressors, we show that Ridge and Lasso regression performed marginally better than Random forest, followed closely by Support Vector Machines. The objective of this study was to predict the target salary and to analyse the overall employability of an engineering graduate. This methodology of data-driven approach can also serve as a foundation for future studies towards prediction of salary and placements, and identification of key features linked to employability of candidates.",2018,
Behavioral and Physiological Responses of Calves to Marshalling and Roping in a Simulated Rodeo Event,"Rodeos are public events at which stockpeople face tests of their ability to manage cattle and horses, some of which relate directly to rangeland cattle husbandry. One of these is calf roping, in which a calf released from a chute is pursued by a horse and rider, who lassoes, lifts and drops the calf to the ground and finally ties it around the legs. Measurements were made of behavior and stress responses of ten rodeo-naÃ¯ve calves marshalled by a horse and rider, and ten rodeo-experienced calves that were roped. NaÃ¯ve calves marshalled by a horse and rider traversed the arena slowly, whereas rodeo-experienced calves ran rapidly until roped. Each activity was repeated once after two hours. Blood samples taken before and after each activity demonstrated increased cortisol, epinephrine and nor-epinephrine in both groups. However, there was no evidence of a continued increase in stress hormones in either group by the start of the repeated activity, suggesting that the elevated stress hormones were not a response to a prolonged effect of the initial blood sampling. It is concluded that both the marshalling of calves naÃ¯ve to the roping chute by stockpeople and the roping and dropping of experienced calves are stressful in a simulated rodeo calf roping event.",2016,Animals : an Open Access Journal from MDPI
Shear wave splitting beneath the central Tien Shan and tectonic implications,"[1]Â Shear-wave splitting analyses were performed at 30 seismic stations in the central Tien Shan and its vicinity. Fast orientation at most of the stations on the Tien Shan range is ENE-WSW, parallel to the strike of the Tien Shan. It largely reflects a coherent deformation of the lithosphere in response to the NS shortening. Anisotropy with NNE-SSW fast axes is observed within adjacent basins, around the Issyk Kul and near the northwestern Tarim, where the lithosphere is presumably strong, less deformed. The NNE-SSW orientation, which coincides with the surface velocity of the central Tien Shan, can be best explained by shear between the lithosphere and asthenosphere on a regional scale rather than small-scale convection as proposed in previous studies. The dextral Talasso-Fergana fault seems to have little effect on anisotropy at its nearby stations, suggesting this fault is probably confined to the shallow part of the lithosphere.",2006,Geophysical Research Letters
A multivariate regression approach to association analysis of a quantitative trait network,"MOTIVATION
Many complex disease syndromes such as asthma consist of a large number of highly related, rather than independent, clinical phenotypes, raising a new technical challenge in identifying genetic variations associated simultaneously with correlated traits. Although a causal genetic variation may influence a group of highly correlated traits jointly, most of the previous association analyses considered each phenotype separately, or combined results from a set of single-phenotype analyses.


RESULTS
We propose a new statistical framework called graph-guided fused lasso to address this issue in a principled way. Our approach represents the dependency structure among the quantitative traits explicitly as a network, and leverages this trait network to encode structured regularizations in a multivariate regression model over the genotypes and traits, so that the genetic markers that jointly influence subgroups of highly correlated traits can be detected with high sensitivity and specificity. While most of the traditional methods examined each phenotype independently, our approach analyzes all of the traits jointly in a single statistical method to discover the genetic markers that perturb a subset of correlated traits jointly rather than a single trait. Using simulated datasets based on the HapMap consortium data and an asthma dataset, we compare the performance of our method with the single-marker analysis, and other sparse regression methods that do not use any structural information in the traits. Our results show that there is a significant advantage in detecting the true causal single nucleotide polymorphisms when we incorporate the correlation pattern in traits using our proposed methods.


AVAILABILITY
Software for GFlasso is available at http://www.sailing.cs.cmu.edu/gflasso.html.",2009,Bioinformatics
Following the money in epilepsy therapeutics.,"To the Editor: Gagne et al., in â€œRefilling and Switching of Antiepileptic Drugs and Seizure-Related Events,â€ provide an illuminating review on the consequences of refilling prescriptions and switching of antiepileptic drugs, showing that refilling itself, but not brand-name/generic switching per se was associated with an increase in seizure-related adverse events.1 Not directly examined, however, is the impact that medication cost may have on seizure-related adverse events. It is likely that cost plays an important role in both refill-associated and switchingassociated adverse events. The study used data from British Columbia, Canada. These data likely underestimate the impact of refillassociated adverse events in the United States because more patients here have difficulty in meeting medication costs. Cost-related medication nonadherence is a major public health problem in the United States, much more so than in Canada.2 In 2007, Canadaâ€™s rate of costrelated medication nonadherence was 8%, whereas in the United States, it was 23%. In the United States, cost-related medication nonadherence for those with private insurance was 16%, and for the uninsured, it was 43%. Antiepileptic drug prices can vary by an order of magnitude, and it is a rare patient these days who is fully shielded from medication costs, regardless of insurance status. For example, my own insurance has prescription copays that range from $10 to 60. As many as 30â€“50% of patients with epilepsy miss >20% of their doses, and there is a threefold higher mortality in patients who are nonadherent to medication regimens than in those who are adherent.3 It is hard to imagine that the <5% variance in bioavailability between and within brands is clinically as relevant to patient outcomes as the effects of cost-related medication nonadherence. I would have liked to see Gagne et al. distinguish between generic-to-brand and brand-to-generic switches as one measure of a possible cost impact on seizure-related adverse events. In medical school and during residency, physicians are trained to prescribe from a menu without prices and to give each patient â€œthe bestâ€ as they perceive it. The American Academy of Neurology has taken the position that it is dangerous to allow generic substitution of antiepileptic drugs without prior â€œinformed consentâ€ of physicians and patients.4 In my view, there is insufficient data for meaningful â€œinformed consentâ€ of this sort. Further, there is no â€œclinical equipoiseâ€ between different agents when some are much cheaper than others and evidence of superiority is lacking. The report by Gagne et al. provides an important and authoritative counterweight to industryfunded studies highlighting alleged risks of generic antiepileptic drugs.5â€“7 Patients pay the price for their doctorsâ€™ menu selections. They will be more likely to delay refills and skip doses of expensive medications. Promotion of the use of branded drugs over cheaper generic alternatives is dangerous business, with epilepsy patientsâ€™ lives at stake.",2010,Clinical pharmacology and therapeutics
Antiretroviral Therapy: New Mechanistic and Therapeutic Insights for HIV Single-Entity and Combination Drug Products.,"The infectivity of cell-free HIV-1 is consistently reported to be less than 0.1% and the mechanisms influencing this low infectivity are not yet fully understood. Some hypothesize that this observed low infectivity results from the presence of defective HIV-1 particles, formed from mutations introduced in the reverse transcription step of virus replication. Using molecularly cloned HIV-1 that is capable of only a single round of infection, we can bypass the reverse transcription step during virus production in order to investigate this hypothesis and the extent to which infectivity can be influenced by other factors during virus production. Herein we show that optimal infectivity is obtained by harvesting virions from culture media 18 hours after transfection, with complete media changes 4-6 hours post-transfection. This optimal infectivity appears to primarily result from the emergence of â€œnoninfectiousâ€ virus particles at later time points. Our data demonstrate that HIV-1 infectivity increases in the presence of 18 greater amounts of viral envelope glycoproteins. Thus, the larger proportion of â€œnoninfectiousâ€ virus particles apparent in harvested virus cultures at later time points may be attributable to the production of virus lacking sufficient amounts of envelope glycoproteins. However, although we demonstrate ways by which virus infectivity can be optimized by varying parameters to enhance culture and production conditions, the overall infectivity of HIV-1 remains low. This suggests that the observed low infectivity of the virus is principally attributable to other, potentially-related, mechanism(s). Introduction Infectivity, a unitless number that quantitates the proportion of virus particles that are infectious, is a critical parameter for characterizing the human immunodeficiency virus type 1 (HIV-1). The infectivity of HIV-1 is consistently reported to be less than 0.1% [22] [23] [24]. There is not yet a clear, confirmatory understanding of why HIV-1 possesses such low infectivity, but there is a great deal of literature suggesting that one of the primary limitations in the establishment of a productive HIV infection is the target cell engagement and/or entry step. In fact, several engineered/artificial methods for increasing interactions between virions and target cells have been documented to result in greater infection efficiency. Spinoculation, a method of inoculating cells with virions using centrifugation, for example, results in greater amounts of cellassociated virus and a proportional increase in virus replication [27]. Furthermore, inoculating virions in the presence of positively-charged molecules 19 (e.g., polybrene) has been shown to enhance adsorption of virus particles onto target cells and result in greater infectivity [24] [28] [29]. And the overexpression adhesion molecules on virion-producer cells has also been shown to result in virions that possess more adhesion molecules (acquired from budding out of producer cells) and enhanced ability to interact with and infect target cells [30] [31] [32]. Therefore, it is plausible that HIV-1 infectivity is primarily limited by the ability to efficiently engage target cells. On the other hand, however, some have hypothesized that this observed low infectivity results from the presence of defective HIV-1 particles, formed from mutations introduced in the reverse transcription step of virus replication [25]. This theory is supported by the well-documented, naturally high error rate of HIV reverse transcriptase [49] [50] [51]. Furthermore, the identification of defense mechanisms, such as APOBEC3 cytidine deaminases, in the host cell that can influence the rate of mutation during the reverse transcription step of virus replication also strongly point to the possibility that the low infectivity of HIV-1 might be due to the presence of large numbers of defective virions resulting from mutations introduced to the proviral DNA during reverse transcription [52] [53]. Using molecularly cloned HIV-1 that is capable of only a single round of infection, we can bypass the reverse transcription step during virus production and essentially eliminate the influence of reverse transcriptase errors and APOBEC3 in order to investigate this hypothesis and determine the extent to which infectivity can be influenced by other factors during virus production. 20 Herein we specifically demonstrate that the culture conditions commonly used to produce cell-free HIV-1 in cultured media significantly influence the resulting infectivity of virions and can result in the production of a greater proportion of defective virus over time. These results emphasize the optimal conditions for producing cell-free virus and point to a potential molecular mechanism, related to the alternate theory of inefficient virus-cell interactions, which may more significantly impede the infectivity of HIV-1 in plasma or cultured media. Materials and Methods Production of single-cycle HIV-1 virions. Virions were produced by transfecting HEK 293T/17 cells (ATCC, Manassas, VA) as previously described [54]. Briefly, 293T were cultured at 37C with 5% CO2 in DMEM supplemented with 10% FBS (HyClone Laboratories, Logan, UT) and seeded overnight in culture media. Using the TransIT LT-1 transfection reagent (MirusBio, Madison, WI), HIV virions carrying free EGFP were generated by transfection with variable amounts of pNL4-3Eplasmid, pNL4-3E-MA-EGFP-CA plasmid and pcDNA3.1REC. In certain experiments, after a specified number of hours of incubation at 37C, the culture media (with transfection reagents) was removed and replaced with fresh culture media. The 37C incubation continued for an additional period. At the experimentallydetermined time point post transfection, the culture media, containing EGFPlabeled single-cycle virions was collected and filtered through a 0.45-mm syringe filter (Millex-HV PVDF, Millipore). The filtrate was then aliquoted on ice, flash21 frozen in a dry ice/ ethanol bath and stored in a -80C freezer. Subsequent analysis using p24 ELISA (HIV-1 p24 Antigen Capture Kit, Advanced Bioscience Laboratories, Rockville, MD) was conducted to determine the number of virus particles in a specified volume, assuming 2,500 molecules of p24 per virion. Calculating the Infectivity of free-EGFP HIV-1. Infection assay in TZM-bl cell line. The infectivity of virions was calculated by normalizing the virus titer (i.e., number of infectious units in a certain volume) to the physical number of virus particles present in the specified volume. The titer was be determined by using an established Î²-galactosidasebased infection assay. This assay relies upon the TZM-bl indicator cell line a genetically engineered HeLa-derived cell line that expresses CD4, CXCR4, and CCR5 [55] [56]. TZM-bl cells function as indicators of HIV infection because they also possess Luciferase and bacterial Î²-galactosidase reporter genes, which are driven by an HIV LTR promoter. This LTR promoter is induced to express the reporter enzymes following HIV infection when the viral protein Tat (Transactivator of transcription) is produced [57]. Thus, in theory, only infected cells will express the reporter genes. These infected cells can be detected by providing a chromogenic substrate for either of the reporter enzymes. For these studies, infected cells will be quantitated by using 5-bromo-4-chloro-3-indolyl-Î²D-galactopyranoside (X-gal) as a substrate for Î²-galactosidase. Î²galactosidase catalyzes the hydrolysis of X-gal and produces a blue-colored byproduct, 5,5'-dibromo-4,4'-dichloro-indigo. 22 The infectious titer of HIV-1 virions can therefore be determined by enumerating the number of blue TZM-bl cells following inoculation with various dilutions of the virus stock in accordance with the following equation: mL Units Infectious (mL) Volume Inoculum mL 1 Factor Dilution Cells TZMbl Blue # Titer ï€½ ï‚´ ï€½ TZM-bl cells (cat#8129, NIH AIDS Research and Reference Reagent Program) were cultured at 37C with 5% CO2 in DMEM supplemented with 10% FBS (HyClone Laboratories, Logan, UT). Prior to tenth passage, TZM-bl cells were trypsinized, counted, sedimented by centrifugation at 1,000g for 5 minutes and resuspended in DMEM with 10% FBS. 8X10 TZM-bl cells in a 1-ml culture volume were seeded in each well of a 12-well plate one day prior to infection. EGFP-labeled HIV-1 particles (obtained from stocks stored at -80C) were added to these pre-seeded aliquots of TZM-bl cells in 100 Î¼l of DMEM with 10% FBS and DEAE dextran (final concentration 20 mg/ml). The virion and TZM-bl cell mixture was incubated at 37C for 2hrs with gentle rocking every 30 min. At the end of two hours, 1 ml of complete media was added to each well and the incubation was continued at 37C for 48 hours with 5% CO2. After 2 days of incubation, cells were fixed in 2% gluteraldehyde at room temperature for five minutes. Cells were then washed three times with PBS, and stained for 50 min at 37C using cell staining solution provided in the beta-galactosidase staining kit (Mirus Bio, Madison, WI). Cells were washed three times with milliQ water and",2015,
"Sexual patterns in the labroid fishes of the Western Caribbean, II, the parrotfishes (Scaridae)","Warner, Robert R., and D. Ross Robertson. Sexual Patterns in the Labroid Fishes of the Western Caribbean, I: The Wrasses (Labridae). Smithsonian Contributions to Zoology, number 254, 27 pages, 11 figures, 7 tables, 1978.â€”We report here on the results of a two year investigation into the interactions of sex change, coloration, and mating behavior in nine Caribbean wrasses (Bodianus rufus, Halichoeres bivittatus, H. garnoti, H. maculipinna, H. pictus, H. poeyi, H. radiatus, Clepticus parrae, and Thalassoma bifasciatum). For each species, we outline (1) the distribution of sexual types according to size and coloration, (2) the testis weights of different types of males, (3) characteristic habitat and relative abundance, (4) the social and mating system, and (5) breeding seasonality. In all dichromatic labrid species, sex change from female to male appears to precede a change of coloration into the bright terminal phase. Two wrasses (B. rufus and C. parrae) that lack non-sex-changed (primary) males have mating systems in which terminal phase males can effectively control the spawning of the females. Other labrid species have lek-type mating systems, within which small primary males interfere in the spawnings of large terminal males. This activity is reflected in the high testis weight of these small males, who face sperm competition in mating. There is a general correlation between population density (measured by relative abundance) and frequency of primary males. OFFICIAL PUBLICATION DATE is handstamped in a limited number of initial copies and is recorded in the Institution's annual report, Smithsonian Year. SERIES COVER DESIGN: The coral Montastrea cavernosa (Linnaeus). Library of Congress Cataloging in Publication Data Warner, Robert R. Sexual patterns in the labroid fishes of the western Caribbean. (Smithsonian contributions to zoology ; no. 254) Bibliography: p.",1978,
Simultaneous Signal Subspace Rank and Model Selection with an Application to Single-snapshot Source Localization,"This paper proposes a novel method for model selection in linear regression by utilizing the solution path of $\ell_{1}$ regularized least-squares (LS) approach (i.e., Lasso). This method applies the complex-valued least angle regression and shrinkage (c-LARS) algorithm coupled with a generalized information criterion (GIC) and referred to as the c-LARS-GIC method. c-LARS-GIC is a two-stage procedure, where firstly precise values of the regularization parameter, called knots, at which a new predictor variable enters (or leaves) the active set are computed in the Lasso solution path. Active sets provide a nested sequence of regression models and G I C then selects the best model. The sparsity order of the chosen model serves as an estimate of the model order and the LS fit based only on the active set of the model provides an estimate of the regression parameter vector. We then consider a source localization problem, where the aim is to detect the number of impinging source waveforms at a sensor array as well to estimate their direction-of-arrivals (DoA-S $)$ using only a single-snapshot measurement. We illustrate via simulations that, after formulating the problem as a grid-based sparse signal reconstruction problem, the proposed c-LARS-GIC method detects the number of sources with high probability while at the same time it provides accurate estimates of source locations.",2018,2018 26th European Signal Processing Conference (EUSIPCO)
Improving the assessment of measurement invariance: Using regularization to select anchor items and identify differential item functioning.,"A common challenge in the behavioral sciences is evaluating measurement invariance, or whether the measurement properties of a scale are consistent for individuals from different groups. Measurement invariance fails when differential item functioning (DIF) exists, that is, when item responses relate to the latent variable differently across groups. To identify DIF in a scale, many data-driven procedures iteratively test for DIF one item at a time while assuming other items have no DIF. The DIF-free items are used to anchor the scale of the latent variable across groups, identifying the model. A major drawback to these iterative testing procedures is that they can fail to select the correct anchor items and identify true DIF, particularly when DIF is present in many items. We propose an alternative method for selecting anchors and identifying DIF. Namely, we use regularization, a machine learning technique that imposes a penalty function during estimation to remove parameters that have little impact on the fit of the model. We focus specifically here on a lasso penalty for group differences in the item parameters within the two-parameter logistic item response theory model. We compare lasso regularization with the more commonly used likelihood ratio test method in a 2-group DIF analysis. Simulation and empirical results show that when large amounts of DIF are present and sample sizes are large, lasso regularization has far better control of Type I error than the likelihood ratio test method with little decrement in power. This provides strong evidence that lasso regularization is a promising alternative for testing DIF and selecting anchors. (PsycINFO Database Record (c) 2020 APA, all rights reserved).",2020,Psychological methods
Purification and Characterization of Cell-associated Glucosyltransferase Synthesizing Insoluble Glucan from Streptococcus mutans,"Streptococcus mutans Ingbritt (serotype c) was shown to have a significant amount of cellassociated glucosyltransferase activity which synthesizes water-insoluble glucan from sucrose. The enzyme was extracted from the washed cells with SDS, renatured with Triton X-100, adsorbed to 1,3-a-~-glucan gel, and then eluted with SDS. The enzyme preparation was electrophoretically homogeneous, and the specific activity was 7-3 i.u. (mg protein)-'. The enzyme had an M, of 158000 as determined by SDS-PAGE, and was a strongly hydrophilic protein, as judged by its amino acid composition. The enzyme gradually aggregated in the absence of SDS. The enzyme had an optimum pH of 6.5 and a K , value of 16.3 mM for sucrose. Activity was stimulated 1.7-fold by dextran T10, but was not stimulated by high concentrations of ammonium sulphate. Below a sodium phosphate buffer concentration of 50 mM, activity was reduced by 75%. This enzyme synthesized an insoluble D-glucan consisting of 76 mol% 1,3-alinked glucose and 24 mol% 1,6-a-linked glucose.",2008,
A Novel Traffic Prediction System based on Floating Car Data and Machine Learning,"Intelligent Transportation Systems have become a necessity with the increasing number of cars running, especially in the urban roads. This paper presents a novel system capable to forecast the traffic in the urban road networks. This study aims to analyze the traffic patterns based on Floating Car Data using a supervised learning approach. The prediction of the mean travel time and the mean waiting time is used to evaluate the proposed method with the R-Square evaluation metric for Lasso and residuals. We also evaluate the prediction performance when we reduce the number of connected vehicles. Based on these evaluations, the proposed system can forecasts well the traffic conditions in the urban road networks with a good accuracy, only with 10% of connected vehicles. Such a system presents a technical means for the road managers to better understand the travel patterns and helps authorities to make a strategic decisions.",2019,
Degrees of freedom for off-the-grid sparse estimation,"A central question in modern machine learning and imaging sciences is to quantify the number of effective parameters of vastly over-parameterized models. The degrees of freedom is a mathematically convenient way to define this number of parameters. Its computation and properties are well understood when dealing with discretized linear models, possibly regularized using sparsity. In this paper, we argue that this way of thinking is plagued when dealing with models having very large parameter spaces. In this case it makes more sense to consider ""off-the-grid"" approaches, using a continuous parameter space. This type of approach is the one favoured when training multi-layer perceptrons, and is also becoming popular to solve super-resolution problems in imaging. Training these off-the-grid models with a sparsity inducing prior can be achieved by solving a convex optimization problem over the space of measures, which is often called the Beurling Lasso (Blasso), and is the continuous counterpart of the celebrated Lasso parameter selection method. In previous works, the degrees of freedom for the Lasso was shown to coincide with the size of the smallest solution support. Our main contribution is a proof of a continuous counterpart to this result for the Blasso. Our findings suggest that discretized methods actually vastly over-estimate the number of intrinsic continuous degrees of freedom. Our second contribution is a detailed study of the case of sampling Fourier coefficients in 1D, which corresponds to a super-resolution problem. We show that our formula for the degrees of freedom is valid outside of a set of measure zero of observations, which in turn justifies its use to compute an unbiased estimator of the prediction risk using the Stein Unbiased Risk Estimator (SURE).",2019,ArXiv
Discrimination of Pancreatic Serous Cystadenomas From Mucinous Cystadenomas With CT Textural Features: Based on Machine Learning,"Objectives: This study was designed to estimate the performance of textural features derived from contrast-enhanced CT in the differential diagnosis of pancreatic serous cystadenomas and pancreatic mucinous cystadenomas. Methods: Fifty-three patients with pancreatic serous cystadenoma and 25 patients with pancreatic mucinous cystadenoma were included. Textural parameters of the pancreatic neoplasms were extracted using the LIFEx software, and were analyzed using random forest and Least Absolute Shrinkage and Selection Operator (LASSO) methods. Patients were randomly divided into training and validation sets with a ratio of 4:1; random forest method was adopted to constructed a diagnostic prediction model. Scoring metrics included sensitivity, specificity, accuracy, and AUC. Results: Radiomics features extracted from contrast-enhanced CT were able to discriminate pancreatic mucinous cystadenomas from serous cystadenomas in both the training group (slice thickness of 2 mm, AUC 0.77, sensitivity 0.95, specificity 0.83, accuracy 0.85; slice thickness of 5 mm, AUC 0.72, sensitivity 0.90, specificity 0.84, accuracy 0.86) and the validation group (slice thickness of 2 mm, AUC 0.66, sensitivity 0.86, specificity 0.71, accuracy 0.74; slice thickness of 5 mm, AUC 0.75, sensitivity 0.85, specificity 0.83, accuracy 0.83). Conclusions: In conclusion, our study provided preliminary evidence that textural features derived from CT images were useful in differential diagnosis of pancreatic mucinous cystadenomas and serous cystadenomas, which may provide a non-invasive approach to determine whether surgery is needed in clinical practice. However, multicentre studies with larger sample size are needed to confirm these results.",2019,Frontiers in Oncology
Mean field analysis of sparse reconstruction with correlated variables,"Sparse reconstruction algorithms aim to retrieve high-dimensional sparse signals from a limited number of measurements. A common example is LASSO or Basis Pursuit where sparsity is enforced using an â„“<sub>1</sub>-penalty together with a cost function ||y - Hx||<sup>2</sup><sub>2</sub>. For random design matrices H, a sharp phase transition boundary separates the `good' parameter region where error-free recovery of a sufficiently sparse signal is possible and a `bad' regime where the recovery fails. However, theoretical analysis of phase transition boundary of the correlated variables case lags behind that of uncorrelated variables. Here we use replica trick from statistical physics to show that when an N-dimensional signal x is K-sparse and H is M Ã— N dimensional with the covariance E[H<sub>ia</sub>H<sub>jb</sub>] = 1/M C<sub>ij</sub>D<sub>ab</sub>, with all D<sub>aa</sub> = 1, the perfect recovery occurs at M ~ Ïˆ<sub>K</sub> (D) K log(N/M) in the very sparse limit, where Ïˆ<sub>K</sub> (D) â‰¥ 1, indicating need for more observations for the same degree of sparsity.",2016,2016 24th European Signal Processing Conference (EUSIPCO)
Transcription Factor Profiling to Predict Recurrence-Free Survival in Breast Cancer: Development and Validation of a Nomogram to Optimize Clinical Management,"Breast cancer (BC) is the most frequently diagnosed cancer and the leading cause of cancer-related death in young women. Several prognostic and predictive transcription factor (TF) markers have been reported for BC; however, they are inconsistent due to small datasets, the heterogeneity of BC, and variation in data pre-processing approaches. This study aimed to identify an effective predictive TF signature for the prognosis of patients with BC. We analyzed the TF data of 868 patients with BC in The Cancer Genome Atlas (TCGA) database to investigate TF biomarkers relevant to recurrence-free survival (RFS). These patients were separated into training and internal validation datasets, with GSE2034 and GSE42568 used as external validation sets. A nine-TF signature was identified as crucially related to the RFS of patients with BC by univariate Cox proportional hazard analysis, least absolute shrinkage and selection operator (LASSO) Cox regression analysis, and multivariate Cox proportional hazard analysis in the training dataset. Kaplanâ€“Meier analysis revealed that the nine-TF signature could significantly distinguish high- and low-risk patients in both the internal validation dataset and the two external validation sets. Receiver operating characteristic (ROC) analysis further verified that the nine-TF signature showed a good performance for predicting the RFS of patients with BC. In addition, we developed a nomogram based on risk score and lymph node status, with C-index, ROC, and calibration plot analysis, suggesting that it displays good performance and clinical value. In summary, we used integrated bioinformatics approaches to identify an effective predictive nine-TF signature which may be a potential biomarker for BC prognosis.",2020,
Distributionally Robust Semi-supervised Learning,"We propose a novel method for semi-supervised learning based on data-driven distributionally robust optimization (DRO) using optimal transport metrics. Our proposed method enhances generalization error by using the non-labeled data to restrict the support of the worst case distribution in our DRO formulation. We enable the implementation of the DRO formulation by proposing a stochastic gradient descent algorithm which allows to easily implement the training procedure. We demonstrate the improvement in generalization error in semi-supervised extensions of regularized logistic regression and square-root LASSO. Finally, we include a discussion on the large sample behavior of the optimal uncertainty region in the DRO formulation. Our discussion exposes important aspects such as the role of dimension reduction in semi-supervised learning.",2017,arXiv: Machine Learning
The 2012 International Vocabulary of Metrology: â€œVIMâ€,"One of the most important events in the last decade for thefuture of measurement is without any doubt the thirdedition of the International Vocabulary of Metrology,commonly called â€˜â€˜VIMâ€™â€™ (from the French title â€˜â€˜Vocab-ulaire International de MeÂ´trologieâ€™â€™) [1]. It was developedin Working Group 2 (WG 2) of the Joint Committee forGuides on Metrology (JCGM) that consists of:â€¢ BIPM, International Bureau for Weights and Measures,â€¢ IEC, International Electrotechnical Committee,â€¢ IFCC, International Federation for Clinical Chemistryand Laboratory Medicine,â€¢ ILAC, International Laboratory AccreditationCooperation,â€¢ ISO, International Organization for Standardization,â€¢ IUPAC, International Union for Pure and AppliedChemistry,â€¢ IUPAP, International Unionfor Pure and Applied Physics,â€¢ OIML, International Organization for Legal Metrology.The formal structure of the membership was intended toguarantee that the resulting â€˜â€˜Guides for Metrologyâ€™â€™ VIMand GUM (guide to the expression of uncertainty in mea-surement) [2] would be formally examined, approved, andformally backed up by international organizations andtherefore be internationally representative. It was alsoexpected that the presence of international professionalassociations such as IEC, IFCC, IUPAC, and IUPAP wouldpromote its implementation on the worldwide scene.The ï¬rst VIM sometimes called â€˜â€˜VIM 1,â€™â€™ was releasedin 1984 [3]. The second edition, sometimes called â€˜â€˜VIM2,â€™â€™ was published in 1993/1995 [4]. The VIM that is nowavailable is the third version, called â€˜â€˜VIM 3â€™â€™. It wasreleased in 2008 and has been re-issued with editorialcorrections on 16 February 2012 [1].VIM 1 and VIM 2 were mainly conceived by physicistsand engineers for measurements in physics and engineer-ing. Chemical measurement was considered to some degreein VIM 2, thanks to the presence of an IFCC representativeand by the fact that clinical chemistryâ€”possibly throughits active Clinical Chemistry Division in the IUPACâ€”hadalready made considerable progress in the introduction ofmetrological principles in clinical measurement. In general,the growth of more metrological insight in â€˜â€˜measurementâ€™â€™in chemistry evolved considerably in the period 1970â€“2010and is still in full development [5].A ï¬rst feature of VIM 3 is the change of title: â€˜â€˜Inter-national Vocabulary of Metrologyâ€”Basic and generalconcepts and associated termsâ€™â€™ whereas the title of VIM 1and VIM 2 was very different: â€˜â€˜International vocabulary ofbasic and general terms in metrologyâ€™â€™ (emphasis by me).Stressing that concepts are to be deï¬ned, and not terms, isone of the basic clariï¬cations by VIM 3. Without com-monly deï¬ned concepts, there is no possibility of validlytranslating the term associated with this concept from onelanguage into a similarly understood term in another lan-guage. That had to be a justiï¬cation in its own right for anysuccessor of VIM 2. Introducing this in VIM 3 requiredconsiderable study and discussions.A second feature of VIM 3 is the concept â€˜â€˜measurementuncertaintyâ€™â€™ (entry 2.26 in [1]) [2]. That change of think-ing was formally initiated by the International Committee",2012,Accreditation and Quality Assurance
Boosted Network Classifiers for Local Feature Selection,"Like all models, network feature selection models require that assumptions be made on the size and structure of the desired features. The most common assumption is sparsity, where only a small section of the entire network is thought to produce a specific phenomenon. The sparsity assumption is enforced through regularized models, such as the lasso. However, assuming sparsity may be inappropriate for many real-world networks, which possess highly correlated modules. In this paper, we illustrate two novel optimization strategies, namely, boosted expectation propagation (BEP) and boosted message passing (BMP), which directly use the network structure to estimate the parameters of a network classifier. BEP and BMP are ensemble methods that seek to optimize classification performance by combining individual models built upon local network features. Neither BEP nor BMP assumes a sparse solution, but instead they seek a weighted average of all network features where the weights are used to emphasize all features that are useful for classification. In this paper, we compare BEP and BMP with network-regularized logistic regression models on simulated and real biological networks. The results show that, where highly correlated network structure exists, assuming sparsity adversely effects the accuracy and feature selection power of the network classifier.",2012,IEEE Transactions on Neural Networks and Learning Systems
Topological Models for Open-Knotted Protein Chains Using the Concepts of Knotoids and Bonded Knotoids,"In this paper we introduce a method that offers a detailed overview of the entanglement of an open protein chain. Further, we present a purely topological model for classifying open protein chains by also taking into account any bridge involving the backbone. To this end, we implemented the concepts of planar knotoids and bonded knotoids. We show that the planar knotoids technique provides more refined information regarding the knottedness of a protein when compared to established methods in the literature. Moreover, we demonstrate that our topological model for bonded proteins is robust enough to distinguish all types of lassos in proteins.",2017,Polymers
Computer-Intensive Statistics: A Promising Interplay between Statistics and Computer Science,"Editorial Statistics and computer science have grown as separate disciplines with little interaction for the past several decades. This however, has changed radically in recent years with the availability of massive and complex datasets in medicine, social media, and physical sciences. The statistical techniques developed for regular datasets simply cannot be scaled to meet the challenges of big data, notably the computational and statistical curses of dimensionality. The dire need to meet the challenges of big data has led to the development of statistical learning, machine learning and deep learning techniques. Rapid improvements in the speed and lower costs of statistical computation in recent years have freed statistical theory from its two serious limitations: the widespread assumption that the data follow the bell-shaped curve and exclusive focus on measures, such as mean, standard deviation, and correlation whose properties could be analyzed mathematically [1]. Computer-intensive statistical techniques have freed practical applications from the constraints of mathematical tractability and today can deal with most problems without the restrictive assumption of Gaussian distribution. These methods can be classified into frequentist and Bayesian methods. The former methods utilize the sample information only while the latter methods utilize both the sample and prior information. Frequentist statistical methods have benefitted enormously from the interaction of statistics with computer science. A very popular computer-intensive method is the bootstrap for estimating the statistical accuracy of a measure, such as correlation in a single sample. The procedure involves generating a very large number of samples with replacement from the original sample. Bootstrap as a measure of statistical accuracy has been shown to be extremely reliable in theoretical research [2,3]. Another widely used computer-intensive method for measuring the accuracy of statistical methods is cross validation. It works non-parametrically without the need for probabilistic modelling and measures the mean-squared-error for the test sample using the training sample to evaluate the performance of various machine learning methods for selecting the best method. Other frequentist statistical methods that rely on a powerful computing environment include jackknife for estimating bias and variance of an estimator, classification and regression trees for prediction, generalized linear models for parametric modelling with continuous, discrete or count response [4], generalized additive models for flexible semi-parametric regression modeling [5], the LASSO method for Cox proportional hazard regression in high dimensional settings [6], and EM algorithm [7] for finding iteratively the maximum likelihood or maximum a posteriori (MAP) estimates of parameters in complex statistical models with latent variables, alternating between performing an expectation (E) step, which evaluates the expectation of the log-likelihood evaluated using the current estimate for the parameters, and a maximization (M) step, which computes parameters maximizing the expected log-likelihood found on the E step. Bagging, random forests, and boosting [8,9] are some relatively recent developments in machine learning which use large amounts of data to fit a very rich class of functions to the data almost automatically. These methods represent a fitted model as a sum of regression trees. A regression tree by itself is a fairly weak prediction model, so these methods greatly improve prediction performance by constructing ensembles of either deep trees under random forests or shallow trees under boosting. Support vector machine (SVM) [10], an approach for linear and nonlinear classification developed in computer science, has been found to perform very well and is widely used by statisticians and data scientists. Neural networks are a class of learning methods developed separately in statistics and artificial intelligence, which use a computer-based model of human brain to perform complex tasks. These methods have found applications across several disciplines, including medicine, geosciences, hydrology, engineering, business, and economics. Some common statistical models, such as multiple linear regression, logistic regression, and linear discriminant analysis for classifying binary response are akin to neural networks. The main idea underlying these methods is to extract linear combinations of inputs as derived features and model the output as a nonlinear function of these features called the activation function. Bayesian statistical methods have also benefited greatly from computer-intensive methods, notably the Markov Chain Monte Carlo (MCMC) approach [11], which is a class Article Information",2018,
Studies of the adaptive network-constrained linear regression and its application,"The network-constrained criterion is one of the fundamental variable selection models for high-dimensional data with correlated features. It is distinguished from others in that it can select features and simultaneously encourage global smoothness of the coefficients over the network via penalizing the weighted sum of squares of the scaled difference of the coefficients between neighbor vertices. However, because more features were selected while it was applied for the process of analysis of the ""China Stock Market Financial Database-Financial?Ratios"", the so-called adaptive network-constrained criterion was proposed to tackle the problem via assigning various weights to the lasso penalty. Similar to the adaptive lasso, the proposed model enjoys consistency in variable selection if the weights have been given correctly in advance. The simulations show that the proposed model performed better than the other variable selection techniques mentioned in the paper with regards to model fitting; meanwhile, it selected fewer features than the network-constrained criterion. Furthermore, the mean value of the cross-validation likelihood and the number of selected features are tested to be accurate enough for practical applications.",2015,Comput. Stat. Data Anal.
gLOP: A Cleaner Dirty Model for Multitask Learning,"Multitask learning (MTL) was originally defined by Caruana (1997) as â€œan approach to inductive transfer that improves learning for one task by using the information contained in the training signals of other related tasksâ€. In the linear model setting this is often realized as joint feature selection across tasks, where features (but not necessarily coefficient values) are shared across tasks. In later work related to MTL Jalali (2010) observed that sharing all features across all tasks is too restrictive in some cases, as commonly used composite absolute penalties (like the `1,âˆž norm) encourage not only common feature selection but also common parameter values between settings. Because of this, Jalali proposed an alternative â€œdirty modelâ€ that can leverage shared features even in the case where not all features are shared across settings. The dirty model decomposes the coefficient matrix Î˜ into a row-sparse matrix B and an elementwise sparse matrix S in order to better capture structural differences between tasks. Multitask learning problems arise in many contexts, and one of the most pertinent of these is healthcare applications in which we must use data from multiple patients to learn a common predictive model. Often it is impossible to gather enough data from any one patient to accurately train a full predictive model for that patient. Additionally, learning in this context is complicated by the presence of individual differences between patients as well as population-wide effects common to most patients, leading to the need for a dirty model. Two additional challenges for methods applied in the healthcare setting include the need for scalability so that the model can work with big data, and the need for interpretable models. While Jalali gives us a dirty model, this method does not scale as well as many other commonly used methods like the Lasso, and does not have a clean interpretation. This is particularly true in the healthcare domain, as this model does not allow us to interpret coefficients in relation to all settings. Because B coefficients in the dirty model paradigm are not required to be the same for all settings for a particular feature, departures from the global model may be captured in B or S leading to ambiguity in interpreting potential main effects. We propose a â€œcleanerâ€ dirty model gLOP (global/LOcal Penalty) that is capable of representing global effects between settings as well as local setting-specific effects, much like the ANalysis Of VAriance (ANOVA) test in inferential statistics. However, the goal of the ANOVA is not to build an accurate predictive model, but to identify coefficients that are non-zero at a given level of statistical significance. By combining the dirty modelâ€™s decomposed Î˜ matrix and the underlying concept behind the ANOVA, we get the best of both worlds: an interpretable predictive model that can accurately recover the underlying structure of a given problem. gLOP is structured as a coordinate minimization problem",2014,
Application of deep learning in genomic selection,"Genomic selection (GS) is a marker-assisted selection approach to enhance quantitative traits in breeding population in which whole genome single-nucleotide polymorphisms (SNPs) markers can be used to predict breeding values (BV). GS has been proved to increase breeding efficiency in both plant and animal breeding, such as dairy cattle, pig, rice, soybean and loblolly pine. Here, we propose a deep-learning model using convolutional neural network (CNN) to predict genomic estimated breeding value (GEBV) and also to investigate neighboring SNP effects within linkage disequilibrium. We have applied our models on two datasets: 1) grain yield (YLD) trait on Glycine Max (soybean) nested association mapping (NAM) dataset and 2) stem height (HT) trait on a Pinus taeda (loblolly pine) dataset. The SoyNAM population contains 4,313 SNPs from 5,139 individuals with a trait heritability of 0.345 and the Loblolly Pine population contains 4,853 SNPs from 861 individuals with a trait heritability of 0.31. Our deep-learning model was tested with a 10-fold cross-validation, run in parallel on graphic processing units (GPUs). Our model prediction accuracy, which was calculated by Pearson Correlation between GEBV and observed values, outperforms traditional statistical RR-BLUP, Bayesian LASSO and BayesA models. The results indicate that deep-learning model is efficient in accurately computing breeding values and simultaneously studying nearby SNP effects from CNNs. It also indicates powerful potential in interpreting phenotype-genotype associations over the entire genome.",2017,2017 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)
MATHEMATICAL ENGINEERING TECHNICAL REPORTS A Structural Model on a Hypercube Represented by Optimal Transport,"We propose a flexible statistical model for high-dimensional quantitative data on a hypercube. Our model, called the structural gradient model (SGM), is based on a one-to-one map on the hypercube that is a solution for an optimal transport problem. As we show with many examples, SGM can describe various dependence structures including correlation and heteroscedasticity. The maximum likelihood estimation of SGM is effectively solved by the determinant-maximization programming. In particular, a lasso-type estimation is available by adding constraints. SGM is compared with graphical Gaussian models and mixture models.",2009,
Multilevel Modeling with Structured Penalties for Classification from Imaging Genetics Data,"In this paper, we propose a framework for automatic classification of patients from multimodal genetic and brain imaging data by optimally combining them. Additive models with unadapted penalties (such as the classical group lasso penalty or \(\ell _1\)-multiple kernel learning) treat all modalities in the same manner and can result in undesirable elimination of specific modalities when their contributions are unbalanced. To overcome this limitation, we introduce a multilevel model that combines imaging and genetics and that considers joint effects between these two modalities for diagnosis prediction. Furthermore, we propose a framework allowing to combine several penalties taking into account the structure of the different types of data, such as a group lasso penalty over the genetic modality and a \(\ell _2\)-penalty on imaging modalities. Finally, we propose a fast optimization algorithm, based on a proximal gradient method. The model has been evaluated on genetic (single nucleotide polymorphisms - SNP) and imaging (anatomical MRI measures) data from the ADNI database, and compared to additive models [13, 15]. It exhibits good performances in AD diagnosis; and at the same time, reveals relationships between genes, brain regions and the disease status.",2017,
Weighted Robust Lasso and Adaptive Elastic Net Method for Regularization and Variable Selection in Robust Regression with Optimal Scaling Transformations,"In this paper, the weight least absolute deviation adaptive lasso optimal scaling method (WLAD-CATREG adaptive lasso) and weight least absolute deviation adaptive elastic net regression with optimal scaling method (WLAD-CATREG adoptive elastic net) will introduced, which is combined of weight least absolute deviation regression (WLAD-CATREG) and adaptive lasso (A-Lasso) or adaptive elastic net regression (A-Elastic net) with optimal scaling. Thus (WLAD-CATREG adoptive elastic net) method aim to automatically select variable, aspire to gropes effect and erase the bad effect of leverage points and outliers simultaneously, these aims cannot be achieved by (WLAD-CATREG), adaptive lasso regression (A-Lasso), weight robust adaptive lasso regression (WLAD-CATREG adoptive lasso), Weight least absolute deviation elastic net regression (WLAD-CATREG elastic net). Simulation study will be running to validated superiority of the (WLAD-CATREG adoptive Lasso) and (WLAD-CATREG adoptive elastic net).",2017,American Journal of Mathematics and Statistics
[Quality of care and sexually transmitted infections algorithm acceptability in Burkina Faso].,"BACKGROUND
To assess sexually-transmitted infections (STIs) quality of care, syndromic approach acceptability and applicability by patients and health workers in Burkina Faso.


METHOD
Three approaches were used: simulated patients method to assess quality of STIs care, patients interview and focus discussion with health workers to assess syndromic approach acceptability and applicability.


RESULTS
Sixty-two anonymous visits were made in 17 Bobo-Dioulasso primary care clinics. Overall, history taking were assessed in 77.4% of visits, 47% patients were physically examined. Women (71%) were examined more frequently than men (41%) (P=0.01), 42% of patients were not examined in an isolated room. Medication was prescribed for 87.1% of the patients but only 37.5% of the treatments were applied according to national recommendations. Counselling was poor concerning critical messages regarding risk of HIV transmission, STI prevention. Patients and health workers found the syndromic approach acceptable and applicable, but the question of sexual behaviour was considered difficult to address.


CONCLUSION
Quality of STIs care is poor in Burkna Faso. Staff training must emphasize interpersonal communication and motivation, with introduction of a sexually-transmitted infection syndrome package consisting of drugs and condoms in order to improve syndromic case management.",2003,Revue d'epidemiologie et de sante publique
Photoshop Elements 5: The Missing Manual,"Foreword The Missing Credits Introduction Part One: Introductory Elements Chapter 1. Finding Your Way Around Elements The Welcome Screen Organizing Your Photos Photo Downloader Editing Your Photos Your Elements Tools Bins and Palettes Getting Help and the How To Palette Escape Routes Getting Started in a Hurry Chapter 2. Importing, Managing, and Saving Your Photos Importing from Cameras The Photo Downloader Opening Stored Images Working with PDF Files Scanning Photos Capturing Video Frames Creating a New File Picking a Document Size Choosing Resolution Choosing Color Mode Choosing Your File's Contents Using the Organizer The Photo Browser Creating Categories and Tags Creating Collections Searching for Photos Searching by Metadata Saving Your Work The File Formats Elements Understands Changing the File Format Backing Up Your Files Saving to CDs and DVDs Organizer Backups Chapter 3. Rotating and Resizing Your Photos Straightening Scanned Photos Straightening Two or More Photos at a Time Straightening Individual Photos Rotating Your Images Rotating and Flipping Options Straightening the Contents of Your Image Straighten Tool Free Rotate Cropping Pictures Using the Crop Tool Cropping Your Image to an Exact Size Cropping with the Marquee Tool Zooming and Repositioning Your View Image Views The Zoom Tool The Hand Tool Changing the Size of Your Image Resizing Images for Email and the Web Resizing for Printing Adding Canvas Chapter 4. The Quick Fix The Quick Fix Window The Quick Fix Toolbox The Quick Fix Control Panel Different Views: After vs. Before and After Editing Your Photos Fixing Red Eye Smart Fix Adjusting Lighting and Contrast Color Sharpening Quick Fix Suggested Workflow Adjusting Skin Tones Part Two: Elemental Elements Chapter 5. Making Selections Making Quick Selections Selecting Rectangular and Elliptical Areas Selecting Irregularly Sized Areas Controlling the Selection Tools The Magic Wand The Lasso Tools Selecting with a Brush The Magic Selection Brush The Selection Brush Selecting Objects from an Image's Background Changing and Moving Selections Inverting a Selection Making a Selection Larger or Smaller Moving Selections Saving Selections Chapter 6. Layers: The Heart of Elements Understanding Layers The Layers Palette The Background Creating Layers Adding a Layer Deleting Layers Duplicating a Layer Copying and Cutting from Layers Managing Layers Making Layers Invisible Adjusting Transparency Locking Layers Blend Mode Rearranging Layers Aligning and Distributing Layers Grouping and Linking Layers Merging and Flattening Layers Fill and Adjustment Layers Adding Fill and Adjustment Layers Layer Masks Moving Layers Between Images Part Three: Retouching Chapter 7. Basic Image Retouching Fixing Exposure Problems Deciding Which Exposure Fix to Use Fixing Major Exposure Problems The Shadows/Highlights Command Controlling the Colors You See Calibrating Your Monitor Choosing a Color Space Using Levels Understanding the Histogram Adjusting Levels: The Eyedropper Method Adjusting Levels: The Slider Controls Removing Unwanted Color Using the Color Cast Tool Using Color Variations Choosing the Color You Want The Color Picker The Eyedropper Tool The Color Swatches Palette Sharpening Your Images Unsharp Mask Adjust Sharpness The High-Pass Filter The Sharpen Tool Chapter 8. Elements for Digital Photographers The RAW Converter Using the RAW Converter White Balance Adjusting Exposure, Shadows, Brightness, Contrast, and Saturation Adjusting Sharpness and Reducing Noise Converting to DNG Photo Filter Processing Multiple Files Choose Your Files File Renaming Changing Image Size and File Type Applying Quick Fix Commands Attaching Labels Chapter 9. Retouching 102: Fine-Tuning Your Images Fixing Blemishes The Spot Healing Brush: Fixing Small Areas The Healing Brush: Fixing Larger Areas The Clone Stamp Applying Patterns The Healing Brush The Pattern Stamp Curves: Enhancing Tone and Contrast Making Your Colors More Vibrant Using the Hue/Saturation Dialog Box Adjusting Saturation with the Sponge Tool Changing the Color of an Object Using an Adjustment Layer Replacing Specific Colors Using a Brush to Replace Colors Special Effects Chapter 10. Removing and Adding Color Method One: Making Color Photos Black and White Method Two: Removing Color from a Photo Creating Spot Color Erasing Colors from a Duplicate Layer Removing Color from Selections Using a Layer Mask and the Saturation Slider Colorizing a Black-and-White Photo Tinting an Entire Photo Chapter 11. Creating Panoramas and Correcting Perspective Creating Panoramas Selecting Files and Merging Them Adjusting Your Photos Fine-Tuning Your Panorama Finishing Up: Creating Your Panorama Correcting Lens Distortion Transforming Images Skew, Distort, Perspective Free Transform Part Four: Artistic Elements Chapter 12. Drawing with Brushes, Shapes, and Other Tools Picking and Using a Basic Brush Modifying Your Brush Saving Modified Brush Settings The Specialty Brushes Making a Custom Brush The Impressionist Brush The Pencil Tool The Paint Bucket Dodging and Burning Dodging Burning Blending and Smudging Blend Modes The Smudge Tool The Eraser Tool Using the Eraser The Magic Eraser The Background Eraser Drawing with Shapes Rectangle and Rounded Rectangle Ellipse Polygon Line Tool The Custom Shape Tool The Shape Selection Tool The Cookie Cutter Chapter 13. Filters, Effects, Layer Styles, and Gradients Using Filters Applying Filters Filter Categories Useful Filter Solutions Adding Effects Adding Layer Styles Applying Gradients The Gradient Tool Gradient Fill Layer Editing a Gradient Saving Gradients Gradient Maps Chapter 14. Type in Elements Adding Type to an Image Type Options Creating Text Editing Type Warping Type The Warp Text Dialog Box Adding Special Effects Text Effects Type Gradients Applying the Liquify Filter to Type Type Masks: Setting an Image in Type Using the Type Mask Tools Creating Outlined Type Part Five: Sharing Your Images Chapter 15. Creating Projects Photo Layouts Creating Multi-Page Documents Adding Favorites to the Artwork and Effects Palette Photo Book Pages Album Pages Greeting Cards CD Jacket DVD Jacket CD/DVD Label Online Creations Chapter 16. Printing Your Photos Getting Ready to Print Ordering Prints Ordering Prints Online Printing at Home (The Editor) Page Setup Print Preview Printing at Home (The Organizer) Printing Multiple Photos Contact Sheets Picture Package Labels Chapter 17. Elements and the Web Image Formats and the Web Saving Images for the Web or Email Using Save For Web Previewing Images and Adjusting Color Creating Animated GIFs Emailing Your Photos Emailing Images Sending Photos to Other Gear Sharing Photos with Yahoo Maps Chapter 18. Photo Galleries, Slideshows, and Flipbooks Photo Galleries Creating a Photo Gallery Slideshows Simple PDF Slideshow Custom Slide Show Flipbooks Part Six: Additional Elements Chapter 19. Beyond the Basics Graphics Tablets Free Stuff from the Internet When You Really Need Photoshop Beyond This Book Part Seven: Appendixes Appendix A. The Organizer, Menu by Menu Appendix B. The Editor, Menu by Menu Appendix C. Installation and Troubleshooting Index",2006,
Bayesian Non--Negative Regularised Regresssion,"This paper proposes a novel Bayesian approach to the problem of variable selection and shrinkage in high dimensional sparse nonâ€“negative linear regression models. The regularisation method is an extension of the LASSO which has been re- cently cast in a Bayesian framework by Park and Casella (2008). Moreover, to deal with the additional problem of variable selection we propose a Stochastic Search Variable Selection (SSVS) method that relies on a dirac spikâ€“andâ€“slab prior where the slab component induces the sparse nonâ€“negative regularisation. The methodol- ogy is then applied to the problem of passive index tracking of large dimensional index in stock markets without short sales.",2017,
Establishment of a Genomic-Clinicopathologic Nomogram for Predicting Early Recurrence of Hepatocellular Carcinoma After R0 Resection,"A high rate of postoperative recurrence, especially early recurrence (ER) occurring within 1 year, seriously impedes patients with hepatocellular carcinoma (HCC) from achieving long-term survival. This study aimed to establish a genomic-clinicopathologic nomogram for precisely predicting ER in HCC patients after R0 resection. Two reliable datasets from The Cancer Genome Atlas (TCGA) and the Gene Expression Omnibus (GEO) databases were selected as the training and validation cohorts, respectively. The prognostic genes related to ER were screened out by univariate Cox regression analysis and differential expression analysis. The gene-based prognostic index was constructed using LASSO and Cox regression analyses, and its independent prognostic value was assessed by Kaplan-Meier and multivariate Cox analyses. Gene set enrichment analysis (GSEA) was performed to explore the biological pathways related to the prognostic index. Finally, the nomogram integrating all the independent prognostic factors was established and comprehensively evaluated by calibration plots, the C-index, receiver operating characteristic curves, and decision curve analysis. Nine dysregulated and prognostic genes related to ER (ZNF131, TATDN2, TXN, DDX55, KPNA2, ZNF30, TIMELESS, SFRP1, and COLEC11) were identified (all P <â€‰0.05). The prognostic index model based on the 9 genes was successfully constructed using the TCGA cohort and showed a certain capability to discriminate the ER group from the non-ER group (P <â€‰0.05) and good independent prognostic value in terms of predicting poor early recurrence-free survival (P <â€‰0.05). Eight biological pathways significantly related to ER were identified by GSEA, such as â€œcell cycleâ€, â€œhomologous recombinationâ€ and â€œp53 signaling pathway.â€ The genomic-clinicopathologic nomogram integrating the 9-gene-based prognostic index and TNM stage displayed significantly higher predictive accuracy and clinical application value than that of TNM stage model both in the training and validation cohorts (all P <â€‰0.05). The novel genomic-clinicopathologic nomogram may be a convenient and powerful tool for accurately predicting ER in HCC patients after R0 resection.",2020,Journal of Gastrointestinal Surgery
Stability of model selection for high-dimensional data,"The analysis of data generated by high throughput technologies such as DNA microarrays has markedly renewed the statistical methodology for multiple testing and feature selection in regression or classification issues. Such data are characterized by both their high-dimension, as the number of measured features is close to several thousands whereas the sample size is about some tens, and their heterogeneity, as the true signal and several confusing factors (uncontrolled and unobserved) are often observed at the same time. In such a framework, the usual statistical approaches are questioned and can lead to misleading decisions for example. Some recent papers (Efron 2007, Leek and Storey 2007 and 2008; Friguet et al, 2009 ) have focused on the negative impact of data heterogeneity on the consistency of the ranking which results from multiple testing procedures. This presentation aims at showing that data heterogeneity also a effects the stability of supervised classification model selection which is often used to identify relevant subsets of features. Key characteristics of selection methods are both classification or prediction performance and reproducibility of the selected variables to perturbation in the data. It is first shown that selected subsets using well-known procedures such as LASSO (Tibshirani, 1996) are subject to a high variability. The stability of this selection method is compared through a simulation study, considering several scenario of dependence between variables: independence, block dependence, factor structure and Toeplitz design (as also considered in Meinshausen and Buhlmann, 2010). Simulation studies show that most usual methods do not select theoretical best predictors and that interesting performances of classification are performed only when a high number of variables are selected. As suggested in Friguet et al. (2009), a supervised factor model is proposed to identify a low-dimensional linear kernel which captures data dependence and new strategies for model selection are deduced. This new strategy is finally shown to improve stability of the usual methods. Indeed, interesting performances of classification are reached for a smaller number of selected variables and best theoretical predictors are more often selected for structures with a high degree of dependence.",2013,
The Royal Navy in the Pacific Islands,"THE STATUS AND PRESTIGE OF 19TH CENTURY BRITAIN WERE FIRMLY BASED UPON the command of sea power; as the greatest thalassocracy the world had yet seen Britain was able to impose upon that world her concepts of Free Trade and the Pax Britannica. The fact, and, sometimes, the fiction of her maritime supremacy deeply influenced the lives of vast numbers of humanity, both civilized and primitive. French admirals, colonial governors, island chiefs and African slaves were equally aware of the presence of that power which, according to their condition, they welcomed or feared but which they could never ignore. Indeed, the influence of the Royal Navy is everywhere to be seen but particularly in the development of Australian society, whose social and political concepts were in the main fashioned in an atmosphere of security from external threat that was the gift of English sea power. Few societies have been so privileged as to be able to concentrate their energies and re sources almost entirely upon the problems of peaceful internal development without the overwhelming distraction and expense of self defence. The purpose of this article is to look briefly at one aspect of the so called Pax Britannica at close quarters, as shown in the attitudes and behaviour of Royal Navy captains stationed in the waters of the Pacific Ocean where, during the middle 50 years of the last century, they were usually the sole representatives of English power. When seen against the great philosophical debates of the century, the actions of such relatively junior officers in a remote ocean might seem naive and unsophisticated, and the moral principles that motivated them either jejune or irrelevant. Yet these officers were also men of their time and their beliefs and decisions often reflected the deep seated convictions of their society. Indeed the very isolation they suffered, as a consequence of distance and slow communications, lends a special significance to their behaviour. Invested with a substantial part of the imperium of the world's greatest power these commanders, faced with the inseparable nature of power and responsibility, were often called upon to meet philosophical and moral dilemmas without any clear guidance from others. The more remote his station and the deeper his comprehension of the issues confronting him, the less probable was it that an officer would find a complete answer, yet answers he was compelled to 3",1968,Journal of Pacific History
ICT per il Cultural Heritage: possibili interazioni con SITAR.,"The advanced Information and Communication Technologies, combined with the development of applications based on artificial intelligence, open new possibilities to investigate in depth the Cultural Heritage (CH). The main objective of this process is to promote the integrated knowledge of CH within its context, so that it becomes a factor of growth in the cultural, social and economic system, in specific geographical areas. Through GRID computing it is possible the direct access by web to distributed databases, creating a network of different archives. Also a virtual reality reconstruction of areas and ontologies, additional capabilities designed to support intelligent fruition and multilingualism, for some time extensively are investigated with significant results. The archaeological heritage has been the subject of investigation and study at ENEA-UTICT in a number of project activities. This contribution intends to propose conceptual and methodological reflections for a fruitful interaction with SITAR. 1. INTRODUZIONE Favorire la conoscenza integrata del bene e del suo contesto affinche diventi un fattore di crescita culturale, sociale ed economica del sistema territoriale in cui si colloca, e uno dei principali obiettivi per chi opera nellâ€™Information and Communication Technology (ICT) e applica tali tecnologie ai Beni Culturali. ENEA e impegnata da oltre venti anni in attivita volte alla conoscenza, conservazione, fruizione e valorizzazione del Patrimonio Culturale del Paese (patrimonio culturale.enea.it). In tale lasso di tempo i ricercatori ENEA, operando a fianco degli esperti del settore dei Beni Culturali, al mondo della ricerca e al tessuto produttivo delle imprese, hanno messo a frutto un processo di adattamento, orientamento ed ampliamento delle proprie conoscenze tecnologiche per meglio rispondere alle specifiche richieste che sempre piu spesso vedono il settore dei Beni e delle Attivita Culturali al centro dello sviluppo economico sostenibile di territori e comunita. Tra le piu significative tecnologie e metodologie, che ENEA ha sviluppato negli ambiti istituzionali e che hanno ricaduta e applicazione nel settore dei Beni Culturali, si possono evidenziare: le tecniche di diagnostica avanzata, di caratterizzazione morfologica e strutturale dei materiali, di protezione sismica, il monitoraggio e lâ€™analisi di dati microclimatici, la modellazione e la ricostruzione virtuale, le applicazioni di intelligenza artificiale. In particolare, lâ€™Unita Tecnica â€œSviluppo Sistemi per lâ€™Informatica e lâ€™ICTâ€ (ENEA-UTICT), con le sue competenze e sistemi innovativi, consente agli esperti del patrimonio culturale di approfondire la conoscenza e la conservazione di un bene, aprendo nuove possibilita di indagine (Migliori et al. 2012). La creazione di reti di archivi complementari, attraverso GRID computazionale, permette lâ€™accesso diretto su web a basi di dati distribuite, che raccolgono informazioni sui materiali costitutivi dei beni culturali e rendono possibili, con lâ€™ausilio della modellazione, della simulazione e di ricostruzioni virtuali 3D, la previsione dei fenomeni di degrado e la classificazione di danneggiamenti non visibili ad occhio nudo. Anche la sperimentazione di tecnologie per una fruizione â€œintelligenteâ€ di informazione culturale costituisce un ambito di ricerca da tempo ampliamente indagato con significativi risultati. Il patrimonio archeologico e stato oggetto di indagine e studio presso lâ€™ENEA-UTICT in numerose attivita progettuali riguardanti sia la ricostruzione virtuale del suo aspetto originale, che la fruizione virtuale e remota (Bordoni et al. 2012). Tale contributo intende proporre delle riflessioni concettuali e metodologiche per una fruttuosa interazione con SITAR, collaborazione sancita anche attraverso la stipula di una convezione quadro trilaterale (SSBAR ENEA GARR). Nel seguito vengono presentate rilevanti attivita svolte nellâ€™ambito dellâ€™infrastruttura ICT di ENEA-GRID e nella realizzazione di piattaforme della conoscenza in cui sono definite delle nuove forme di fruizione, di condivisione e di modalita dâ€™uso. 2. PIATTAFORMA TECNOLOGICA ICT PER I BENI CULTURALI Le piattaforme e i sistemi ICT distribuiti presso i centri di ricerca dellâ€™ENEA consentono l'accesso ai dati e la condivisione di tutte le risorse informatiche hardware e software che compongono la ENEA-GRID (http://www.eneagrid.enea.it). Strumenti, quali laser scanner 3D di varie tipologie, insieme alle risorse di calcolo e di visualizzazione virtuale disponibili nei vari ""Laboratori Virtuali"" costituiscono la base per la salvaguardia, la documentazione e la fruizione del patrimonio culturale. In particolare, negli ultimi tempi le azioni progettuali si sono concentrate prevalentemente sullo svolgimento delle attivita del progetto IT@CHA tecnologie italiane per applicazioni avanzate nei Beni Culturali, finanziato dal Programma Operativo Nazionale (PON) â€œRicerca e Competitivita 2007-2013â€. Il programma di attivita del progetto e dedicato al miglioramento dello stato di conoscenza e di fruizione del patrimonio culturale, il partenariato si compone di piccole e medie imprese, enti di ricerca e universita italiane. L'obiettivo principale del progetto e lo studio, la messa a punto prototipale e la sperimentazione di tecnologie (strumenti e sistemi) e metodologie (procedure e linee guida) innovative che trovano applicazione nelle diverse fasi del processo di gestione di un bene culturale:",2015,
Immune Landscape of Invasive Ductal Carcinoma Tumor Microenvironment Identifies a Prognostic and Immunotherapeutically Relevant Gene Signature,"Background: Invasive ductal carcinoma (IDC) is a clinically and molecularly distinct disease. Tumor microenvironment (TME) immune phenotypes play crucial roles in predicting clinical outcomes and therapeutic efficacy. Method: In this study, we depict the immune landscape of IDC by using transcriptome profiling and clinical characteristics retrieved from The Cancer Genome Atlas (TCGA) data portal. Immune cell infiltration was evaluated via single-sample gene set enrichment (ssGSEA) analysis and systematically correlated with genomic characteristics and clinicopathological features of IDC patients. Furthermore, an immune signature was constructed using the least absolute shrinkage and selection operator (LASSO) Cox regression algorithm. A random forest algorithm was applied to identify the most important somatic gene mutations associated with the constructed immune signature. A nomogram that integrated clinicopathological features with the immune signature to predict survival probability was constructed by multivariate Cox regression. Results: The IDC were clustered into low immune infiltration, intermediate immune infiltration, and high immune infiltration by the immune landscape. The high infiltration group had a favorable survival probability compared with that of the low infiltration group. The low-risk score subtype identified by the immune signature was characterized by T cell-mediated immune activation. Additionally, activation of the interferon-Î± response, interferon-Î³ response, and TNF-Î± signaling via the NFÎºB pathway was observed in the low-risk score subtype, which indicated T cell activation and may be responsible for significantly favorable outcomes in IDC patients. A random forest algorithm identified the most important somatic gene mutations associated with the constructed immune signature. Furthermore, a nomogram that integrated clinicopathological features with the immune signature to predict survival probability was constructed, revealing that the immune signature was an independent prognostic biomarker. Finally, the relationship of VEGFA, PD1, PDL-1, and CTLA-4 expression with the immune infiltration landscape and the immune signature was analyzed to interpret the responses of IDC patients to immunotherapy. Conclusion: Taken together, we performed a comprehensive evaluation of the immune landscape of IDC and constructed an immune signature related to the immune landscape. This analysis of TME immune infiltration landscape has shed light on how IDC respond to immunotherapy and may guide the development of novel drug combination strategies.",2019,
Covariance Estimation for High Dimensional Data Vectors Using the Sparse Matrix Transform,"Covariance estimation for high dimensional vectors is a classically difficult problem in statistical analysis and machine learning. In this paper, we propose a maximum likelihood (ML) approach to covariance estimation, which employs a novel sparsity constraint. More specifically, the covariance is constrained to have an eigen decomposition which can be represented as a sparse matrix transform (SMT). The SMT is formed by a product of pairwise coordinate rotations known as Givens rotations. Using this framework, the covariance can be efficiently estimated using greedy minimization of the log likelihood function, and the number of Givens rotations can be efficiently computed using a cross-validation procedure. The resulting estimator is positive definite and well-conditioned even when the sample size is limited. Experiments on standard hyperspectral data sets show that the SMT covariance estimate is consistently more accurate than both traditional shrinkage estimates and recently proposed graphical lasso estimates for a variety of different classes and sample sizes.",2008,
A direct approach to sparse discriminant analysis in ultra-high dimensions,"Sparse discriminant methods based on independence rules, such as the nearest shrunken centroids classifier (Tibshirani et al., 2002) and features annealed independence rules (Fan & Fan, 2008), have been proposed as computationally attractive tools for feature selection and classification with high-dimensional data. A fundamental drawback of these rules is that they ignore correlations among features and thus could produce misleading feature selection and inferior classification. We propose a new procedure for sparse discriminant analysis, motivated by the least squares formulation of linear discriminant analysis. To demonstrate our proposal, we study the numerical and theoretical properties of discriminant analysis constructed via lasso penalized least squares. Our theory shows that the method proposed can consistently identify the subset of discriminative features contributing to the Bayes rule and at the same time consistently estimate the Bayes classification direction, even when the dimension can grow faster than any polynomial order of the sample size. The theory allows for general dependence among features. Simulated and real data examples show that lassoed discriminant analysis compares favourably with other popular sparse discriminant proposals. Copyright 2012, Oxford University Press.",2012,Biometrika
Combining Sparse Group Lasso and Linear Mixed Model Improves Power to Detect Genetic Variants Underlying Quantitative Traits,"Genome-Wide association studies (GWAS), based on testing one single nucleotide polymorphism (SNP) at a time, have revolutionized our understanding of the genetics of complex traits. In GWAS, there is a need to consider confounding effects such as due to population structure, and take groups of SNPs into account simultaneously due to the ""polygenic"" attribute of complex quantitative traits. In this paper, we propose a new approach SGL-LMM that puts together sparse group lasso (SGL) and linear mixed model (LMM) for multivariate associations of quantitative traits. LMM, as has been often used in GWAS, controls for confounders, while SGL maintains sparsity of the underlying multivariate regression model. SGL-LMM first sets a fixed zero effect to learn the parameters of random effects using LMM, and then estimates fixed effects using SGL regularization. We present efficient algorithms for hyperparameter tuning and feature selection using stability selection. While controlling for confounders and constraining for sparse solutions, SGL-LMM also provides a natural framework for incorporating prior biological information into the group structure underlying the model. Results based on both simulated and real data show SGL-LMM outperforms previous approaches in terms of power to detect associations and accuracy of quantitative trait prediction.",2019,Frontiers in Genetics
Regularized logistic regression without a penalty term: An application to cancer classification with microarray data,"Research highlights? EDAs can be used to find regularized logistic classifiers. It avoids the determination of the regularization term. ? EDA is not influenced by large number of covariates. ? Yields to significant better performance on AUC measure, compared to ridge and Lasso logistic regressions. Regularized logistic regression is a useful classification method for problems with few samples and a huge number of variables. This regression needs to determine the regularization term, which amounts to searching for the optimal penalty parameter and the norm of the regression coefficient vector. This paper presents a new regularized logistic regression method based on the evolution of the regression coefficients using estimation of distribution algorithms. The main novelty is that it avoids the determination of the regularization term. The chosen simulation method of new coefficients at each step of the evolutionary process guarantees their shrinkage as an intrinsic regularization. Experimental results comparing the behavior of the proposed method with Lasso and ridge logistic regression in three cancer classification problems with microarray data are shown.",2011,Expert Syst. Appl.
Variable-Group Selection on Estimated Metabolites of Curcuma aeruginosa Related to Antioxidant Activity by Using Group Lasso Regression,"A metabolite may be expressed on a group of variables in mass-spectrometry experiments. Evaluation on metabolite effects should consider this group. Group lasso regression can be used to evaluate these groups. It shrinks some regression coefficients to zero by intermediate penalty on OLS loss function. The data used were antioxidant activity and mass/charge ion from LC-MS output of Curcuma aeruginosa compositions of 3 areas in Java. The significance metabolite groups were 148,060, 202,179, 204,159, 228,123, 238,150, 246,133, 312,274, and 398,335. Keywordsâ€” Variable-group selection, group lasso regression, antioxidant, Curcuma aeruginosa",2018,
BTLLasso: A Common Framework and Software Package for the Inclusion and Selection of Covariates in Bradley-Terry Models,"In paired comparison models, the inclusion of covariates is a tool to account for the heterogeneity of preferences and to investigate which characteristics determine the preferences. Although methods for the selection of variables have been proposed no coherent framework that combines all possible types of covariates is available. There are three different types of covariates that can occur in paired comparisons, the covariates can either vary over the subjects, the objects or both the subjects and the objects of the paired comparisons. This paper gives an overview over all possible types of covariates in paired comparisons and introduces a general framework to include covariate effects into BradleyTerry models. For each type of covariate, appropriate penalty terms that allow for sparser models and, therefore, easier interpretation are proposed. The whole framework is implemented in the R package BTLLasso. The main functionality and the visualization tools of the package are introduced and illustrated by real data sets.",2019,Journal of Statistical Software
Analysis of longitudinal data using varying-coefficient model,In this study' a model selection procedure for varying-coefficient model based on longitudinal data is proposed to distinguish three types of variables: variables not in the model' variables in the model with time-independent coefficients and variables in the model with time-varying coefficients. To identify these three kinds of variables simultaneously' we extend the present variable selection method from cross-sectional data to longitudinal data. This method combines the B-spline function approximation and Adaptive-Lasso penalty to perform variable selection and do nonparametric estimation simultaneously. Validity is illustrated with a set of simulation experiments' and results indicate the proposed variable selection procedure performs well in distinguishing the real type of independent variables.,2017,2017 36th Chinese Control Conference (CCC)
A computationally cheaper method for blind speech separation based on AuxIVA and incomplete demixing transform,This paper proposes a modification of an auxiliary-function based algorithm for Independent Vector Analysis (AuxIVA) by applying it to a constrained set of frequency bins where the activity of (speech) signals is high. The de-mixing transform obtained by this approach is incomplete. Its completion is done through the solution of a convex optimization problem known as LASSO. The experiments with blind separation of speech signals show that the proposed method can be twice faster and sometimes even slightly more accurate in terms of signal-to-noise ratio than the original algorithm applied to all frequencies.,2016,2016 IEEE International Workshop on Acoustic Signal Enhancement (IWAENC)
Lasso environment model combination for robust speech recognition,"In this paper, we propose a novel acoustic model adaptation method for noise robust speech recognition. Model combination is a common way to adapt acoustic models to a target test environment. For example, the mean supervectors of the adapted model are obtained as a linear combination of mean supervectors of many pre-trained environment-dependent acoustic models. Usually, the combination weights are estimated using a maximum likelihood (ML) criterion and the weights are nonzero for all the mean supervectors. We propose to estimate the weights by using Lasso (least absolute shrinkage and selection operator) which imposes an L1 regularization term in the weight estimation problem to shrink some weights to exactly zero. Our study shows that Lasso usually shrinks to zero the weights of those mean supervectors not relevant to the test environment. By removing some nonrelevant supervectors, the obtained mean supervectors are found to be more robust against noise distortions. Experimental results on Aurora-2 task show that the Lasso-based mean combination consistently outperforms ML-based combination.",2012,"2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
"A simple ""lasso"" for intraocular foreign bodies.","Three patients had foreign bodies in their anterior chambers following penetrating ocular injuries. These foreign bodies were removed by a closed chamber technique using a simple loop. The loop was created by a 22-gauge intravenous cannula and a 7-0 polypropylene suture. Retained cilia in one patient and metallic foreign bodies in two patients were removed using this intraocular ""lasso."" Sutures were not placed at the incision sites at the end of the surgery. This is an inexpensive and easy to prepare technique that introduces minimal surgical trauma. In addition, two hands are not needed for loop manipulation. This technique may be an excellent alternative for removal of small intraocular foreign bodies.",1999,Ophthalmic surgery and lasers
Examining a Syndemics Network Among Young Latino Men Who Have Sex with Men,"Although studies consistently find that syndemic indicators are additively associated with increased HIV/STI risk behavior (e.g., condomless anal sex; CAS) among men who have sex with men (MSM), information is lacking about how syndemic indicators are associated with each other. Young Latino MSM are one of the most at-risk groups for acquiring HIV in the U.S. Understanding the associations of syndemic indicators with each other and with CAS may improve understanding of how to enhance sexual and behavioral health in this population. Network analysis using the graphical LASSO (glasso) algorithm was employed to explore associations between CAS and syndemic indicators among 139 young Latino MSM. Structural and psychosocial syndemic indicators were assessed via self-report. CAS was defined as the number of partners in the past 3 months with whom one engaged in CAS. Results of the network analysis suggested the variables with the highest centrality were unstable housing, prison history, childhood sexual abuse, and CAS. Specific significant associations included links between CAS and alcohol use (bâ€‰=â€‰0.40), childhood sexual abuse and unstable housing (bâ€‰=â€‰âˆ’â€‰0.75), alcohol use and childhood sexual abuse (bâ€‰=â€‰0.40), and substance use and intimate partner violence (bâ€‰=â€‰0.43). This pattern of interconnectedness demonstrates the potential for network analysis to examine nuanced interrelationships of syndemic indicators. The specific associations in this sample raise the question whether a primary focus of interventions should address the more central syndemic indicators for this population, such as alcohol use and unstable housing, and whether this would, via downstream effects, affect other aspects of behavioral health in this population.",2019,International Journal of Behavioral Medicine
Panel data segmentation under finite time horizon,"Abstract We study the nonparametric change point estimation for common changes in the means of panel data. The consistency of estimates is investigated when the number of panels tends to infinity but the sample size remains finite. Our focus is on weighted denoising estimates, involving the group fused LASSO, and on the weighted CUSUM estimates. Due to the fixed sample size, the common weighting schemes do not guarantee consistency under (serial) dependence and most typical weightings do not even provide consistency in the i.i.d. setting when the noise is too dominant. Hence, on the one hand, we propose a consistent covariance-based extension of existing weighting schemes and discuss straightforward estimates of those weighting schemes. The performance will be demonstrated empirically in a simulation study. On the other hand, we derive sharp bounds on the change to noise ratio that ensure consistency in the i.i.d. setting for classical weightings.",2015,Journal of Statistical Planning and Inference
A deep auto-encoder model for gene expression prediction,"BackgroundGene expression is a key intermediate level that genotypes lead to a particular trait. Gene expression is affected by various factors including genotypes of genetic variants. With an aim of delineating the genetic impact on gene expression, we build a deep auto-encoder model to assess how good genetic variants will contribute to gene expression changes. This new deep learning model is a regression-based predictive model based on the MultiLayer Perceptron and Stacked Denoising Auto-encoder (MLP-SAE). The model is trained using a stacked denoising auto-encoder for feature selection and a multilayer perceptron framework for backpropagation. We further improve the model by introducing dropout to prevent overfitting and improve performance.ResultsTo demonstrate the usage of this model, we apply MLP-SAE to a real genomic datasets with genotypes and gene expression profiles measured in yeast. Our results show that the MLP-SAE model with dropout outperforms other models including Lasso, Random Forests and the MLP-SAE model without dropout. Using the MLP-SAE model with dropout, we show that gene expression quantifications predicted by the model solely based on genotypes, align well with true gene expression patterns.ConclusionWe provide a deep auto-encoder model for predicting gene expression from SNP genotypes. This study demonstrates that deep learning is appropriate for tackling another genomic problem, i.e., building predictive models to understand genotypesâ€™ contribution to gene expression. With the emerging availability of richer genomic data, we anticipate that deep learning models play a bigger role in modeling and interpreting genomics.",2017,BMC Genomics
The influence function of penalized regression estimators,"To perform regression analysis in high dimensions, lasso or ridge estimation are a common choice. However, it has been shown that these methods are not robust to outliers. Therefore, alternatives as penalized M-estimation or the sparse least trimmed squares (LTS) estimator have been proposed. The robustness of these regression methods can be measured with the influence function. It quantifies the effect of infinitesimal perturbations in the data. Furthermore, it can be used to compute the asymptotic variance and the mean-squared error (MSE). In this paper we compute the influence function, the asymptotic variance and the MSE for penalized M-estimators and the sparse LTS estimator. The asymptotic biasedness of the estimators make the calculations non-standard. We show that only M-estimators with a loss function with a bounded derivative are robust against regression outliers. In particular, the lasso has an unbounded influence function.",2015,Statistics
Adaptive Semi-varying Coefficient Model Selection,"Identification of constant coefficients in a semi-varying coefficient model is an important issue (Zhang et al (2002)). We propose a novel method for this by combining local polynomial smoothing (Fan and Zhang (1999)) with shrinkage estimation (Tibshirani (1996)). Unlike the stepwise procedure (Xia, Zhang, and Tong (2004)), our method can identify the constant coefficients and estimate the model simultaneously. By imposing the adaptive LASSO penalty and starting with the Nadaraya-Watson estimator, the method can identify the constant coefficients and varying coefficients consistently, and estimate the model with oracle efficiency (Fan and Li (2001)).",2012,Statistica Sinica
Iterative hard thresholding for model selection in genome-wide association studies.,"A genome-wide association study (GWAS) correlates marker and trait variation in a study sample. Each subject is genotyped at a multitude of SNPs (single nucleotide polymorphisms) spanning the genome. Here, we assume that subjects are randomly collected unrelateds and that trait values are normally distributed or can be transformed to normality. Over the past decade, geneticists have been remarkably successful in applying GWAS analysis to hundreds of traits. The massive amount of data produced in these studies present unique computational challenges. Penalized regression with the â„“1 penalty (LASSO) or minimax concave penalty (MCP) penalties is capable of selecting a handful of associated SNPs from millions of potential SNPs. Unfortunately, model selection can be corrupted by false positives and false negatives, obscuring the genetic underpinning of a trait. Here, we compare LASSO and MCP penalized regression to iterative hard thresholding (IHT). On GWAS regression data, IHT is better at model selection and comparable in speed to both methods of penalized regression. This conclusion holds for both simulated and real GWAS data. IHT fosters parallelization and scales well in problems with large numbers of causal markers. Our parallel implementation of IHT accommodates SNP genotype compression and exploits multiple CPU cores and graphics processing units (GPUs). This allows statistical geneticists to leverage commodity desktop computers in GWAS analysis and to avoid supercomputing.


AVAILABILITY
Source code is freely available at https://github.com/klkeys/IHT.jl.",2017,Genetic epidemiology
Overwintering effects on the spring bloom dynamics of phytoplankton,"The influence of winter on the selection of dominant taxa for the phytoplankton spring bloom was studied in batch culture experiments. Different natural phytoplankton assemblages from different phases of the temperate zone winter were exposed to varying periods of darkness (0, 6/7, 13 and 19 weeks) followed by a re-exposure to saturating light intensity for 14 days to experimentally simulate the onset of spring. The results showed that dark incubation has a strong effect on shaping the phytoplankton community composition. Many taxa disappeared in the absolute darkness. Dark survival ability might be an important contributing factor for the success of diatoms in spring. Different phytoplankton starting assemblages were dominated by the same bloom-forming diatoms, Skeletonema marinoi and Thalassosira spp., after dark incubation for only 6 weeks, irrespective of the high dissimilarities between phytoplankton communities. The growth capacity of surviving phytoplankton is almost unimpaired by darkness. Similar growth rates as that before darkness could be resumed for the surviving taxa with a potential lag time of 1â€“7 days dependent on taxon and the duration of darkness.",2017,Journal of Plankton Research
User's Manual: AFWL One-Dimensional Plasma Simulation Particle Codes.,"Abstract : The report is intended primarily as a user's manual for a series of five computer codes for the numberical simulation of one-dimentsional collisionless plasma phenomena. These codes solve, in effect, the one-dimensional Vlassov equation by following the trajectories of simulation particles under the influence of electromagnetic fields. Thus, the particle codes describes are particularly useful for investigating micro-instabilities, such as those encountered in laser-plasma interactions, collisionless shocks, and plasma confinement. A discussion of numerical techniques is includes. The report also includes a listing of the CDC FORTRAN 4 codes, including machine language subroutines for CDC 6000 and 7000 machines. The microfilm routines used to obtain phase space plots and energy graphs employ the system routine PLOTQ, designed specifically for the CDC 280 mirofilmer. (Author)",1972,
Closed-Form Deterministic End-to-End Performance Bounds for the Generalized Processor Sharing Scheduling Discipline,"The Generalized Processor Sharing (GPS) schedulingdiscipline is an important scheduling mechanism that can support both class isolation and bandwidth sharing among different service classes, thus making itan appealing choice for networks providing multiple services with Quality-of-Service guarantees. In this paper, we study a broad classof GPS networks known as Consistent Relative Session Treatment}(CRST) GPS networks and establish closed-form end-to-end performance boundsfor CRST GPS networks. This result generalizes the results of Parekhand Gallager (1994) where simple, closed-form end-to-end performancebounds are derived for a special sub-class of CRST GPS networks, theso-called Rate Proportional Processor Sharing (RPPS) GPS networks, but performance bounds for the general CRST GPS networks do not haveclosed-form. Our result is obtained through the notion of CRSTpartition, which in fact yields a broader class of CRST GPS networksthan the one originally defined in (Parekh and Gallager, 1993). Moreover,our approach is quite general. It not only applies to the deterministicanalysis of GPS networks, but can also be employed in the study of GPSnetworks in a stochastic setting.",1998,Journal of Combinatorial Optimization
A Data-Dependent Weighted LASSO Under Poisson Noise,"Sparse linear inverse problems appear in a variety of settings, but often the noise contaminating observations cannot accurately be described as bounded by or arising from a Gaussian distribution. Poisson observations in particular are a characteristic feature of several real-world applications. Previous work on sparse Poisson inverse problems encountered several limiting technical hurdles. This paper describes a novel alternative analysis approach for sparse Poisson inverse problems that 1) sidesteps the technical challenges present in previous work, 2) admits estimators that can readily be computed using off-the-shelf LASSO algorithms, and 3) hints at a general framework for broad classes of noise in sparse linear inverse problems. At the heart of this new approach lies a weighted LASSO estimator for which data-dependent weights are based on Poisson concentration inequalities. Unlike previous analyses of the weighted LASSO, the proposed analysis depends on conditions which can be checked or shown to hold in general settings with high probability. 2000 Math Subject Classification: 60E15, 62G05, 62G08, and 94A12.",2019,IEEE Transactions on Information Theory
Variable Selection for Partial Linear Single-Index Model with M-Estimation,"In this paper, a method of variable selection for partial linear single-index model is proposed, which is based on the M-estimation and the adaptive LASSO. And its oracle property is established and proved. Unlike the existing M-estimator of the partial linear single-index model, the unknown link function is approximated by B-spline. Keywords-partial linear single-index model; M-estimation; variable selection; B-spline",2016,
"A review of the genus Semiocladius sublette et wirth, 1980 (Diptera: Chironomidae)","A generic diagnosis for the genus Semiocladius Sublette et Wirth and a key to the male imagines is given. The systematic position of the genus is close to Thalassosmittia Strenzke et Remmert based on imagines and pupa while the tentatively associated larva possibly could belong to another genus with marine larvae. Semiocladius crassipennis (Skuse), S. endocladiae (Tokunaga) and S. brevicornis (Tokunaga) are redescribed in both sexes, and the presumptive larva of 5. kuscheli Sublette et Wirth redescribed. A pupa collected from Lord Howe Island in the Western Pacific Ocean is described, illustrated and tentatively assigned to the genus. The generic diagnosis for pupae is expanded to include this species.",1997,Aquatic Insects
