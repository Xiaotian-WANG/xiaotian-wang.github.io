title,abstract,year,journal
Assessment of Imputation Methods for Missing Gene Expression Data in Meta-Analysis of Distinct Cohorts of Tuberculosis Patients.,"The growth of publicly available repositories, such as the Gene Expression Omnibus, has allowed researchers to conduct meta-analysis of gene expression data across distinct cohorts. In this work, we assess eight imputation methods for their ability to impute gene expression data when values are missing across an entire cohort of Tuberculosis (TB) patients. We investigate how varying proportions of missing data (across 10%, 20%, and 30% of patient samples) influence the imputation results, and test for significantly differentially expressed genes and enriched pathways in patients with active TB. Our results indicate that truncating to common genes observed across cohorts, which is the current method used by researchers, results in the exclusion of important biology and suggest that LASSO and LLS imputation methodologies can reasonably impute genes across cohorts when total missingness rates are below 20%.",2020,Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing
PP-112 Compliance to Anticoagulant Therapy and Time in Therapeutic Range in Patients with Non-valvular Atrial Fibrillation: Results from TREQ-AF Study,"The American Journal of Cardiology MARCH 26e29, 2015 11 IN AND C PP-111 A New Tool for Catheter Ablation: Lasso Electroporation Catheter. Omer Uz, Haluk Un, Ugur Kucuk, Hilal Olgun Kucuk, Ejder Kardesoglu, Zafer Isilak, Mehmet Uzun. Gulhane Military Medical Academy, Haydarpasa Hospital, Department Of Cardiology, Istanbul, Turkey; Siyami Ersek Thoracic and Cardiovascular Surgery Center Department of Cardiology, Istanbul, Turkey. Currently, radiofrequency (RF) energy is the most widely accepted and used method for arrythmia ablation. However, RF ablation has potential disadvantages such as endocardial disruption, charring, platelet activation and thrombus formation, Alternative energy sources have been proposed for catheter ablation. Cryoablation, microwave, laser and ultrasound may be able to create longer, deeper, and more controlled lesions. Irreversible electroporation is a nonthermal ablative tool that uses direct electrical pulses to create irreversible membrane pores and eventual cell death. At present there is no commercially avaiable cardiac eletroporation cathater. But limited animal studies utilizing modified standart ablation cathaters have been reported. Here we present design of a lasso electroporation cathater. Defibrillators are widely used to terminate life-threatening arrhythmias. If defibrillator shock-induced transmembrane potentials are large enough, they can cause irreversible tissue damage due to electroporation. We use 10-polar lasso cathater. Positive and negative poles of all electrode pairs were connected in parallel (Figure 1A). And we connected these poles to transcutaneous pacing patch. Direct current electricity transmitted to lasso cathater when we applied shock from defibrillator paddles to the transcutaneous pacing patch(Figure 1B). We test our cathater in saline solution. Whenever we applied shock, bubbles were seen around the electrode pairs which mean our lasso electroporation cathater is functioning properly(Figure 1C). To our knowledge this is the first lasso electroporation cathater reported in literature. We applied local ethic committee for further animal studies. Future studies will establish the efficacy of this new and promising technology.",2015,American Journal of Cardiology
A Portrait of the Computer as a Young Artist,"From Emojis to Manga, from western adverts to â€œforeignâ€ brand consciousness, visual products are continuing their near instantaneous circulation around the globe. Especially their apparent â€œnaturalnessâ€ and freedom from translation is appealing. But here also lies the problem: many of the consumers of these images are oblivious to the fact that these materials have been constructed by social actors with specific backgrounds and specific agendas in mind; thus, especially their â€œforeignâ€ receptions create challenges, including ethical ones. In order to properly study these fairly new phenomena, a different kind of terminology is needed, not one that relies on older media concepts, but one that does them justice in terms of their contextual and technological complexity, multivalence and mobility. I will propose the term â€œVisionBytesâ€ for these phenomena. These denote complex visual arrays, oftentimes of foreign cultural origin and consisting of still or moving images. They circulate within a system of non-photography as sketched by FranÃ§ois Laruelle (2013) and are akin to the â€œobjectsâ€ described in Quentin Meillassoux' Beyond Finitude (2010). Invariably, they touch on issues of belonging, identity, exclusion, globalisation, human and AI rights, all points featuring strongly in this text. Already today, these images have begun participating in the preparations for the gaze of the (technological) Other, of a possible singularity which for the first time will allow humans to re-view themselves and thus be seen by non-human intelligent others, a trajectory already taking its course. As so often, art is at the forefront of these mediated upheavals. In the final part of the article, I will examine a number of recent art pieces/installations from a 2016 Art Fair in Shanghai, from the 2017 Dokumenta 14 in Kassel, and from an ongoing internet project. These select pieces all point to an ever more life-permeating media future where wanting to merely live with media will never do.",2018,The Journal of cultural studies
"A dwarf walrus from the Miocene of Baja California Sur, Mexico","Here, we describe the odobenid Nanodobenus arandai gen. et sp. nov., based on a nearly complete left mandible from the mid to late Miocene Tortugas Formation in Baja California Sur. Nanodobenus is distinguished among odobenids by displaying a unique combination of plesiomorphic and derived characters, such as narrow mandibular symphysis, well-developed genial tuberosity, bilobed canine and p2 roots, bulbous post-canine teeth with the paraconid, protoconid and hypoconid, and smooth lingual cingula. Moreover, it is characterized by its small adult body length, which is estimated at about 1.65â€‰m. Throughout the Mioceneâ€“Pliocene odobenids are characterized by an increase in body size, especially after the extinction of desmatophocids in the late Miocene. The small size of Nanodobenus departs from this trend, demonstrating that there was greater size disparity among odobenids in the midâ€“late Miocene than previously thought. It is hypothesized that Nanodobenus occupied a niche that was later on occupied by similar-sized otariids, such as Thalassoleon mexicanus, which occurs sympatrically with large odobenids in the overlying Almejas Formation.",2018,Royal Society Open Science
Square-root lasso: pivotal recovery of sparse signals via conic programming,"We propose a pivotal method for estimating high-dimensional sparse linear regression models, where the overall number of regressors p is large, possibly much larger than n, but only s regressors are significant. The method is a modification of the lasso, called the square-root lasso. The method is pivotal in that it neither relies on the knowledge of the standard deviation Ïƒ nor does it need to pre-estimate Ïƒ. Moreover, the method does not rely on normality or sub-Gaussianity of noise. It achieves near-oracle performance, attaining the convergence rate Ïƒl(s/n) log pr-super-1/2 in the prediction norm, and thus matching the performance of the lasso with known Ïƒ. These performance results are valid for both Gaussian and non-Gaussian errors, under some mild moment restrictions. We formulate the square-root lasso as a solution to a convex conic programming problem, which allows us to implement the estimator using efficient algorithmic methods, such as interior-point and first-order methods. Copyright 2011, Oxford University Press.",2011,Biometrika
Developing small-area predictions for smoking and obesity prevalence in the United States for use in Environmental Public Health Tracking.,"BACKGROUND
Globally and in the United States, smoking and obesity are leading causes of death and disability. Reliable estimates of prevalence for these risk factors are often missing variables in public health surveillance programs. This may limit the capacity of public health surveillance to target interventions or to assess associations between other environmental risk factors (e.g., air pollution) and health because smoking and obesity are often important confounders.


OBJECTIVES
To generate prevalence estimates of smoking and obesity rates over small areas for the United States (i.e., at the ZIP code and census tract levels).


METHODS
We predicted smoking and obesity prevalence using a combined approach first using a lasso-based variable selection procedure followed by a two-level random effects regression with a Poisson link clustered on state and county. We used data from the Behavioral Risk Factor Surveillance System (BRFSS) from 1991 to 2010 to estimate the model. We used 10-fold cross-validated mean squared errors and the variance of the residuals to test our model. To downscale the estimates we combined the prediction equations with 1990 and 2000 U.S. Census data for each of the four five-year time periods in this time range at the ZIP code and census tract levels. Several sensitivity analyses were conducted using models that included only basic terms, that accounted for spatial autocorrelation, and used Generalized Linear Models that did not include random effects.


RESULTS
The two-level random effects model produced improved estimates compared to the fixed effects-only models. Estimates were particularly improved for the two-thirds of the conterminous U.S. where BRFSS data were available to estimate the county level random effects. We downscaled the smoking and obesity rate predictions to derive ZIP code and census tract estimates.


CONCLUSIONS
To our knowledge these smoking and obesity predictions are the first to be developed for the entire conterminous U.S. for census tracts and ZIP codes. Our estimates could have significant utility for public health surveillance.",2014,Environmental research
Efectes d'un programa d'activitat fÃ­sica sobre la memÃ²ria en la gent gran,"La tesi esta plantejada amb dos objectius fonamentals. Un primer objectiu ha estat el delaborar un programa dintervencio per a la millora de la memoria que sha anomenat Programa Motricitat i Memoria. Aquest programa es presenta com una aportacio pedagogica i didactica del treball corporal amb gent gran de cara a buscar la manera de millorar o de prevenir la perdua benigna de la memoria, mitjancant la practica organitzada de lactivitat fisica. Aquest programa es un punt de partida per a organitzar i delimitar totes aquelles propostes corporals que fan especial incidencia en el treball de la memoria, proposant una taxonomia del treball motriu i cognoscitiu que te en compte les estrategies i el tipus de memoria, i que permet poder classificar, ordenar i clarificar tot aquell treball motriu que incideix en la memoria.
Un segon objectiu, de tipus experimental, es el de posar en practica aquest programa amb grups de gent gran i comparar els resultats amb grups de gent gran que no han participat en aquest programa. Aquesta part experimental sha realitzat amb una mostra de persones majors de 60 anys i amb grups naturals, es a dir, amb grups de gent gran que ja estaven participant del programa dactivitat fisica de lAssociacio Esportiva Sarria- St. Gervasi de Barcelona.
Sha realitzat un disseny quasi experimental -de tipus pre- test, post- test- o sigui shan passat uns test a principi de lestudi i shan passat els mateixos test a finals de lestudi, deixant una epoca dentrenament entre els pre i post test.
Per tal de comparar els efectes del programa, es va organitzar lestudi en tres grups:
Grup Control, format per una mostra inicial de 47 persones grans que assistien al casal pero que no estaven participant de cap programa dactivitat fisica, Grup Experimental 1 amb 59 persones que estaven participant del grups dactivitat fisica i que seguien el programa de Motricitat i Memoria, i el Grup Experimental 2 format per una mostra inicial de 45 persones que assistien als grups dactivitat fisica, pero que no seguien el treball de Motricitat i Memoria, simplement seguien un programa dactivitat fisica mes generic, sense fer especial incidencia en la memoria. La mitjana dedat de tots els grups va ser de (X= 73,57 anys), tenint una edat minima de 60 anys i una maxima de 90 anys.
La tesi presenta dos estudis diferenciats, realitzats de forma parallela i amb la mateixa mostra.
Inicialment es va passar un test de caracteristiques personals, aquest test va servir per situar i descriure la mostra motiu de lestudi.
Per a lestudi 1: Tests Cognoscitius: Test Mini mental State, on es valoren aspectes com: Orientacio. Memoria immediata. Atencio. Record i Llenguatge. Test de Digits de Wais. Test de la Figura. Test de Depressio Geriatrica de Yesavage.
Per a lestudi 2: Test de Memoria Motriu, dissenyat per a aquest estudi.
A nivell descriptiu, sha observat com el Grup Experimental 1 -grup que ha seguit lentrenament amb el programa Motricitat i Memoria- ha obtingut una millora en totes les proves a la fase post- test respecte de la fase pre- test. A nivell inferencial i tenint en compte que es consideren com a significatius els valors (p < 0,05) hi ha diferencies estadisticament significatives en els test cognoscitius del Mini Mental dorientacio, datencio, de record i en el test de la Figura i el Test de Depressio.
En lestudi 2, lanalisi inferencial ens diu que el Grup Experimental 1 es el grup que aconsegueix millores estadisticament significatives en la prova amb el brac habil respecte dels resultats dels altres dos grups.
Els resultats obtinguts en aquesta tesi ens apunten que la participacio en programes dactivitat fisica que es treballi la memoria, poden ajudar a millorar i/o a mantenir aquesta qualitat. A la vegada, volem destacar que les persones grans han pres consciencia dels beneficis que li pot aportar la practica continuada dactivitat fisica i de lactivitat cognoscitiva per tal de mantenir lautonomia i la qualitat de vida.
"" SUMMARY:
This thesis is worked towards two basic aims: the first one has been an intervention program to obtain better memory, called Motricity and Memory Program
This program is a contribution of a work given to old people in a way pedagogic and didactic by physical exercises, to prevent loose of memory.
The second part, is experimental type: we compare groups of people who has done the program with others not included.
We took a sample of people older than 60 years and natural groups, this people are in Association Sportive Sarria- St. Gervasi de Barcelona and are doing physical fitness as usual.
The middle average 73- 74 years. Minimum 60 years old and maximum 90 years old.
We have done an experimental design modelling in sequences: pre- test, post- test; we pan a test at the beginning and at the end (the same test), and leaving a period of time of practices between tests.
Cognoscibility test, Test Mini Mental State examination, where we value:
Orientation - Immediate memory- Attention- Remember- Language.
Test Numbers Wais.
Figure test.
Geriatric Depression test.
Results: When physical activity is worked with memory exercises, they can help to improve or maintain memory.
And we prove that old people are conscious physical activity with cognocitive activity improves their quality of live and daily autonomy.",2005,
Predicting Growth and Carcass Traits in Swine Using Microbiome Data and Machine Learning Algorithms,"In this paper, we evaluated the power of microbiome measures taken at three time points over the growth test period (weaning, 15 and 22 weeks) to foretell growth and carcass traits in 1039 individuals of a line of crossbred pigs. We measured prediction accuracy as the correlation between actual and predicted phenotypes in a five-fold cross-validation setting. Phenotypic traits measured included live weight measures and carcass composition obtained during the trial as well as at slaughter. We employed a null model excluding microbiome information as a baseline to assess the increase in prediction accuracy stemming from the inclusion of operational taxonomic units (OTU) as predictors. We further contrasted performance of models from the Bayesian alphabet (Bayesian Lasso) as well machine learning approaches (Random Forest and Gradient Boosting) and semi-parametric kernel models (Reproducing Kernel Hilbert space). In most cases, prediction accuracy increased significantly with the inclusion of microbiome data. Accuracy was more substantial with the inclusion of microbiome information taken at weeks 15 and 22, with values ranging from approximately 0.30 for loin traits to more than 0.50 for backÂ fat. Conversely, microbiome composition at weaning resulted in most cases in marginal gains of prediction accuracy, suggesting that later measures might be more useful to include in predictive models. Model choice affected predictions marginally with no clear winner for any model/trait/time point. We, therefore, suggest average prediction across models as a robust strategy in fitting microbiome information. In conclusion, microbiome composition can effectively be used as a predictor of growth and composition traits, particularly for fatness traits. The inclusion of OTU predictors could potentially be used to promote fast growth of individuals while limiting fat accumulation. Early microbiome measures might not be good predictors of growth and OTU information might be best collected at later life stages. Future research should focus on the inclusion of both microbiome as well as host genome information in predictions, as well as the interaction between the two. Furthermore, the influence of theÂ microbiome on feed efficiency as well as carcass and meat quality should be investigated.",2019,Scientific Reports
An efficient Monte Carlo EM algorithm for Bayesian lasso,"The lasso is a popular technique of simultaneous estimation and variable selection in many research areas. The marginal posterior mode of the regression coefficients is equivalent to estimates given by the non-Bayesian lasso when the regression coefficients have independent Laplace priors. Because of its flexibility of statistical inferences, the Bayesian approach is attracting a growing body of research in recent years. Current approaches are primarily to either do a fully Bayesian analysis using Markov chain Monte Carlo (MCMC) algorithm or use Monte Carlo expectation maximization (MCEM) methods with an MCMC algorithm in each E-step. However, MCMC-based Bayesian method has much computational burden and slow convergence. Tan et al. [An efficient MCEM algorithm for fitting generalized linear mixed models for correlated binary data. J Stat Comput Simul. 2007;77:929â€“943] proposed a non-iterative sampling approach, the inverse Bayes formula (IBF) sampler, for computing posteriors of a hierarchical model in the structure of MCEM. Motivated by their paper, we develop this IBF sampler in the structure of MCEM to give the marginal posterior mode of the regression coefficients for the Bayesian lasso, by adjusting the weights of importance sampling, when the full conditional distribution is not explicit. Simulation experiments show that the computational time is much reduced with our method based on the expectation maximization algorithm and our algorithms and our methods behave comparably with other Bayesian lasso methods not only in prediction accuracy but also in variable selection accuracy and even better especially when the sample size is relatively large.",2014,Journal of Statistical Computation and Simulation
"The Stream Algorithm: Computationally Efficient Ridge-Regression via Bayesian Model Averaging, and Applications to Pharmacogenomic Prediction of Cancer Cell Line Sensitivity","Computational efficiency is important for learning algorithms operating in the ""large p, small n"" setting. In computational biology, the analysis of data sets containing tens of thousands of features (""large p""), but only a few hundred samples (""small n""), is nowadays routine, and regularized regression approaches such as ridge-regression, lasso, and elastic-net are popular choices. In this paper we propose a novel and highly efficient Bayesian inference method for fitting ridge-regression. Our method is fully analytical, and bypasses the need for expensive tuning parameter optimization, via cross-validation, by employing Bayesian model averaging over the grid of tuning parameters. Additional computational efficiency is achieved by adopting the singular value decomposition reparametrization of the ridge-regression model, replacing computationally expensive inversions of large p Ã— p matrices by efficient inversions of small and diagonal n Ã— n matrices. We show in simulation studies and in the analysis of two large cancer cell line data panels that our algorithm achieves slightly better predictive performance than cross-validated ridge-regression while requiring only a fraction of the computation time. Furthermore, in comparisons based on the cell line data sets, our algorithm systematically out-performs the lasso in both predictive performance and computation time, and shows equivalent predictive performance, but considerably smaller computation time, than the elastic-net.",2014,Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing
Structural features and petroleum prospects of the southwestern Indol-Kuban' trough,"The Koren'kovo reconnaissance well in the southwestern part of the Kerch' Peninsula penetrated Quaternary and Neogene rocks and bottomed out at 4953 m in Maykopian sediments. The unexpected increase in the thickness of the exposed complexes, in combination with seismic-exploration data, indicates that a complex polygenetic depression formed in front of and on the percilines of the orogens of the Northwestern Caucassus and the High Crimea during Oligocene-Pliocenetime. The depression was filled with molassoid sequences. We regard the southeastern part of the peninsula as a fragment of the northwestern margin of the southern branch of this depression. In the basal strata of the Maykopian Series of the South Kerch' trough, there are basal sand-silt layers with satisfactory reservoir properties. Continued exploration here for hydrocarbon pools in the local uplifts of the Kerch' Peninsula and the adjacent water area of the Black Sea is proposed.",1982,International Geology Review
The high-dimensional statistical analysis of sparse Group Lasso,"The high-dimensional statistical properties of sparse Group Lasso are studied by analyzing the properties of loss function and penalty function,and selecting the appropriate regularization parameter Î»n. The bounds of estimates of sparse Group Lasso are obtained. When the loss function and the penalty function satisfy some certain conditions,the unified error bounds of any solution Î¸Î»n and unknown parameter Î¸*are derived.",2014,Journal of Northwest University
Linking demography with drivers: climate and competition,"Summary 
 
In observational demographic data, the number of measured factors that could potentially drive demography (such as daily weather records between two censuses) can easily exceed the number of independent observations. Thus, identifying the important drivers requires alternatives to standard model selection and variable selection methods. 
Spline methods that estimate smooth functions over continuous domains (such as space or time) have the potential to resolve high-dimensional problems in ecological systems. We consider two examples that are important for many plant populations: competition with neighbours that vary in size and distance from the focal individual and climate variables during a window of time before a response (growth, survival, etc.) is measured. 
For competition covariates, we use a simulation study based on empirical data to show that a monotone spline estimate of competition kernels via approximate AIC returns very accurate estimates. We then apply the method to long-term, mapped quadrat data on the four dominant species in an Idaho (US) sagebrush steppe community. 
For climate predictors and their temporal lags, we use simulated data sets to compare functional smoothing methods with competing linear (LASSO) or machine learning (random forests) methods. Given sufficient data, functional smoothing methods outperformed the other two methods. 
Functional smoothing methods can advance data-driven population modelling by providing alternatives to specifying competition kernels a priori and to arbitrarily aggregating continuous environmental covariates. However, there are important open questions related to modelling of nonlinear climate responses and size Ã— climate interactions.",2016,Methods in Ecology and Evolution
"Clustering, Classification, and Factor Analysis in High Dimensional Data Analysis","Clustering, classification, and factor analysis are three popular data mining techniques. In this dissertation, we investigate these methods in high dimensional data analysis. Since there are much more features than the sample sizes and most of the features are non-informative in high dimensional data, dimension reduction is necessary before clustering or classification can be made. In the first part of this dissertation, we reinvestigate an existing clustering procedure, optimal discriminant clustering (ODC; Zhang and Dai, 2009), and propose to use cross-validation to select the tuning parameter. Then we develop a variation of ODC, sparse optimal discriminant clustering (SODC) for high dimensional data, by adding a group-lasso type of penalty to ODC. We also demonstrate that both ODC and SDOC can be used as a dimension reduction tool for data visualization in cluster analysis. In the second part, three existing sparse principal component analysis (SPCA) methods, Lasso-PCA (L-PCA), Alternative Lasso PCA (ALPCA), and sparse principal component analysis by choice of norm (SPCABP) are applied to a real data set the International HapMap Project for AIM selection to genome-wide SNP data, the classification accuracy is compared for them and it is demonstrated that SPCABP outperforms the other two SPCA methods. Third, we propose a novel method called sparse factor analysis by projection (SFABP) based on SPCABP, and propose to use cross-validation method for the selection of the tuning parameter and the number of factors. Our simulation studies show that SFABP has better performance than the unpenalyzed factor analysis when they are applied to classification problems. INDEX WORDS: Cluster analysis, Classification, Cross-validation, High-dimensional data, Optimal score, Principal components analysis, Tuning parameter, Variable selection, Factor Analysis CLUSTERING, CLASSIFICATION, AND FACTOR ANALYSIS IN HIGH DIMENSIONAL DATA ANALYSIS",2013,
A baited underwater video technique to assess shallow-water Mediterranean fish assemblages: Methodological evaluation,"A baited underwater video (BUV) system for the study of reef-associated fish populations on shallow (10â€“20 m) rocky habitats in the western Mediterranean was assessed at four locations in Spain and two in France. We describe the apparatus and optimal deployment times for video sampling. Different species had different response times to the bait, with four response groups identified. Examination of species accumulation curves and fish abundance estimates over time revealed that a period of approximately 20 min deployment was sufficient to capture most species on video. The technique sampled a wide variety of species, with 51 species belonging to 33 families recorded. Nine species of fish appeared regularly at the bait in relatively high numbers, and consist of six carnivores (Serranus cabrilla, Serranus scriba, Coris julis, Diplodus annularis, Diplodus vulgaris, Thalassoma pavo), two planktivores (Chromis chromis, Boops boops) and one omnivore (Oblada melanura). However, abundance estimates for other species were generally very low (mean b1 per location). Comparison of results from BUV with those obtained by Underwater Visual Census (UVC) at the same locations suggests that although BUVestimates species richness reliably, UVC is the more suitable technique for estimating the abundance of shallow-water reef fish in the Mediterranean. BUV improvements are suggested to optimise its use in deeper waters where UVC using scuba is inoperable. Â© 2007 Elsevier B.V. All rights reserved.",2007,Journal of Experimental Marine Biology and Ecology
Optimal Schatten-q and Ky-Fan-k Norm Rate of Low Rank Matrix Estimation,"In this paper, we consider low rank matrix estimation using either matrix-version Dantzig Selector $\hat{A}_{\lambda}^d$ or matrix-version LASSO estimator $\hat{A}_{\lambda}^L$. We consider sub-Gaussian measurements, $i.e.$, the measurements $X_1,\ldots,X_n\in\mathbb{R}^{m\times m}$ have $i.i.d.$ sub-Gaussian entries. Suppose $\textrm{rank}(A_0)=r$. We proved that, when $n\geq Cm[r^2\vee r\log(m)\log(n)]$ for some $C>0$, both $\hat{A}_{\lambda}^d$ and $\hat{A}_{\lambda}^L$ can obtain optimal upper bounds(except some logarithmic terms) for estimation accuracy under spectral norm. By applying metric entropy of Grassmann manifolds, we construct (near) matching minimax lower bound for estimation accuracy under spectral norm. We also give upper bounds and matching minimax lower bound(except some logarithmic terms) for estimation accuracy under Schatten-q norm for every $1\leq q\leq\infty$. As a direct corollary, we show both upper bounds and minimax lower bounds of estimation accuracy under Ky-Fan-k norms for every $1\leq k\leq m$.",2014,arXiv: Machine Learning
Prediction of pathological mutations in proteins: the challenge of integrating sequence conservation and structure stability principles,"The recent drop in genome sequencing costs has created a promising horizon for the development of genomic medicine. Within the biomedical environment, sequencing data are increasingly used for disease diagnosis and prognosis, treatment development, counseling, and so on. Many of these applications rely on the identification of disease causing variants. This is a particularly challenging problem because of the large number and wide variety of sequence variants identified in sequencing projects, and also because we only have a limited understanding of the physicochemical/biochemical properties that differentiate neutral from pathologic variants. Nonetheless, these last years have witnessed important methodologicaladvancesforoneclassofvariants,thosecorrespondingtochanges in the amino-acid sequence of proteins. Proteins are a main constituent of living systems. We know that although their biological properties are essentially determined by the amino-acid sequence, not all the changes in this sequence have the same impact. Some are neutral, but others affect protein function and lead to disease. A large body of evidence shows that whether one or the other is the case that depends on properties such as mutation location in the protein structure, interspecies conservation, and so on. Mutation prediction methods based on these features have good success rates, in the 70â€90% range, although representation over time suggests there is a performance plateau that would limit their applicability. In light of the most recent advances in the field, and after reviewing the foundations of prediction methods, we discuss the existence of this performance threshold and how it can be overcomed. C ï¿½ 2013 John Wiley & Sons, Ltd.",2014,Wiley Interdisciplinary Reviews: Computational Molecular Science
Robust Variable Selection in Functional Linear Models,"We consider the problem of selecting functional variables using the L1 regularization in a functional linear regression model with a scalar response and functional predictors in the presence of outliers. Since the LASSO is a special case of the penalized least squares regression with L1-penalty function it suffers from the heavy-tailed errors and/or outliers in data. Recently, the LAD regression and the LASSO methods have been combined (the LAD-LASSO regression method) to carry out robust parameter estimation and variable selection simultaneously for a multiple linear regression model. However variable selection of the functional predictor based on LASSO fails since multiple parameters exist for a functional predictor . Therefore group LASSO is used for selecting grouped variables rather than individual variables. In this study we extend the LADgroup LASSO to a functional linear regression model with a scalar response and functional predictors. We illustrate the LADgroup LASSO on both simulated and real data.",2014,
Full Quantification of Left Ventricle via Deep Multitask Learning Network Respecting Intra- and Inter-Task Relatedness,"Cardiac left ventricle (LV) quantification is among the most clinically important tasks for identification and diagnosis of cardiac diseases, yet still a challenge due to the high variability of cardiac structure and the complexity of temporal dynamics. Full quantification, i.e., to simultaneously quantify all LV indices including two areas (cavity and myocardium), six regional wall thicknesses (RWT), three LV dimensions, and one cardiac phase, is even more challenging since the uncertain relatedness intra and inter each type of indices may hinder the learning procedure from better convergence and generalization. In this paper, we propose a newly-designed multitask learning network (FullLVNet), which is constituted by a deep convolution neural network (CNN) for expressive feature embedding of cardiac structure; two followed parallel recurrent neural network (RNN) modules for temporal dynamic modeling; and four linear models for the final estimation. During the final estimation, both intra- and inter-task relatedness are modeled to enforce improvement of generalization: (1) respecting intra-task relatedness, group lasso is applied to each of the regression tasks for sparse and common feature selection and consistent prediction; (2) respecting inter-task relatedness, three phase-guided constraints are proposed to penalize violation of the temporal behavior of the obtained LV indices. Experiments on MR sequences of 145 subjects show that FullLVNet achieves high accurate prediction with our intra- and inter-task relatedness, leading to MAE of 190 mm\(^2\), 1.41 mm, 2.68 mm for average areas, RWT, dimensions and error rate of 10.4% for the phase classification. This endows our method a great potential in comprehensive clinical assessment of global, regional and dynamic cardiac function.",2017,ArXiv
Turning a Blind Eye? on the Political Economy of Environmental Regulation in China,"This paper provides a political economy interpretation of environmental data manipulation in China using a unique panel dataset that combines city-level air pollution, economic indicators as well as resumÃ©-type characteristics of local officials. We quantify the magnitude of data manipulation for each city/year using a novel censored MLE strategy and then link city-level manipulation behavior with party secretary and mayor characteristics using LASSO shrinkage. We find that having elite-educated party secretaries is associated with increased manipulation of air quality data at the city level. The results are consistent with economic growth and promotion concerns providing potential explanations for our",2018,
Empirical-Bayes Approaches to Recovery of Structured Sparse Signals via Approximate Message Passing,"In recent years, there have been massive increases in both the dimensionality and sample sizes of data due to ever-increasing consumer demand coupled with relatively inexpensive sensing technologies. These high-dimensional datasets bring challenges such as complexity, along with numerous opportunities. Though many signals of interest live in a high-dimensional ambient space, they often have a much smaller inherent dimensionality which, if leveraged, lead to improved recoveries. For example, the notion of sparsity is a requisite in the compressive sensing (CS) field, which allows for accurate signal reconstruction from sub-Nyquist sampled measurements given certain conditions. When recovering a sparse signal from noisy compressive linear measurements, the distribution of the signalâ€™s non-zero coefficients can have a profound effect on recovery mean-squared error (MSE). If this distribution is apriori known, then one could use computationally efficient approximate message passing (AMP) techniques that yield approximate minimum MSE (MMSE) estimates or critical points to the maximum a posteriori (MAP) estimation problem. In practice, though, the distribution is unknown, motivating the use of robust, convex algorithms such as LASSOâ€“which",2015,
A computational model for the limit analysis of three-dimensional masonry structures,"This paper extends previous work on the limit analysis of ductile frames and plane masonry arches to the limit analysis of three-dimensional masonry structures. A lower-bound approach is developed which can handle three-dimensional collapse mechanisms involving any combination of sliding, twisting and hingeing at the block interfaces. A computer program for determining the collapse load of such structures is used to study (a) the equilibrium limits of a block with four contact points resting on an inclined plane and (b) the collapse of a semicircular arch of four blocks. The paper also describes experimental and computational work on a radially symmetric model dome of 380 blocks subject to foundation settlement.SommarioIl presentre contributo estende al campo delle structture tridimensionali in muratura un precedente lavoro sull'analisi limite di telai duttili ed archi in muratura piani. Si e' sviluppato un approccio statico che analizza meccanismi di collasso tridimensionale ottenuti per combinazione dei meccanismi semplici di scorrimento e rotazione nel piano e fuori dal piano delle superfici di interfaccia tra i blocchi. Si descrivono (a) i limiti di equilibrio di un blocco con 4 punti di contatto su base inclinata, (b) le condizioni di collasso di un arco semicircolare costituito da quattro blocchi, applicando un programma di calcolo redatto per l'analisi e la definizione del carico di collasso di tali strutture. La terza parte dell'articolo presenta il lavoro sperimentale e di calcolo sviluppato su un modello di cupola a simmetria radiale costituita da 380 blocchi soggetta a cedimenti fondali.",1992,Meccanica
"Mixtures, envelopes and hierarchical duality","Summary 
 
We develop a connection between mixture and envelope representations of objective functions that arise frequently in statistics. We refer to this connection by using the term â€˜hierarchical dualityâ€™. Our results suggest an interesting and previously underexploited relationship between marginalization and profiling, or equivalently between the Fenchelâ€“Moreau theorem for convex functions and the Bernsteinâ€“Widder theorem for Laplace transforms. We give several different sets of conditions under which such a duality result obtains. We then extend existing work on envelope representations in several ways, including novel generalizations to varianceâ€“mean models and to multivariate Gaussian location models. This turns out to provide an elegant missing data interpretation of the proximal gradient method, which is a widely used algorithm in machine learning. We show several statistical applications in which the framework proposed leads to easily implemented algorithms, including a robust version of the fused lasso, non-linear quantile regression via trend filtering and the binomial fused double-Pareto model. Code for the examples is available on GitHub at https://github.com/jgscott/hierduals.",2014,Journal of The Royal Statistical Society Series B-statistical Methodology
A Process Ontology for Business Intelligence,"This paper presents oXPDL, a process interchange ontology based on the standardised XML Process Definition Language (XPDL). XPDL was introduced to allow process model exchange between information systems, most of which are based on proprietary workflow models. In its current form, XPDL allows only syntactic vendor-specific extensions without clear semantics, has only limited support for informational and organisational modelling aspects and cannot be interlinked to existing standardised knowledge bases. Our process interchange ontology oXPDL explicitly models the complete semantics of XPDL process models in a standard Web ontology language. The oXPDL ontology has a strong focus on the reuse and integration of existing standard ontologies such as PSL, RosettaNet, SUMO and eClassOWL. We present the ontology and the accompanying tool to automatically translate an XPDL process model to its corresponding oXPDL. The oXPDL process models may be used for integrated process analysis, by querying and reasoning over multiple models, each of which may originate from different information systems, in combination with business rules described in background ontologies. Digital Enterprise Research Institute (DERI), National University of Ireland, Galway, IDA Business Park, Lower Dangan, Galway, Ireland. E-mail: {armin.haller, mateusz.marmolowski, walid.gaaloul}@deri.org. AI department, VU Amsterdam, de Boelelaan 1081a, 1081 HV Amsterdam, The Netherlands. E-mail: eoren@few.vu.nl Acknowledgements: This material is based upon works supported by the Science Foundation Ireland under Grant No. SFI/02/CE1/I131 and No. SFI/04/BR/CS0694. Copyright c Â© 2008 by the authors DERI TR 2008-04-01 I",2008,
"Glarborg â€“ en glashytte fra renÃ¦ssancen i Gribskov, NordsjÃ¦lland","Glarborg â€“ a Renaissance glassworks from the forest of Gribskov, Northern ZealandBy Liv Appel and Arne Jouttijarvi 
Archaeo-metrical analysis of finds originating from archaeological investigations carried out by Gilleleje Museum at Glarborg in the Gribskov forest from 2003-2006 show, that both slags, parts of ovens, glass pots and shards of glass all stem from a Renaissance glassworks. The precise position of the glasseworks itself has not yet been established, however as cultivation in the forest area had ceassed already by the end of the 18th century, the glassworks site could well be preserved undisturbed and thus be a viable object for further research investigations. The first part of the placename Glarborg stems from the Old Danish word glar, meaning glass, whilst the last part borg refers to an older fortified site nearby. When the name Glarborg first crops up in the written sources in 1555, the site was most probably younger than the other settlement sites in the forest founded in the 11th -12th century. The Danish King Christian III (1534-1559) undoubtedly required a supply of glass for his royal castles in Elsinore and Cophenhagen and the citing of a glassworks in the Gribskov forest would have been an obvious choice, as it had been a crown posscession from the beginning of the medieval period. By all accounts it seems, that the glassworks was located in a natural forest clearing created by a storm, with ample supplies of wood for firing charcoal to heat the glassovens. Archaeo-metrical analysis shows, that the glassmaker and his workers had been brought in from Hessen. The glassmaker possibly received the tenancy of the farm, that was created by forest clearence before 1555 and which took the name of the glassworks. Both the archaeological and the written sources indicate, that there was an unusal level of affluence at Glarborg right up until the end of the 17th century. Itâ€™s possible, that the tenant farmer managed to earn enough money and was shown soo much good will by both the kings officers and the King himself â€“ in the short time that glass was produced at Glarborg â€“ that he sucessfully built up the farms animal flocks, which consequently ensured the farms affluence right through the 17th century.",2010,
Robust Covariance Matrix Estimation and Sparse Bias Estimation for Multipath Mitigation,"Multipath is an important source of error when using global navigation satellite systems (GNSS) in urban environment, leading to biased measurements and thus to false positions. This paper treats the GNSS navigation problem as the resolution of an overdetermined system, which depends on the receiver's position, velocity, clock bias, clock drift, and possible biases affecting GNSS measurements. We investigate a sparse estimation method combined with an extended Kalman filter to solve the navigation problem and estimate the multipath biases. The proposed sparse estimation method assumes that only a part of the satellites are affected by multipath, i.e., that the unknown bias vector is sparse in the sense that several of its components are equal to zero. The natural way of enforcing sparsity is to introduce an l1 regularization ensuring that the bias vector has zero components. This leads to a least absolute shrinkage and selection operator (LASSO) problem, which is solved using a reweighted-l1 algorithm. The weighting matrix of this algorithm is defined as functions of the carrier to noise density ratios and elevations of the different satellites. Moreover, the smooth variations of multipath biases versus time are enforced using a regularization based on total variation. For estimating the noise covariance matrix, we use an iterative reweighted least squares strategy based on the so-called Danish method. The performance of the proposed method is assessed via several simulations conducted on different real datasets.",2018,
Historical anecdote related to chemical tests for intoxication.,"During the preparation of an article dealing with the life and work of Erik M.P. Widmark (1), we were given access to his original documents, various letters and correspondence, and reprints of his scientific papers. This was made possible through the kindness and cooperation of Mrs. Daisy Widmark, wife of the late Dr. Per-Erik Widmark, E.M.P. Widmark's son. We report here a historical event gleaned from correspondence dated from 1931-1932 between the U.S. Department of Justice, Bureau of Prohibition and Professor Erik M.P. Widmark that concerns tests for drunkenness based on chemical analysis of blood. The U.S. Department of Justice was disturbed about the increasing involvement of alcohol in traffic accidents and the problems associated with gathering tangible evidence for prosecuting drunken drivers. The head of the research division, Mr. E.P. Sanford, contacted Dr. Walter R. Miles of the Institute of Human Relations, Yale University, to discuss the problem of ""driving under the influence of liquor"" and the role of alcohol in traffic accidents. Miles was considered the leading authority in the United States on the subject of alcohol-induced impairment of body functions. His classic book, Alcohol and Human Efficiency, which was published by the Carnegie Institution of Washington in 1924, is still interesting reading. In his letter of response, Dr. Miles recommended that the Department of Justice contact Professor Erik Widmark at the University of Lund in Sweden, adding, ""I believe it is in Dr. Widmark's laboratory where they are at present analyzing blood samples secured in connection with automobile accidents in Sweden. Widmark has developed what is known as a micromethod, so that sufficient blood can be secured from a needle prick to serve as sample for analysis."" Accordingly, the chief of the research division at the Department of Justice, Bureau of Prohibition wrote to Widmark and, in a letter dated July 22, 1931, mentioned the problem of testing for drunkenness: ""Such tests in our various states vary from smelling the offenders breath to making him walk a chalk line, but no scientific test apparently is applied."" The letter from the Department of Justice continued: ""We are particularly anxious to know what the alcoholic content of the blood must be before a person can be described as being under the influence of alcohol. Some median line must have been established on one side of which an offender is not under the influence of liquor and on the other side of which he may be said to be intoxicated. Just what that line is we should like to know."" Obviously, Widmark could not provide an unequivocal answer to this question, although in his reply dated August 7, 1931, he included reprints of several articles and information about the Swedish system of blood-alcohol testing for law enforcement purposes. Widmark also included an original manuscript, which he requested be sent for publication to the Journal of the American MedicalAssociation. This manuscript was entitled ""The Swedish System for MedicoForensic Determination of the Alcohol Content of the Blood. 1. The Theoretical Basis."" The manuscript was submitted to the Journal on August 27, 1931, by the Department of Justice. However, in a response from the editors dated September 21, 1931, the paper was rejected with the following comment: ""The technical character of this manuscript would seem to make it better suited to one of the special publications in the field of laboratory medicine than to a journal of general circulation, such as ours."" The editors also mentioned an acute shortage of space in the journal at that time and that practically everything that was sent for publication was returned. E.M.P. Widmark therefore joins the ranks of other famous scientists such as Sir Hans Krebs and Rosalind Yalow, both winners of the Nobel Prize, in having an important paper rejected by a scientific journal. The paper by Krebs described the biochemistry of the citric acid cycle and was rejected by the journal Nature; Yalow's paper on radioimmunoassay was rejected by the Journal of Clinical lnvestigation (2). Widmark was informed of the editors' decision, and the manuscript was apparently never published: this is the only known example of Widmark's work being rejected by a scientific journal. Unfortunately, we have been unable to find this manuscript among Widmark's remaining papers. Nevertheless, the Department of Justice was interested in Widmark's article and made copies of it for its own purposes. In some later correspondence, Widmark sent the Department of Justice a copy of his classic German monograph that had just been published, ""Die theoretischen",1996,Journal of analytical toxicology
Spatial Lasso With Applications to GIS Model Selection,"Geographic information systems (GIS) organize spatial data in multiple two-dimensional arrays called layers. In many applications, a response of interest is observed on a set of sites in the landscape, and it is of interest to build a regression model from the GIS layers to predict the response at unsampled sites. Model selection in this context then consists not only of selecting appropriate layers, but also of choosing appropriate neighborhoods within those layers. We formalize this problem as a linear model and propose the use of Lasso to simultaneously select variables, choose neighborhoods, and estimate parameters. Spatially dependent errors are accounted for using generalized least squares and spatial smoothness in selected coefficients is incorporated through use of a priori spatial covariance structure. This leads to a modification of the Lasso procedure, called spatial Lasso. The spatial Lasso can be implemented by a fast algorithm and it performs well in numerical examples, including an applicat...",2010,Journal of Computational and Graphical Statistics
Natural microbial diversity in superficial sediments of Milazzo Harbor (Sicily) and community successions during microcosm enrichment with various hydrocarbons.,"Hydrocarbon-contaminated superficial sediments collected from the Harbor of Milazzo (Tirrenean Sea, northern Sicily), a zone strongly affected by anthropogenic activities, were examined for in situ biodegradative capacities. A culture-independent molecular phylogenetic approach was used to study the influence of hydrocarbon and nutrient addition on the activity and diversity of the indigenous microbiota during a microcosm evaluation. The autochthonous microbial community in non-polluted sediments was represented by eubacterial phylotypes grouped within Proteobacteria, CFB and Firmicutes. The archaeal domain was represented by members of Marine Group I of Crenarchaeota. The majority of recovered sequences was affiliated with heterotrophic genera Clostridium and Vibrio, typical members of eutrophic coastal environments. Amendments of hydrocarbons and mineral nutrients to microcosms dramatically changed the initial diversity of the microbial community. Only bacterial phylotypes affiliated with Proteobacteria and CFB division were detected. The decrease in diversity observed in several microcosms could be explained by the strong selection for microorganisms belonging to group of marine hydrocarbonoclastic gamma-Proteobacteria, namely Alcanivorax, Cycloclasticus, Marinobacter, Marinobacterium/Neptunomonas and Thalassolituus. This study demonstrated that nutrient amendment to hydrocarbon-contaminated superficial sediments enhanced the indigenous microbial biodegradation activity and that highly specialized marine hydrocarbonoclastic bacteria, representing a minor fraction in the natural microbial community, play an important role in the biodegradation of petroleum hydrocarbons accidentally entering the coastal environment.",2005,Environmental microbiology
Pathwise least angle regression and a significance test for the elastic net,"Least angle regression (LARS) by Efron et al. (2004) is a novel method for constructing the piece-wise linear path of Lasso solutions. For several years, it remained also as the de facto method for computing the Lasso solution before more sophisticated optimization algorithms preceded it. LARS method has recently again increased its popularity due to its ability to find the values of the penalty parameters, called knots, at which a new parameter enters the active set of non-zero coefficients. Significance test for the Lasso by Lockhart et al. (2014), for example, requires solving the knots via the LARS algorithm. Elastic net (EN), on the other hand, is a highly popular extension of Lasso that uses a linear combination of Lasso and ridge regression penalties. In this paper, we propose a new novel algorithm, called pathwise (PW-)LARS-EN, that is able to compute the EN knots over a grid of EN tuning parameter Î± values. The developed PW-LARS-EN algorithm decreases the EN tuning parameter and exploits the previously found knot values and the original LARS algorithm. A covariance test statistic for the Lasso is then generalized to the EN for testing the significance of the predictors. Our simulation studies validate the fact that the test statistic has an asymptotic Exp(1) distribution.",2017,2017 25th European Signal Processing Conference (EUSIPCO)
Abstract 4177: Identification of pathways relevant for metastatic site prediction in prostate cancer,"Proceedings: AACR Annual Meeting 2014; April 5-9, 2014; San Diego, CA

Background and Significance: We address the problem of metastatic site prediction in prostate adenocarcinoma (PRAD), with a specific focus on identifying molecular pathways that are activated in association with the homing to a particular metastatic site. The approach can reveal the molecular mechanisms in metastatic cancer while also providing clues about potential drug targets. Further experimental validation of our findings may lead to the discovery of novel therapies for patients who are in the advanced stages of disease.

Methods: We downloaded four PRAD datasets that contained met-site information from the Gene Expression Omnibus (GEO) and trained multi-class predictors on this set. The predictors were then evaluated on patient samples collected as part of the Stand Up To Cancer (SU2C) initiative. Standard normalization techniques were used to remove batch effects associated with non-biological factors such as the institution from which the materials were collected and/or assays conducted.

We focused our attention on linear models due to their straightforward interpretation: higher weights indicate stronger association of the corresponding genomic features with a particular metastatic site. To identify pathways implicated by the relevant genomic features, we employed model regularization via group LASSO. This technique groups genes according to their pathway membership using the PathwayCommons database. The regularizer (penalty trading accurate classification with model complexity) sets the weights of an entire group to zero if those groups were uninformative for met-site prediction and non-zero otherwise.

Results: We trained a multi-class linear predictor to recognize lymphatic node, liver and bone metastatic sites from gene expression data. The resulting model gave rise to two linear signatures: one that distinguished liver mets from the rest, and another that distinguished lymph node mets from the rest. The signatures were enriched for pathways commonly associated with liver development and liver progenitor cells, as well as pathways involved in integrin interactions on the cell surface. Based on the latter, we hypothesize that the up-regulation of particular integrin-signaling pathways may be responsible for driving the tendency of metastatic PRAD cells to prefer one site over another. We are currently in the process of investigating whether there is further evidence of this hypothesis in the SU2C data, as well as comparing group LASSO to other regularization techniques that also incorporate prior pathway information.

Conclusion: We used linear methods to identify several pathways that may be responsible for localization of metastatic prostate adenocarcinoma cells to specific tissues. Our empirical results provide evidence that integrin-signaling may play a key role in this process. We are working on robustness evaluation of these findings, as well as experimental validation with our SU2C collaborators.

Citation Format: Adrian Bivol, Kiley Graim, Evan Paull, Dan Carlin, Robert Baertsch, Artem Sokolov, Josh Stuart. Identification of pathways relevant for metastatic site prediction in prostate cancer. [abstract]. In: Proceedings of the 105th Annual Meeting of the American Association for Cancer Research; 2014 Apr 5-9; San Diego, CA. Philadelphia (PA): AACR; Cancer Res 2014;74(19 Suppl):Abstract nr 4177. doi:10.1158/1538-7445.AM2014-4177",2014,Cancer Research
An Improved Version of Logistic Bayesian LASSO for Detecting Rare Haplotype-Environment Interactions with Application to Lung Cancer,"The importance of haplotype association and gene-environment interactions (GxE) in the context of rare variants has been underlined in voluminous literature. Recently, a software based on logistic Bayesian LASSO (LBL) was proposed for detecting GxE, where G is a rare (or common) haplotype variant (rHTV)-it is called LBL-GxE. However, it required relatively long computation time and could handle only one environmental covariate with two levels. Here we propose an improved version of LBL-GxE, which is not only computationally faster but can also handle multiple covariates, each with multiple levels. We also discuss details of the software, including input, output, and some options. We apply LBL-GxE to a lung cancer dataset and find a rare haplotype with protective effect for current smokers. Our results indicate that LBL-GxE, especially with the improvements proposed here, is a useful and computationally viable tool for investigating rare haplotype interactions.",2015,Cancer Informatics
Vanilla Lasso for sparse classification under single index models,"This paper study sparse classification problems. We show that under single-index models, vanilla Lasso could give good estimate of unknown parameters. With this result, we see that even if the model is not linear, and even if the response is not continuous, we could still use vanilla Lasso to train classifiers. Simulations confirm that vanilla Lasso could be used to get a good estimation when data are generated from a logistic regression model.",2015,arXiv: Statistics Theory
Chapter 1 Parallel Classification on Shared-Memory Systems,"An important task of data mining can be thought of as the proce ss of assigning things to predefined categories or classes â€“ a process called Classification. Since the classes are predefined this is also known as Supervised Induction. The input for the classification system consists of a set of e xample records, called a training set , where each record consists of several fields or attributes. Attributes are either continuous, coming from an ordered domain, or categorical , coming from an unordered domain. One of the attributes, cal led theclassifying attribute, indicates the classor label to which each example belongs. The induced model con sists of patterns that are useful in class discrimination. Once indu ce , the model can help in the automatic prediction of new unclassified data. Classification has been identified a s an important problem in the emerging field of data mining (Agrawal, Imielinski, & Swami 1993). It has im portant applications in diverse domains like retail target marketing, customer retention, fraud detect ion and medical diagnosis (Michie, Spiegelhalter, & Taylor 1994). Classification is a well-studied problem (see (Weiss & Kulik owski 1991; Michie, Spiegelhalter, & Taylor 1994) for excellent overviews) and several models have been proposed over the years, which include neural networks (Lippmann 1987), statistical models like line ar/quadratic discriminants (James 1985), decision trees (Breimanet al. 1984; Quinlan 1993) and genetic algorithms (Goldberg 1989) . Among these models, decision trees are particularly suited for data mining (Agr awal, Imielinski, & Swami 1993; Mehta, Agrawal, & Rissanen 1996). Decision trees can be constructed relativ ly fast compared to other methods. Another advantage is that decision tree models are simple and easy to understand (Quinlan 1993). Moreover, trees can be easily converted into SQL statements that can be used t o access databases efficiently (Agrawal et al. 1992). Finally, decision tree classifiers obtain similar, a nd sometimes better, accuracy when compared with other classification methods (Michie, Spiegelhalter, & Tay lor 1994). We have therefore focused on building scalable and parallel decision-tree classifiers. While there has been a lot of research in classification in the past, the focus had been on memoryresident data, thus limiting their suitability for mining o ver large databases. Recent work has targeted the",2009,
Stagewise Lasso,"Many statistical machine learning algorithms minimize either an empirical loss function as in AdaBoost, or a penalized empirical loss as in Lasso or SVM. A single regularization tuning parameter controls the trade-off between fidelity to the data and generalizability, or equivalently between bias and variance. When this tuning parameter changes, a regularization ""path"" of solutions to the minimization problem is generated, and the whole path is needed to select a tuning parameter to optimize the prediction or interpretation performance. Algorithms such as homotopy-Lasso or LARS-Lasso and Forward Stagewise Fitting (FSF) (aka e-Boosting) are of great interest because of their resulted sparse models for interpretation in addition to prediction. 
 
In this paper, we propose the BLasso algorithm that ties the FSF (e-Boosting) algorithm with the Lasso method that minimizes the L1 penalized L2 loss. BLasso is derived as a coordinate descent method with a fixed stepsize applied to the general Lasso loss function (L1 penalized convex loss). It consists of both a forward step and a backward step. The forward step is similar to e-Boosting or FSF, but the backward step is new and revises the FSF (or e-Boosting) path to approximate the Lasso path. In the cases of a finite number of base learners and a bounded Hessian of the loss function, the BLasso path is shown to converge to the Lasso path when the stepsize goes to zero. For cases with a larger number of base learners than the sample size and when the true model is sparse, our simulations indicate that the BLasso model estimates are sparser than those from FSF with comparable or slightly better prediction performance, and that the the discrete stepsize of BLasso and FSF has an additional regularization effect in terms of prediction and sparsity. Moreover, we introduce the Generalized BLasso algorithm to minimize a general convex loss penalized by a general convex function. Since the (Generalized) BLasso relies only on differences not derivatives, we conclude that it provides a class of simple and easy-to-implement algorithms for tracing the regularization or solution paths of penalized minimization problems.",2007,J. Mach. Learn. Res.
Restricted Eigenvalue from Stable Rank with Applications to Sparse Linear Regression,"High-dimensional settings, where the data dimension (d) far exceeds the number of observations (\(n\)), are common in many statistical and machine learning applications. Methods based on \(\ell_1\)-relaxation, such as Lasso, are very popular for sparse recovery in these settings. Restricted Eigenvalue (RE) condition is among the weakest, and hence the most general, condition in literature imposed on the Gram matrix that guarantees nice statistical properties for the Lasso estimator. It is natural to ask: what families of matrices satisfy the RE condition? Following a line of work in this area, we construct a new broad ensemble of dependent random design matrices that have an explicit RE bound. Our construction starts with a fixed (deterministic) matrix \(X \in \mathbb{R}^{n \times d}\) satisfying a simple stable rank condition, and we show that a matrix drawn from the distribution \(X \Phi^\top \Phi\), where \(\Phi \in \mathbb{R}^{m \times d}\) is a subgaussian random matrix, with high probability, satisfies the RE condition. This construction allows incorporating a fixed matrix that has an easily verifiable condition into the design process, and allows for generation of compressed design matrices that have a lower storage requirement than a standard design matrix. We give two applications of this construction to sparse linear regression problems, including one to a compressed sparse regression setting where the regression algorithm only has access to a compressed representation of a fixed design matrix \(X\).",2018,
On prediction with the LASSO when the design is not incoherent,"The LASSO estimator is an $\ell_1$-norm penalized least-squares estimator, which was introduced for variable selection in the linear model. When the design matrix satisfies, e.g. the Restricted Isometry Property, or has a small coherence index, the LASSO estimator has been proved to recover, with high probability, the support and sign pattern of sufficiently sparse regression vectors. Under similar assumptions, the LASSO satisfies adaptive prediction bounds in various norms. The present note provides a prediction bound based on a new index for measuring how favorable is a design matrix for the LASSO estimator. We study the behavior of our new index for matrices with independent random columns uniformly drawn on the unit sphere. Using the simple trick of appending such a random matrix (with the right number of columns) to a given design matrix, we show that a prediction bound similar to \cite[Theorem 2.1]{CandesPlan:AnnStat09} holds without any constraint on the design matrix, other than restricted non-singularity.",2012,arXiv: Statistics Theory
Serum microRNA-based prediction of responsiveness to eribulin in metastatic breast cancer,"The identification of biomarkers for predicting the responsiveness to eribulin in patients with metastatic breast cancer pretreated with an anthracycline and a taxane remains an unmet need. Here, we established a serum microRNA (miRNA)-based prediction model for the emergence of new distant metastases after eribulin treatment. Serum samples were collected from metastatic breast cancer patients prior to eribulin treatment and comprehensively evaluated by miRNA microarray. The prediction model for estimating eribulin efficacy was established using the logistic LASSO regression model. Serum samples were collected from 147 patients, of which 52 developed at least one new distant metastasis after eribulin monotherapy and 95 did not develop new distant metastases. A combination of eight serum miRNAs (miR-4483, miR-8089, miR-4755-3p, miR-296-3p, miR-575, miR-4710, miR-5698 and miR-3160-5p) predicted the appearance of new distant metastases with an area under the curve of 0.79, sensitivity of 0.69 and specificity of 0.82. The serum levels of miR-8089 and miR-5698 were significantly associated with overall survival after the initiation of eribulin treatment. The present study provides evidence that serum miRNA profiling may serve as a biomarker for the responsiveness to eribulin and for predicting the development of new distant metastases in metastatic breast cancer.",2019,PLoS ONE
An ensemble of LSTM neural networks for high-frequency stock market classification,"We propose an ensemble of longâ€“shortâ€term memory (LSTM) neural networks for intraday stock predictions, using a large variety of technical analysis indicators as network inputs. The proposed ensemble operates in an online way, weighting the individual models proportionally to their recent performance, which allows us to deal with possible nonstationarities in an innovative way. The performance of the models is measured by area under the curve of the receiver operating characteristic. We evaluate the predictive power of our model on several US largeâ€cap stocks and benchmark it against lasso and ridge logistic classifiers. The proposed model is found to perform better than the benchmark models or equally weighted ensembles.",2019,Journal of Forecasting
Introduction to the genlasso package,"We present a short tutorial and introduction to using the R package genlasso, which is used for computing the solution path of the generalized lasso problem discussed in Tibshirani and Taylor (2011). Use cases of the generalized lasso include the fused lasso over an arbitrary graph, and trend fitting of any given polynomial order. Our implementation includes a function to solve the generalized lasso is its most general form, as well as special functions to cover the fused lasso and trend filtering subproblems. The general implementation maintains and updates a matrix factorization to successively solve related linear systems; the specialized implementations forego this update routine and exploit subproblem structure, which improves both stability and speed of the computation. Standard S3 methods such as plot, predict, and coef are also included to assist in studying the output solution path objects.",2012,
Joint User Selection and Precoding in Multiuser MIMO Systems via Group LASSO,"Joint user selection and precoding in multiuser MIMO settings can be interpreted as group sparse recovery in linear models. In this problem, a signal with group sparsity is to be reconstructed from an underdetermined system of equations. This paper utilizes this equivalent interpretation and develops a computationally tractable algorithm based on the method of group LASSO. Compared to the state of the art, the proposed scheme shows performance enhancements in two different respects: higher achievable sum-rate and lower interference at the non-selected user terminals.",2019,"2019 IEEE 30th Annual International Symposium on Personal, Indoor and Mobile Radio Communications (PIMRC)"
Special Issue on INFORMS 2015 Annual Meeting,"E ach year, the Institute for Operations Research and the Management Sciences (INFORMS) Annual Meeting provides an excellent platform for researchers world-wide to present their state-of-the-art research in quality and reliability. The objective of this Special Issue is to introduce the recent advancements in quality and reliability methodological studies as well as applications in a form of full-length papers expanded from the presentations in the INFORMS 2015 Annual Meeting (November 1â€“4, Philadelphia, PA). We are very grateful for the contributions of the authors who submitted papers. After two to three rounds of rigorous reviews, 10 papers were accepted for publication. Among them, five papers focus on process monitoring and fault identification. The paper by Turkoz et al. proposes a new fault identification method when a high dimensional process is out-of-control. The proposed method combines the support vector data description-based test statistic with an adaptive step-down procedure to identify the faults. By using a nonparametric one-class classification method, the proposed approach does not rely on any distribution assumption. Compared with the existing distribution free methods, the proposed method has much more stable performance when the number of faults is more than one. The paper by Choe et al. focuses on the off-line change-point detection problem for time-series data. It adopts the Thresholded Least Absolute Shrinkage and Selection Operator (LASSO) techniques to control the false positives. The authors demonstrated the superior performance of the proposed method by comparing with several benchmark methods based on both simulations and a case study related to solar panel performance. The paper proposed by Zang, Wang, and Jin is handling with the issue of monitoring of processes based on unaligned profiles. While existing works focus on monitoring of well-aligned profiles, this paper develops new algorithms for monitoring unaligned profiles with varying sampling points. For this, they propose a robust dynamic time warping algorithm for profile alignment that are robust to noises and shift signals. And then, they propose a penalization-based charting algorithm that gives more effective performance in shift detection. In order to illustrate their proposed framework, they applied it for unaligned profile monitoring to the ingot growth process and monitoring heating power profiles. The paper by Abdella et al. proposes the double Exponentially Weighted Moving Average based procedure to evaluate the quality of a process based on polynomial quality profiles. The simulations studies have revealed the distinctive performance of their proposed double Exponentially Weighted Moving Average based control charts in quickly detecting changes in the second-order polynomial profiles. Their extensive simulation studies are based on two shift patterns in the polynomial quality profiles: changes in the coefficients of the regression parameters and changes in the process variability. Timely detection of whether a data stream reaches the steady state is critical in various fields. The paper by Hou, Wu, and Chen presents a new online steady state detection algorithm under the Bayesian framework based on a multiple change-point state space formulation and the sequential Monte Carlo methods. In order to reduce the variance of Monte Carlo estimation and enhance the computational efficiency, they propose a Rao-Blackwellization technique. Both artificially simulated signals and a real data example from the ultrasonic-cavitation based nanoparticle dispersion process are used to demonstrate the robustness of their proposed algorithms for various types of signals with different levels of noises. Five other papers address other important quality and reliability issues such as forecasting, maintenance, and robust design. The paper by Xiang and Coit proposes a multi-criteria optimization model to jointly minimize the burning and maintenance costs of a product with heterogeneous subpopulations. The preventive maintenance is allowed to be imperfect. Two papers in this special issue use hybrid models for forecasting. The paper by Xu et al. develops a hybrid Autoregressive integrated moving average-Linear regression (ARIMA-LR) method that combines the ARIMA model and the linear regression model in a sequential manner to capture both the seasonal trend and effects of predictors in time series data. Real-world case studies regarding two Chinese emergence departments are conducted to illustrate the forecasting performance of the developed method. The paper by Xin et al. proposes a hybrid model of singular spectrum analysis and support vector regression to predict failure time series. A stepwise grid search algorithm is used to find optimal tuning parameters in the hybrid model. The paper by Soh, Kim, and Yum develops a multivariate loss function approach to multi-characteristic robust design problems with an appropriately defined signal-to-noise (SN) ratio. Existing methods do not properly consider the varianceâ€“covariance structures among performance characteristics and/or do not preserve the original properties of the Taguchi SN ratio. The paper by Zhang, Yang, and Xin proposes a semi-parametric microstructure modeling method that can capture the variation across different microstructure samples. Existing methods only consider a single microstructure sample, while the unit-to-unit variability among different samples is ignored. Their proposed model can be used for both isotropic materials, and anisotropic materials in which the microstructure properties in the vertical direction and those in the horizontal direction are different. They illustrate their proposed methods by applying them for the quality control of real-life dual phase steels.",2016,Quality and Reliability Eng. Int.
Joint variable selection and network modeling for detecting eQTLs,"Abstract In this study, we conduct a comparison of three most recent statistical methods for joint variable selection and covariance estimation with application of detecting expression quantitative trait loci (eQTL) and gene network estimation, and introduce a new hierarchical Bayesian method to be included in the comparison. Unlike the traditional univariate regression approach in eQTL, all four methods correlate phenotypes and genotypes by multivariate regression models that incorporate the dependence information among phenotypes, and use Bayesian multiplicity adjustment to avoid multiple testing burdens raised by traditional multiple testing correction methods. We presented the performance of three methods (MSSL â€“ Multivariate Spike and Slab Lasso, SSUR â€“ Sparse Seemingly Unrelated Bayesian Regression, and OBFBF â€“ Objective Bayes Fractional Bayes Factor), along with the proposed, JDAG (Joint estimation via a Gaussian Directed Acyclic Graph model) method through simulation experiments, and publicly available HapMap real data, taking asthma as an example. Compared with existing methods, JDAG identified networks with higher sensitivity and specificity under row-wise sparse settings. JDAG requires less execution in small-to-moderate dimensions, but is not currently applicable to high dimensional data. The eQTL analysis in asthma data showed a number of known gene regulations such as STARD3, IKZF3 and PGAP3, all reported in asthma studies. The code of the proposed method is freely available at GitHub (https://github.com/xuan-cao/Joint-estimation-for-eQTL).",2020,Statistical Applications in Genetics and Molecular Biology
Das Problem der WahlmoÌˆglichkeiten von PruÌˆfungsfaÌˆchern hinsichtlich der Vergleichbarkeit von Ergebnissen der zentralen AbschlusspruÌˆfung in Finnland,"JERO, Vol. 8, No. 2 (2016) Abstract The article by Kupiainen, Marjanen and HautamÃ¤ki focuses on the upper secondary matriculation examination in Finland as a school leaving and university entrance examination. The presented research addresses the question of whether increased choice of the subject-specifi c examinations has the potential to undermine the comparability of examination results and to direct studentsâ€™ choices not only in the examination but already beforehand at school. The authors refer to Finlandâ€™s tradition of more than 160 years of a national examination connecting the academic track of upper secondary schools with universities. The authors explain the Finnish system by describing the adoption of a course-based (vs. classor year-based) curriculum for the three-year upper secondary education and the subsequent reforms in the matriculation examination. This increases studentsâ€™ choices considerably with regard to the subject-specifi c exams included in the examination (a minimum of four). As a result, high-achieving students compete against each other in the more demanding subjects while the less able share the same normal distribution of grades in the less demanding subjects. As a consequence, students tend to strategic exam-planning, which in turn aff ects their study choices at school, often to the detriment of the more demanding subjects and, subsequently, of studentsâ€™ career opportunities, endangering the traditional national objective of an all-round pre-academic upper secondary education. 1 This contribution provides an overview of Finnish upper secondary education and of the matriculation examination (cf. Klein, 2013) while studying three separate but related issues by using data from several years of Finnish matriculation results: â€¢ the relation of the matriculation examination and the curriculum; â€¢ the problems of comparability vis-Ã -vis university entry due to the increased choice within the examination;",2016,
Treatment of 16 patients with bullous pemphigoid with oral tetracycline and niacinamide and topical clobetasol.,"siveness to purified protein derivative of tuberculin. Arch Dermatol 1993:129:469-73. 5. Kuramoto Y, Aiba S, Tagami H, et al. Erythema induramm of Bazin as a type of tuberculid. J Am Acad Dermatol 1990;22:612-6. 6. Schneider JW, Geiger DH. Rossouw DJ, et al. Mycobacteriurn tuberculosis DNA in erythema induratum of Bazin. Lancet 1993:342:747-8. 7. Degitz K, Stein M, Thomas P. et al. Aetiology of tuberculids. Lancet 1993;341:239-40. 8. Eisenach KD, Cave MD, Bates JH. et al. Polymerase chain reaction amplification of a repetitive DNA sequence specific for Mycobacterium tuberculosis. J Infect Dis 1990: 161:977-81. 9. Irgang S. Nodose erythema induramm (Bazin's disease). N Y State J Med 1964;64:2580-3. 10. Feiwel M, Munro DD. Diagnosis and treatment of erythema induramm (Bazin). BMJ 1965;1:1109-11. 11. Lebel M. Lassonde M. Erythema induratum of B azin. J Am Acad Dermatol 1986; 14:738-42. 12. Webster GF, Toso SM, Hegemann L. Inhibition of a model of in vitro granuloma formation by tetracyclines and ciprofloxacin: involvement of protein kinase C. 1994:130:74852. 13. Tefford ED. Lesions of the skin and subcutaneous tissue in diseases of the peripheral circulation. Arch Dermatol Syph 1937;36:952-63. 14. Eberhartinger C Das Problem des Erythema Induramm Bazin. Arch Klin Exp Dermatol 1963;217:196-254. 15. Van der Lug L. Some remarks about tuberculosis of the skin and tuberculids. Dermatologica 1965;131:266-75. 16. Forstrom L. Hannuksela K. Antituberculous treatment of erythema induratum of Bazin. Acta Derm Venereol (Stockh) 1970:50:143-7.",1997,Journal of the American Academy of Dermatology
Insufficiency of Instantaneousstrength Determinations Forfailurerateprediction,"The failure ofanelementsubjected strengths attimetmustbegreater thantheapplied toasteady applied stressisassociated withthe stress, reflecting thefactthatstrengths arede- time-wise deterioration oftheelement's strength.terminedforonlythoseelements whichsurvived Attheinstant offailure, theelementcannolongertotimet. performtospecifications. Thestrength ofthe Thestrength ofanelementisanexternally ob- elementcanbeobtained ataspecified point intime servedquantity measuredinthesameunitsasthe byabruptly modifying theapplied stresstoalevel applied stress.Itisapparent thattheeventuality whichcausestheelementtofail.Theinstantane - ofafailure signifies thatthestrength ofanele- ousstrengths determined bythisprocedure are mentdeteriorates withtimeuntil thestrength investigated forthepossibility ofreducing thetest coincides withtheapplied stress. Thetimeinstant timenecessary inobtaining thefailure rateasso- offailure ofanelementsubjected toa steady stress ciated withexponentially distributed failure times. dependsuponthespecific functional formofthe A simple, physically plausible classofstrength strength deterioration andwill, ingeneral, bea deterioration functions isconsidered andthedis- randomquantity having aprobability distribution. tributional character oftheinstantaneous strengthsElements whosetimestofailure conformtothe atanytimeinstant isdetermined. Itisfoundthat exponential distribution havethefeature thattheir| X whentheunderlying distribution oftimetofailurefailure timesareindifferent topasthistory and {iV- t' isexponential, theinstantaneous strengths have thusexhibit theproperty ofretaining precisely the distributions whichareinvariant withtime.This samefailure timedistribution irrespective ofthe result indicates thatnocluetotheformofstrengthtime-wise extent ofsuccessfully completed past deterioration canbeobtained bytesting forthe history. Itiswellknownthatnootherfailure dis- instantaneous strengths. Thustheconclusion is tribution hasthisproperty. reached, inthecaseoftheexponential failure dis- Theexponential failure distribution isare- tribution, thatinstantaneous destruction testsfor markabledistribution whichleadstomanyunique strength determination areineffective forassess- properties, someofwhichareanalyzed inthesub- ingtheunderlying failure rate. sequent sections. Itisshowninthefollowing sec- tions, however, thatbyvirtue ofitsuniquenature, theexponential distribution canobscurethedeter-",1965,
Sparse Regression Algorithm for Activity Estimation in $\gamma$ Spectrometry,"We consider the counting rate estimation of an unknown radioactive source, which emits photons at times modeled by an homogeneous Poisson process. A spectrometer converts the energy of incoming photons into electrical pulses, whose number provides a rough estimate of the intensity of the Poisson process. When the activity of the source is high, a physical phenomenon known as pileup effect distorts direct measurements, resulting in a significant bias to the standard estimators of the source activities used so far in the field. We show in this paper that the problem of counting rate estimation can be interpreted as a sparse regression problem. We suggest a post-processed, non-negative, version of the Least Absolute Shrinkage and Selection Operator (LASSO) to estimate the photon arrival times. The main difficulty in this problem is that no theoretical conditions can guarantee consistency in sparsity of LASSO, because the dictionary is not ideal and the signal is sampled. We therefore derive theoretical conditions and bounds which illustrate that the proposed method can none the less provide a good, close to the best attainable, estimate of the counting rate activity. The good performances of the proposed approach are studied on simulations and real datasets.",2010,IEEE Transactions on Signal Processing
Spanish in Abraham Fraunceâ€™s Arcadian Rhetorike and the Political Context of the Summer of 1588,"Abraham Fraunceâ€™s (1558â€“1593) The Arcadian Rhetorike went to press as the Armada approached Englandâ€™s shores. Usually studied as a conduit for the circulation of Renaissance poetry, Fraunce was the first to publish excerpts of Sir Philip Sidneyâ€™s (1554â€“1586) writing that had circulated previously only in manuscript. This article asks: is Fraunceâ€™s excerption of Sidney driven by more than his search for patronage? Is it instead patriotic? The Arcadian Rhetorikeâ€™s juxtaposition of languages, and particularly English and Spanish poetry, reveals an underlying concern with the nationalist fervor that characterized England in 1588. Rather than a mere pedagogic and at times pedantic manual, The Arcadian Rhetorike may also be propagandaâ€”a call to arms advocating for emulation of Spanish verse that, counterintuitively, resists Spainâ€™s martial advancements. Printed just two years after Englandâ€™s role in the Netherlands expanded and Fraunceâ€™s patron, Sidney, was killed as he fought against the influence of Philip II, The Arcadian Rhetorike signals its relevance to the growing English antipathy for Spain from the moment it invokes the name of Arcadia. Continuity between three seemingly unrelated aspects of The Arcadian Rhetorike reflects upon the tense moment in which it was printed: its publication of Sidneyâ€™s writing; the passages it excerpts from Garcilaso de la Vegaâ€™s (1501â€“1536) poetry; and its apparent reliance on the nationalist edition of Obras de Garci Lasso (1580) of Fernando de Herrera (1534â€“1597)â€”a patriotic Spanish poet known for his verse celebrating contemporary military leaders. Thus this article will establish Fraunceâ€™s political motivations and expand understanding of Sidneyâ€™s posthumous importance as a political symbol in the developing English conflict with Spain.",2017,Studies in Philology
The electrophysiological characteristics and ablation treatment of patients with paroxysmal sustained atrial fibrillation caused by rapid focal activation originating from the pulmonary veins,"Objective To report the electrophysiological characteristics and ablation treatment of four patients with sustained atrial fibrillation originating from pulmonary veins. Methods Four patients with frequent episodes and ECG recording of atrial fibrillation were involved in this study. Two multiple electrode catheters were placed in the high right atrium (HRA) and coronary sinus (CS) for mapping and stimulation. Two decapolar ring designed catheters (Lasso mapping catheter, Cordis Co) were positioned into the pulmonary veins (PV) for mapping of the activation after atrial transseptal puncture and angiography of PV. The cycle length and regularity of the local activation were measured at the bipolar electrograms from HRA, CS and PVs during spontaneous or induced atrial fibrillation. The target pulmonary vein was that where the earliest activity was appeared at the onset of atrial fibrillation or the most irregular activity was recorded during atrial fibrillation. Radiofrequency energy was delivered at the junctions of the target PV and left atrium during atrial fibrillation. The endpoint of procedure was the termination of atrial fibrillation and disappearance of focal activity from PV. Results The rapid focal activation was originated from right superior (3 patients) and left superior (1 patient) PV. Target PV had rapid and irregular activation which had frequent episodes of cycle length shortening. In four patients the atrial fibrillation was terminated during 1 to 18 delivering, and in three of them the rapid activity disappeared at the same time of atrial fibrillation termination. In another patient with the rapid activation from PV during sinus rhythm the focal activity disappeared after PV focal ablation. No recurrence of atrial fibrillation during follow up of 4 to 17 months. Conclusions It could be concluded that focal activity originated from PV was an important mechanism of paroxysmal atrial fibrillation. Radiofrequency energy ablation to the ostium of target PV could isolate the activity from PV to left atrium or abolish the focal activity to terminate atrial fibrillation.",2004,Chinese Journal of Cardiology
-1 - Neighborhood Properties Are Important Determinants of 1 Temperature Sensitive Mutations 2 3 4 5,"20 Temperature-sensitive (TS) mutants are powerful tools to study gene function in 21 vivo. These mutants exhibit wild-type activity at permissive temperatures and reduced 22 activity at restrictive temperatures. Although random mutagenesis can be used to 23 generate TS mutants, the procedure is laborious and unfeasible in multicellular 24 organisms. Further, the underlying molecular mechanisms of the TS phenotype are 25 poorly understood. To elucidate TS mechanisms, we used a machine learning 26 methodï£§logistic regressionï£§to investigate a large number of sequence and structure 27 features. We developed and tested 133 features, describing properties of either the 28 mutation site or the mutation site neighborhood. We defined three types of 29 neighborhood using sequence distance, Euclidean distance, and topological distance. 30 We discovered that neighborhood features outperformed mutation site features in 31 predicting TS mutations. The most predictive features suggest that TS mutations tend 32 to occur at buried and rigid residues, and are located at conserved protein domains. 33 The environment of a buried residue often determines the overall structural stability of 34 a protein, thus may lead to reversible activity change upon temperature switch. We 35 developed TS prediction models based on logistic regression and the Lasso 36 regularized procedure. Through a tenfold cross-validation, we obtained the area 37 under the curve of 0.91 for the model using both sequence and structure features. 38 Testing on independent datasets suggested that the model predicted TS mutations with 39 a 50% precision. In summary, our study elucidated the mechanism of TS mutants and 40 suggested the importance of neighborhood properties in determining TS mutations. 41 We further developed models to predict TS mutations derived from single amino acid 42 substitutions. In this way, TS mutants can be efficiently obtained through 43 experimentally introducing the predicted mutations. 44 45-3",2011,
Hydrotherapy: A forgotten Australian therapeutic modality,"Put simply, hydrotherapy, also called hydrothermal therapy or medical hydrology, is the use of water, in any of its forms, for the maintenance of health or the treatment of disease. Balneotherapy is a branch of this therapy that specifically studies baths and their medical uses, with a large focus on the healing aspects of various mineral contents (though this itself used to be a subbranch called crenology, which was limited to the science and use of mineral spring waters). Thalassotherapy is another branch which refers to the use of seawater and seascapes for healing.",2013,Australian Journal of Medical Herbalism
Two-Stage Fuzzy Multiple Kernel Learning Based on Hilbertâ€“Schmidt Independence Criterion,"Multiple kernel learning (MKL) is a principled approach to kernel combination and selection for a variety of learning tasks, such as classification, clustering, and dimensionality reduction. In this paper, we develop a novel fuzzy multiple kernel learning model based on the Hilbertâ€“Schmidt independence criterion (HSIC) for classification, which we call HSIC-FMKL. In this model, we first propose an HSIC Lasso-based MKL formulation, which not only has a clear statistical interpretation that minimum redundant kernels with maximum dependence on output labels are found and combined, but also enables the global optimal solution to be computed efficiently by solving a Lasso optimization problem. Since the traditional support vector machine (SVM) is sensitive to outliers or noises in the dataset, fuzzy SVM (FSVM) is used to select the prediction hypothesis once the optimal kernel has been obtained. The main advantage of FSVM is that we can associate a fuzzy membership with each data point such that these data points can have different effects on the training of the learning machine. We propose a new fuzzy membership function using a heuristic strategy based on the HSIC. The proposed HSIC-FMKL is a two-stage kernel learning approach and the HSIC is applied in both stages. We perform extensive experiments on real-world datasets from the UCI benchmark repository and the application domain of computational biology which validate the superiority of the proposed model in terms of prediction accuracy.",2018,IEEE Transactions on Fuzzy Systems
Pancreatic adenocarcinoma: quantitative CT features are correlated with fibrous stromal fraction and help predict outcome after resection,"To identify quantitative imaging features of contrast-enhanced computed tomography (CE-CT) that may be prognostically favorable after resection of smaller (â‰¤â€‰30 mm) pancreatic ductal adenocarcinomas (PDACs) located at head. This retrospective study included two independent cohorts (discovery cohort, nâ€‰=â€‰212; test cohort, nâ€‰=â€‰100) of patients who underwent resection of head PDACs â‰¤â€‰30 mm and preoperative CE-CT. We examined tumor and surrounding parenchymal attenuation differences (deltas), and tumor attenuation changes across phases (ratios). Semantic features of PDACs were evaluated by two radiologists. Clinicopathologic and imaging features for predicting disease-free survival (DFS) and overall survival (OS) were analyzed via multivariate Lasso-penalized Cox proportional-hazards models. Survival rates were derived by Kaplan-Meier method. Imaging features achieved C-indices of 0.766 (discovery cohort) and 0.739 (test cohort) for DFS, and 0.790 (discovery cohort) and 0.772 (test cohort) for OS estimates through incorporation of clinicopathologic features. The most decisive imaging feature was delta 3, denoting attenuation differences between tumor and surrounding pancreas at pancreatic phase (DFS: HRâ€‰=â€‰2.122; OS: HRâ€‰=â€‰2.375; both pâ€‰<â€‰0.001). Compared with inconspicuous (low-delta-3, <â€‰28 HU) tumors, conspicuous (high-delta-3) tumors correlated significantly with more aggressive histologic grades (pâ€‰=â€‰0.014) and less extensive tumor fibrous stromal fractions (pâ€‰<â€‰0.001). Patients with low-delta-3 tumors â‰¤â€‰20 mm experienced the most favorable outcomes (DFS, 36 months; OS, 42 months), whereas those with high-delta-3 tumors fared poorly, regardless of tumor size (DFS, 12 months; OS, 19 months). Quantifiable CT imaging features reflect heterogeneous fibrous stromal fractions and histologic grades of PDAC at head locations that help stratify patients with disparate clinical outcomes. â€¢ Quantitative and semantic imaging features achieved promising results for the prognosis of resected PDAC (â‰¤â€‰30 mm) at head location, through incorporation of clinicopathologic features. â€¢ Attenuation difference at tumor-parenchyma interface (delta 3) emerged as the most decisive imaging feature, enabling further stratification of patients into distinct prognostic subtypes by tumor size. â€¢ High delta 3 signifies sharper contrast between tumor and surrounding pancreas, correlating with more aggressive histologic grades and less extensive tumor fibrous stromal fractions.",2020,European Radiology
Analysis on User Activity in Compressed Sensing based Random Access,"In Compressed Sensing based Random Access CHannel (CS-RACH) protocol, a base station leverages compressed sensing technique to detect the active users in the cell coverage and estimate the channel gain between the users and the base station. In a real communication scenario, activity of a specific user usually varies time to time and thus can be seen as a random variable following ON/OFF distribution. Meanwhile, the performance of compressed sensing technique is dependent on the sparsity of the estimating vector, which is closely related to the user activity in CS-RACH scenario. In this perspective, we analyze the condition of the user activity for the stable operation of the protocol. Particularly, we use the least absolute shrinkage and selection operator (LASSO) approach, which gives a closed form expressions of the sparsity condition for the successful active user detection in an asymptotic manner. As a result, we obtain the condition of the user activity for stable operation of CS-RACH and verify the result with numerical simulations.",2019,2019 IEEE Wireless Communications and Networking Conference (WCNC)
Group Lasso Regularized Deep Learning for Cancer Prognosis from Multi-Omics and Clinical Features,"Accurate prognosis of patients with cancer is important for the stratification of patients, the optimization of treatment strategies, and the design of clinical trials. Both clinical features and molecular data can be used for this purpose, for instance, to predict the survival of patients censored at specific time points. Multi-omics data, including genome-wide gene expression, methylation, protein expression, copy number alteration, and somatic mutation data, are becoming increasingly common in cancer studies. To harness the rich information in multi-omics data, we developed GDP (Group lass regularized Deep learning for cancer Prognosis), a computational tool for survival prediction using both clinical and multi-omics data. GDP integrated a deep learning framework and Cox proportional hazard model (CPH) together, and applied group lasso regularization to incorporate gene-level group prior knowledge into the model training process. We evaluated its performance in both simulated and real data from The Cancer Genome Atlas (TCGA) project. In simulated data, our results supported the importance of group prior information in the regularization of the model. Compared to the standard lasso regularization, we showed that group lasso achieved higher prediction accuracy when the group prior knowledge was provided. We also found that GDP performed better than CPH for complex survival data. Furthermore, analysis on real data demonstrated that GDP performed favorably against other methods in several cancers with large-scale omics data sets, such as glioblastoma multiforme, kidney renal clear cell carcinoma, and bladder urothelial carcinoma. In summary, we demonstrated that GDP is a powerful tool for prognosis of patients with cancer, especially when large-scale molecular features are available.",2019,Genes
Structure of 3-Chlorotropolone,"C7H5C102, Mr = 156.6, monoclinic, P21/c, a = 8.476 (2), b = 12.241 (2), c = 8.170 (2) A, /3 -- 126.47 Â°, V= 681.7 A 3, z = 4, Dx = 1.525 g cm -3, A(Cu Ka) = 1.54184 A, /z = 44.63 cm-~, F(000) = 320, T = 293 K, R -- 0.046 for 833 reflections with I > 3tr(I). The hydroxylic proton forms a bifurcated hydrogen bond with carbonyl O atoms, one branch intramolecular and the other intermolecular. The latter intermolecular branches form a hydrogen- bonded dimer, which is roughly planar. Introduction. The structures of tropolone and some tropolone derivatives have been studied. It has been shown that 5,isopropyltropolone (Berg, Karlasson, (P~.tti & Wiehager, 1976) as well as tropolone manouchi & Sasada, 1973) forms a bifurcated hydrogen bond with the carbonyl O atoms, one branch being intramolecular and other intermolecu- lar. On the other hand, 4-isopropyltropone does not form a dimer, but O...O hydrogen bonds of 2.8 A link the molecules in a chain (Derry & Hamor, 1972). Very recently, we have measured the electronic spectra of the isolated 3-chlorotropolone (Tsuji, Sekiya, Nishimura, Mori & Takeshita, 1991) and 3-bromotropolone (Sekiya, Sasaki, Nishimura, Mori & Takeshita, 1990). The observation of tunnel splitting provides conclusive evidence for the de- localization of the proton. We have found that the hydroxylic proton is delocalized in 3-chlorotro- polone, whereas the proton is localized in 3- bromotropolone. It has been suggested that the localization of the hydroxylic proton strongly depends on the planarity of the molecule. In order to examine the effect of the substitution of a C1 atom on the molecular and crystal structure, an X-ray",1991,Acta Crystallographica Section C-crystal Structure Communications
Cmar_a_186914 8731..8741,"*These authors contributed equally to this work Purpose: In recent years, there has been an increase in the incidence of small renal masses (SRMs) and nephrectomy was the standard management of this disease in the past. Currently, the use of active surveillance has been recommended as an alternative option in the case of some patients with SRMs due to its heterogenicity. However, limited studies focused on the regarding risk stratification. Therefore, in the current study, we developed a nomogram for the purpose of predicting the presence of high-grade SRMs on the basis of the patient information provided (clinical information, hematological indicators, and CT imaging data). Patients and methods: A total of 329 patients (consisting of development and validation cohort) who had undergone nephrectomy for SRMs between January 2013 and May 2016 retrospectively were recruited for the present study. All preoperative information, including clinical predictors, hematological indicators, and CT predictors, were obtained. Lasso regression model was used for data dimension reduction and feature selection. Multivariable logistic regression analysis was applied for the establishment of the predicting model. The performance of the nomogram was assessed with respect to its calibration and discrimination properties and externally validated. Results: The predictors used in the assessment of the nomogram included tumor size, CT tumor contour, CT necrosis, CT tumor exophytic properties, and CT collecting system oppression. Based on these parameters, the nomogram was evaluated to have an effective discrimination and calibration ability, and the C-index was found to be 0.883 after internal validation and 0.887 following external validation. Conclusion: Based on the aforementioned findings, it can be concluded that CT imagingâ€“ based preoperative nomogram is an effective predictor of SRMs and hence can be used in the preoperative evaluation of SRMs, due to its calibration and discrimination abilities.",2019,
Paradigms and falsification in earthquake engineering,"Particular reference is made to the works by E. Torroja and by A. M. Freudenthal, that can be seen as the father-works of a new paradigm in the study of structural engineering: the probabilistic approach to structural safety. The key idea of this approach is, according to Freudenthal, the selection of an â€œacceptable risk of failureâ€. The criteria that have been proposed for this selection in the case of constructions in earthquake-prone areas are discussed.Attention is then paid to the scientific reliability of seismic hazard analysis at a particular site, following the teachings of modern philosophy of science. The problem is discussed in the frame of the theory of falsification, due to K. Popper and I. Lakatos: it is recognised that the results of a seismic hazard analysis are not falsifiable; hence, they would not have a scientific status. In order to overcome this problem, it is proposed to shift the attention from the final results of the hazard analysis, which are not directly falsifiable, to the procedures which lead to that result. A method for falsifying these procedures is proposed.SommarioVengono ricordati i lavori di E. Torroja e di A. M. Freudenthal, ai quali si puÃ² far risalire la nascita di un nuovo paradigma nello studio delle costruzioni civili: l'impostazione probabilistica del problema della sicurezza. L'idea chiave di questa impostazione, secondo Freundenthal, Ã¨ quella della scelta di un â€œrischio accettabile di collassoâ€™. Vengono quindi discussi i criteri che sono stati proposti per addivenire a tale scelta nel caso di costruzioni in zona sismicamente attiva.Viene poi presa in considerazione l'attendibilitÃ  scientifica delle analisi di pericolositÃ  sismica locale, secondo i canoni della moderna filosofia della scienza. Il problema viene riguardato dal punto di vista della teoria della falsificazione dovuta a K. Popper ed a I. Lakatos: si riconosce che i risultati di una analisi di pericolositÃ  sismica non sono falsificabili e quindi non avrebbero dignitÃ  scientifica. Al fine di superare questa difficoltÃ , viene osservato che, se non sono direttamente falsificabili i risultati delle analisi, sono invece falsificabili le procedure che a tali risultati conducono. Viene anche proposto un metodo per la falsificazione delle procedure.",1991,Meccanica
Simultaneous variable and factor selection via sparse group lasso in factor analysis,"ABSTRACT This paper considers variable and factor selection in factor analysis. We treat the factor loadings for each observable variable as a group, and introduce a weighted sparse group lasso penalty to the complete log-likelihood. The proposal simultaneously selects observable variables and latent factors of a factor analysis model in a data-driven fashion; it produces a more flexible and sparse factor loading structure than existing methods. For parameter estimation, we derive an expectation-maximization algorithm that optimizes the penalized log-likelihood. The tuning parameters of the procedure are selected by a likelihood cross-validation criterion that yields satisfactory results in various simulation settings. Simulation results reveal that the proposed method can better identify the possibly sparse structure of the true factor loading matrix with higher estimation accuracy than existing methods. A real data example is also presented to demonstrate its performance in practice.",2019,Journal of Statistical Computation and Simulation
Structured Multi-class Feature Selection for Effective Face Recognition,"This paper addresses the problem of real time face recognition in unconstrained environments from the analysis of low quality video frames. It focuses in particular on finding an effective and fast to compute (that is, sparse) representation of faces, starting from classical Local Binary Patterns (LBPs). The two contributions of the paper are a new formulation of Group LASSO for structured feature selection (MCGroup LASSO) to cope directly with multi-class settings, and a face recognition pipeline based on a representation derived from MC-GrpLASSO. We present an extensive experimental analysis on two benchmark datasets, MOBO and Choke Point, and on a more complex dataset acquired in-house over a large temporal span. We compare our results with state-of-the-art approaches and show the superiority of our method in terms of both performances and sparseness of the obtained solution.",2013,
Fixed Point Theorems for a Class of Mappings Governed By Strictly Contractive Implicit Function,"AMS Mathematics Subject Classiï¬cation(2000): 47H10, 54H25, 54E50Abstract. The purpose of this paper is two fold. Firstly, we deï¬ne contractive (strictcontraction) mappings and introduce property (E.A) in 2-metric spaces. Secondly, wesuggest a sharpened and enriched form of a recent metrical common ï¬xed theorem dueto ï¬rst author proved for a class of mappings governed by strictly contractive implicitfunctions involving a pair of mappings. In order to demonstrate the utility of this classof strict contractions, we have added some natural examples of strictly contractivefunctions besides proving common ï¬xed point theorems in 2-metric spaces (for a pairof weakly compatible mappings) for the esteemed class of strict contractions.Keywords: 2-metric space; Property (E.A); Fixed point; Weakly compatible mappingsand implicit function.",2010,
"Palynostratigraphy, palaeoenvironments and kerogen assessment of Mid-Cretaceous Ezeaku Shales succession from River Obey in Umudi-Lokpanta, Abia State, Southeastern Nigeria","There have been disparities about the age of the Eze-Aku Formation, Anambra Basin south eastern Nigeria. Eleven surface samples of the Eze-Aku Shales from the River Obey in Umudi village, Lokpanta Abia state were subjected to standard palynological, palynofacies, kerogen preparation and analysis. A diversified palynoflora with preponderance of typical Late Cenomanian to Early Turonian diagnostic palynomorphs were recovered. Common taxa were Elaterosporites protensus, cf. Elateropollenites jardinei, Steevesipollenites binodosus, Steevesipollenites grambasti, Gnetaceaepollenites barghoornii, G. diversus, G. clathratus, Galeocornea causea, Classopollis spp., Podocarpidites herbstii, Fraxionoipollenites venustus, Cretaceaporites mulleri, Cicatricosisporites venustus, and the dinoflagellate cysts Callaiosphaeridium trycherium, Oligosphaeridium pulcherrimum, O. complex, O. albertense, Pseudoceratium pelliferum, Florentinia resex, Heterosphaeridium difficile, Surculosphaeridium longifurcatum, Cribroperidinium orthoceras and different species of Dinogymnium. Angiosperm pollens were scarce while gymnosperms were moderate due to the evolutionary trend of the flowering plants in the early Cretaceous. Structured phytoclasts with sparse amorphous organic matter which indicated deposition in nearshore to marginal marine environments dominated the palynofacies. The sporomorph colour indicated mature oil and gas generation potentials.",2019,
"Investigating the association between stress. Anxiety and geophagy among pregnant women in mwanza, Tanzania","Geophagy, the craving and intentional consumption of soil, is common especially among pregnant women in some low- and middle-income settings. Soils may contain a variety of non-nutritive components such as heavy metals and microbes or substances that interfere with gastrointestinal absorptive processes, posing health risks to pregnant women. Several hypotheses regarding the practice have been proposed but very few have examined the role of maternal stress. The practice of geophagy may help to alleviate stress or anxiety during gestation from perceived dietary or other pregnancy-related concerns. In this study, we evaluated several measures of maternal stress (general anxiety, Pregnancy-Related Anxiety Scores (10-item revised) and Perceived Stress Scores) and other covariates in relation to geophagic behaviour in early pregnancy in 227 women (12-19 weeks gestation) recruited from two hospitals in the Nyamagana district of Mwanza City, Tanzania. Geophagy was reported by 24.7% of the pregnant women. LASSO regression identified the self-reported treatment of nausea or vomiting during pregnancy (adjusted ORâ€¯=â€¯3.12, 95%CI: 1.43 to 6.83), paternal education level (adjusted ORâ€¯=â€¯2.79, 95%CI: 1.32 to 5.87 for primary or lower education level), antenatal hospital site (adjusted ORâ€¯=â€¯3.71, 95%CI: 1.78 to 7.75), prescription drug use prior to pregnancy (adjusted ORâ€¯=â€¯1.76, 95%CI: 0.87 to 3.56) and general anxiety (feeling worried, tense or anxious in the past four weeks) (adjusted ORâ€¯=â€¯1.81, 95%CI: 0.88 to 3.72) as predictors of geophagic behaviour. Given that relatively little has been done to examine geophagy in relation to the public health risk it may pose to pregnant women, these findings suggest the need for further investigations regarding maternal stress.",2019,Appetite
Elasmobranchs from Marine and Freshwater Environments in Colombia: A Review,"INTRODUCTIONColombia occupies 401,042 mi 2 (1,038,699 km 2) of the north western end of South America. Its western shore is bordered by the Pacific Ocean, and its northern shore borders the Atlantic. Inland waters are characterized by rivers, streams, floodplains, lakes and lagoons. These habitats are important for elasmobranch species. Elasmobranch studies are in their infancy due to the lack of basic information on their ecology, population dynamics, feeding, conservation, and reproduction; however, recently (2000-present), an increasing number of investigations have been carried out on Colombian elasmobranch biology. These studies have focused mainly on marine elasmobranchs (Navia et al., 2006a,b,c,d 2009, 2012; Mejia-Falla et al., 2006, 2007; Acero et al., 2007; Mejia-Falla and Navia, 2009; Grijalba-Bendeck and Acevedo 2009; Payan et al., 2010a) and freshwater sting rays (Lasso et al., 2010). Other investigations have focused on species native to Colombian rivers such as the Magdalena, Cauca, Catatumbo, Amazon, among others and their tributaries (Mojica-C, 1999; Lasso et al., 2004, 2009; Mojica et al., 2005, 2006; Villa-Navarro et al., 2006; Maldonado-Ocampo et al., 2006, 2008; Mejia-Falla et al., 2009). The number of marine and freshwater elasmobranchs species studied is low compared to the total number of elasmobranchs reported for Colombia (124); however, there is a positive trend in the number of recent shark and ray investigations in both Pacific and Caribbean Oceans. Little is known about freshwater elasmobranchs as evidenced by few publications on the topic. Ecology and feeding habits have been examined using stomach content analyses and food web interactions (Gomez et al., 2003; Mejia-Falla et al., 2006; Lopez et al., 2009; Navia et al., 2006a,d, 2007, 2009, 2010a, 2011, 2012; Payan et al., 2011b) in the Colombian Pacific (Gomez et al., 2003; Mejia-Falla et al., 2006; Lopez et al., 2009; Navia et al., 2006d, 2007, 2009, 2010, 2011; Payan et al., 2011) and Caribbean Oceans (Tellez et al., 2006; Polo-Silva and Grijalba-Bendeck, 2008; Grijalba-Bendeck et al., 2009; Moreno et al., 2009) as well as on freshwater sting rays (Mejia-Falla et al., 2009). Length weight relationships (Mejia-Falla and Navia 2011a) and reproductive biology studies have focused on reproductive stages (immature and mature) in both females and males (Acevedo et al., 2007; Grijalba-Bendeck et al., 2008, 2009; Moreno et al., 2008; Navia et al., 2006d, 2009; Mejia-Falla et al. 2011), ray reproductive system description (Acero et al., 2008) and embryo malformation (Mejia-Falla et al., 2011). These studies were conducted in the Caribbean (Acevedo et al., 2007; Grijalba-Bendeck et al., 2008, 2009; Moreno et al., 2008; Acero et al., 2008) and Pacific Colombian Oceans (Navia et al., 2006d, 2009; Mejia-Falla et al., 2011; Payan et al., 2010a). Freshwater sting ray reproductive information has focused on relatively few species (Mejia-Falla et al., 2009; Argumentado, 2008). Migration studies have focused on the endangered, Sphyrna lewini (Griffith and Smith 1834) (Bessudo et al., 2011). Fisheries information has been reported by several governmental, private, non-governmental and other institutions or organizations (Garcia et al., 2007; Grijalba-Bendeck et al., 2008; Moreno et al., 2008; Caldas et al., 2009; FAO, 2011; Navia et al., 2009, Navia and Mejia-Falla 2006a, 2010b). Broad taxonomic information related to fisheries has been gathered by Colombian environmental and governmental institutions since 1957 (Caldas et al., 2009; Mejia-Falla and Navia 2011b). This information, however, is unreliable as elasmobranch catches are not discriminated at the species level or fishing zone (Caldas et al., 2009). Other sources of fishery information have been provided by non-governmental organizations (Funadacion MALPELO and SQUALUS) and studies by Mejia- Falla et al. (2006), Garcia et al. (2007), and Grijalba-Bendeck et al. (2008). Funadacion MALPELO and SQUALUS have provided fishing, economic and social related information for the Colombian Pacific Ocean (Mejia-Falla and Navia, 2011b). â€¦",2013,"Current Politics and Economics of the United States, Canada, and Mexico"
"Ã‰tude du comportement alimentaire de la pintade locale ( Numida meleagris , L.) Ã  lâ€™Ouest du Burkina-Faso","Les asticots representent une alternative pour faire face a lâ€™inaccessibilite des intrants sources de proteine en aviculture. Lâ€™objectif de cette etude est dâ€™analyser le comportement alimentaire des pintades locales en presence dâ€™asticots seches et dâ€™autres aliments servis en mode dâ€™assiette anglaise ou cafeteria. Douze pintades adultes ont ete individuellement reparties dans un dispositif bloc completement randomise a trois traitements (08 h, 12 h et 16 h) et en quatre repetitions. Deux experiences successives ont ete conduites. Au cours de la premiere, les asticots seches ont ete associes aux grains de sorgho et de mais concasses et servis aux pintades. Pendant la deuxieme, un aliment complet a remplace le mais. Chaque service de duree 30 min a ete suivi dâ€™une distribution de lâ€™aliment complet en deux temps : 01 h 30 min apres et pendant la claustration (de 19 h a 07 h 30 min). La consommation journaliere individuelle et la proportion de chaque aliment dans la ration ont ete determinees. Les resultats ont montre que les oiseaux preferent plus les grains de cereales (entre 95,56% et 98,13%) que les asticots seches (moins de 5%) servis concurrentiellement avec les cereales. Dâ€™autres investigations sont necessaires pour promouvoir lâ€™utilisation des asticots dans lâ€™aviculture. 
Â© 2020 International Formulae Group. All rights reserved. 
Mots cles: Aviculture traditionnelle, nutrition aviaire, asticots, Bobo Dioulasso 
English Title: Feeding behaviour of local guinea fowl (Numida meleagris, L.) in western Burkina Faso 
English Abstract 
Maggots represent an alternative to deal with the inaccessibility of protein source inputs in poultry farming. The study aimed at analyzing the feeding behaviour of local guinea fowl which received dried maggots and other feeds in cafeteria feeding. Twelve adult guinea fowls were assigned individually in a completely randomized block design with three treatments (08 am, 12 am and 04 pm) and four replications per treatment. Two consecutive experiments were carried out. In the first experiment, dried maggots were choice-fed with sorghum and cracked maize grain and served to guinea fowl. In the second experiment, a complete layer diet replaced the cracked maize. In all treatments, the cafeteria test lasted 30 minutes, followed by a two-step distribution of the complete diet: 01 h 30 min after and during the confinement at night (from 07 pm to 07:30 am). The daily individual consumption and the proportion of each food in the ration were determined. The results showed that birds prefer cereal grains (between 95.56% and 98.13%) more than dried maggots (less than 5%) served concurrently with cereals. Further investigations are needed to promote the use of maggots in poultry farming. 
Â© 2020 International Formulae Group. All rights reserved. 
Keywords: Traditional poultry farming, avian nutrition, maggots, Bobo-Dioulasso",2020,International Journal of Biological and Chemical Sciences
Tissue-guided LASSO for prediction of clinical drug response using preclinical samples,"Prediction of clinical drug response (CDR) of cancer patients, based on their clinical and molecular profiles obtained prior to administration of the drug, can play a significant role in individualized medicine. Machine learning models have the potential to address this issue but training them requires data from a large number of patients treated with each drug, limiting their feasibility. While large databases of drug response and molecular profiles of preclinical in-vitro cancer cell lines (CCLs) exist for many drugs, it is unclear whether preclinical samples can be used to predict CDR of real patients. We designed a systematic approach to evaluate how well different algorithms, trained on gene expression and drug response of CCLs, can predict CDR of patients. Using data from two large databases, we evaluated various linear and non-linear algorithms, some of which utilized information on gene interactions. Then, we developed a new algorithm called TG-LASSO that explicitly integrates information on samples' tissue of origin with gene expression profiles to improve prediction performance. Our results showed that regularized regression methods provide better prediction performance. However, including the network information or common methods of including information on the tissue of origin did not improve the results. On the other hand, TG-LASSO improved the predictions and distinguished resistant and sensitive patients for 7 out of 13 drugs. Additionally, TG-LASSO identified genes associated with the drug response, including known targets and pathways involved in the drugs' mechanism of action. Moreover, genes identified by TG-LASSO for multiple drugs in a tissue were associated with patient survival. In summary, our analysis suggests that preclinical samples can be used to predict CDR of patients and identify biomarkers of drug sensitivity and survival.",2020,PLoS Computational Biology
Dispositional Flow and Performance in Brazilian Triathletes,"Flow is a mental state characterized by total immersion and focus in an activity; performing it pleasurably. Such a state is considered optimal for performance. The present study analyzed the relationship between dispositional flow and performance in triathletes. The sample consisted of 328 athletes (294 males and 34 females; mean age of 37.42 Â± 7.18 years) competing in the Ironman Brazil - FlorianÃ³polis - South American Championship 2017. Instruments were an identification sheet, the Dispositional Flow Scale (DFS-2) and athletes' total race times. Data were analyzed using R, through the Shapiro-Wilk normality test, Mann-Whitney's U, Spearman Correlation, and Network Analysis [Least Absolute Shrinkage and Selection Operator (LASSO)], using strength, closeness, and betweenness as centrality measurements. Results show a positive correlation between age and practice time (r = 0.34), inverse relationship between practice time and total race time (r = -0.25), and inverse correlations between race time and 05 of the 09 flow dimensions (r between -0.17 and -0.11), suggesting better performances were related to more practice time and higher disposition to flow. Flow conditions, flow characteristics, individual characteristics, and performance were separately grouped in the network structure. Challenge-skill balance was the most influential node, with the highest closeness and betweenness values; challenge-skill balance, clear goals, control, and action-awareness merge directly influenced better race times. Sample's top 50 performers had significantly higher disposition to challenge-skill balance, clear goals, control and feedback. Practical implications of flow mechanisms are discussed. Dispositional flow was positively related to objective performance in Brazilian triathletes.",2019,Frontiers in Psychology
Comparison of learning-based wastewater flow prediction methodologies for smart sewer management,"Abstract Situational awareness in sanitary sewer systems requires accurate flow information at different spatial locations in a city. It is especially desirable to predict flows across a wastewater network in response to heavy rainfall events in addition to regular consumption patterns. Typically, complicated hydraulic models that suffer from difficulties in parameter identification and high computational burden are used for flow prediction. Recently, data-driven approaches have been employed for flow prediction. In this paper, we first design and then compare the performance of three data-driven methods to predict flow: (1) Artificial Neural Network (ANN); (2) Long-Short Term Memory (LSTM); (3) Least Absolute Shrinkage and Selection Operator (LASSO). We test the performance of these machine and statistical learning techniques using data gathered from the City of Springfield. While, all three data-driven methodologies provide acceptable prediction performance, we observe that LSTM outperforms ANN due to the inherent memory integrated with a feedback structure. The statistical learning approach, i.e., LASSO regression not only offers good prediction performance, but also helps identify the key spatial and temporal features that influence flow at any specific location. This added information could aid in remediation activities. Another key contribution of this paper is that we quantify the value of groundwater data in flow prediction. Specifically, including groundwater data as an additional input enhances flow prediction performance in all three methods. Lastly, to better predict flows corresponding to rare events (e.g., 50 or 100 year rainfall events) we use a resampling approach (known as SmoteR) to modify the training dataset. Simulation results indicate that the resampling technique is effective in improving prediction performance.",2019,Journal of Hydrology
CSS Artistry: A Web Design Master Class,"Andy Clarke has an uncanny ability to make you reconsider what you know and wonder why you hadnt always done it his way. Dave Shea, creator of the CSS Zen GardenAnd its those uncanny abilities that make this Master Classone-on-one instruction from Andy Clarke on camera, coupled with his bestselling full-color book containing hundreds more visual examplesthe best way to absorb Andys amazing techniques and best practices for Web design.The book and workshop DVD-ROM deliver the ultimate CSS learning experience for intermediate to advanced Web designers looking to create beautiful and accessible designs. Inside youll find: Proven techniques for creating inspiring designs that use lean, semantic markup and CSS Numerous examples that use a visual approach to explain coding techniques The latest advances in Web design, including the emerging CSS3 specification A new design workflow for building successful prototypes and effective grids that works well for all team members Instruction on advanced topics including typography, microformats, advanced CSS selectors, CSS3 modules, and layout techniques that employ floats, positioning, and margins Inspiration from leading designers who embrace Web standards in their own work Example files included so you can follow alongTotal running time: 2 hours, 34 minutes",2008,
Corporate default prediction,"In the literature of predicting corporate default, it is an ad-hoc process to select the predictors and different models often use different predictors. We study the predictors of U.S corporate default by Forward Stepwise and Lasso model selection methods. Out of 30 candidate default predictors that have been used in the default-predicting literature, we identify a set of eight default predictors that have strong effects in predicting default using the U.S corporate default data from 1984-2009. We compare the eight default predictors' predicting effect over the past three major economic recessions and find that the recession in early 1990 and the recent sub-prime mortgage crisis share some common default characteristics, while the recession in 2000 is different from the other two. We then present a decision-based default prediction framework where we incorporate the default forecaster's loss utility into default classification and derive an optimal decision rule for this classification problem. By combining the default forecaster's loss utility into Support Vector Machines(SVMs), we show that minimizing the utility adjusted hinge loss is consistent with minimizing utility adjusted classification loss. Our empirical classification result of the decision-based Support Vector Machines demonstrates more classification accuracy and flexibilities in meeting different default forecasters' goals in comparison to traditional statistical methods.",2011,
"Monobromoisophakellin, a New Bromopyrrole Alkaloid from the Caribbean Sponge Agelas sp.","A detailed analysis of the chemical constituents of a Caribbean specimen of Agelas sp. was carried out. Four brominated compounds (1-4) were isolated and one of them was identified as a new bromopyrrole metabolite, monobromoisophakellin (1). The structure of 1 was determined using spectroscopic methods. All compounds were tested for their antifeedant activity against the Caribbean reef fish Thalassoma bifasciatum in an aquarium assay.",2002,Zeitschrift fÃ¼r Naturforschung C
"Realism, Anti-Realism, and Materialism","Quentin Meillassoux has recently leveled a controversial attack on critical philosophy and the transcendental turn through his concept of correlationism. This critique is motivated by the attempt to move away from a philosophy of human finitude towards a speculative materialism. In this paper I argue that Meillassouxâ€™s understanding of correlationism does not adequately depict the critical turn, especially in regards to the distinction between the epistemological problem of realism and the problem of materialism. I attempt to show that by reading the critical turn as anti-epistemological one can think of its turn away from things themselves towards relation and mediation in ways that are not necessarily against either science or materialism.",2011,Angelaki
ClustOfVar : an R package for dimension reduction via clustering of variables. Application in supervised classification and variable selection in gene expressions data,"The main goal of this work is to tackle the problem of dimension reduction for high-dimensional supervised classication. The motivation is to handle gene expression data. The proposed method works in 2 steps. First, one eliminates redundancy using clustering of variables, based on the R-package ClustOfVar. This first step is only based on the exploratory variables (genes). Second, the synthetic variables (summarizing the clusters obtained at the first step) are used to construct a classifier (e.g. logistic regression, LDA, random forests). We stress that the first step reduces the dimension and gives linear combinations of original variables (synthetic variables). This step can be considered as an alternative to PCA. A selection of predictors (synthetic variables) in the second step gives a set of relevant original variables (genes). Numerical performances of the proposed procedure are evaluated on gene expression datasets. We compare our methodology with LASSO and sparse PLS discriminant analysis on these datasets.",2013,
Pancreatic stone protein,"To The Editor: We read with interest the paper by Harary et al (1) regarding the gastric retention of enteric-coated sulfasalazine tablets. Two main consequences were possible aggravation of obstruction by the pillassociated physical barrier and unpredictability of the therapeutic benefit of the tablet. Recently, a 52-year-old woman, with a six-year history of Crohn's disease involving the ileum and colonic hepatic flexure, was admitted to our Unit with a four-month history of recurrent and selflimited episodes of intestinal obstruction. Upper gastrointestinal series showed an ileum grossly affected, but allowing transit of barium to the colon. She was treated with parenteral hyperalimentation and then with enteral nutrition by continuous infusion with a good tolerance. Parenteral corticosteroids and oral sulfasalazine (one 500-mg tablet four times daily) were administered. The patient was operated on six weeks after admission because obstruction did not resolve, in fact it had even worsened. At operation, Crohn's disease involving ileum and colonic hepatic flexure was confirmed, then corroborated by histologic assessment. A lot of complete sulfasalazine tablets were observed in the intestinal lumen, proximal to the obstructing area. In our case, we think that altered intestinal transit time and pH were probably the causes for the absence of disintegration of the tablets, thus contributing to the maintenance or aggravation of the obstruction by a physical barrier phenomenon and precluding the therapeutic effect of the drug. In patients with small bowel Crohn' s disease with a partial obstruction to the transit of intestinal contents, oral tablets should be used with caution. VICENTE GARRIGUES, MD JULIO PONCE, M D JOAQUIN BERENGUER, M D Gastroenterology Unit Hospital ""La Fe"" Valencia, Spain",2005,Digestive Diseases and Sciences
Computational method to predict esophageal temperature elevations during pulmonary vein isolation.,"BACKGROUND
The esophagus is in close proximity to the posterior wall of the left atrium, which renders it susceptible to thermal injury during radiofrequency (RF) ablation procedures for atrial fibrillation (AF). Real-time assessment of esophageal position and temperature (TÂ°) during pulmonary vein (PV) isolation has not been extensively explored.


OBJECTIVE
To develop a protocol that allows estimation of the potential for, and avoidance of, esophageal heating.


METHODS
In consecutive patients who underwent PV isolation, a thermal probe was used to monitor TÂ° fluctuations in the esophagus during application of RF energy. The tip of the thermal probe was positioned at the level of the targeted PV and RF was discontinued for TÂ° rise >0.5Â°C. The proximity of individual PVs to the esophagus was measured from the temperature probe tip to the closest posterior part of the Lasso catheter from review of biplane projections (left anterior oblique 60Â° and right anterior oblique 30Â°). These raw distances were entered into the Pythagorean theorem and the actual distance between the esophageal thermal probe and PV antrum was determined.


RESULTS
The study cohort included 44 patients (60 Â± 11 years, 61% male, 57% lone AF). The thermal probe in the esophagus was closer to the left-sided PVs (left common pulmonary vein: 20.9 Â± 13 mm, left upper pulmonary vein: 20.5 Â± 11 mm, left lower pulmonary vein: 23.4 Â± 10 mm) than the right-sided ones (right common pulmonary vein: 31.0 Â± 11 mm, right upper pulmonary vein: 41.9 Â± 18 mm, right lower pulmonary vein: 34.5 Â± 16 mm). A TÂ° increase >0.5Â° C occurred during 116/1,495 (7.8%) deliveries. A TÂ° rise was more likely during ablation of left-sided PVs than right-sided PVs (55% vs 10%, P < 0.0001) and when RF was delivered â‰¤ 24 mm from the esophagus (sensitivity 91%, specificity 81%, positive predictive value 75%, and negative predictive value 93%).


CONCLUSION
A thermal probe placed in the esophagus provides real-time TÂ° monitoring and anatomic localization. A TÂ° rise is more likely during ablation of left PVs and during RF deliveries within 24 mm of the esophageal thermal probe.",2010,Pacing and clinical electrophysiology : PACE
Linear regression in the Bayesian framework,"These notes aim at clarifying different strategies to perform linear regression from given dataset. Methods like the weighted and ordinary least squares, ridge regression or LASSO are proposed in the literature. The present article is my understanding of these methods which are, according to me, better unified in the Bayesian framework. The formulas to address linear regression with these methods are derived. The KIC for model selection is also derived in the end of the document.",2019,arXiv: Methodology
Community-based capacity building in rural Senegal.,"In 1992 a Senegalese Muslim woman with experience in community development founded LAssociation Rurale de Lutte Contre le SIDA (ARLS) a community-based association to fight HIV/AIDS. She and the other founders trained themselves as HIV/AIDS educators and used their own resources to initiate IEC (information education and communication) activities in six villages within 6 months. By 1994 the group received a grant which enabled them to receive training and expand their activities. The volunteers became active in HIV/AIDS prevention because they realized that the disease could undo all that they had accomplished in their previous development work. One of the first things they did was approach the local Islamic elders to make them aware of the dangers and the proposed organization. HIV/AIDS is currently at ""preepidemic"" levels in Senegal but poverty and labor migration place the country at risk. ARLS awareness sessions involve whole villages and promote fidelity and abstinence as the primary prevention tactics. Condom use is also demonstrated. ARLS also trains village-level volunteers to carry on the education and the group has inspired the growth of similar organizations in other parts of Senegal. While villagers still resist condom use and condoms are in short supply some behaviors and attitudes are changing and villagers are seeking more information as well as condoms. Noting the link between poverty and labor migration ARLS is offering literacy training and is seeking funding for village-level income-producing activities.",1995,
18F-FDG PET image biomarkers improve prediction of late radiation-induced xerostomia.,"BACKGROUND AND PURPOSE
Current prediction of radiation-induced xerostomia 12months after radiotherapy (Xer12m) is based on mean parotid gland dose and baseline xerostomia (Xerbaseline) scores. The hypothesis of this study was that prediction of Xer12m is improved with patient-specific characteristics extracted from 18F-FDG PET images, quantified in PET image biomarkers (PET-IBMs).


PATIENTS AND METHODS
Intensity and textural PET-IBMs of the parotid gland were collected from pre-treatment 18F-FDG PET images of 161 head and neck cancer patients. Patient-rated toxicity was prospectively collected. Multivariable logistic regression models resulting from step-wise forward selection and Lasso regularisation were internally validated by bootstrapping. The reference model with parotid gland dose and Xerbaseline was compared with the resulting PET-IBM models.


RESULTS
High values of the intensity PET-IBM (90th percentile (P90)) and textural PET-IBM (Long Run High Grey-level Emphasis 3 (LRHG3E)) were significantly associated with lower risk of Xer12m. Both PET-IBMs significantly added in the prediction of Xer12m to the reference model. The AUC increased from 0.73 (0.65-0.81) (reference model) to 0.77 (0.70-0.84) (P90) and 0.77 (0.69-0.84) (LRHG3E).


CONCLUSION
Prediction of Xer12m was significantly improved with pre-treatment PET-IBMs, indicating that high metabolic parotid gland activity is associated with lower risk of developing late xerostomia. This study highlights the potential of incorporating patient-specific PET-derived functional characteristics into NTCP model development.",2018,Radiotherapy and oncology : journal of the European Society for Therapeutic Radiology and Oncology
A simulation based method for assessing the statistical significance of logistic regression models after common variable selection procedures,"ABSTRACT Classification models can demonstrate apparent prediction accuracy even when there is no underlying relationship between the predictors and the response. Variable selection procedures can lead to false positive variable selections and overestimation of true model performance. A simulation study was conducted using logistic regression with forward stepwise, best subsets, and LASSO variable selection methods with varying total sample sizes (20, 50, 100, 200) and numbers of random noise predictor variables (3, 5, 10, 15, 20, 50). Using our critical values can help reduce needless follow-up on variables having no true association with the outcome.",2017,Communications in Statistics - Simulation and Computation
"La expansiÃ³n urbana y procesos sociales en Yerba Buena (Gran San Miguel de TucumÃ¡n, TucumÃ¡n) - Urban sprawl and social processes in Yerba Buena (Gran San Miguel de TucumÃ¡n, TucumÃ¡n)","Argentina in the booming housing was closed in the late eighties and early nineties occurred in major cities: Buenos Aires (2003 Janoschka; Svampa 2003), Rosario (Braga et al 2003), Mendoza (Roitman 2003) and TucumÃ¡n (Malizia and Paolasso 2007). The phenomenon of the closed housing is also reproduced on a smaller scale in cities such as Greater San Miguel de TucumÃ¡n-GSMT (Mertins 1995). This paper analyzes the spatial distribution of housing closed, changes in urban structure and social processes resulting from the interaction between those inside and those outside. It focuses on the town of Yerba Buena, located west of GSMT, since the largest number of closed neighborhoods of cities; in 2005 and had built 45 such ventures. At present in this area combines social housing, illegal occupation of land, settlements and residential developments for the middle class and upper (MÃ¼ller 2000/01; Mertins 1995). The latter drive the formation of a CBD in rapid expansion and the construction of shopping centers, entertainment and others in advance of its borders. This expansion produces, in turn, the fragmentation of the city that is manifested in a tendency towards a ""city of islands,"" as expressed Janoschka (2002) in his new model of analysis of the Latin American metropolis. Palabras claves: Urban territorial management / Closed urbanizations / Yerba Buena Gran San Miguel de TucumÃ¡n. AÃ±o 4 Vol 2 NÃºmero 5 ISSN 1852 0006",2008,
A permutation approach for selecting the penalty parameter in penalized model selection.,"We describe a simple, computationally efficient, permutation-based procedure for selecting the penalty parameter in LASSO-penalized regression. The procedure, permutation selection, is intended for applications where variable selection is the primary focus, and can be applied in a variety of structural settings, including that of generalized linear models. We briefly discuss connections between permutation selection and existing theory for the LASSO. In addition, we present a simulation study and an analysis of real biomedical data sets in which permutation selection is compared with selection based on the following: cross-validation (CV), the Bayesian information criterion (BIC), scaled sparse linear regression, and a selection method based on recently developed testing procedures for the LASSO.",2015,Biometrics
Experimental study and Random Forest prediction model of microbiome cell surface hydrophobicity,"Experimental study and prediction model of microbiome cell surface hydrophobicity.Expected Measurement Moving Average Machine Learning model to predict CSH.Random Forest prediction model with 12 features and test R-squared of 0.992. The cell surface hydrophobicity (CSH) is an assessable physicochemical property used to evaluate the microbial adhesion to the surface of biomaterials, which is an essential step in the microbial biofilm formation and pathogenesis. For the present in vitro fermentation experiment, the CSH of ruminal mixed microbes was considered, along with other data records of pH, ammonia-nitrogen concentration, and neutral detergent fibre digestibility, conditions of surface tension and specific surface area in two different time scales. A dataset of 170,707 perturbations of input variables, grouped into two blocks of data, was constructed. Next, Expected Measurement Moving Average Machine Learning (EMMA-ML) models were developed in order to predict CSH after perturbations of all input variables. EMMA-ML is a Perturbation Theory method that combines the ideas of Expected Measurement, Box-Jenkins Operators/Moving Average, and Time Series Analysis. Seven regression methods have been tested: Multiple Linear regression, Generalized Linear Model with Stepwise Feature Selection, Partial Least Squares regression, Lasso regression, Elastic Net regression, Neural Networks regression, and Random Forests (RF). The best regression performance has been obtained with RF (EMMA-RF model) with an R-squared of 0.992. The model analysis has shown that CSH values were highly dependent on the in vitro fermentation parameters of detergent fibre digestibility, ammonia nitrogen concentration, and the expected values of cell surface hydrophobicity in the first time scale.",2017,Expert Syst. Appl.
A phylogenetic analysis of recent anseriform genera using morphological characters,"--A phylogenetic analysis of all Recent genera of the Anseriformes using 120 morphological characters supports much of the current consensus regarding intraordinal relationships. I found that (1) Anseranas hould be placed in a monotypic family; (2) Dendrocygna, Thalassornis, geese and swans, and Stictonetta re paraphyletic to the rest of the Anatidae; (3) Cereopsis the sister group to Anser and Branta, and Coscoroba is the sister group to Cygnus and Olor; (4) Plectropterus i the sister group to the Tadorninae (shelducks) and the Anatinae (typical ducks); (5) the shelducks are monophyletic and include Sarkidiornis (provisionally), Malacorhynchus, Hymenolaimus, Merganetta, and Tachyeres; (6) the tribe ""Cairinini"" (""perching ducks"") is an unnatural, polyphyletic assemblage and is rejected; (7) the dabbling ducks (including the smaller ""perching ducks"") comprise an unresolved, probably paraphyletic group; (8) tribal monophyly of the pochards (including Marmaronetta nd Rhodonessa), sea ducks (including the eiders), and stiff-tailed ducks (including Heteronetta) is confirmed; and (9) the retention of Mergellus and resurrection of Noraonyx are recommended based on clarifications of intratribal relationships. Problematic groups, effects of homoplasy, phenetic comparisons, life-history correlates, biogeographic patterns, and fossil species are discussed, and a phylogenetic classification of Recent genera is proposed. Received 18 November 1985, accepted 2 April 1986. THE order Anseriformes is considered to comprise the families Anhimidae (2 genera, 3 species) and Anatidae (approximately 43 genera and 150 species). The family Anatidae is undoubtedly one of the best-studied groups of birds, owing largely to the historical importance of waterfowl for hunting (Weller 1964a), domestication (Delacour 1964a), and aviculture (Delacour 1964b). The classification of the Anatidae proposed by Delacour and Mayr (1945) has been followed, with only minor revisions, in recent decades (e.g. Delacour 1954, 1956, 1959, 1964c; Johnsgard 1961a, 1962, 1965a, 1978, 1979; Woolfenden 1961; Frith 1967; Bellrose 1976; Palmer 1976; A.O.U. 1983; Bottjer 1983; Scott 1985). Perhaps the most innovative aspect of this system (inspired by the works of Salvadori 1895; Phillips 1922, 1923, 1925; and Peters 1931) was the erection of ""tribes,"" groups of genera that were considered to be closely related within the subfamilies of the Anatidae. These tribes became the primary focus of subsequent works on anatid classification, many of which addressed the tribal assignments of problematic genera (e.g. Humphrey and Butsch 1958; Johnsgard 1960a, 1961b; Humphrey and Ripley 1962; Davies and Frith 1964; Raikow 1971; Kear and Murton 1973). Most authors assumed the validity of the tribes and used them as working units in phylogenetic analyses of the family (e.g. Johnsgard 1961a, Bottjer 1983). A few workers named additional tribes (Moynihan 1958, Delacour 1959, Woolfenden 1961, Weller 1968b) or attempted to test the naturalness of those originally proposed (Cotter 1957, Woolfenden 1961, Brush 1976). Behavioral characters have been accorded considerable weight in classifications of waterfowl. Delacour and Mayr (1945) based their revision on characters they considered to be ""non-adaptive,"" including behavioral displays, nesting and feeding habits, and selected morphological characters (e.g. posture, body proportions, head shape, syringeal bulla). Reliance on comparative ethology in anatid systematics was furthered by the studies of Lorenz (19511953), McKinney (1953), and Myres (1959) and was increased significantly by Johnsgard (1960a-c, 1961a-d, 1962, 1964, 1965a, b, 1966a, b, 1967, 1978), whose work was largely ethological and influenced profoundly by that of Delacour (1954, 1956, 1959, 1964c). This emphasis, work on interspecific hybridization 737 The Auk 103: 737-754. October 1986 738 BRADLEY C. LIVEZEY [Auk, Vol. 103 (Sibley 1957; Gray 1958; Johnsgard 1960d, 1963), and study of plumage patterns of downy young (Delacour 1954, 1956, 1959; Frith 1955, 1964b; Kear 1967) were prompted in part by the opportunity to observe waterfowl in avicultural collections. Other data used in the classification of waterfowl include syringeal anatomy (Humphrey 1955, 1958; Johnsgard 1961e), cytogenetics (Yamashina 1952), serology (Cotter 1957, Bottjer 1983), osteology (DeMay 1940, Verheyen 1955, Humphrey and Butsch 1958, Woolfenden 1961, Humphrey and Ripley 1962, Raikow 1971), feather lice (Timmermann 1963), eggshell structure (Tyler 1964), egg-white proteins (Sibley 1960, Sibley and Ahlquist 1972), feather proteins (Brush 1976), royology (Zusi and Bentz 1978), lipids from the uropygial gland (Jacob and Glaser 1975, Jacob 1982), and mitochondrial DNA (Kessler and Avise 1984). These studies, with the possible exceptions of those by Lorenz (1953) and Kessler and Avise (1984), estimated the evolutionary relationships of groups by assessments of overall similarities; no attempts were made to determine primitive conditions or to distinguish shared primitive characters from shared derived characters (""special"" similarity). Moreover, the ""evolutionary trees"" presented in most of these works lack references to the specific characters used to support the branching patterns (e.g. Delacour and Mayr 1945; Johnsgard 1961a, 1978; Woolfenden 1961). I performed a phylogenetic (cladistic) analysis of Recent genera of Anseriformes using 120 morphological characters. I present a hypothetical evolutionary tree for the order, consider the taxonomic implications, and discuss selected life-history and biogeographic correlates and the classification of selected fossil species. Many of the characters were described first in the pioneering work of Woolfenden (1961), to whom I dedicate this paper.",1986,The Auk
