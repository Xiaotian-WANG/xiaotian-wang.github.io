title,abstract,year,journal
Linear ablation and circumferential isolation of the pulmonary vein guided by electroanatomic mapping system in patients with atrial fibrillation,"Objective The aim of this study is to use two Lasso catheter placed in ipsilateral two pulmonary veins to demonstrate the clear isolation of the pulmonary vein (PV) and the para-ostium of PV.Methods Thirteen patients [males in 8,females in 5,(56Â±8) years] with atrial fibrillation (AF)were referred for elctrophysiological study and radiofrequency catheter ablation.Among them,eight patients had frequent paroxysmal AF(1~20 years) and 5 patients had persistent AF(1~4 yeras).After reconstruction of the left atrium using Carto system during pacing at distal end of coronary sinus,two Lasso-catheters were located in right superior and inferior PVs,simultaneously.Then,circumferential ablation around two right PVs was performed at a distance of 1 cm away from the ostium of PVs.The endpoint was complete conduction block between left atrium and PVs and the para-ostium of PV as indicated by loss of PV potentials during energy delivery.After completion of the right PV isolation,the same approach for isolation of the left PV and the para-ostium of PV was performed.Results Successful isolation of PVs during sinus rhythm in 7 patients with paroxysmal AF,and during AF and sinus rhythm in 5 patients with persistent AF and 1 patients with paroxysmal AF.AF was terminated during energy delivery in 3 patients.External cardioversion was needed in the three re-maining patients.The procedure time was (256Â±56) minutes, with the X-ray exposure time (39Â±11) minutes.No complication occurred during the procedure.After (104Â±50) days of follow-up in the out-patient clinic,atypical atrial flutter occurred in 1 patients,others were free of AF.Conclusion The present approach with evidence-based isolation of the PV and the para-ostium of the PV is effective and safe.Large-scale clinical trial with long-term follow-up is needed to document the efficacy of the this approach.",2005,Chinese Journal of Cardiac Arrhythmias
Prevalence and risk factors for hearing loss in high-risk neonates in Germany.,"AIM
Hearing loss in infants is often diagnosed late, despite universal screening programmes. Risk factors of hearing impairment in high-risk neonates, identified from population-based studies, can inform policy around targeted screening. Our aim was to determine the prevalence and the risk factors of hearing loss in a high-risk neonatal population.


METHODS
This was a retrospective cohort study of neonates hospitalised at the University Hospital Cologne, Germany from January 2009 to December 2014 and were part of the newborn hearing screening programme. Multivariable regression analyses using the lasso approach was performed.


RESULTS
Data were available for 4512 (43% female) neonates with a mean gestational age at birth of 35.5Â weeks. The prevalence of hearing loss was 1.6%, and 42 (0.9%) neonates had permanent hearing loss. Craniofacial anomalies, hyperbilirubinaemia requiring exchange transfusion, oxygen supplementation after 36Â weeks of gestation and hydrops fetalis showed associations with permanent hearing loss.


CONCLUSION
Our findings of risk factors for hearing loss were consistent with other studies. However, some commonly demonstrated risk factors such as perinatal infections, meningitis, sepsis and ototoxic drugs did not show significant associations in our cohort. Targeted screening based on risk factors may help early identification of hearing loss in neonates.",2019,Acta paediatrica
Confidence Bands in Quantile Regression and Generalized Dynamic Semiparametric Factor Models,"In many applications it is necessary to know the stochastic fluctuation of the maximal deviations of the nonparametric quantile estimates, e.g. for various parametric models check. Uniform confidence bands are therefore constructed for nonparametric quantile estimates of regression functions. The first method is based on the strong approximations of the empirical process and extreme value theory. The strong uniform consistency rate is also established under general conditions. The second method is based on the bootstrap resampling method. It is proved that the bootstrap approximation provides a substantial improvement. The case of multidimensional and discrete regressor variables is dealt with using a partial linear model. A labor market analysis is provided to illustrate the method. High dimensional time series which reveal nonstationary and possibly periodic behavior occur frequently in many fields of science, e.g. macroeconomics, meteorology, medicine and financial engineering. One of the common approach is to separate the modeling of high dimensional time series to time propagation of low dimensional time series and high dimensional time invariant functions via dynamic factor analysis. We propose a two-step estimation procedure. At the first step, we detrend the time series by incorporating time basis selected by the group Lasso-type technique and choose the space basis based on smoothed functional principal component analysis. We show properties of this estimator under the dependent scenario. At the second step, we obtain the detrended low dimensional stochastic process (stationary), but it also poses an important question: is it justified, from an inferential point of view, to base further statistical inference on the estimated stochastic time series? We show that the difference of the inference based on the estimated time series and â€œtrueâ€ unobserved time series is asymptotically negligible, which finally allows one to study the dynamics of the whole high-dimensional system with a low dimensional representation together with the deterministic trend. We apply the method to our motivating empirical problems: studies of the dynamic behavior of temperatures (further used for pricing weather derivatives), implied volatilities and risk patterns and correlated brain activities (neuroeconomics related) using fMRI data, where a panel version model is also presented.",2010,
Learning Networked Exponential Families with Network Lasso,We propose networked exponential families to jointly leverage the information in the topology as well as the attributes (features) of networked data points. Networked exponential families are a flexible probabilistic model for heterogeneous datasets with intrinsic network structure. These models can be learnt efficiently using network Lasso which implicitly pools or clusters the data points according to the intrinsic network structure and the local likelihood. The resulting method can be formulated as a non-smooth convex optimization problem which we solve using a primal-dual splitting method. This primal-dual method is appealing for big data applications as it can be implemented as a highly scalable message passing algorithm.,2019,ArXiv
The effect of machine learning regression algorithms and sample size on individualized behavioral prediction with functional connectivity features,"Abstract Individualized behavioral/cognitive prediction using machine learning (ML) regression approaches is becoming increasingly applied. The specific ML regression algorithm and sample size are two key factors that nonâ€trivially influence prediction accuracies. However, the effects of the ML regression algorithm and sample size on individualized behavioral/cognitive prediction performance have not been comprehensively assessed. To address this issue, the present study included six commonly used ML regression algorithms: ordinary least squares (OLS) regression, least absolute shrinkage and selection operator (LASSO) regression, ridge regression, elasticâ€net regression, linear support vector regression (LSVR), and relevance vector regression (RVR), to perform specific behavioral/cognitive predictions based on different sample sizes. Specifically, the publicly available restingâ€state functional MRI (rsâ€fMRI) dataset from the Human Connectome Project (HCP) was used, and wholeâ€brain restingâ€state functional connectivity (rsFC) or rsFC strength (rsFCS) were extracted as prediction features. Twentyâ€five sample sizes (ranged from 20 to 700) were studied by subâ€sampling from the entire HCP cohort. The analyses showed that rsFCâ€based LASSO regression performed remarkably worse than the other algorithms, and rsFCSâ€based OLS regression performed markedly worse than the other algorithms. Regardless of the algorithm and feature type, both the prediction accuracy and its stability exponentially increased with increasing sample size. The specific patterns of the observed algorithm and sample size effects were well replicated in the prediction using reâ€testing fMRI data, data processed by different imaging preprocessing schemes, and different behavioral/cognitive scores, thus indicating excellent robustness/generalization of the effects. The current findings provide critical insight into how the selected ML regression algorithm and sample size influence individualized predictions of behavior/cognition and offer important guidance for choosing the ML regression algorithm or sample size in relevant investigations. HighlightsIndividualized prediction is influenced by regression algorithm and sample size.LASSO regression performed worse than other algorithms using rsFC feature.OLS regression performed worse than other algorithms using rsFCS feature.Prediction accuracy and its stability exponentially increased with sample size.The observed algorithm and sample size effects are robust and generalizable.",2018,NeuroImage
On the Distribution of the Adaptive LASSO,"model in finite samples as well as in the large-sample limit. We show that these distributions are typically highly non-normal regardless of the choice of tuning of the estimator. The uniform convergence rate is obtained and shown to be slower than n 1/2 in case the estimator is tuned to perform consistent model selection. Moreover, we derive confidence intervals based on the adaptive LASSO and also discuss the questionable statistical relevance of the â€™oracleâ€™-property of this estimator. Simulation results for the non-orthogonal case complement and confirm our theoretical findings for the orthogonal case. Finally, we provide an impossibility result regarding the estimation of the distribution function of the adaptive LASSO estimator.",2009,
Artemether-lumefantrine versus amodiaquine plus sulfadoxine-pyrimethamine for uncomplicated falciparum malaria in Burkina Faso: a randomised non-inferiority trial,"BACKGROUND
Artemisinin-based combination regimens are widely advocated for malarial treatment, but other effective regimens might be cheaper and more readily available. Our aim was to compare the risk of recurrent parasitaemia in patients given artemether-lumefantrine with that in those given amodiaquine plus sulfadoxine-pyrimethamine for uncomplicated malaria.


METHODS
We enrolled 521 patients aged 6 months or older with uncomplicated falciparum malaria in Bobo-Dioulasso, Burkina Faso. Patients were randomly assigned to receive standard doses of either artemether-lumefantrine (261) or amodiaquine plus sulfadoxine-pyrimethamine (260) for 3 days. Primary endpoints were the risks of treatment failure within 28 days, either unadjusted or adjusted by genotyping to distinguish recrudescence from new infection. The study is registered at controlled-trials.gov with the identifier ISRCTN54261005.


FINDINGS
Of enrolled patients, 478 (92%) completed the 28-day study. The risk of recurrent symptomatic malaria was lowest in the group given amodiaquine plus sulfadoxine-pyrimethamine (1.7%vs 10.2%; risk difference 8.5%; 95% CI 4.3-12.6; p=0.0001); as was the risk of recurrent parasitaemia (4.7%vs 15.1%; 10.4%; 5.1-15.6; p=0.0002). Nearly all recurrences were due to new infections. Recrudescences were four late treatment failures with artemether-lumefantrine and one early treatment failure with amodiaquine plus sulfadoxine-pyrimethamine. Both regimens were safe and well tolerated, with pruritus more common with amodiaquine plus sulfadoxine-pyrimethamine than with artemether-lumefantrine. Each regimen selected for new isolates with mutations that have been associated with decreased drug susceptibility.


INTERPRETATION
Amodiaquine plus sulfadoxine-pyrimethamine was more effective than was artemether-lumefantrine for the treatment of uncomplicated malaria. For regions of Africa where amodiaquine plus sulfadoxine-pyrimethamine continues to be effective, this less expensive and more available regimen should be considered as an alternative to blanket recommendations for artemisinin-based combination treatment for malaria.",2007,The Lancet
Impact and implications of the shortfall in California's K-12 education budget,"California's education shortfall can be directly related to a recession in the state and national economy. Recent developmentsin the California and national economy as they relate to education are discussed. Eighteen persons were personallyinterviewed with the open ended question, ""From your position, whatis theimpactand implications ofCalifornia's shortfall in the KÂ­ 12education budget?"" Subjects represented the district,coimty,and state education commtmity. The responses wentbeyond the usual""tighten the belt""tactics dted by school districts as the usualimpacts and implications of a budget shortfall. The responses covered a wide range ofconcerns with some ofthe shortfall'simpactsimplicating the need for structural change for relief. Law and policy changes atthe state level were cited as structiiral changes possibly broughtaboutbythe shortfall's impacts. Ofthese,AB1200has potentialfor significant change as well as the recentState Supreme Coiirt ruling allowing districts to charge bus fees. iv Impactand Implications Acknowledgement Ithas been an exciting time in mylife to be encouraged to write and share the results ofmyresearch with others. Iacknowledge all myfriends and family who encouraged me on a continuing basis to keep pursuing new challenges. For this particular project,I would like to describe afew key situations during my graduate studiesin school administration thatI believe broughtthis projectto its currentform. It started with a book list thatincluded these writers: Warren Bennis, Peter Drucker,John Naisbitt and Patricia Abiirdene, and Tom Peters. I was engrossed overly so butIknew I wasin the right program and the works of those writers made mefeel my past experience would help me contribute to my new field ofstudy. High standards and expectationsin MargotBurke's class and anintroduction to use ofthePublication ManualoftheAmerican PsychologicalAssociation got me offto a good first quarterin the program. A research class with Joseph Turpin made me realize thatmy ability to identify key personsfor solving research problems and interviewing those people would be a valuable skillin research. Kenneth Lane,advisor for my field studies,gave me plenty oflatitude to investigate questions aboutelementary and secondary school administration in a program ofshadowing school administrators attwo school campuses. Two professors provided the necessary encouragementthat allows me to speak to you through this media atthis moment. William E.Camp and David 0.Stine volunteered to be my first and second readersfor this project. Dr. V Impactand Implications Camp specializesin school finance. The opportimities he mentioned in the two finance courses Itook from him showed me a variety ofwaysthatmy education could be used in both the public schools and in the private sector. The projects assigned in his class opened the doors to meeting school administrators and discussing with them current school finance issues. It was during the time I was enrolled in his classes thatI developed a specific interestin school finance. Icredit Dr.Camp withinfluencing my decision to pmsue school finance research after completion ofthis current project. Dr. Stine reminds me ofafriend ofmine who always has five exciting volunteer opportunities to mention to you and you can hardly resist doingthem all. Dr. Stine has not quite metmyfriend Grace Lieberman's record,butthe opportunities I have pursued at his suggestion have contributed greatly to my current projects. On myfirst meeting with Dr.Stine ata San Bernardino County Superintendent's meeting,Ilearned about student membershipin the Association ofCalifornia School Administrators and he offered to be a reader for my masters thesis. I pmsued both ofthese offers. Another offer he made wasto recommend anyonein the School and Commimity Relations Classfor participation in a Western Association ofSchools and Colleges school accreditation. I took him up on this offer and had the opportunity to participate on an eight memberteam doing a WASC/CDE accreditation for a large urban high school. This particular experience was one thatsynthesized many areas ofstudyin the educational administration program into a meaningful whole. Impactand Implications",1992,
Maximizing Interpretability and Cost-Effectiveness of Surgical Site Infection (SSI) Predictive Models Using Feature-Specific Regularized Logistic Regression on Preoperative Temporal Data,"This study describes a novel approach to solve the surgical site infection (SSI) classification problem. Feature engineering has traditionally been one of the most important steps in solving complex classification problems, especially in cases with temporal data. The described novel approach is based on abstraction of temporal data recorded in three temporal windows. Maximum likelihood L1-norm (lasso) regularization was used in penalized logistic regression to predict the onset of surgical site infection occurrence based on available patient blood testing results up to the day of surgery. Prior knowledge of predictors (blood tests) was integrated in the modelling by introduction of penalty factors depending on blood test prices and an early stopping parameter limiting the maximum number of selected features used in predictive modelling. Finally, solutions resulting in higher interpretability and cost-effectiveness were demonstrated. Using repeated holdout cross-validation, the baseline C-reactive protein (CRP) classifier achieved a mean AUC of 0.801, whereas our best full lasso model achieved a mean AUC of 0.956. Best model testing results were achieved for full lasso model with maximum number of features limited at 20 features with an AUC of 0.967. Presented models showed the potential to not only support domain experts in their decision making but could also prove invaluable for improvement in prediction of SSI occurrence, which may even help setting new guidelines in the field of preoperative SSI prevention and surveillance.",2019,Computational and Mathematical Methods in Medicine
ANovelTechnique forModeling Radiation Effects inSolar Cells Utilizing SILVACOÂ®Virtual WaferFabrication Software,"A noveltechnique formodeling advanced solar cells usingSilvaco' Virtual Wafer Fabrication software hasbeenpreviously introducedfl. Overthepastthree yearsthenewmodeling approach has beenextended tocover modeling ofadvanced multijunction cells, design andoptimizations ofthese devices, aswell asdesign ofnewquad-junction cells2'3"". Inthis paper, theATLASdevice simulator fromSilvaco International hasbeendemonstrated tohavethepotentialfor predicting theeffects of electron radiation insolar cells bymodeling material defects. A gallium arsenide solar cell was simulated inATLASandcompared toanactual cell withradiation defects identified usingdeep level transient spectroscopy techniques. Thecell datawerecompared forvarious fluence levels ofI MeV electron radiation andtheresults haveshownanaverage ofonlyfivepercent difference between experimental andsimulated cell output characteristics. Theseresults demonstrate that ATLASsoftware canbeaviable toolfor predicting solar cell degradation duetoelectron radiation.",2005,
Der Drifte Internationale Kongress fÃ¼r Philosophie.,"Vom 31. August bis zum 5. September v. J. tagte in Heidelberg der III. Internationale Kongress fÃ¼r Philosophie unter zahlreicher Beteiligung (ca. 400) des Inund besonders des Auslandes. Der PrÃ¤sident des Kongresses, Wilhelm Windelband, wies in seiner ErÃ¶ffnungsrede auf den Zusammenhang zwischen der Philosophie und der Grsammtkultur hin, den er darin sieht, dass die Philosophie wie jede andere Wissenschaft durch die begriffliche Arbeit, die ihr formales Wesen ausmacht, an der einheitlichen Selbsterfassung und Selbstgestaltung des menschlichen Kulturbewusstseins mitwirkt. In dem BewÃ¼sstsein, an dieser grossen Aufgabe der SelbstverstÃ¤ndigung einer wahrhaft humanen Gesamtkultur mitzuarbeiten, erblickt er zugleich dasjenige Moment, welches die Vertreter der verschiedensten philosophischen Richtungen vereinigt und somit die Grundlage bildet fÃ¼r ihr Zusammenwirken auf einem Kongress fÃ¼r Philosophie. Die wissenschaftliche Arbeit des Kongresses wurde in 4 allgemeinen Sitzungen und in den Tagessitzungen der 7 Sektionen fÃ¼r: L Geschichte der Philosophie unter Vorsitz von XavierLoon (Paris) undPetsch (Heidelberg); . Allgemeine Philosophie, Metaphysik und Naturphilosophie unter Vorsitz von KÃ¼lpe (WÃ¼rzburg) und Drews (Karlsruhe); III. Psychologie unter Vorsitz von MÃ¼nsterberg (Harvard) und Hellpach (Karlsruhe); IV. Logik und Erkenntnistheorie unter Vorsitz von Maier (TÃ¼bingen) und Lask (Heidelberg); V. Ethik und Soziologie unter Vorsitz von Lasson (Berlin) und Bauch (Halle); VI. Ã„sthetik unter Vorsitz von Cohn (Freiburg) und Vossler (Heidelberg) und VII. ReligionsphÃ¼osophie unter Vorsitz von Troeltsch (Heidelberg) und Schwarz (Halle) geleistet. In den allgemeinen Sitzungen wurden die 5 HauptvortrÃ¤ge des Kongresses gehalten, deren Inhalt hier nur ganz kurz angegeben werden kann. Der erste Vortrag von Josiah Royce (Cambridge U. S. A.) behandelte: â€žDas Problem der Wahrheit im Lichte der neueren Forschung"". Nach Royce lassen sich die verschiedenen Theorien Ã¼ber die Wahrheit in 3 Kategorien einteilen auf Grund der Motive, denen sie ihre Entstehung verdanken. Diese sind: Instrumentalismus, Individualismus und absoluter Pragmatismus. WÃ¤hrend die beiden ersteren den relativistischen Theorien zu Grunde liegen, enthÃ¤lt letzterer, welcher sich hauptsÃ¤chlich in den modernen Forschungen auf dem Gebiet der reinen Mathematik und Logik geltend gemacht hat, eine Tendenz zur Anerkennung absoluter Wahrheiten. Royce zeigt ferner, dass eine versÃ¶hnende Synthese dieser 3 Motive mÃ¶glich iÃŸt, weil jedes derselben irgend eine Forin des Voluntarismus zur*",1909,
Bayesian Modeling and Inference for Quantile Mixture Regression,"Author(s): Yan, Yifei | Advisor(s): Kottas, Athanasios | Abstract: The focus of this work is to develop a Bayesian framework to combine information from multiple parts of the response distribution characterized with different quantiles. The goal is to obtain a synthesized estimate of the covariate effects on the response variable as well as to identify the more influential predictors. This framework naturally relates to the traditional quantile regression, which studies the relationship between the covariates and the conditional quantile of the response variable and serves as an attractive alternative to the more widely used mean regression methods. We achieve the objectives through constructing a Bayesian mixture model using quantile regressions as the mixture components.The first stage of the research involves the development of a parametric family of distributions to provide the mixture kernel for the Bayesian quantile mixture regression. We derive a new family of error distributions for model-based quantile regression called generalized asymmetric Laplace distribution, which is constructed through a structured mixture of normal distributions. The construction enables fixing specific percentiles of the distribution while, at the same time, allowing for varying mode, skewness and tail behavior. This family provides a practically important extension of the asymmetric Laplace distribution, which is the standard error distribution for parametric quantile regression. We develop a Bayesian formulation for the proposed quantile regression model, including conditional lasso regularized quantile regression based on a hierarchical Laplace prior for the regression coefficients, and a Tobit quantile regression model.Next, we develop the main framework to model the conditional distribution of the response with a weighted mixture of quantile regression components. We specify a common regression coefficient vector for all components to synthesize information from multiple parts of the response distribution, each modeled with one quantile regression component. The goal is to obtain a combined estimate of the predictive effect of each covariate. We consider the following two choices of kernel densities for the mixture model. When the probability of the quantile in each regression component is known, we model the components with the generalized asymmetric Laplace distribution, as its shape parameter introduces flexibility in shape and skewness to the kernel; else when the quantile probabilities are unknown, we use the asymmetric Laplace distribution as kernel density and view its skewness parameter, which is also the quantile probability of the component, as a random quantity and estimate it from the data. Under each kernel density, we formulate the hierarchical structure of the mixture weights and develop the approach to the posterior inference. We consider both parametric and nonparametric priors for the framework, and explore inferences for the number of components to be included. We demonstrate the performance of the method in identification of influential variables with simulation examples and illustrate the posterior predictive inferences in a realty price data from the Boston metropolitan area.Finally, we extend the framework to apply the methods to specific problems in survival analysis and epidemiology. Both applications involve analyses of two cohorts, which oftentimes exhibit differing responses given the same predictor input. We adapt the proposed framework to model the survival data with right-censoring. For applications in epidemiology, we study the ordering properties of the mixture kernels and incorporate stochastic ordering in the two-cohort mixture framework through structured priors, which conforms with the assumption in certain circumstances of receiver operating characteristic curve estimation. With the adapted models, we carry out cohort-specific identification of influential variables and gain insights into the contribution in estimation and prediction from different parts of the response distribution, which are depicted by the corresponding quantile regression components. We illustrate the applications with a time-to-event data set on length of stay at nursing home and two disease diagnosis data sets, one on adolescent depression and the other on cattle epidemics.",2017,
Dietâ€Related Metabolites Associated with Cognitive Decline Revealed by Untargeted Metabolomics in a Prospective Cohort,"SCOPE
Untargeted metabolomics may reveal preventive targets in cognitive aging, including within the food metabolome.


METHODS AND RESULTS
A case-control study nested in the prospective Three-City study included participants aged â‰¥65 years and initially free of dementia. We contrasted 209 cases of cognitive decline and 209 controls (matched for age, gender and educational level) with slower cognitive decline over up to 12 years. This article is protected by copyright. All rights reserved Using a bootstrap-enhanced LASSO regression on the baseline serum metabolomes analyzed with LC-QTOF, we identified a signature of 22 metabolites associated with subsequent cognitive decline. The signature included three coffee metabolites, a biomarker of citrus intake, a cocoa metabolite, two metabolites putatively derived from fish and wine, three medium-chain acylcarnitines, glycodeoxycholic acid, lysoPC(18:3), trimethyllysine, glucose, cortisol, creatinine and arginine. Adding the 22 metabolites to a reference predictive model for cognitive decline (conditioned on age, gender and education and including ApoE-Îµ4, diabetes, BMI and number of medications) substantially increased the predictive performance: cross-validated Area Under the Receiver Operating Curve = 75% [95% CI 70-80%] compared to 62% [95% CI 56-67%].


CONCLUSIONS
Our untargeted metabolomics study supports a protective role of specific foods (e.g., coffee, cocoa, fish) and various alterations in the endogenous metabolism responsive to diet in cognitive aging.",2019,Molecular Nutrition & Food Research
ColoGuideEx: a robust gene classifier specific for stage II colorectal cancer prognosis.,"BACKGROUND AND AIMS
Several clinical factors have an impact on prognosis in stage II colorectal cancer (CRC), but as yet they are inadequate for risk assessment. The present study aimed to develop a gene expression classifier for improved risk stratification of patients with stage II CRC.


METHODS
315 CRC samples were included in the study. Gene expression measurements from 207 CRC samples (stage I-IV) from two independent Norwegian clinical series were obtained using Affymetrix exon-level microarrays. Differentially expressed genes between stage I and stage IV samples from the test series were identified and used as input for L1 (lasso) penalised Cox proportional hazards analyses of patients with stage II CRC from the same series. A second validation was performed in 108 stage II CRC samples from other populations (USA and Australia).


RESULTS
An optimal 13-gene expression classifier (PIGR, CXCL13, MMP3, TUBA1B, SESN1, AZGP1, KLK6, EPHA7, SEMA3A, DSC3, CXCL10, ENPP3, BNIP3) for prediction of relapse among patients with stage II CRC was developed using a consecutive Norwegian test series from patients treated according to current standard protocols (n=44, p<0.001, HR=18.2), and its predictive value was successfully validated for patients with stage II CRC in a second Norwegian CRC series collected two decades previously (n=52, p=0.02, HR=3.6). Further validation of the classifier was obtained in a recent external dataset of patients with stage II CRC from other populations (n=108, p=0.001, HR=6.5). Multivariate Cox regression analyses, including all three sample series and various clinicopathological variables, confirmed the independent prognostic value of the classifier (pâ‰¤0.004). The classifier was shown to be specific to stage II CRC and does not provide prognostic stratification of patients with stage III CRC.


CONCLUSION
This study presents the development and validation of a 13-gene expression classifier, ColoGuideEx, for prognosis prediction specific to patients with stage II CRC. The robustness was shown across patient series, populations and different microarray versions.",2012,Gut
How to Use Fewer Markers in Admixture Studies,"Summary Swiss Fleckvieh has been established from 1970 as a composite of Simmental and Red Holstein Friesian cattle. Breed composition is currently reported based on pedigree information. Information on ancestry informative molecular markers potentially provides more accurate information. For the analysis Illumina Bovine SNP50 Beadchip data for 495 bulls were used. Markers were selected based on diff erence in allele frequencies in the pure populations, using FST as an indicator. Performance of sets with decreasing number of markers was compared. Th e scope of the study was to see how much we can reduce the number of markers based on FST to get a reliability that is close to that with the full set of markers. On these sets of markers hidden Markov models (HMM) and methods used in genomic selection (BayesB, partial least squares regression, LASSO variable selection) were applied. Correlations of admixture levels were estimated and compared with admixture levels based on pedigree information. FST chosen SNP gave very high correlations with pedigree based admixture. Only when using 96 and 48 SNP with the highest F ST , correlations dropped to 0.92 and 0.90, respectively.",2011,
AP 2-9 Does epidural anaesthesia decrease desaturations frequency during postoperative period ?,"Materials and Methods: 60 patients with femur fractures (ASA III) were randomly allocated in 2 groups to receive hypobaric (2 ml isobaric 0,5% bupivacaine+4 ml twice-distilled water) for unilateral (Group 1) or isobaric (3-3,5 ml 0,5%) bupivacaine for bilateral (Group 2) spinal anesthesia. After spinal anesthesia epidural catheter was placed for postoperative analgesia. In case of first signs of sensitivity recovery epidural catheter was used for extension of anesthesia â€“ patients received 4 ml 0,5% isobaric bupivacaine. The difference between groups in incidence of hypotension and/or bradicardia, inadequate anesthesia and needs in additional epidural anesthesia due to short duration of spinal block were compared with analysis of proportions: Fisherâ€™s test, Relative Risk (RR), Number Needed to Treat (NNT), Number Needed to Harm (NNH). Results and Discussion: Both groups had similar characteristics for sex, age, surgery, and duration of surgery. Significant different in hypotension and/ or bradycardia was observed (two-tailed Fisherâ€™s p=0,012): 1 from 30 patients in Group 1 and 9 from 30 patient in Group 2, RR = 9 (95%C.I. 1,21-66,75), OR=12,43 (95%C.I. 1,47-363,05), NNT=3,75. In uSA Group number of patients who needed in additional epidural anesthesia was significantly greater: 9/30 vs. 2/30 (p=0,042), RR=4,5 (95%C.I. 1,06-19,11), OR=6 (1,05-61,03), NNH=4,3. Using uSA considerably reduces risk of hypotension and/or bradicardia in high risk trauma patients with femur fractures but due to shorter duration of blockade often requires additional anesthesia. It can be suggested for potential long-term surgery. Epidural or subarachnoid catheterization solves this potential problem. Conclusion(s): uSA with hypobaric bupivacaine for high risk trauma patients with femur fractures reduces risk of hypotension but requires epidural or subarachnoid catheterization for additional anesthesia in case of long-term surgery. Acknowledgements: â€œPartners in Healthâ€ and Prof. V.V. Vlassov for Summer School â€œReseasch effectiveness in biomedicineâ€.",2010,
Japanese-Automobile Data,"Japanese-automobile data consist of 29 regular and 15 small cars with six independent variables, such as the emission rate (X1), price (X2), number of seats (X3), CO2 (X4), fuel (X4), and sales (X6). The following points are important for this book: (1) LSD discrimination: We can easily recognize that these data are LSD because X1 and X3 can separate two classes completely by two boxâ€“whisker plots. (2) Problem 3: The forward stepwise procedure selects X1, X2, X3, X4, X5, and X6 in this order. Although MNM of Revised IP-OLDF and NM of QDF are zeroes in the one-variable model (X1), QDF misclassifies all regular cars as small cars after X3 enters the model because the X3 value in small cars is four (Problem 3). These data are very suitable for explaining Problem 3 because they are easier than examination scores that use 100 items. (3) Explanation of Method 2 by these data: When we discriminate six microarray datasets by eight LDFs, only Revised IP-OLDF can naturally make the feature-selection and reduce the high-dimensionnal gene space to the small gene subspace that is a linearly separable model. We call these subspaces, â€œMatroska.â€ We establish the Matroska feature-selection method for the microarray dataset (Method 2), and the data consist of several disjoint small Matroskas with MNM = 0. Because LSD discrimination is not popular now and Method 2 has several unknown ideas, we explain these ideas by these data in addition to the Swiss banknote data from Chap. 6 and Student linearly separable data in Chap. 4. If the data are LSD, the full model is the largest Matroska that contains many smaller Matroskas in it. We already know that the smallest Matroska (the basic gene set or subspase, BGS) can describe the Matroska structure completely because MNM decreases monotonously. On the other hand, LASSO attempts to make feature-selection. If it cannot find BGS in the dataset, it cannot explain the dataset structure. Therefore, LASSO researchers have better examine their method by two common data before examining microarray datasets. If they are not successful in these ordinary data, it is not logical for them to expect a successful result for gene analysis. In particular, Japanese-automobile data are simple data for feature-selection because only two one-variable models are linearly separable and BGSs.",2016,
Adapting the Diagonalization Method by Reformulations,"Extending the planADbased paradigm for auto-mated theorem proving, we developed in previ-ous work a declarative approach towards rep-resenting methods in a proof planning frame-work to support their mechanical modification.This paper presents a detailed study of a classof particular methods, embodying variations ofa mathematical technique called diagonaliza-tion. The purpose of this paper is mainly two-fold. First we demonstrate that typical math-ematical methods can be represented in ourframework in a natural way. Second we illus-trate our philosophy of proof planning: besidesplanning with a fixed repertoire of methods,metaADmethods create new methods by modify-ing existing ones. With the help of three differ-ent diagonalization problems we present an ex-ample trace protocol of the evolution of meth-ods: an initial method is extracted from a par-ticular successful proof. This initial method isthen reformulated for the subsequent problems,and more general methods can be obtained byabstracting existing methods. Finally we comeup with a fairly abstract method capable ofdealing with all the three problems, since it cap-tures the very key idea of diagonalization.",1999,
Purification and characterization of an organic solvent-tolerant alkaline cellulase from a halophilic isolate of Thalassobacillus,An extracellular cellulase from Thalassobacillus sp. LY18 was purified 4.5-fold with a recovery of 21Â % and a specific activity of 52.4Â UÂ mgâˆ’1 protein. Its molecular mass was 61Â kDa estimated by SDS-PAGE. It was an endoglucanase for soluble cellulose with optimal activity was at 60Â Â°C and pH 8 with 10Â % (w/v) NaCl. It was stable from 30 to 80Â Â°C and from pH 7 to 11 with NaCl from 5 to 17.5Â % (w/v). EDTA inhibited activity indicating it was a metalloenzyme. Inhibition by diethyl pyrocarbonate and Î²-mercaptoethanol suggested that histidine residues and disulfide bonds may play important roles in its catalytic function. The cellulase was highly active in non-ionic surfactants and was stable in water-insoluble organic solvents with log PowÂ â‰¥Â 2.13.,2012,Biotechnology Letters
Short- to Mid-term Day-Ahead Electricity Price Forecasting Using Futures,"Due to the liberalization of markets, the change in the energy mix and the surrounding energy laws, electricity research is a dynamically altering field with steadily changing challenges. One challenge especially for investment decisions is to provide reliable short to mid-term forecasts despite high variation in the time series of electricity prices. This paper tackles this issue in a promising and novel approach. By combining the precision of econometric autoregressive models in the short-run with the expectations of market participants reflected in future prices for the short- and mid-run we show that the forecasting performance can be vastly increased while maintaining hourly precision. We investigate the day-ahead electricity price of the EPEX Spot for Germany and Austria and setup a model which incorporates the Phelix future of the EEX for Germany and Austria. The model can be considered as an AR24-X model with one distinct model for each hour of the day. We are able to show that future data contains relevant price information for future time periods of the day-ahead electricity price. We show that relying only on deterministic external regressors can provide stability for forecast horizons of multiple weeks. By implementing a fast and efficient lasso estimation approach we demonstrate that our model can outperform several other models in the literature.",2018,arXiv: Statistical Finance
Deregressed EBV as the response variable yield more reliable genomic predictions than traditional EBV in pure-bred pigs,"BackgroundGenomic selection can be implemented by a multi-step procedure, which requires a response variable and a statistical method. For pure-bred pigs, it was hypothesised that deregressed estimated breeding values (EBV) with the parent average removed as the response variable generate higher reliabilities of genomic breeding values than EBV, and that the normal, thick-tailed and mixture-distribution models yield similar reliabilities.MethodsReliabilities of genomic breeding values were estimated with EBV and deregressed EBV as response variables and under the three statistical methods, genomic BLUP, Bayesian Lasso and MIXTURE. The methods were examined by splitting data into a reference data set of 1375 genotyped animals that were performance tested before October 2008, and 536 genotyped validation animals that were performance tested after October 2008. The traits examined were daily gain and feed conversion ratio.ResultsUsing deregressed EBV as the response variable yielded 18 to 39% higher reliabilities of the genomic breeding values than using EBV as the response variable. For daily gain, the increase in reliability due to deregression was significant and approximately 35%, whereas for feed conversion ratio it ranged between 18 and 39% and was significant only when MIXTURE was used. Genomic BLUP, Bayesian Lasso and MIXTURE had similar reliabilities.ConclusionsDeregressed EBV is the preferred response variable, whereas the choice of statistical method is less critical for pure-bred pigs. The increase of 18 to 39% in reliability is worthwhile, since the reliabilities of the genomic breeding values directly affect the returns from genomic selection.",2011,"Genetics, Selection, Evolution : GSE"
Application of Machine Learning for Predicting Clinically Meaningful Outcome After Arthroscopic Femoroacetabular Impingement Surgery,"Background: Hip arthroscopy has become an important tool for surgical treatment of intra-articular hip pathology. Predictive models for clinically meaningful outcomes in patients undergoing hip arthroscopy for femoroacetabular impingement syndrome (FAIS) are unknown. Purpose: To apply a machine learning model to determine preoperative variables predictive for achieving the minimal clinically important difference (MCID) at 2 years after hip arthroscopy for FAIS. Study Design: Case-control study; Level of evidence, 3. Methods: Data were analyzed for patients who underwent hip arthroscopy for FAIS by a high-volume fellowship-trained surgeon between January 2012 and July 2016. The MCID cutoffs for the Hip Outcome Scoreâ€“Activities of Daily Living (HOS-ADL), HOSâ€“Sport Specific (HOS-SS), and modified Harris Hip Score (mHHS) were 9.8, 14.4, and 9.14, respectively. Predictive models for achieving the MCID with respect to each were built with the LASSO algorithm (least absolute shrinkage and selection operator) for feature selection, followed by logistic regression on the selected features. Study data were analyzed with PatientIQ, a cloud-based research and analytics platform for health care. Results: Of 1103 patients who met inclusion criteria, 898 (81.4%) had a minimum of 2-year reported outcomes and were entered into the modeling algorithm. A total of 74.0%, 73.5%, and 79.9% met the HOS-ADL, HOS-SS, and mHHS threshold scores for achieving the MCID. Predictors of not achieving the HOS-ADL MCID included anxiety/depression, symptom duration for >2 years before surgery, higher body mass index, high preoperative HOS-ADL score, and preoperative hip injection (all P < .05). Predictors of not achieving the HOS-SS MCID included anxiety/depression, preoperative symptom duration for >2 years, high preoperative HOS-SS score, and preoperative hip injection, while running at least at the recreational level was a predictor of achieving HOS-SS MCID (all P < .05). Predictors of not achieving the mHHS MCID included history of anxiety or depression, high preoperative mHHS score, and hip injections, while being female was predictive of achieving the MCID (all P < .05). Conclusion: This study identified predictive variables for achieving clinically meaningful outcome after hip arthroscopy for FAIS. Patient factors including anxiety/depression, symptom duration >2 years, preoperative intra-articular injection, and high preoperative outcome scores are most consistently predictive of inability to achieve clinically meaningful outcome. These findings have important implications for shared decision-making algorithms and management of preoperative expectations after hip arthroscopy for FAI.",2019,The American Journal of Sports Medicine
Graph Classification Based on Sparse Graph Feature Selection and Extreme Learning Machine,"Identification and classification of graph data is a hot research issue in pattern recognition. The conventional methods of graph classification usually convert the graph data to vector representation which ignore the sparsity of graph data. In this paper, we propose a new graph classification algorithm called graph classification based on sparse graph feature selection and extreme learning machine. The key of our method is using lasso to select sparse feature because of the sparsity of the corresponding feature space of the graph data, and extreme learning machine (ELM) is introduced to the following classification task due to its good performance. Extensive experimental results on a series of benchmark graph datasets validate the effectiveness of the proposed methods.",2017,Neurocomputing
Woundinfections acquired fromadisperser ofan unusual strain ofStaphylococcus aureus,"SYNOPSISSevenpost-operative woundswere infected witha strain ofStaphylococcus aureus, probably acquired fromatheatre orderly whosuffered fromadrygeneralized eczema.Theorderly was anasal andheavy skincarrier, andwasshowntobeadisperser oftheepidemic strain. Infection was probably acquired fromairborne contamination intheoperating theatre, since theorderly did notscrub-up. Theepidemic strain (phage type80/81 at1,000 routine testdilution) was sensitive topenicillin andresistant totetracycline andnovobiocin. Neomycin resistance was variable. Thisstrain was foundtoloseresistance toneomycin whensubcultured intheabsence oftheantibiotic. Nasalorskincarriers ofStaphylococcus aureusin theoperating theatre haveoften beenincriminated assources ofpost-operative woundinfection (e.g., Devenish and Miles, 1939;Blowers, Mason, Wallace, andWalton, 1955;Shooter, Griffiths, Cook,andWilliams, 1957; Penikett, Knox,and Liddell, 1958;Browne, Ryan,Glassow, Martin, andShouldice, 1959). Although thepassage of organisms through punctures ingloves maybethe major factor, theactual mechanism oftransfer from carrier topatient israrely certain; evidence ofinfection fromunscrubbed persons intheoperating roomisunusual (Sompolinsky, Hermann, Oeding, andRippon, 1957; Walter, Kundsin, andBrubaker, 1963; Bassett, Ferguson, Hoffman, Walton, Blowers, andConn,1963). Inaddition topersons withinfections orcertain skindiseases, somestaphylococcalcarriers withapparently normalskinrelease manyparticles carrying Staph. aureus intotheenvironment (Duguid andWallace, 1948; Hareand Thomas, 1956; NobleandDavies, 1965; Bethune, Blowers, Parker, andPask, 1965), andoneofthese dispersers intheoperating theatre wouldbea special hazard. Thepresent investigation describes post-operative woundinfections probably acquired fromatheatre orderly whowasanasal andskin carrier ofStaph. aureus. Dispersal ofstaphylococci intotheenvironment bythecarrier wasstudied inthelaboratory. Thestaphylococcus isolated fromtheinfections andthecarrier wasan unusual strain showing",1967,
Desain Didaktis Konsep Operasi Hitung Bilangan Pecahan Pada Pembelajaran Matematika Smp,"Penelitian ini dilatarbelakangi oleh learning obstacle yang dialami oleh siswa dan juga banyaknya kasus siswa sulit melakukan operasi hitung pada bilangan pecahan. Berdasarkan hal tersebut maka penelitian ini dibuat dengan tujuan untuk meminimalisir learning obstacle yang ada dengan membuat desain didaktis sebagai alternatif yang dapat digunakan dalam pembelajaran. Desain didaktis disusun dengan memperhatikan learning trajectory dan teori belajar yang relevan dan disesuaikan dengan learning obstacle yang ada. Terdapat 4 desain didaktis pada konsep operasi hitung bilangan pecahan, yaitu yang pertama mengenai kaitan antara pecahan desimal dan pecahan biasa, kedua mengenai operasi penjumlahan dan pengurangan bilangan pecahan, ketiga mengenai operasi perkalian dan pembagian bilangan pecahan, dan keempat mengenai mengurutkan bilangan pecahan. Desain didaktis tersebut diimplementasikan kepada siswa kelas VII Sekolah Menengah Pertama. Metode penelitian yang digunakan adalah kualitatif dengan teknik pengumpulan data triangulasi melalui observasi, wawancara, dan dokumentasi. Dari hasil penelitian, diperoleh bahwa beberapa learning obstacle siswa dapat diantisipasi sedangkan masih terdapat learning obstacle yang belum mampu diantisipasi yaitu learning obstalce dalam mengurutkan bilangan pecahan. Berdasarkan hasil implementasi, dilakukan perbaikan kembali pada desain didaktis awal yang kemudian disebut desain didaktis revisi dan dapat dijadikan salah satu alternatif bahan ajar dalam pembelajaran operasi hitung bilangan pecahan baik pecahan biasa ataupun pecahan desimal.;---The research was distributed by the learning obstacle and also many cases of difficult students perform arithmetic operations on fractions. Based on that, this research has been created aiming to minimize learning obstacle which exist, with creating lesson design as an alternative that can be use in learning. Lesson design was drwan up thus show didactical learning trajectory and learningâ€™s theory that are relevant and matched to the learning obstacle which exist. There are four design on the concept of arithmetic operations fractions, i.e. the first regarding the linkage between decimal fractions and common fraction, the second regarding operations of addition and subtraction of fractions, the third regarding multiplication and division of the fractions, and the fourt regarding ordering fractions. The lesson design was implemented to students of VII clas junior high school. The research methods used qualitative data collection techniques with triangulation through observation, interviews, and documentation. Based on the result of this research, provided that some students learning obstacle can be anticipated while there is still a learning obstacle that has not been able to anticipate i.e learning obstacle regarding ordering fractions. Based on the result of implementation, carried out the repairs back on the design of earlu lasson design later called lesson design revision and can serve as one alternative learning materials in learning arithmatic operations both common fractions or a decimal fraction.",2017,
Probabilistic Signal Recovery and Random Matrices,"Abstract : Our research program spanned several areas of mathematics and data science. In the area of highdimensionalinference, we showed that classical methods for linear regression (such as Lasso) areapplicable for non-linear data. This surprising finding has already found several applications in theanalysis of genetic, fMRI and proteomic data, compressed sensing, coding and quantization. In the area ofnetwork analysis, we showed how to detect communities in sparse networks by using semidefiniteprogramming and regularized spectral clustering. In high dimensional convex geometry, we studied thecomplexity of convex sets. In numerical linear algebra, we analyzed the fastest known randomizedapproximation algorithm for computing the permanents of matrices with non-negative entries. Incomputational graph theory, we studied a randomized algorithm for estimating the number of perfectmatchings in general graphs. In random matrix theory, we established delocalization of eigenvectors for awide class of random matrices, proved a sharp invertibility result for sparse random matrices, showed howto improve the norm of a general random matrix by removing a small submatrix, and developed a simpleand general tool for bounding the deviation of random matrices on arbitrary geometric sets. This has applications for dimension reduction, regression and compressed sensing.",2016,
Exploiting Hidden Persistent Structures in Multivariate Tensor-Based Morphometry and Its Application to Detecting White Matter Abnormality in Maltreated Children,"We present novel multivariate tensor-based morphometry (TBM) for characterizing white matter abnormalities. Traditionally TBM is used in quantifying tissue volume changes in a massive univariate fashion. At each voxel, the Jacobian determinant obtained from TBM is used as the response variable in a general linear model (GLM) and a test statistic is constructed. However, this obvious approach cannot be used in testing, for instance, if the change in one voxel is related to other voxels. To address this limitation of univariate-TBM, we propose a novel multivariate framework for more complex relational hypotheses across brain regions. To develop multivariate-TBM, it is necessary to regularize ill-conditioned covariance matrix by incorporating sparse penalty. Unfortunately, most sparse models like compressed sensing, sparse likelihood and LASSO cause a serious computational bottleneck. The computational bottleneck can be bypassed by exploiting hidden persistent structures in the sparse models. The proposed methods are applied to quantify abnormal white matter in maltreated children to show multivariate-TBM combined with persistent homology can extract additional information that cannot be obtained in univariate-TBM.",2012,
On Shrinkage Estimation: Non-orthogonal Case,"In this paper, we consider the estimation of the parameters of the non-orthogonal regression model, when we suspect a sparsity condition. We provide with a comparative performance characteristics of the primary penalty estimators, namely, the ridge and the LASSO, with the least square estimator, restricted LSE, preliminary testÂ  and Stein-type ofÂ  estimators, when the dimension of the parameterÂ  space is less than the dimensionÂ  of the sample space. Using the principle of marginal distribution theory, the analysis of risks leads to the following conclusions: (i) ridge estimator outperforms least squares, preliminary test and Stein-type estimators uniformly, (ii) The restricted least squares estimator and LASSO are competitive, although LASSO lags behind the restricted least squares estimator uniformly. Both estimators outperform the least squares, preliminary test, and Stein-type estimators in a subspace, respectively.Â  (iii) The lower bound risk expression of LASSO does not depend on the threshold parameter. (iv) Performance of the estimators depends upon the size of numbers of active coefficients, non-active coefficients, and the divergence parameter. In support of our conclusion, we prepare some tables and graphs relevant to the properties of the estimators.",2018,"Statistics, Optimization and Information Computing"
Large-scale Feature Selection of Risk Genetic Factors for Alzheimer's Disease via Distributed Group Lasso Regression,"Genome-wide association studies (GWAS) have achieved great success in the genetic study of Alzheimer's disease (AD). Collaborative imaging genetics studies across different research institutions show the effectiveness of detecting genetic risk factors. However, the high dimensionality of GWAS data poses significant challenges in detecting risk SNPs for AD. Selecting relevant features is crucial in predicting the response variable. In this study, we propose a novel Distributed Feature Selection Framework (DFSF) to conduct the large-scale imaging genetics studies across multiple institutions. To speed up the learning process, we propose a family of distributed group Lasso screening rules to identify irrelevant features and remove them from the optimization. Then we select the relevant group features by performing the group Lasso feature selection process in a sequence of parameters. Finally, we employ the stability selection to rank the top risk SNPs that might help detect the early stage of AD. To the best of our knowledge, this is the first distributed feature selection model integrated with group Lasso feature selection as well as detecting the risk genetic factors across multiple research institutions system. Empirical studies are conducted on 809 subjects with 5.9 million SNPs which are distributed across several individual institutions, demonstrating the efficiency and effectiveness of the proposed method.",2017,ArXiv
Reducing Wait Time Prediction In Hospital Emergency Room: Lean Analysis Using a Random Forest Model,"Most of the patients visiting emergency departments face long waiting times due to overcrowding which is a major concern across the hospital in the United States. Emergency Department (ED) overcrowding is a common phenomenon across hospitals, which leads to issues for the hospital management, such as increased patient's dissatisfaction and an increase in the number of patients choosing to terminate their ED visit without being attended to by a medical healthcare professional. Patients who have to Leave Without Being Seen (LWBS) by doctors often leads to loss of revenue to hospitals encouraging healthcare professionals to analyze ways to improve operational efficiency and reduce the operational expenses of an emergency department. To keep patients informed of the conditions in the emergency room, recently hospitals have started publishing wait times online. Posted wait times help patients to choose the ED which is least overcrowded thus benefiting patients with shortest waiting time and allowing hospitals to allocate and plan resources appropriately. This requires an accurate and efficient method to model the experienced waiting time for patients visiting an emergency medical services unit. In this thesis, the author seeks to estimate the waiting time for low acuity patients within an ED setting; using regularized regression methods such as Lasso, Ridge, Elastic Net, SCAD and MCP; along with tree-based regression (Random Forest). For accurately capturing the dynamic state of emergency rooms, queues of patients at various stage of ED is used as candidate predictor variables along with time patient's arrival time to account for diurnal variation. Best waiting time prediction model is selected based on the analysis of historical data from the hospital. Tree-based regression model predicts wait time of",2017,
Deciphering the genomic architecture of the stickleback brain with a novel multilocus gene-mapping approach.,"Quantitative traits important to organismal function and fitness, such as brain size, are presumably controlled by many small-effect loci. Deciphering the genetic architecture of such traits with traditional quantitative trait locus (QTL) mapping methods is challenging. Here, we investigated the genetic architecture of brain size (and the size of five different brain parts) in nine-spined sticklebacks (Pungitius pungitius) with the aid of novel multilocus QTL-mapping approaches based on a de-biased LASSO method. Apart from having more statistical power to detect QTL and reduced rate of false positives than conventional QTL-mapping approaches, the developed methods can handle large marker panels and provide estimates of genomic heritability. Single-locus analyses of an F2 interpopulation cross with 239 individuals and 15 198, fully informative single nucleotide polymorphisms (SNPs) uncovered 79 QTL associated with variation in stickleback brain size traits. Many of these loci were in strong linkage disequilibrium (LD) with each other, and consequently, a multilocus mapping of individual SNPs, accounting for LD structure in the data, recovered only four significant QTL. However, a multilocus mapping of SNPs grouped by linkage group (LG) identified 14 LGs (1-6 depending on the trait) that influence variation in brain traits. For instance, 17.6% of the variation in relative brain size was explainable by cumulative effects of SNPs distributed over six LGs, whereas 42% of the variation was accounted for by all 21 LGs. Hence, the results suggest that variation in stickleback brain traits is influenced by many small-effect loci. Apart from suggesting moderately heritable (h2 Â â‰ˆÂ 0.15-0.42) multifactorial genetic architecture of brain traits, the results highlight the challenges in identifying the loci contributing to variation in quantitative traits. Nevertheless, the results demonstrate that the novel QTL-mapping approach developed here has distinctive advantages over the traditional QTL-mapping methods in analyses of dense marker panels.",2017,Molecular ecology
High-dimensional grouped folded concave penalized estimation via the LLA algorithm,"Abstract The group folded concave penalization problems have been shown to process the satisfactory oracle property theoretically. However, it remains unknown whether the optimization algorithm for solving the resulting nonconvex problem can find such oracle solution among multiple local solutions. In this paper, we extend the well-known local linear approximation (LLA) algorithm to solve the group folded concave penalization problem for the linear models. We prove that, with the group LASSO estimator as the initial value, the two-step LLA solution converges to the oracle estimator with overwhelming probability, and thus closing the theoretical gap. The results are high-dimensional which allow the group number to grow exponentially, the true relevant groups and the true maximum group size to grow polynomially. Numerical studies are also conducted to show the merits of the LLA procedure.",2019,Journal of The Korean Statistical Society
"New Information on the Pterosaur Tupandactylus imperator, with Comments on the Relationships of Tapejaridae","A new specimen of Tupandactylus imperator, comprising an incomplete skull with associated lower jaw, is described. The material is the best preserved specimen of this species known so far and provides new information on the anatomy of this pterodactyloid pterosaur, especially with respect to the morphology of the lower jaw, the first one formally described for the species. Also, the new specimen shows an extensive preservation of soft tissues such as the soft-tissue component of the headcrest, ramphoteca associated with the premaxillae and lower jaw, as well as probable pycnofibres. A phylogenetic analysis was performed in order to test the relationships of the taxon within Tapejaridae. The results of the analysis support Tapejaridae, as well as monophyly of Tapejarinae and Thalassodrominae.",2011,
FMRI analysis of cocaine addiction using k-support sparsity,"In this paper, we explore various sparse regularization techniques for analyzing fMRI data, such as LASSO, elastic net and the recently introduced k-support norm. Employing sparsity regularization allow us to handle the curse of dimensionality, a problem commonly found in fMRI analysis. We test these methods on real data of both healthy subjects as well as cocaine addicted ones and we show that although LASSO has good prediction, it lacks interpretability since the resulting model is too sparse, and results are highly sensitive to the regularization parameter. We find that we can improve prediction performance over the LASSO using elastic net or the k-support norm, which is a convex relaxation to sparsity with an â„“2 penalty that is tighter than the elastic net. Elastic net and k-support norm overcome the problem of overly sparse solutions, resulting in both good prediction and interpretable solutions, while the k-support norm gave better prediction performance. Our experimental results support the general applicability of the k-support norm in fMRI analysis, both for prediction performance and interpretability.",2013,2013 IEEE 10th International Symposium on Biomedical Imaging
"Late Cretaceous palynofloras from the southern Laurasian margin in the Xigaze region, Xizang (Tibet)","Palaeontology plays an indispensable role in interpreting the sedimentary and tectonic history of the convergence of India and Laurasia and its consequences. Hitherto, spores and pollen grains have not been used very much in this connection. In this paper, we consider the palynology of two sedimentary successions in Xizang (Tibet) that reflect the early stages of this convergence. One of these is near the town of Xigaze, in Xigaze County (the eastern site), and the other is in Zhongba County (the western site). The palynomorph assemblages from these sites are broadly comparable in being dominated by gymnosperm pollen taxa, especially Classopollis and Exesipollenites. Angiosperm pollen grains are more common at the eastern site, especially representatives of the genus Proteacidites. The assemblages are considered to be late Late Cretaceous (SantonianeMaastrichtian) in age and, in common with the lithologies from which they were recovered, reflect a hot, arid or semi-arid climate comparable to that prevailing in South China during this period. 2007 Elsevier Ltd. All rights reserved.",2008,Cretaceous Research
Estimating Non-stationary Spatial Covariance Matrix using Multi-resolution Knots,"Providing a best linear unbiased predictor (BLUP) is always a challenge for a nonrepetitive, irregularly spaced, spatial data. The estimation process as well as prediction involves inverting an n Ã— n covariance matrix, which computationally requires O(n3). Studies showed the potential observed process covariance matrix can be decomposed into two additive matrix components, measurement error and an underlying process which can be non-stationary. The non-stationary component is often assumed to be fixed but low rank. This assumption allows us to write the underlying process as a linear combination of fixed numbers of spatial random effects, known as fixed rank kriging (FRK). The benefit of smaller rank has been used to improve the computation time as O(nr2), where r is the rank of the low rank covariance matrix. In this work we generalize FRK, by rewriting the underlying process as a linear combination of n random effects, although only a few among these are actually responsible to quantify the covariance structure. Further, FRK considers the covariance matrix of the random effect can be represented as product of rÃ— r cholesky decomposition. The generalization leads us to a n Ã— n cholesky decomposition and use a group-wise penalized likelihood where each row of the lower triangular matrix is penalized. More precisely, we present a two-step approach using group LASSO type shrinkage estimation technique for estimating the rank of the covariance matrix and finally the matrix itself. We investigate our findings over a set of simulation study and finally apply to a rainfall data obtained on Colorado, US.",2016,
Determinants of Bond Risk Premia,"This paper provides new evidence on links between bond risk premia and macroeconomic fundamentals. Using a two-step adaptive group lasso procedure, we extract a new, parsimonious, and intuitive macro factor â€”â€” a combination of the employment, housing, financial, and inflation factors â€”â€” from 131 macroeconomic series. The new factor not only has strong forecast power for excess bond returns but also subsumes the existing macro-based return predictors. It is also unspanned, suggesting a need for term structure models that can accommodate both unspanned latent and macro factors. Results from a nite-sample analysis reject the null hypothesis that bond risk premia are uncorrelated with the macroeconomy.",2010,
Lasso based gene selection for linear classifiers,"Selecting a subset of genes with strong discriminative power is a very important step in classification problems based on gene expression data. Lasso is known to have automatic variable selection ability in linear regression analysis. This paper uses Lasso to select most informative genes to represent the class label as a linear function of the gene expression data. The selected genes are further used to fit linear classifiers for tumor classification. The proposed approach (gene selection and linear classification) was applied to 5 publicly available cancer datasets. Compared to other methods in literature, the proposed method achieves similar or higher classification accuracy with fewer genes.",2009,2009 IEEE International Conference on Bioinformatics and Biomedicine Workshop
Machine Learning Methods in Gastroenterology.,"2 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 Dear Sir: We read with interest the article by Hsu et al, recently published in Gastroenterology. Predictive scores are very useful to develop more proper screening strategies and the paper by Hsu et al is of paramount importance, because it incorporates information from common genetic susceptibility loci. The correct merging of clinical, epidemiologic, and genetic parameters certainly adds to the performance of the final predictive risk model. Hsu et al included 27 validated common colorectal cancer susceptibility loci identified from genome-wide association studies together with clinical parameters such as age, sex, family history, and history of endoscopy (often neglected in previous works). However, in our opinion, some statistical concerns should be raised with regard to the methodology adopted in building the model. The authors quantify the association of colorectal cancer risk with baseline and genetic parameters by means of logistic regression and express it in terms of odds ratios and 95% CIs. The accuracy of classic logistic regression models may be impaired if many predictor variables are included (particularly when genetic parameters such as susceptibility loci or alleles are considered). In these circumstances, this results in inaccurate regression coefficients and large CIs. Sometimes, when the number of predictors is comparable with or even greater than the number of subjects enrolled, certain models will not converge at all (the calculation for the prediction cannot be performed and the equation cannot be solved). The study by Hsu et al has the great strength to present a very large sample size both with regard to training set and validation cohort. However, we think that a proper approach to address the aforementioned limitation of classic regression would be to apply a least absolute shrinkage and selection operator (LASSO) technique. The LASSO is a regression method that penalizes the absolute size of regression coefficients (or equivalently constrains the sum of the absolute values of the estimates), thus leading to a situation where some of the parameter estimates may be exactly zero. The larger the penalty applies, the further the estimates are shrunk toward zero. In this way, LASSO facilitates variable selection by incorporating only those variables most strongly correlated with the outcome of interest. In creating a more condensed clinical prediction model that only includes variables highly associated with the outcome, the condensed model can outperform a more comprehensive approach, particularly in presence of a high number of parameters.",2015,Gastroenterology
Least squares approximation with a diverging number of parameters,"Regularized regression with the l1 penalty is a popular approach for variable selection and coefficient estimation. For a unified treatment of the l1-constrained model selection, Wang and Leng (2007) proposed the least squares approximation method (LSA) for a fixed dimension. LSA makes use of a quadratic expansion of the loss function and takes full advantage of the fast Lasso algorithm in Efron etÃ‚ al. (2004). In this paper, we extend the fixed dimension LSA to the situation with a diverging number of parameters. We show that LSA possesses the oracle properties under appropriate conditions when the number of variables grows with the sample size. We propose a new tuning parameter selection method which achieves the oracle properties. Extensive simulation studies confirmed the theoretical results.",2010,Statistics & Probability Letters
LASSO-based feature selection and naÃ¯ve Bayes classifier for crime prediction and its type,"For centuries, crime has been viewed as random because it is based on human behavior; even now, it incorporates an excessive number of factors for current machine learning models to forecast accurately. In this work, we tend to discuss the early crime prediction results from a model developed using the data from the Chicago crime dataset. In any case, with a superior execution future crime is to anticipated accurately, it is a testing assignment as a result of the increase in several crimes in present days. Therefore, the crime foreseeing method is foremost, and it identifies the future crimes and number of crimes are degraded. In this paper, we built up a model to anticipate future crime occurrences at a future time and also predict which type of crime may be happening in a given area. First, we analyze how certain crime features like given a date, time and some geologically important relevant features such as latitude and longitude. Second, we discuss several analytics techniques we used to find meaning in our data, such as LASSO feature selection analysis, classification models like naÃ¯ve Bayes and SVM. Finally, we select the best model for foreseeing crime type and seriousness of the crime for giving different features.",2019,Service Oriented Computing and Applications
Discovery of pre-therapy 2-deoxy-2-18F-fluoro-D-glucose positron emission tomography-based radiomics classifiers of survival outcome in non-small-cell lung cancer patients,"PurposeThe aim of this multi-center study was to discover and validate radiomics classifiers as image-derived biomarkers for risk stratification of non-small-cell lung cancer (NSCLC).Patients and methodsPre-therapy PET scans from a total of 358 Stage Iâ€“III NSCLC patients scheduled for radiotherapy/chemo-radiotherapy acquired between October 2008 and December 2013 were included in this seven-institution study. A semi-automatic threshold method was used to segment the primary tumors. Radiomics predictive classifiers were derived from a training set of 133 scans using TexLAB v2. Least absolute shrinkage and selection operator (LASSO) regression analysis was used for data dimension reduction and radiomics feature vector (FV) discovery. Multivariable analysis was performed to establish the relationship between FV, stage and overall survival (OS). Performance of the optimal FV was tested in an independent validation set of 204 patients, and a further independent set of 21 (TESTI) patients.ResultsOf 358 patients, 249 died within the follow-up period [median 22 (range 0â€“85) months]. From each primary tumor, 665 three-dimensional radiomics features from each of seven gray levels were extracted. The most predictive feature vector discovered (FVX) was independent of known prognostic factors, such as stage and tumor volume, and of interest to multi-center studies, invariant to the type of PET/CT manufacturer. Using the median cut-off, FVX predicted a 14-month survival difference in the validation cohort (NÂ =â€‰204, pÂ =â€‰0.00465; HRâ€‰=â€‰1.61, 95% CI 1.16â€“2.24). In the TESTI cohort, a smaller cohort that presented with unusually poor survival of stage I cancers, FVX correctly indicated a lack of survival difference (NÂ =â€‰21, pÂ =â€‰0.501). In contrast to the radiomics classifier, clinically routine PET variables including SUVmax, SUVmean and SUVpeak lacked any prognostic information.ConclusionPET-based radiomics classifiers derived from routine pre-treatment imaging possess intrinsic prognostic information for risk stratification of NSCLC patients to radiotherapy/chemo-radiotherapy.",2018,European Journal of Nuclear Medicine and Molecular Imaging
Connectivity-based parcellation of putamen using resting state fMRI data,"In this paper, we present a novel framework for parcellation of a brain region into functional sub-regions based on connectivity patterns between brain regions. The proposed method takes the prior neurological information into consideration and aims at finding spatially continuous and functionally consistent sub-regions in a given brain area. The proposed framework relies on 1) a sparse spatially regularized fused lasso regression model for encouraging spatially and functionally adjacent voxels to share similar regression coefficients despite of spatial noise; 2) an iterative voxels (groups) merging and adaptive parameter tuning process; and 3) a Graph-Cut optimization algorithm for assigning overlapped voxels into separate sub-regions. With spatial information incorporated, spatially continuous and functionally consistent sub-regions could be obtained and further used for subsequent brain connectivity analysis.",2015,2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI)
"Comparing Performance of Selected Teaching Hospitals in Kerman and Shiraz Universities of Medical Sciences, Iran, Using Pabon-Lasso Chart","Background: Statistical control is one of the tools in performance assessment and comparison of hospitals. In this regard the Pabon Lasso graph is used as a technique in hospitals. The current study aimed to compare the performance of teaching hospitals of Kerman and Shiraz Universities of Medical Sciences, Iran, in 2007 using the Pabon Lasso tools. 
 
Methods: This was a descriptive cross-sectional study. Eight teaching hospitals in Kerman and Shiraz were selected through purposive sampling. The data gathering instrument was the standard data form for hospital activities that had been verified by the Ministry of Health and Medical Education. Three indicators related to Pabon Lasso chart including Bed Occupancy Rate, Average Length of Stay and Bed Turnover Rate were calculated using the Excel software. Finally, the Pabon Lasso graph was used to rank the performance of the selected hospitals in terms of the indicators. 
 
Results: Two out of the 8 hospitals (25%) including Shafa and Khalili fell in the second quarter and 4 out of the 8 hospitals (50%) including Shahid Bahonar, Shahid Faghihi, Namazi and Afzalipour were placed in the third quarter of the chart. Also 2 out of the 8 hospitals were positioned in the fourth quarter. Overall, a separate comparison of the three indicators showed that Shiraz teaching hospitals have better efficiency and performance than Kerman teaching hospitals. 
 
Conclusion: Pabon Lasso graph can be used as a suitable tool in the performance assessment of hospitals. Also, it is recommended that these functional indicators be prioritized in the annual evaluation of hospitals. 
Keywords: Performance, Pabon-Lasso graph, Evaluation, Teaching hospital",2012,
Super learning for daily streamflow forecasting: Large-scale demonstration and comparison with multiple machine learning algorithms,"Daily streamflow forecasting through data-driven approaches is traditionally performed using a single machine learning algorithm. Existing applications are mostly restricted to examination of few case studies, not allowing accurate assessment of the predictive performance of the algorithms involved. Here we propose super learning (a type of ensemble learning) by combining 10 machine learning algorithms. We apply the proposed algorithm in one-step ahead forecasting mode. For the application, we exploit a big dataset consisting of 10-year long time series of daily streamflow, precipitation and temperature from 511 basins. The super learner improves over the performance of the linear regression algorithm by 20.06%, outperforming the ""hard to beat in practice"" equal weight combiner. The latter improves over the performance of the linear regression algorithm by 19.21%. The best performing individual machine learning algorithm is neural networks, which improves over the performance of the linear regression algorithm by 16.73%, followed by extremely randomized trees (16.40%), XGBoost (15.92%), loess (15.36%), random forests (12.75%), polyMARS (12.36%), MARS (4.74%), lasso (0.11%) and support vector regression (-0.45%). Based on the obtained large-scale results, we propose super learning for daily streamflow forecasting.",2019,ArXiv
"Recruitment of a Coral Reef Fish: Roles of Settlement, Habitat, and Postsettlement Losses","Multiple processes typically influence patterns of abundance. Despite this widely accepted view, many studies continue to approach ecological questions from a single- factor, or, at most, a two-factor perspective. Here, I evaluate the consequences of consid- ering, separately and jointly, the effects of three factors (larval settlement, reef resources, and postsettlement losses) on spatial patterns of abundance of a marine reef fish, the six bar wrasse (Thalassoma hardwicke). Using correlational methods commonly employed in single-factor studies, I show that local patterns of abundance of juvenile wrasse could be attributed entirely to either (1) patterns of abundance of settlement habitat, or (2) patterns of larval settlement. This result occurred because habitat and presumed larval delivery covaried in space. I manipulated abundance of settlement habitat in a field experiment to uncouple this covariation and found subsequent settlement to be simultaneously influenced by both factors. However, joint effects of habitat and settlement failed to account for patterns of abundance of juvenile wrasse without also considering a third factor-postsettlement losses-which were density-dependent and substantially modified patterns of settlement. These results illustrate (1) how multifactorial explanations may be falsely refuted when incomplete sets of multiple factors are considered, and (2) how single-factor explanations may misrepresent underlying multifactorial causation of ecological patterns. Uncovering the interactive role of multiple factors in determining ecological patterns of interest requires a shift from single-factor approaches to more pluralistic perspectives.",2001,Ecology
Multiple single nucleotide polymorphism analysis using penalized regression in nonlinear mixed-effect pharmacokinetic models.,"CONTEXT
Studies on the influence of single nucleotide polymorphisms (SNPs) on drug pharmacokinetics (PK) have usually been limited to the analysis of observed drug concentration or area under the concentration versus time curve. Nonlinear mixed effects models enable analysis of the entire curve, even for sparse data, but until recently, there has been no systematic method to examine the effects of multiple SNPs on the model parameters.


OBJECTIVE
The aim of this study was to assess different penalized regression methods for including SNPs in PK analyses.


METHODS
A total of 200 data sets were simulated under both the null and an alternative hypothesis. In each data set for each of the 300 participants, a PK profile at six sampling times was simulated and 1227 genotypes were generated through haplotypes. After modelling the PK profiles using an expectation maximization algorithm, genetic association with individual parameters was investigated using the following approaches: (i) a classical stepwise approach, (ii) ridge regression modified to include a test, (iii) Lasso and (iv) a generalization of Lasso, the HyperLasso.


RESULTS
Penalized regression approaches are often much faster than the stepwise approach. There are significantly fewer true positives for ridge regression than for the stepwise procedure and HyperLasso. The higher number of true positives in the stepwise procedure was accompanied by a higher count of false positives (not significant).


CONCLUSION
We find that all approaches except ridge regression show similar power, but penalized regression can be much less computationally demanding. We conclude that penalized regression should be preferred over stepwise procedures for PK analyses with a large panel of genetic covariates.",2013,Pharmacogenetics and genomics
The International Market Of Steel And Iron In 2014-2015 Period. Previsions 2016,"Looking at the broad picture, the world crude steel production rose with 2,8% in2015 (1.623 million tonnes) comparative with 2014 (1.670 million tonnes) confirm World SteelAssociation-WSA).The first producer of world is China, with 804 million tonnes in 2015 comparative 823million tonnes in 2014 (-2,3%).In the international top of steelmakers lists (40 countries) Romania occupe the position 37,with 3,3 million tonnes crude steel in 2015 comparative 3,2 million tonnes in 2014 (+5,4%).Though, at world level, the iron and steel industry to confront with more problems in thelast years: overcapacities, fall in the prices transactions of iron and steel products and the costs toimport the raw materials.To observe that, on 2014, the introduction of stricts regulations for the protection ofenvironmental medium, to determine a reduction of capacities for the production of crude steel, thecosts with modernization of steel works.",2015,"Impact of Socio-economic and Technological Transformations at National, European and International Level"
Measuring Hospital Efficiency Through Pabon Lasso Analysis: An Empirical Study in Kemang Medical Care (KMC),"Introduction: Hospital is most costly in health care system. Provisions of optimal service require that hospital administrators identify hospital efficient performance based on relevant indicator. This study used the Pabon Lasso Analysis to access the efficiency of KMC and to identify strategies toward an improving efficiency KMC.Methods: This cross-sectional descriptive study involved all the four years performance of KMC Hospital. Data on Average Length Of Stay (ALOS), Bed Occupancy Rate (BOR) and Bed Turn Over (BTO). Data collected from 2011-2014 based on medical record.Result: The overall average length of stay and bed turn over represents a satisfactory level of efficiency. ALOS 2-3 days and BTO 42-47 times. KMC hospital located in Zone II because BOR only 28-31%. This study represents good satisfactory level of efficiency KMC demonstrated, short stay, high turnover but low occupancy.Discussion: This study showed the KMC hospital has generally high performance as indicated by Pabon Lasso Analysis. The administrator should find a strategy for measuring bed occupation rate for increasing hospital efficiency.",2015,
Effects of the aquarium fish industry in Costa Rica on populations of the Cortez rainbow wrasse Thalassoma lucasanum,"Costa Rica hosts an active industry for the collection of marine ornamental reef fish that are supplied to the international aquarium fish trade. Little is known about the effects that collection activities may be having upon target species in Costa Rica, although research elsewhere gives reason for concern. Thalassoma lucasanum (Cortez rainbow wrasse) is an important species in this fishery. Costa Rican collectors prize the vibrantly colored terminal phase T. lucasanum individuals, but also collect the less dramatic initial phase T. lucasanum. We measured the density of T. lucasanum and the length of terminal phase individuals on highly collected reefs close to a fishing village and on nearby less-collected reefs located within the Guanacaste Conservation Area. Our findings show that densities of T. lucasanum were significantly lower and that terminal phase individuals were significantly smaller on the reefs near the fishing village where collection pressure was high. These findings indicate that even moderate amounts of relatively non-destructive aquarium fish extraction can negatively affect targeted populations. We present some suggestions for the reform of the aquarium reef fish industry in Costa Rica, which we believe would improve the long-term sustainability of this fishery. 
 
En Costa Rica la extraccion de peces ornamentales de arrecife, para abastecer el mercado internacional, es una industria importante. Se sabe poco acerca del efecto de esta extraccion sobre las especies explotadas en Costa Rica, pero investigaciones realizadas en otros sitios sugieren que puede tener un impacto negativo. Thalassoma lucasanum (vieja de Cortes) es importante para esta actividad. Los vibrantes colores que presentan los individuos de T. lucasanum en su fase terminal los hace muy apreciados, aunque tambien se recolectan individuos en su fase inicial menos atractiva. Se compararon la densidad de las poblaciones de T. lucasanum y la longitud de sus individuos en fase terminal, en arrecifes cercanos a una comunidad de pescadores, donde se realiza una extraccion intensiva, y en arrecifes adyacentes dentro del Area de Conservacion Guanacaste, donde la explotacion es limitada. La densidad de T. lucasanum fue significativamente menor y los individuos en fase terminal presentaron talla menor en los arrecifes donde la presion de extraccion es mas intensa. Estos resultados indican que aun la extraccion relativamente no destructiva de peces ornamentales, en cantidades moderadas, puede afectar negativamente las poblaciones objetivo. Se sugieren algunas modificaciones a los metodos de explotacion de estas especies en Costa Rica que podrian conducir hacia la sustentabilidad de la actividad en el largo plazo.",2008,Ciencias Marinas
"Urban Water Recreation: Experiences, Place Meanings, and Future Issues","city sounds. In stark contrast to most urban images,water resources afford aesthetic relief among a variety of other individual andcommunity benefits, including recreation. Water-based recreation experiences inurban environments contribute to citizenâ€™s quality of life through opportunitiesfor enhanced health and wellness, environmental protection, and stimulated eco-nomic development. Beyond the benefits afforded through recreation experiences,however, are the meanings urban dwellers and visitors attach to these recreationresources. Both the recreation experiences and the meanings attached to theserecreation environments have short- and long-term implications for water resourcemanagers, policy makers, and urban planners. These urban water-based recreationexperiences and resources, their place meanings, and future issues associated withthem are the subject of this chapter.Prior to addressing urban water-based recreation experiences and place mean-ings, it is appropriate to understand the magnitude of water-based recreation.Beyond serving municipalities as water supplies and modes of transportation, waterresources provide significant recreation services. Nationally, the majority of US cit-izens report visiting a waterside area (60%; NSRE 2000). In addition to visitinga waterside area, a variety of recreation activities depend upon water resources.For example, in 2006, 29.9 million US residents, 16 years and older, participatedin recreational fishing; an additional 71 million enjoyed wildlife viewing (USFWS2007). At the state level, 735 million visitors enjoyed state parks, many of whichinclude water-based activities such as swimming, boating, and fishing (NationalAssociation of State Park Directors 2004). In terms of the urban water-based recre-ation experience, the total area covered by urban parkland in the US exceeds onemillion acres and recreational use of this area is significant. For example, LincolnPark in Chicago hosts more than 12 million users each year while New Yorkâ€™s",2009,
Sources and the Circulation of Renaissance Music,"Contents: Introduction Part I Scribes and the Making of Manuscripts: Manuscript structure in the Dufay era, Charles Hamm Simon Mellet, scribe of Cambrai cathedral, Liane Curtis A contemporary perception of early 15th-century style: Bologna Q15 as a document of scribal editorial initiative, Margaret Bent The origins of the Chigi Codex: the date, provenance, and original ownership of Rome, Biblioteca Vaticana, Chigiana, C.VIII.234, Herbert Kellman Jean Michel, Maistre Jhan and a chorus of beasts: old light on some Ferrarese music manuscripts, Joshua Rifkin. Part II Sources, Politics and Transmission: European politics and the distribution of music in the early 15th century, Reinhard Strohm A gift of madrigals and chansons: the Winchester Part Books and the courtship of Elizabeth I by Erik XIV of Sweden, Kristine K. Forney Danish diplomacy and the dedication of Giardino novo II (1606) to King James I, Susan G. Lewis [Hammond]. Part III Sources and the Transmission of Repertory: The early Tudor court, the provinces and the Eton Choirbook, Magnus Williamson Antwerp's role in the reception and dissemination of the madrigal in the North, Kristine K. Forney. Part IV Patrons and Collectors: The purpose of the gift: for display or for performance?, Stanley Boorman Music in the library of Johannes Klein, Tom R. Ward Music for the nuns of Verona: a story about MS DCCLXI of the Biblioteca Capitolare in Verona, Howard Mayer Brown The salon as marketplace in the 1550s: patrons and collectors of Lasso's secular music, Donna G. Cardamone. Part V Music Printing: 1501-1528: The 500th anniversary of the first music printing: a history of patronage and taste in the early years, Stanley Boorman The printing contract for the Libro primo de musica de la salamandra (Rome 1526), Bonnie J. Blackburn. Part VI Printing and Printing Houses after 1528: The Libro Primo of Constanzo Festa, James Haar Twins, cousins, and heirs: relationships among editions of music printed in 16th-century Venice, Mary S. Lewis Thoughts on the popularity of printed music in 16th-century Italy, Stanley Boorman The Burning Salamander: assigning a printer to some 16th-century music prints, Jane A. Bernstein. Part VII The Financial Side of Music Printing: Financial arrangements and the role of printer and composer, Jane A. Bernstein The Venetian privilege and music-printing in the 16th century, Richard J. Agee Orlando di Lasso, composer and print entrepreneur, James Haar Music selling in late 16th-century Florence: the bookshop of Piero di Giuliano Morosi, Tim Carter Name index.",2012,
[Homicide by strangulation with a lasso sling].,"A 50-year-old physically handicapped man was found dead on his wheelchair in a short passage on the ground floor of a high-rise housing development. The lattice doors at the both ends of the passage were closed and the wheelchair with the body was found at the door to an adjoining indoor parking place for bicycles. A noose of a vinyl plastic cord was found wound around the neck; the other end of the cord was tied through the lattice door to a handrail on the wall of the parking place. The wheelchair was stopped sideways; its chair back was in contact with a doorpost. The body which was slipped from the seat of the wheelchair was incompletely suspended by the noose. The cause of death primarily was diagnosed as hanging. The police suspected murder and arrested a 21-year-old male assailant. According to his confession, the assailant pushed the wheelchair through the lattice door in the passage, stopped it at the door and braked it. Immediately thereafter he returned to the parking place and locked the lattice door behind him. After having made a kind of lasso using a cord taken off from a bicycle which has been sitting there, he reached out his hands through the lattice door and lassoed the man on the wheelchair from behind. Finally, he tied another end of the lasso to the handrail. It was considered that the body of the victim was slipped down in the stage of convulsion and incompletely suspended by the noose.",1995,Archiv fur Kriminologie
Allocation strategies for high fidelity models in the multifidelity regime,"We propose a novel approach to allocating resources for expensive simulations of high fidelity models when used in a multifidelity framework. Allocation decisions that distribute computational resources across several simulation models become extremely important in situations where only a small number of expensive high fidelity simulations can be run. We identify this allocation decision as a problem in optimal subset selection, and subsequently regularize this problem so that solutions can be computed. Our regularized formulation yields a type of group lasso problem that has been studied in the literature to accomplish subset selection. Our numerical results compare performance of algorithms that solve the group lasso problem for algorithmic allocation against a variety of other strategies, including those based on classical linear algebraic pivoting routines and those derived from more modern machine learning-based methods. We demonstrate on well known synthetic problems and more difficult real-world simulations that this group lasso solution to the relaxed optimal subset selection problem performs better than the alternatives.",2019,SIAM/ASA J. Uncertain. Quantification
Machine Learning for Noise Sensor Placement and Full-Chip Voltage Emergency Detection,"Power supply fluctuation can be potential threat to the correct operations of processors, in the form of voltage emergency that happens when supply voltage drops below a certain threshold. Noise sensors (with either analog or digital outputs) can be placed in the nonfunction area of processors to detect voltage emergencies by monitoring the runtime voltage fluctuations. Our work addresses two important problems related to building a sensor-based voltage emergency detection system: 1) offline sensor placement, i.e., where to place the noise sensors so that the number and locations of sensors are optimized in order to strike a balance between design cost and chip reliability and 2) online voltage emergency detection, i.e., how to use these placed sensors to detect voltage emergencies in the hotspot locations. In this paper, we propose integrated solutions to these two problems, respectively, for analog and digital (more specifically, binary) sensor outputs, by exploiting the voltage correlation between the sensor candidate locations and the hotspot locations. For the analog case, we use the Group Lasso and an ordinary least squares approach; for the binary case, we integrate the Group Lasso and the SVM approach. Experimental results show that, our approach can achieve 2.3Xâ€“2.7X better voltage emergency detection results on average for analog outputs when compared to the state-of-the-art work; and for the binary case, on average our methodology can achieve up to 21% improvement in prediction accuracy compared to an approach called max-probability-no-prediction.",2017,IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems
Benefits of dimension reduction in penalized regression methods for high-dimensional grouped data: a case study in low sample size,"MOTIVATION
In some prediction analyses, predictors have a natural grouping structure and selecting predictors accounting for this additional information could be more effective for predicting the outcome accurately. Moreover, in a high dimension low sample size framework, obtaining a good predictive model becomes very challenging. The objective of this work was to investigate the benefits of dimension reduction in penalized regression methods, in terms of prediction performance and variable selection consistency, in high dimension low sample size data. Using two real datasets, we compared the performances of lasso, elastic net, group lasso, sparse group lasso, sparse partial least squares (PLS), group PLS and sparse group PLS.


RESULTS
Considering dimension reduction in penalized regression methods improved the prediction accuracy. The sparse group PLS reached the lowest prediction error while consistently selecting a few predictors from a single group.


AVAILABILITY AND IMPLEMENTATION
R codes for the prediction methods are freely available at https://github.com/SoufianeAjana/Blisar.


SUPPLEMENTARY INFORMATION
Supplementary data are available at Bioinformatics online.",2019,Bioinformatics
Pain Management in Children with Severe Neurological Impairment (421),"of last resort. What is more controversial, however, is palliative sedation to unconsciousness for patients whose physical pain may be adequately managed, but whose existential suffering cannot be otherwise abated by aggressive multidisciplinary care. This session offers participants the opportunity to think about how work in existential psychology and philosophy can inform ethical decision-making and policy development related to palliative sedation. The debate over sedation for existential suffering rarely takes the time to articulate what is meant by the term existential suffering. Indeed, more often than not the meaning of the term is simply implied: suffering of or from a nonphysical etiology. In other words, suffering that is not equivalent to or arising from physical pain. While there are a handful of conceptual models of existential suffering in the psychology and palliative care literature, there is no commonly used definition or set of diagnostic criteria to identify suffering as existential. Yet, several professional societies including the American MedicalAssociationhave issuedpositionstatements opposing the use of palliative sedation for existential suffering. The diagnosis and treatment of existential suffering offers an opportunity for translational work between the humanities, social sciences, and palliative care. The literature and practice of psycho-oncology offers one example of such translation. Drawing on the work of Irvin Yolom in existential psychology, Breitbart and colleagues have developed interventions for suffering that translate Yalomâ€™s four realms of existential concern into clinical interventions for advanced cancer patients. The panel will further investigate ways to translate conceptualizations of suffering in existential philosophy and psychology in ways that can more robustly inform the debate on the use of palliative sedation for existential suffering.",2010,Journal of Pain and Symptom Management
Regularized rank-based estimation of high-dimensional nonparanormal graphical models,"A sparse precision matrix can be directly translated into a sparse Gaussian graphical model under the assumption that the data follow a joint normal distribution. This neat property makes high-dimensional precision matrix estimation very appealing in many applications. However, in practice we often face nonnormal data, and variable transformation is often used to achieve normality. In this paper we consider the nonparanormal model that assumes that the variables follow a joint normal distribution after a set of unknown monotone transformations. The nonparanormal model is much more flexible than the normal model while retaining the good interpretability of the latter in that each zero entry in the sparse precision matrix of the nonparanormal model corresponds to a pair of conditionally independent variables. In this paper we show that the nonparanormal graphical model can be efficiently estimated by using a rank-based estimation scheme which does not require estimating these unknown transformation functions. In particular, we study the rank-based graphical lasso, the rank-based neighborhood Dantzig selector and the rank-based CLIME. We establish their theoretical properties in the setting where the dimension is nearly exponentially large relative to the sample size. It is shown that the proposed rank-based estimators work as well as their oracle counterparts defined with the oracle data. Furthermore, the theory motivates us to consider the adaptive version of the rank-based neighborhood Dantzig selector and the rank-based CLIME that are shown to enjoy graphical model selection consistency without assuming the irrepresentable condition for the oracle and rank-based graphical lasso. Simulated and real data are used to demonstrate the finite performance of the rank-based estimators.",2012,Annals of Statistics
Structured sparsity regularized multiple kernel learning for Alzheimer's disease diagnosis,"Multimodal data fusion has shown great advantages in uncovering information that could be overlooked by using single modality. In this paper, we consider the integration of high-dimensional multi-modality imaging and genetic data for Alzheimer's disease (AD) diagnosis. With a focus on taking advantage of both phenotype and genotype information, a novel structured sparsity, defined by â„“ 1, p-norm (p > 1), regularized multiple kernel learning method is designed. Specifically, to facilitate structured feature selection and fusion from heterogeneous modalities and also capture feature-wise importance, we represent each feature with a distinct kernel as a basis, followed by grouping the kernels according to modalities. Then, an optimally combined kernel presentation of multimodal features is learned in a data-driven approach. Contrary to the Group Lasso (i.e., â„“ 2, 1-norm penalty) which performs sparse group selection, the proposed regularizer enforced on kernel weights is to sparsely select concise feature set within each homogenous group and fuse the heterogeneous feature groups by taking advantage of dense norms. We have evaluated our method using data of subjects from Alzheimer's Disease Neuroimaging Initiative (ADNI) database. The effectiveness of the method is demonstrated by the clearly improved prediction diagnosis and also the discovered brain regions and SNPs relevant to AD.",2019,Pattern recognition
ultielectrode basket catheter : A new tool for curing trial fibrillation ?,"m s a d e d o e r S c N t r c p c s m c l b i l t t c r o Over the last decade, considerable advances have been ade in both the understanding and the curative treatment f atrial fibrillation (AF), with excellent results even in atients with persistent and permanent AF. Pulmonary vein PV) isolation as initially proposed by Haissaguerre et al as several disadvantages and a relatively low success rate, articularly in patients with long-lasting AF. PV isolation is uch less effective, particularly in patients with persistent F, because PV isolation alone affects only triggers of AF. herefore, it has become increasingly clear that to improve he clinical outcome of PV isolation strategy, elimination of V triggers alone may not be sufficient but must be comined with substrate modification. Success rates have mproved with the evolution and development of PV isolaion, but more extensive and deeper lesions have brought the pectra of important complications, such as atrio-esophaeal fistula and cardiac perforation. If PV isolation with ore extensive lesions can produce better outcomes, the rucial question is how extensive the lesions must be to aintain or even increase the success rate while minimizing isk and complications. PV disconnection has been guided y different mapping systems. More recently, circumferenial lesions around PV ostia have been reported in patients ith paroxysmal or persistent AF by using the Constellation ultielectrode basket catheter in combination with the Asronomer nonfluoroscopic three-dimensional navigation ystem. Isolation of PV antra guided by intracardiac echoardiography has been reported as an alternative approach or curing AF, but this technique has some technical limiations. Because the PV antrum has a large diameter, a tationary Lasso catheter located in just one position in rder to locate PV potentials cannot map its circumference. he oblique nature of the antralâ€“left atrium interface makes tabilization of the Lasso catheter difficult, particularly at hallenging sites such as anterior segments of the left PVs r septal segments of the right PVs. Therefore, in patients ndergoing PV antra isolation, catheter stability, although ime-consuming, should be considered a crucial endpoint. In",2006,
Gray matter network differences between behavioral variant frontotemporal dementia and Alzheimer's disease,"We set out to study whether single-subject gray matter (GM) networks show disturbances that are specific for Alzheimer's disease (AD; nÂ = 90) or behavioral variant frontotemporal dementia (bvFTD; nÂ = 59), and whether such disturbances would be related to cognitive deficits measured with mini-mental state examination and a neuropsychological battery, using subjective cognitive decline subjects as reference. AD and bvFTD patients had a lower degree, connectivity density, clustering, path length, betweenness centrality, and small world values compared with subjective cognitive decline. AD patients had a lower connectivity density than bvFTD patients (FÂ = 5.79, pÂ = 0.02; mean Â± standard deviation bvFTD 16.10 Â± 1.19%; mean Â± standard deviation AD 15.64 Â± 1.02%). Lasso logistic regression showed that connectivity differences between bvFTD and AD were specific to 23 anatomical areas, in terms of local GM volume, degree, and clustering. Lower clustering values and lower degree values were specifically associated with worse mini-mental state examination scores and lower performance on the neuropsychological tests. GM showed disease-specific alterations, when comparing bvFTD with AD patients, and these alterations were associated with cognitive deficits.",2017,Neurobiology of Aging
Lasso Screening for Object Categories Recognition Using Multi-directional Context Features,"Image representation using local features and sparse coding (SC) plays a very important role in image classification when the dataset is fairly large. Despite of its worldwide popularity, there are still some improving space in classification efficiency and computational investment in training and coding phrase of SC. In this paper, we put forward a novel object categories recognition method from two aspects. First, the contextual relevance between image patches are fully utilized by merging local feature of every sub-patch with its neighboring ones into strong context features to generate the multiple sparse representations, which are received by the SC and multi-scale max pooling SPM(Spatial Pyramid Matching), respectively. Second, while calculating the sparse coefficients of SC, we need to solve L1-regularized least square problem. Screening out the zero coefficients and discarding the corresponding inactive codewords before solving Lasso problem can remarkably speed up the optimization. The proposed method outperforms state-of-the-art performancein a large number of image categorization experiments on several benchmarks: the ground truth dataset (21 Land-Use database), the event dataset (UIUC-Sport dataset), and the object recognition dataset (Caltech101 dataset).",2015,2015 10th International Conference on Intelligent Systems and Knowledge Engineering (ISKE)
Combined Pbâ€“Sr isotopic analysis in provenancing late Roman iron raw materials in the territory of Sagalassos (SW Turkey),"In early Roman times, iron was likely supplied to the city of Sagalassos through the smelting of close-by hematite ores. In the early Byzantine period, magnetiteâ€“titanite placer sands in some instances could have been exploited for its iron. For the intermediate late Roman period, however, the source of the locally used iron was unknown. Pb and Sr isotopic analyses of iron ores from the area of Camoluk, just south of the territory of Sagalassos, and of late Roman iron artefacts from the antique city itself, reveal a very close resemblance. This makes the use of the Camoluk ores to supply Sagalassos with raw iron in the late Roman period likely. It is also shown that combined Pb and Sr isotopic analyses provides a powerful tool to distinguish chronological groups of iron provenance and a technique that can determine the nature and source of iron raw materials used.",2009,Archaeological and Anthropological Sciences
Accumulation of seagrass beach cast along the Kenyan coast: A quantitative assessment,"Abstract Accumulation of seagrass beach cast material was monitored along the beaches of the Mombasa Marine National Park and Reserve, Kenya between September 1995 and August 1996. Weekly surveys using a rapid visual assessment technique revealed an average total of 93,000Â kg dry weight of beach cast material along a 9.5Â km stretch of beaches in this area. An average of 88Â Â±Â 18% of the beach cast dry weight consisted of seagrass material (88% leaves) while the remainder was composed of the seaweeds Sargassum sp. and Ulva sp. The seagrass Thalassodendron ciliatum (Forsskal) den Hartog constituted the major part (76%) of the seagrass tissue on the beach, followed by Syringodium isoetifolium (Ascherson) Dandy (15%). An average of 19.7Â Â±Â 24.7% ( n Â =Â 90; SEÂ =Â 0.27) of the beach cast consisted of freshly-detached (green) seagrass material. The beach cast material was part of a pool of detached macrophytes in the intertidal zone washed back and forth between the beach and the adjacent reef lagoon with the ebb and flood tides. An average net diffusion factor (DF) of 29.1Â Â±Â 3.5Â g 24 h âˆ’1 was measured in the lagoon using blocks of plaster of Paris, indicating a relatively high degree of exposure to waves and currents. Significantly ( p Â =Â 0.006) larger amounts of beach cast were recorded during spring tide periods compared to neap tide periods. Weekly monitoring at three beach sites (Nyali, Bamburi, Reef) revealed that accumulation of beach cast was markedly seasonal with largest amounts observed during the South-East (SE) monsoon (March to October) and minimal amounts during the North-East (NE) monsoon (November to March). Extrapolation of the monitoring results indicated that the total amount of beach cast along the entire beach (9.5Â km) varied between a minimum of 14,700Â kg dry weight (or 31Â gÂ m âˆ’2 ) during the NE monsoon to a maximum of 1.2 million kg dry weight (or 2.5Â kgÂ m âˆ’2 ) during the SE monsoon. Decomposition of the beach cast material was measured by litter bag experiments. T. ciliatum leaves in litter bags lying on the beach surface showed a decomposition rate ( k ) of 0.017 day âˆ’1 ash-free dry weight (AFDW). The material in the litterbags took 42 days to lose 50% of its initial ash-free dry weight. Burial of litterbags under the sand did not result in a significant reduction of the decomposition rate. Large numbers of amphipods, isopods, nematodes and oligochaetes were associated with the beach cast material. Most dominant were amphipods which had an average density of 23,182Â Â±Â 10,697 animals m âˆ’2 . A positive correlation ( r Â =Â 0.4) was found between faunal density and amount of beach cast material. Above-ground biomass and primary production of seagrass meadows in the adjacent lagoon were 760Â Â±Â 96Â g dry weight m âˆ’2 and 8.2Â Â±Â 2.8Â g dry weight m âˆ’2 day âˆ’1 , respectively. The total net production by the seagrass beds covering 60% of the 20Â km 2 lagoon was estimated to be 36 million kg dry weight year âˆ’1 (or 14.7 million kg C year âˆ’1 ). The turn-over of the beach cast material was in the order of 73 times per year, implying that approximately 6.8 million kg dry weight of seagrass material is being casted on the beach annually. This indicates that approximately 19% of the total seagrass productivity in the lagoon passes through the beach, where exposure to wind and sun, fragmentation, leaching and decomposition contribute to efficient recycling of nutrients.",1999,Aquatic Botany
Non-Invasive Continuous Glucose Monitoring: Identification of Models for Multi-Sensor Systems,"Diabetes is a disease that undermines the normal regulation of glucose levels in the blood. In people with diabetes, the body does not secrete insulin (Type 1 diabetes) or derangements occur in both insulin secretion and action (Type 2 diabetes). In 
spite of the therapy, which is mainly based on controlled regimens of insulin and drug administration, diet, and physical exercise, tuned according to self-monitoring of blood glucose (SMBG) levels 3-4 times a day, blood glucose concentration often exceeds the normal range thresholds of 70-180 mg/dL. While hyperglycaemia mostly affects long-term complications (such as neuropathy, retinopathy, cardiovascular, and heart diseases), hypoglycaemia can be very dangerous in the short-term and, in the worst-case scenario, may bring the patient into hypoglycaemic coma. New scenarios in diabetes treatment have been opened in the last 15 years, when continuous glucose monitoring (CGM) sensors, able to monitor glucose concentration continuously (i.e. with a reading every 1 to 5 min) over several days, entered clinical research. CGM sensors can be used both retrospectively, e.g., to optimize the metabolic control, and in real-time applications, e.g., in the ""smart"" CGM sensors, able to generate alerts when glucose concentrations are predicted to exceed the normal range thresholds or in the so-called ""artificial pancreas"". Most CGM sensors exploit needles and are thus invasive, although minimally. In order to improve patients comfort, Non-Invasive Continuous Glucose Monitoring (NI-CGM) technologies have been widely investigated in the last years and their ability to monitor glucose changes in the human body has been demonstrated under highly controlled (e.g. in-clinic) conditions. 
As soon as these conditions become less favourable (e.g. in daily-life use) several problems have been experienced that can be associated with physiological and environmental 
perturbations. To tackle this issue, the multisensor concept received greater attention in the last few years. A multisensor consists in the embedding of sensors of different nature 
within the same device, allowing the measurement of endogenous (glucose, skin perfusion, sweating, movement, etc.) as well as exogenous (temperature, humidity, etc.) factors. 
The main glucose related signals and those measuring specific detrimental processes have to be combined through a suitable mathematical model with the final goal of estimating glucose non-invasively. White-box models, where differential equations are used to describe the internal behavior of the system, can be rarely considered to combine multisensor measurements because a physical/mechanistic model linking multisensor data 
to glucose is not easily available. A more viable approach considers black-box models, which do not describe the internal mechanisms of the system under study, but rather depict how the inputs (channels from the non-invasive device) determine the output (estimated glucose values) through a transfer function (which we restrict to the class of multivariate linear models). Unfortunately, numerical problems usually arise in the 
identication of model parameters, since the multisensor channels are highly correlated (especially for spectroscopy based devices) and for the potentially high dimension of the 
measurement space. 
The aim of the thesis is to investigate and evaluate different techniques usable for the identication of the multivariate linear regression models parameters linking multisensor data and glucose. In particular, the following methods are considered: Ordinary Least Squares (OLS); Partial Least Squares (PLS); the Least Absolute Shrinkage and Selection Operator (LASSO) based on l1 norm regularization; Ridge regression based on l2 norm regularization; Elastic Net (EN), based on the combination of the two previous norms. As a case study, we consider data from the Multisensor device mainly based on dielectric 
and optical sensors developed by Solianis Monitoring AG (Zurich, Switzerland) which partially sponsored the PhD scholarship. Solianis Monitoring AG IP portfolio is now 
held by Biovotion AG (Zurich, Switzerland). Forty-five recording sessions provided by Solianis Monitoring AG and collected in 6 diabetic human beings undertaken hypo and 
hyperglycaemic protocols performed at the University Hospital Zurich are considered. The models identified with the aforementioned techniques using a data subset are then 
assessed against an independent test data subset. Results show that methods controlling complexity outperform OLS during model test. In general, regularization techniques outperform PLS, especially those embedding the l1 norm (LASSO end EN), because they set many channel weights to zero thus resulting more robust to occasional spikes occurring in the Multisensor channels. In particular, the EN model results the best one, 
sharing both the properties of sparseness and the grouping effect induced by the l1 and l2 norms respectively. In general, results indicate that, although the performance, in terms of overall accuracy, is not yet comparable with that of SMBG enzyme-based needle sensors, the Multisensor platform combined with the Elastic-Net (EN) models is a valid tool for the real-time monitoring of glycaemic trends. An effective application concerns the complement of sparse SMBG measures with glucose trend information within the recently developed concept of dynamic risk for the correct judgment of dangerous events such as hypoglycaemia. 
The body of the thesis is organized into three main parts: Part I (including Chapters 1 to 4), first gives an introduction of the diabetes disease and of the current technologies for NI-CGM (including the Multisensor device by Solianis) and then states the aims of the thesis; Part II (which includes Chapters 5 to 9), first describes some of the issues to be faced in high dimensional regression problems, and then presents OLS, PLS, LASSO, Ridge and EN using a tutorial example to highlight their advantages and drawbacks; Finally, Part III (including Chapters 10-12), presents the case study with the data set and results. Some concluding remarks and possible future developments end the thesis. In particular, a Monte Carlo procedure to evaluate robustness of the calibration procedure for the Solianis Multisensor device is proposed, together with a new cost function to be used for identifying models.",2013,
Using LASSO to Calibrate Non-probability Samples using Probability Samples.,"Using LASSO to Calibrate Non-probability Samples using Probability Samples by Kuang Tsung Chen Chair: Professor Michael R. Elliott Amidst declining response rates and rapidly increasing costs of probability-based sampling, the resurgence of more cost-effective non-probability sampling has prompted survey researchers to explore different adjustment methods for non-probability samples. The current approach attempts to create one single set of survey weights to correct all imbalances within a non-probability sample. One scheme is to generate estimated selection weights by combining the non-probability sample with a large probability-sampling-based dataset with all variables related to propensity of a respondent being in the non-probability sample. In practice, obtaining an appropriate probability sample is costly, and usually there is no way to determine the correct probability of selection for the non-probability sample, or even if all variables are available in the non-probability data to do so. An alternative approach is to adjust the non-probability sample so that the weighted sample totals of a set of variables, known as calibration variables, equal to their Census benchmark totals. Although the method does not require specialized probability-sampling-based data, the resulting calibrated weights can only correct the imbalance with respect to the limited number of Census benchmark variables, which is insufficient for adjusting all errors",2016,
"Texture profile analysis (TPA) of clay/seawater mixtures useful for peloid preparation: Effects of clay concentration, pH and salinity","Abstract The liquid phase of a peloid can be mineral-medicinal, marine or salt lake water. This study was designed to experimentally determine the interaction between two bentonites and one sepiolite, and seawater as well as dilutions thereof to verify the effect of salinity on instrumental texture measurements in clay-water mixtures prepared with these components. In all the clay-water mixtures tested, instrumental hardness and adhesiveness decreased with water content. For a given instrumental hardness or adhesiveness, bentonite retained more water in the mixture with distilled water than with seawater. In contrast, sepiolite retained more water in the mixture with seawater than with distilled water. These differences affected the thermal behaviour of the clay-water mixtures. Instrumental hardness and adhesiveness curves may be a suitable tool to tailor concentrated dispersions and after maturation could be used in thalassos and medical spas as peloids.",2018,Applied Clay Science
"Physical and Functional Interactions between ELL2 and RB in the Suppression of Prostate Cancer Cell Proliferation, Migration, and Invasion12","Elongation factor, RNApolymerase II, 2 (ELL2) is expressed and regulated by androgens in the prostate. ELL2 and ELLassociated factor 2 (EAF2) form a stable complex, and their orthologs in Caenorhabditis elegans appear to be functionally similar. In C. elegans, the EAF2 ortholog eaf-1 was reported to interact with the retinoblastoma (RB) pathway to control development and fertility in worms. Because RB loss is frequent in prostate cancer, ELL2 interaction with RB might be important for prostate homeostasis. The present study explored physical and functional interaction of ELL2 with RB in prostate cancer. ELL2 expression in human prostate cancer specimens was detected using quantitative polymerase chain reaction coupled with laser capture microdissection. Co-immunoprecipitation coupled with deletion mutagenesis was used to determine ELL2 association with RB. Functional interaction between ELL2 andRBwas tested using siRNA knockdown, BrdU incorporation, Transwell, and/or invasion assays in LNCaP, C42, and 22Rv1 prostate cancer cells. ELL2 expression was downregulated in highâ€“Gleason score prostate cancer specimens. ELL2 could be bound and stabilized by RB, and this interaction was mediated through the N-terminus of ELL2 and the C-terminus of RB. Concurrent siRNA knockdown of ELL2 and RB enhanced cell proliferation, migration, and invasion as compared to knockdown of ELL2 or RB alone in prostate cancer cells. ELL2 and RB can interact physically and functionally to suppress prostate cancer progression. Neoplasia (2017) 19, 207â€“215 Address all correspondence to: Zhou Wang. E-mail: wangz2@upmc.edu Contributions: X. Q. designed and performed experiments, and wrote themanuscript. Q. S generated data for Supplemental Figure S2. All authors reviewed and edited the manuscript This work was supported in part by National Institutes of Health grants R01 CA120386, 1P50 CA180995, T32 DK007774, and 1R50 CA211242, and scholarships from the Chinese Scholarship Council (X.Q.) and Tippins Foundation (L.E.P.). This project used the UPCI Tissue and Research Pathology Services and wa supported in part by National Cancer Institute award P30CA047904. Received 28 September 2016; Revised 22 December 2016; Accepted 2 January 2017 Â© 2017 The Authors. Published by Elsevier Inc. on behalf of SOCIETY. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org licenses/by-nc-nd/4.0/). http://dx.doi.org/10.1016/j.neo.2017.01.00",2017,
Bayesian Network Classifier Based on L1 Regularization,"Variable order-based Bayesian network classifiers ignore the information of the selected variables in their sequence and their class label,which significantly hurts the classification accuracy.To address this problem,we proposed a simple and efficient L1 regularized Bayesian network classifier(L1-BNC).Through adjusting the constraint value of Lasso and fully taking advantage of the regression residuals of the information,L1-BNC takes the information of the sequence of selected variables and the class label into account,and then generates an excellent variable ordering sequence(L1 regularization path) for constructing a good Bayesian network classifier by the K2 algorithm.Experimental results show that L1-BNC outperforms existing state-of-the-art Bayesian network classifiers.In addition,in comparison with SVM,Knn and J48 classification algorithms,L1-BNC is also superior to those algorithms on most datasets.",2012,Computer Science
Ascertaining gene flow patterns in livestock populations of developing countries: a case study in Burkina Faso goat,"BackgroundIntrogression of Sahel livestock genes southwards in West Africa may be favoured by human activity and the increase of the duration of the dry seasons since the 1970â€™s. The aim of this study is to assess the gene flow patterns in Burkina Faso goat and to ascertain the most likely factors influencing geographic patterns of genetic variation in the Burkina Faso goat population.ResultsA total of 520 goat were sampled in 23 different locations of Burkina Faso and genotyped for a set of 19 microsatellites. Data deposited in the Dryad repository: http://dx.doi.org/10.5061/dryad.41h46j37. Although overall differentiation is poor (FST = 0.067 Â± 0.003), the goat population of Burkina Faso is far from being homogeneous. Barrier analysis pointed out the existence of: a) genetic discontinuities in the Central and Southeast Burkina Faso; and b) genetic differences within the goat sampled in the Sahel or the Sudan areas of Burkina Faso. Principal component analysis and admixture proportion scores were computed for each population sampled and used to construct interpolation maps. Furthermore, Population Graph analysis revealed that the Sahel and the Sudan environmental areas of Burkina Faso were connected through a significant number of extended edges, which would be consistent with the hypothesis of long-distance dispersal. Genetic variation of Burkina Faso goat followed a geographic-related pattern. This pattern of variation is likely to be related to the presence of vectors of African animal trypanosomosis. Partial Mantel test identified the present Northern limit of trypanosome vectors as the most significant landscape boundary influencing the genetic variability of Burkina Faso goat (p = 0.008). The contribution of Sahel goat genes to the goat populations in the Northern and Eastern parts of the Sudan-Sahel area of Burkina Faso was substantial. The presence of perennial streams explains the existence of trypanosome vectors. The South half of the NakambÃ© river (Southern Ouagadougou) and the Mouhoun river loop determined, respectively, the Eastern and Northern limits for the expansion of Sahelian goat genes. Furthermore, results from partial Mantel test suggest that the introgression of Sahelian goat genes into DjallonkÃ© goat using human-influenced genetic corridors has a limited influence when compared to the biological boundary defined by the northern limits for the distribution of the tsetse fly. However, the genetic differences found between the goat sampled in Bobo Dioulasso and the other populations located in the Sudan area of Burkina Faso may be explained by the broad goat trade favoured by the main road of the country.ConclusionsThe current analysis clearly suggests that genetic variation in Burkina Faso goat: a) follows a North to South clinal; and b) is affected by the distribution of the tsetse fly that imposes a limit to the Sahelian goat expansion due to their trypanosusceptibility. Here we show how extensive surveys on livestock populations can be useful to indirectly assess the consequences of climate change and human action in developing countries.",2011,BMC Genetics
Tractable relaxations and efficient algorithmic techniques for large-scale optimization,"In this thesis, we develop tractable relaxations and efficient algorithms for large-scale optimization. Our developments are motivated by a recent paradigm, Compressed Sensing, which covers a multitude of large-scale, sparsity-oriented convex optimization problems. Compressed sensing is focused on the recovery of sparse or well-concentrated signals from possibly noisy observations in a low-dimensional space. Nowadays, this theory is successfully utilized in many fields ranging from MRI image processing to machine learning, from biology to statistics. In the first chapter of this thesis, we provide a general introduction to compressed sensing and its applications and cover some of the earlier results. 
The majority of results in compressed sensing theory rely on the ability to design/use projection matrices with good recoverability properties. In the second chapter of this thesis, we study the conditions for good recoverability properties of a sensing matrix. We propose necessary and sufficient conditions for a sensing matrix to allow for exact e1-recovery of sparse signals with at most s nonzero entries while utilizing a priori information given in the form of sign restrictions on part of the entries. We express error bounds for imperfect e1-recovery in terms of the characteristics underlying these conditions. These characteristics, although difficult to evaluate, lead to two different verifiable sufficient conditions, which can be efficiently computed via linear programming (LP) and/or semidefinite programming (SDP) and thus generate efficiently computable lower bounds on the level of sparsity, s, for which a given sensing matrix is shown to allow for exact e1-recovery. We analyze the connection between our LP- and SDP-based verifiable sufficient conditions, examine their properties, describe their limits of performance and provide numerical examples comparing them with other verifiable conditions from the literature. Even though our LP- and SDP-based relaxations are presented in CS framework, these techniques are generic and applicable in the case of disjoint bilinear programs. 
In the third chapter, we study the compressed sensing synthesis problem â€“ selecting the minimum number of rows from a given matrix, so that the resulting submatrix possesses certifiably good recovery properties. Starting from the verifiable sufficient conditions, we express the synthesis problem as the problem of approximating a given matrix by a matrix of specified low rank in the uniform norm. We develop a randomized algorithm for efficient construction of rank k approximation of matrices of size m Ã— n achieving accuracy bounds O(1) lnmn /k which hold in expectation or with high probability. We supply a derandomized version of our approximation algorithm and provide numerical results on its performance for the synthesis problem. 
Chapter 4 is dedicated to efficient first-order algorithms for large-scale, well-structured convex optimization problems. Saddle point reformulation is proven to be an effective tool to exploit problem structure for designing computationally efficient algorithms. Building upon their strength, we first demonstrate that the solutions to many large-scale problems arising from compressed sensing recovery, high-dimensional statistical inference, and machine learning can be obtained through solving a series of Bilinear Saddle Point problems (BSPs). We accelerate the solution of associated single-parametric BSP's by utilizing the Mirror Prox algorithm from [101] as a prototype and by replacing precise first order oracle (which becomes quite time-consuming in the extremely large-scale case) by its computationally cheap randomized counterpart. In the overall solution of parametric BSPs, cheap online assessment of solution quality is crucial. Our randomized algorithms come with exact guarantees on solution quality and achieves sublinear time behavior to solve large-scale parametric BSPs. Extensive simulations show that our randomized first-order methods are capable of handling very large-scale applications and improve considerably over the state-of-the-art deterministic algorithms, with benefits amplifying as the sizes of the problems grow. 
In the fifth chapter, we examine a more general sparse estimation problem â€“ estimating a signal from its undersampled observations corrupted with nuisance and stochastic noise. Instead of the standard sparse signal framework, here we work under the assumption that a priori information is presented via a block representation structure of a known linear transform of the signal, and the signal achieves a good approximation in block sparse sense in this representation structure. There are a number of important applications where such a nontrivial sparsifying representation arises naturally such as standard image reconstruction with Total Variation regularization or finding the solution of a linear finite-difference equation with sparse right hand side (â€œevolution of a linear plant corrected from time to time by impulse controlâ€). We show that an extension of the standard compressed sensing results from [79] to this framework is possible. Particularly, we introduce a family of conditions, suggest two new methods of recovery based on block-e 1 minimization and study the most common cases of the block representation structure under which these estimators have efficiently verifiable guaranties of performance. We link our performance estimations to the well known results of compressed sensing by providing connections between our conditions and Restricted Isometry Property. This also establishes connections between new techniques and classical methods such as Lasso and Dantzig Selector. 
We present a summary of conclusions of our study and provide future research directions in the last chapter.",2011,
Isolation of an arrhythmogenic roof vein with the guide of a circular mapping catheter in a case with paroxysmal atrial fibrillation,"2214-0271 B 2016 Heart Rhythm Society. Published by Elsevier Inc. This is an o (http://creativecommons.org/licenses/by-nc-nd/4.0/). of the left atrium (LA) from the right upper lung lobe (roof vein, Figure 1A, B). After reisolation of reconducted PVs (left superior, right superior, and left inferior PVs) and completion of making a roof line (posteriorly to the roof vein), a Lasso catheter (measuring 15 mm in diameter) was placed inside the roof vein (Figure 1B), which showed both passively activated circumferential potentials and nonconducted blocked firings (Figure 2A). Frequent short-coupling atrial premature complexes with conduction to the LA were also documented. A single radiofrequency (RF) energy application (23W) with an irrigation system at the earliest activation site successfully eliminated the circumferential potentials at the ostium of the roof vein (Figure 2B). During 5 months of follow-up, the patient has been free from atrial fibrillation recurrence.",2016,HeartRhythm Case Reports
A Unified and Comprehensible View of Parametric and Kernel Methods for Genomic Prediction with Application to Rice,"One objective of this study was to provide readers with a clear and unified understanding of parametric statistical and kernel methods, used for genomic prediction, and to compare some of these in the context of rice breeding for quantitative traits. Furthermore, another objective was to provide a simple and user-friendly R package, named KRMM, which allows users to perform RKHS regression with several kernels. After introducing the concept of regularized empirical risk minimization, the connections between well-known parametric and kernel methods such as Ridge regression [i.e., genomic best linear unbiased predictor (GBLUP)] and reproducing kernel Hilbert space (RKHS) regression were reviewed. Ridge regression was then reformulated so as to show and emphasize the advantage of the kernel ""trick"" concept, exploited by kernel methods in the context of epistatic genetic architectures, over parametric frameworks used by conventional methods. Some parametric and kernel methods; least absolute shrinkage and selection operator (LASSO), GBLUP, support vector machine regression (SVR) and RKHS regression were thereupon compared for their genomic predictive ability in the context of rice breeding using three real data sets. Among the compared methods, RKHS regression and SVR were often the most accurate methods for prediction followed by GBLUP and LASSO. An R function which allows users to perform RR-BLUP of marker effects, GBLUP and RKHS regression, with a Gaussian, Laplacian, polynomial or ANOVA kernel, in a reasonable computation time has been developed. Moreover, a modified version of this function, which allows users to tune kernels for RKHS regression, has also been developed and parallelized for HPC Linux clusters. The corresponding KRMM package and all scripts have been made publicly available.",2016,Frontiers in Genetics
Using two-stage Approach toClustering,"A grid-based approach toclustering ispresented. approximate thepeculiar structure ofeachcluster, andturn the Eachgrid isahypercube indataspace, andAppriori algorithm is initial taskofclustering alldatatothatofclustering all usedtofindtherepresenting subsets ofeachcluster. Thetwo-representing subsets. Thusthesecond clustering stage isonly stageprocedure -first finding allrepresenting subsets then performed a small size setofrepresenting subsets compared clustering inthesecondstage - isfoundtoperform wellwhen withtheinitial setofdata. compared withdirect clustering ofdata.Theuseofthe Apir isanofldata representing subsets canefficiently find thedatastructure ofthe Apriori iSaninfluential algorithm formining frequent givedataset. Consequently, thenewapproach caneffectively itemsets forallassociation rulesthatarepresent ina overcome theparameter-sensitive problem that isencountered in transaction. LetT bethesetoftransactions whereeach mostoftheconventional grid-based approaches toclustering. At transaction isasubset oftheitem-set I.Anassociation rule is thesametime, ifaproper threshold inthenewapproach ischosen,. s a thecomputation timetocluster a largedataset willfurtheranexpressionoftheformX Y,whereXcIandYcI. decrease greatly. Twoexperiments areusedtoillustrate the TheSupport degree softheruleandConfidence degree a is performances ofthenewproposed approach andverify its merits. respectively defined as",2006,
A LASSO-Based Diagnostic Framework for Multivariate Statistical Process Control,"In monitoring complex systems, apart from quick detection of abnormal changes of system performance and key parameters, accurate fault diagnosis of responsible factors has become increasingly critical in a variety of applications that involve rich process data. Conventional statistical process control (SPC) methods, such as interpretation and decomposition of Hotellingâ€™s T2-type statistic, are often computationally expensive in such high-dimensional problems. In this article, we frame fault isolation as a two-sample variable selection problem to provide a unified diagnosis framework based on Bayesian information criterion (BIC). We propose a practical LASSO-based diagnostic procedure which combines BIC with the popular adaptive LASSO variable selection method. Given the oracle property of LASSO and its algorithm, the diagnostic result can be obtained easily and quickly with a similar computational effort as least squares regression. More importantly, the proposed method does not require making any extra t...",2011,Technometrics
Efficient methods for estimating constrained parameters with applications to regularized (lasso) logistic regression,"Fitting logistic regression models is challenging when their parameters are restricted. In this article, we first develop a quadratic lower-bound (QLB) algorithm for optimization with box or linear inequality constraints and derive the fastest QLB algorithm corresponding to the smallest global majorization matrix. The proposed QLB algorithm is particularly suited to problems to which the EM-type algorithms are not applicable (e.g., logistic, multinomial logistic, and Cox's proportional hazards models) while it retains the same EM ascent property and thus assures the monotonic convergence. Secondly, we generalize the QLB algorithm to penalized problems in which the penalty functions may not be totally differentiable. The proposed method thus provides an alternative algorithm for estimation in lasso logistic regression, where the convergence of the existing lasso algorithm is not generally ensured. Finally, by relaxing the ascent requirement, convergence speed can be further accelerated. We introduce a pseudo-Newton method that retains the simplicity of the QLB algorithm and the fast convergence of the Newton method. Theoretical justification and numerical examples show that the pseudo-Newton method is up to 71 (in terms of CPU time) or 107 (in terms of number of iterations) times faster than the fastest QLB algorithm and thus makes bootstrap variance estimation feasible. Simulations and comparisons are performed and three real examples (Down syndrome data, kyphosis data, and colon microarray data) are analyzed to illustrate the proposed methods.",2008,Comput. Stat. Data Anal.
Recent Soviet books on Turkey,"Edited by A. X. Rafikov, it is named Istoricheskaya literature na turetskom yazike xranyashchayasya v bibliotekax Leningrada {Historical literature in Turkish, preserved in the Leningrad libraries), Leningrad, Soviet Academy of Sciences: 1968; 267 pp. Sh. V. Megrelidze's Voprosi Zakavkazya v istorii Russoâ€Turetskoy voyni 1877â€“1878 {The problems of Transcaucasia in the Russoâ€Turkish war of 1877â€“1878), Tiflis, Gruzinian Academy of Sciences, Metsniyeryeba Press: 1969; 144 pp. Noveyshaya istoriya Turtsii {The recent history of Turkey), Moscow, Nauka Press: 1968; 396 pp. U. N. Rozaliyev's Klassi I klassovaya borba v Turtsii: burzhuaziya i proletariyat (Classes and the classâ€struggle in Turkey: bourgeoisie and proletariat), Moscow, Nauka: 1966; 168 pp. R. P. Korniyenko's Rabocheye dvizheniye v Turtsii 1918â€“1963 {The workersâ€™ movement in Turkey, 1918â€“1963), Moscow, Nauka Press: 1965; 176 pp. V. I. Danilov's Sredniye sloi v politicheskoy zhizni sovremennoy Turtsii {The middle classes in the political life of cont...",1970,Middle Eastern Studies
Distance metric choice can both reduce and induce collinearity in geographically weighted regression,"This paper explores the impact of different distance metrics on collinearity in local regression models such as geographically weighted regression. Using a case study of house price data collected in Ha Ná»™i, Vietnam, and by fully varying both power and rotation parameters to create different Minkowski distances, the analysis shows that local collinearity can be both negatively and positively affected by distance metric choice. The Minkowski distance that maximised collinearity in a geographically weighted regression was approximate to a Manhattan distance with (powerâ€‰=â€‰0.70) with a rotation of 30Â°, and that which minimised collinearity was parameterised with powerâ€‰=â€‰0.05 and a rotation of 70Â°. The results indicate that distance metric choice can provide a useful extra tuning component to address local collinearity issues in spatially varying coefficient modelling and that understanding the interaction of distance metric and collinearity can provide insight into the nature and structure of the data relationships. The discussion considers first, the exploration and selection of different distance metrics to minimise collinearity as an alternative to localised ridge regression, lasso and elastic net approaches. Second, it discusses the how distance metric choice could extend the methods that additionally optimise local model fit (lasso and elastic net) by selecting a distance metric that further helped minimise local collinearity. Third, it identifies the need to investigate the relationship between kernel bandwidth, distance metrics and collinearity as an area of further work.",2020,
"Predation Protection in the Poison-Fang Blenny, Meiacanthus atrodorsalis, and Its Mimics, Ecsenius bicolor and Runula laudandus (Blenniidae)","The large canine teeth in Meiaeanthtts atrodorsalis impart a toxic bite which causes this animal to be rejected as a prey item by several piscivorous fishes. Two morphologically and behaviorally similar species, Rlmttla lattdandtts and Besenitts bieolor, may enjoy predator protection through Batesian mimicry and, for the former species, greater feeding efficiency through aggressive mimicry. A limited number of experiments indicate that the predator, Bpinephaltts merra, may learn to avoid Meiaeanthtts atrodorsalis and its mimics. As POINTED OUT BY WICKLER (1968), mimicry must be considered in terms of three parties. The ""mimic"" resembles some ""model"" in terms of one or several characteristics such as coloration, shape, odor, etc., but utmost importance must be attached to the ""third party"" which is frequently a predator or prey organism. Usually the third party must encounter both model and mimic and must fail to differentiate perfectly between them. For purposes of clarity, the three types of mimicry discussed in this paper are defined briefly below. But it should be clear that, as with many biological phenomena, it is difficult to provide precise definitions (see Wickler, 1968). Batesian mimicry is generally the resemblance of a harmless or palatable species to a harmful or unpalatable one. This provides the mimic with some selective advantage similar to that enjoyed by the model, usually predator protection. Mullerian mimicry involves a similar relationship except that both species possess some undesirable qualities. Their resemblance increases the probability that the third party will encounter one of their ""type"" and learn to avoid all others. Aggressive, or Peckhammian, mimicry 1 Supported by a National Institutes of Health Postdoctoral Fellowship and Atomic Energy Commission contract no. AT(29-2)-226. Hawaii Institute of Marine Biology contribution no. 380. Manuscript received 19 July 1971. 2 University of Hawaii: Department of Zoology, Honolulu, Hawaii 96822; and Hawaii Institute of Marine Biology, P.O. Box 1067, Kaneohe, Oahu, Hawaii 96744. is the resemblance of a ""predatory"" species to a harmless or nonpredatory form. This relationship facilitates the mimic's ""predation"" through deception of the third party, which in this case is its ""prey"" organism. Mimicry in the form of camouflage is known for many species of fish. Cases of Batesian, Mullerian, and aggressive mimicry are comparatively rare. Aspidonttts taeniattts Quoy & Gaimard, the aggressive mimic of the cleaning wrasse, Labroides dimidiatlls (Cuvier & Valenciennes), is the only well-known example of morphological and behavioral mimicry in fishes (Randall, 1955, 1958; Randall and Randall, 1960; Eibl-Eibesfeldt, 1955, 1959; Wickler, 1960, 1961, 1963, 1965a, 1968). Rtt1l1tla azalea Jordan & Bollman resembles T halassoma ltteasanttm (Gill), another cleaning wrasse, but Hobson (1969) concluded that its coloration functions primarily to conceal the blenny within groups of the wrasses in order to facilitate the mimic's attacks on larger fishes. Some other species of Rttnttla have at least superficial resemblance to young labrids. Starck (1969) described Besenitts midas which joins large schools of Anthias sqttamipinnis. It is suspected that its resemblance to these numerous anthiids serves as a protection against predation and allows it to exploit planktonic food sources. Other cases of mimicry have been hypothesized such as baits and lures of antennariid fishes, but there is little evidence to support the inclusion of these fishes as examples of true mimicry according to the criteria suggested by Wickler (1965a, 1968).",1972,
Stratification of TAD boundaries identified in reproducible Hi-C contact matrices reveals preferential insulation of super-enhancers by strong boundaries,"The metazoan genome is compartmentalized in megabase-scale areas of highly interacting chromatin known as topologically associating domains (TADs), typically identified by computational analyses of Hi-C sequencing data. TADs are demarcated by boundaries that are largely conserved across cell types and even across species, although, increasing evidence suggests that the seemingly invariant TAD boundaries may exhibit plasticity and their insulating strength can vary. However, a genome-wide characterization of TAD boundary strength in mammals is still lacking. A systematic classification and characterization of TAD boundaries may generate new insights into their function. In this study, we first use fused two-dimensional lasso as a machine learning method to improve Hi-C contact matrix reproducibility, and, subsequently, we categorize TAD boundaries based on their insulation score. We demonstrate that higher TAD boundary insulation scores are associated with elevated CTCF levels and that they may differ across cell types. Intriguingly, we observe that super-enhancer elements are preferentially insulated by strong boundaries, i.e. boundaries of higher insulation score. Furthermore, we perform a pan-cancer analysis to demonstrate that strong TAD boundaries and super-enhancer elements are frequently co-duplicated in cancer patients. Taken together, our findings suggest that super-enhancers insulated by strong TAD boundaries may be exploited, as a functional unit, by cancer cells to promote oncogenesis.",2017,bioRxiv
The complete genome of the oil emulsifying strain Thalassolituus oleivorans K-188 from the Barents Sea,"Abstract Gammaproteobacterium Thalassolituus oleivorans plays an important role in oil degradation in sea water through emulsifying crude oil and alkanes at low temperatures in polar sea environment. Here we report the complete genome sequence of K-188 strain (VKPM B-9394) isolated in the Barents Sea and compare it with other known Thalassolituus oleivorans strains. The Thalassolituus strains are differed in orthologs number of the genes of alkane degradation, transport proteins, genes of sugar utilization, endonucleases, signaling proteins, transcriptional regulators and presence of CRISPR/Cas locus. Also only the genome of K-188 contains the 3-hydroxyalkanoate synthetase.",2017,Marine Genomics
Proton scattering radiography using an emulsion detector: a feasibility study.,"A least absolute shrinkage and selection operator (LASSO) method was used for feature selection. Model performance was evaluated using Harrellâ€™s concordance-index (c-index). Fitted model included sum entropy (GLCM), high intensity large area emphasis (GLSZM), volume with a minimum relative intensity of 60% of the maximum SUV â€“ AVRI60% (IVH), grey level non-uniformity and long run emphasis (RLGL) and volume (shape) â€“ Table 1. Internal performance of the model was 0.64 (p<0.01), while externally it achieved a performance of 0.61 (p = 0.05) and 0.58 (0.20), with no further calibration done. Maximum and mean SUV had a univariable performance in the training data of 0.51 and 0.55, respectively. The reduced accuracy of the model validation can be associated with dissimilarities among data, particularly the different timing and delivered dose of the second scan. Nevertheless, we do see benefit on a timely assessment of response to radiotherapy using the described imaging analysis, particularly when compared with the limited capacity of humans to infer accurate predictions and risk groups identification (5). From the Radiomics analysis one can optimally benefit from early response metrics based on changes in metabolism measured with FDG-PET, even before anatomic changes become noticeable, while treatment can still be adapted. We developed and validated a predictive model on the percentage variation of Radiomics features, the so-called â€œDelta Radiomicsâ€ concept, from repeated FDG-PET scans of NSCLC patients.",2016,Radiotherapy and Oncology
L1-norm-based principal component analysis with adaptive regularization,"Recently, some L1-norm-based principal component analysis algorithms with sparsity have been proposed for robust dimensionality reduction and processing multivariate data. The L1-norm regularization used in these methods encounters stability problems when there are various correlation structures among data. In order to overcome the drawback, in this paper, we propose a novel L1-norm-based principal component analysis with adaptive regularization (PCA-L1/AR) which can consider sparsity and correlation simultaneously. PCA-L1/AR is adaptive to the correlation structure of the training samples and can benefit both from L2-norm and L1-norm. An iterative procedure for solving PCA-L1/AR is also proposed. The experiment results on some data sets demonstrate the effectiveness of the proposed method. We propose a L1-norm-based principal component analysis with adaptive regularization.We use trace Lasso to regularize the projection vectors.Our mode can simultaneously consider the sparsity and correlation.",2016,Pattern Recognit.
"Efectos toxicolÃ³gicos del cromo, sobre Daphnia magna","Uncontrolled discharges of wastewater to water bodies occur due to lack of planning in both, technology and industry, in adittion, proper zoning. In Ecuador, the Cutuchi River that is located in the cities of Lasso and Latacunga captures the contaminating discharges of different industries, including the tannery industry. The objective of this study is to evaluate the toxicological effects of chromium on Daphnia magna. Synthetic water, prepared in the laboratory, was used and the following concentrations were used: T1 (control, standing water), T2 (0.03 mg/L), T3 (0.038 mg/L), T4 (0.1 mg/L), T5 (0.2 mg/L), T6 (0.684 mg/L), T7 (0.8 mg/L) y T8 (1 mg/L). Treatment concentrations 2, 3 and 6 were taken based on the results in the Cutuchi River, T3 represents the maximum allowable chromium for the preservation of aquatic life in freshwater bodies and the others were established by means of preliminary tests. The basic chromium sulfate- Cr (OH) SO4 was occupied and five repetitions were performed per treatment in 100 ml vessels and 10 replicates for each repetition. Lethal effects were analyzed at 24, 48 y 72 hours and sublethal effects (mobility) at 6 hours and reproduction (at 20 days). According to the results obtained, mortality and reproduction had negative effects with T8 (1 mg/L), however, mobility was altered (lethargy) in all treatments except in the control, so evidence that Daphnia magna presents significant effects as chromium concentration increases and the number of days of exposure.",2019,
Structure learning with large sparse undirected graphs and its applications,"Learning the structures of large undirected graphical models from data is an active research area and has many potential applications in various domains, including molecular biology, social science, marketing data analysis, among others. The estimated structures provide semantic clarity, the possibility of causal interpretation, and ease of integration with a variety of tools. For example, one very important direction in system biology is to discover gene regulatory networks from microarray data (together with other data sources) based on the observed mRNA levels of thousands of genes under various conditions. The basic assumption is that if two genes are co-regulated by the same proteins, then they tend to have similar patterns at the mRNA levels. Thus it is possible to learn a gene regulatory network from microarray data if we treat each gene as a node variable and each condition as a configuration instance. Structure learning for undirected graphs is an open challenge in machine learning. Most probabilistic structure learning approaches enforce sparsity on the estimated structure by penalizing the number of edges in the graph, which leads to a non-convex optimization problem. Thus these approaches have to search for locally optimal solutions through the combinatorial space of structures, which makes them unscalable for large graphs. Furthermore, the local optimal solution they find could be far away from the global optimal solution, especially when the number of configuration instances is small compared with the number of nodes in the graph. This thesis tries to address these issues by developing a novel structure learning approach that can learn large undirected graphs efficiently in a probabilistic framework. We use the Graphical Gaussian Model (GGM) as the underlying model and propose a novel ARD style Wishart prior for the precision matrix of the GGM, which encodes the graph structure we want to learn. With this prior, we can get the MAP estimation of the precision matrix by solving a modified version of Lasso regression and thus achieve a global optimal sparse solution. By proposing a generalized version of Lasso regression, which is called the Feature Vector Machine (FVM), our structure learning model is further extended so that it can capture non-linear dependencies between node variables. In particular, the optimization problem in our model remains convex even in non-linear cases, which makes our solution globally optimal. We have also developed a graph-based classification approach for predicting node labels given network structures, either observed or automatically induced. This approach is especially suitable when edges in the networks contain multiple input features.",2007,
"Sporopollen Assemblages and the Triassic-jurassic Boundary at the Sikeshu Section of the Junggar Basin,xinjiang","The Sikeshu Section is located on the south-western margin of the Junggar Basin, Xinjiang, and about 40 km southwest of Wusu city. Abundant sporopollen were discovered there. The Late Triassicâ€”Jurassic sporopollen assemblages of the Sikeshu Section measured include: 1) Cameratitriletes and Zonotriletes-Protopinus-Cycadopites assemblage from the Upper Triassic Xiaoquangou Group; 2) Cyatheaceae-Pinaceae-Cycadopites assemblage form Lower Jurassic Badaowan Formation; 3) Cyatheaceae-Pinaceae-Quadraeculina-Cycadopites assemblage from Lower Jurassic Sangonghe Formation; 4) Cyatheaceae-Quadraeculina-Piceaepollenites assemblage from the Middle Jurassic Xishanyao Formation; 5) Cyatheaceae-Classopollis-Pinaceae assemblage from the Middle Jurassic Toutunhe Formation. Based on sporopollen assemblage, lithology, sedimentology and regional correlation, the Triassic-Jurassic boundary should be placed at the contact between Beds 38 and 39.",2007,Journal of stratigraphy
Accurately forecasting temperatures in smart buildings using fewer sensors,"Forecasts of temperature in a â€œsmartâ€ building, i.e. one that is outfitted with sensors, are computed from data gathered by these sensors. Model predictive controllers can use accurate temperature forecasts to save energy by optimally using heating, ventilation and air conditioners while achieving comfort. We report on experiments from such a house. We select different sets of sensors, build a temperature model from each set, and compare the accuracy of these models. While a primary goal of this research area is to reduce energy consumption, in this paper, besides the cost of energy, we consider the cost of data collection and management. Our approach informs the selection of an optimal set of sensors for any model predictive controller to reduce overall costs, using any forecasting methodology. We use lasso regression with lagged observations, which compares favourably to previous methods using the same data.",2017,Personal and Ubiquitous Computing
Recoverability of Group Sparse Signals from Corrupted Measurements via Robust Group Lasso,"This paper considers the problem of recovering a group sparse signal matrix $\mathbf{Y} = [\mathbf{y}_1, \cdots, \mathbf{y}_L]$ from sparsely corrupted measurements $\mathbf{M} = [\mathbf{A}_{(1)}\mathbf{y}_{1}, \cdots, \mathbf{A}_{(L)}\mathbf{y}_{L}] + \mathbf{S}$, where $\mathbf{A}_{(i)}$'s are known sensing matrices and $\mathbf{S}$ is an unknown sparse error matrix. A robust group lasso (RGL) model is proposed to recover $\mathbf{Y}$ and $\mathbf{S}$ through simultaneously minimizing the $\ell_{2,1}$-norm of $\mathbf{Y}$ and the $\ell_1$-norm of $\mathbf{S}$ under the measurement constraints. We prove that $\mathbf{Y}$ and $\mathbf{S}$ can be exactly recovered from the RGL model with a high probability for a very general class of $\mathbf{A}_{(i)}$'s.",2015,ArXiv
Discovery and Geological Significance of Classopollis High-Content,"This paper systematically collects and integrates palaeotologic information from downhole Sangonghe formation and out -crops in Junggar basin,and pays mainly attention to study of the distribution of sporopollen assemblage of Classopollis high-con-tent zone in Sangonghe formation.The results show that strata with Classopollis high-content zone are big in thickness and high in content in the foreland of North Tianshan mountain,and become thinning and reducing from the south to the north of Junggar basin.The Classopollis high-content zone in top of lower member of Sangonghe formation in Junggar basin is correlated with San-gonghe formation of Meiyaogou section in Turpan-Hami basin,upper member of Yangxia formation of Kuqa river section in Tarim basin,No.2and3members of Dameigou formation of Dameigou section in Qinghai region,upper part of Tandonggou formation in Minhe basin and Fuxian formation in southern Shaanxi-Gansu-Ningxia basin,reflecting an arid climate occurred in period of Toar-cian in the world.The Classopollis high-content zone is a good marker bed,and can be used as one of markers for distinguishing between the Middle Jurassic and the Lower Jurassic in Junggar basin.",2003,Xinjiang Petroleum Geology
Insufficienza surrenalica acuta nell'adulto,"Lâ€™insufficienza surrenalica acuta e un deficit di steroidi che mette in gioco la prognosi vitale in assenza di un trattamento adeguato a causa della comparsa di uno stato di shock. La diagnosi sara facilmente evocata in presenza di uno stato di shock in un paziente seguito per una malattia di Addison. Puo tuttavia trattarsi anche di un episodio iniziale e lâ€™insufficienza surrenalica acuta puo essere secondaria a unâ€™emorragia delle ghiandole surrenali, per esempio nel quadro di un trattamento anticoagulante. In questo caso bisognera sospettare la diagnosi di fronte a segni poco specifici e ingannevoli quali un collasso accompagnato da febbre, dolori addominali e vomito. Il trattamento e unâ€™urgenza e associa una dose sovrafisiologica di glucocorticoidi e una reidratazione importante. Nel decorso, la conferma diagnostica si basa su un cortisolo plasmatico bassissimo (prelievo eseguito prima dellâ€™inizio del trattamento) e sulla mancanza di risposta nel test al Synacthen Â® . Per certe eziologie possono essere necessari altri test di stimolazione. Il valore del tasso di adrenocorticotrophin hormone guida la ricerca eziologica verso unâ€™insufficienza surrenalica primitiva in caso di tasso elevato o verso unâ€™insufficienza surrenalica secondaria o corticotropa davanti a un tasso normale o basso. La malattia di Addison, legata a una retrazione autoimmune delle ghiandole surrenali o a una tubercolosi, resta la prima causa di insufficienza surrenalica primitiva, anche se molti progressi genetici hanno permesso lâ€™identificazione di nuove eziologie. A lungo termine, la terapia sostitutiva associa lâ€™idrocortisone e, a volte, il fludrocortisone. Lâ€™educazione terapeutica del paziente e fondamentale per prevenire ogni scompenso.",2009,EMC - Urgenze
"Lasso peptide, a highly stable structure and designable multifunctional backbone","Lasso peptide belongs to a new class of natural product with highly compact and stable structure. It has varieties of biological activities, among which the most important one is its antibacterial efficacy. Novel lasso peptides have been constantly discovered and analyzed by advanced techniques, and the biosynthesis or even chemical synthesis of lasso peptide has been studied after learning its constituent amino acids and maturation process. Structural identification of lasso peptide provides information for elucidating the mechanisms of its antibacterial activity and basis for further modifications. Ring of lasso peptide is the key to both its highly compact and stable structure and its intrinsic antibacterial property. The loop has been considered as suitable modification region of lasso peptide, such as V11â€“S18 of MccJ25 being modifiable without disrupting the lasso structure in biosynthesis. The tail is the immunity protein that can export lasso peptide out of its produced strain and serve as a self-protection mechanism at the same time. Most of currently known lasso peptides are non-pathogenic, which implies that the modified lasso peptides are promising candidates for medical applications. Arginine, glycine, and aspartic acid as a ligands of cancer-specific receptor have been grafted to the loop of lasso peptide without losing its bioactivity, and many other targets are expected to be used for lasso peptide modification. Multi-molecular modification and large-scale production need to be studied and solved in future for designing and using multifunctional lasso peptide based on its extraordinary stable structure.",2016,Amino Acids
Prognostic value of an immunohistochemical signature in patients with esophageal squamous cell carcinoma undergoing radical esophagectomy,"Here, we aimed to identify an immunohistochemical (IHC)-based classifier as a prognostic factor in patients with esophageal squamous cell carcinoma (ESCC). A cohort of 235 patients with ESCC undergoing radical esophagectomy (with complete clinical and pathological information) were enrolled in the study. Using the least absolute shrinkage and selection operator (LASSO) regression model, we extracted six IHC features associated with progression-free survival (PFS) and then built a classifier in the discovery cohort (nÂ =Â 141). The prognostic value of this classifier was further confirmed in the validation cohort (nÂ =Â 94). Additionally, we developed a nomogram integrating the IHC-based classifier to predict the PFS. We used the IHC-based classifier to stratify patients into high- and low-risk groups. In the discovery cohort, 5-year PFS was 22.4% (95% CI: 0.14-0.36) for the high-risk group and 43.3% (95% CI: 0.32-0.58) for the low-risk group (PÂ =Â 0.00064), and in the validation cohort, 5-year PFS was 20.58% (95% CI: 0.12-0.36) for the high-risk group and 36.43% (95% CI: 0.22-0.60) for the low-risk group (PÂ =Â 0.0082). Multivariable analysis demonstrated that the IHC-based classifier was an independent prognostic factor for predicting PFS of patients with ESCC. We further developed a nomogram integrating the IHC-based classifier and clinicopathological risk factors (gender, American Joint Committee on Cancer staging, and vascular invasion status) to predict the 3- and 5-year PFS. The performance of the nomogram was evaluated and proved to be clinically useful. Our 6-IHC marker-based classifier is a reliable prognostic tool to facilitate theÂ individual management of patients with ESCC after radical esophagectomy.",2018,Molecular Oncology
What is the relative impact of primary health care quality and conditional cash transfer program in child mortality?,"Evaluate how coverage and quality of primary health care (PHC) and a conditional cash transfer (CCT) program associate with child mortality in Brazil. Multivariate linear regression models and least absolute shrinkage and selection estimator (LASSO) were utilized with the municipal level child mortality rate as the key dependent variable. PHC quality with PHC and CCT coverage were the independent variables. The quality of the Brazilian PHC was assessed using the Brazilian National Program for Access and Quality Improvement in PHC data. PHC and CCT coverage were calculated based on Brazilian official databases. Human developmental index (HDI), municipality size, and country region were used as control variables. A total of 3441 municipalities were evaluated. We found that ESF (EstratÃ©gia SaÃºde da FamÃ­lia) quality variables PLANNING [Family Health Team Planning activities], CITYSUPPORT [municipality support for Family Health Strategy activities], EXAMS [exams offered and priority groups seen by the family health team], and PRENATAL [prenatal care and exams provided by the family health team], as well as HDI, percentage of PHC coverage, percentage of CCT coverage, and population size have significant and negative relationships with 1-year-old child mortality. LASSO regression results confirmed these associations. Quality is an important element of effective social service provision. This exploration represents one of the first investigations into the role of PHC system quality, and how it is related to health outcomes, while also considering PHC and conditional cash transfer program coverage. Quality of PHC, measured by work process variables, plays an important role in child mortality. Efforts on PHC quality and coverage, as well as on CCT program coverage, are important to child mortality reduction. Therefore, this is an important finding to other PHC public health services.",2019,Canadian Journal of Public Health
Sparse Subspace Clustering via Two-Step Reweighted L1-Minimization: Algorithm and Provable Neighbor Recovery Rates,"Sparse subspace clustering (SSC) relies on sparse regression for accurate neighbor identification. Inspired by recent progress in compressive sensing, this paper proposes a new sparse regression scheme for SSC via two-step reweighted $\ell_1$-minimization, which also generalizes a two-step $\ell_1$-minimization algorithm introduced by E. J. Cand\`es et al in [The Annals of Statistics, vol. 42, no. 2, pp. 669-699, 2014] without incurring extra algorithmic complexity. To fully exploit the prior information offered by the computed sparse representation vector in the first step, our approach places a weight on each component of the regression vector, and solves a weighted LASSO in the second step. We propose a data weighting rule suitable for enhancing neighbor identification accuracy. Then, under the formulation of the dual problem of weighted LASSO, we study in depth the theoretical neighbor recovery rates of the proposed scheme. Specifically, an interesting connection between the locations of nonzeros of the optimal sparse solution to the weighted LASSO and the indexes of the active constraints of the dual problem is established. Afterwards, under the semi-random model, analytic probability lower/upper bounds for various neighbor recovery events are derived. Our analytic results confirm that, with the aid of data weighting and if the prior neighbor information is enough accurate, the proposed scheme with a higher probability can produce many correct neighbors and few incorrect neighbors as compared to the solution without data weighting. Computer simulations are provided to validate our analytic study and evidence the effectiveness of the proposed approach.",2019,ArXiv
Asymptotic Analysis of LASSO â€™ s Solution Path with Implications for Approximate Message Passing,"This paper concerns the performance of the LASSO (also knows as basis pursuit denoising) for recovering sparse signals from undersampled, randomized, noisy measurements. We consider the recovery of the signal xo âˆˆ R from n random and noisy linear observations y = Axo + w, where A is the measurement matrix and w is the noise. The LASSO estimate is given by the solution to the optimization problem xo with xÌ‚Î» = arg minx 1 2 â€–y âˆ’ Axâ€–2 + Î»â€–xâ€–1. Despite major progress in the theoretical analysis of the LASSO solution, little is known about its behavior as a function of the regularization parameter Î». In this paper we study two questions in the asymptotic setting (i.e., where N â†’âˆž, nâ†’âˆž while the ratio n/N converges to a fixed number in (0, 1)): (i) How does the size of the active set â€–xÌ‚Î»â€–0/N behave as a function of Î», and (ii) How does the mean square error â€–xÌ‚Î» âˆ’ xoâ€–2/N behave as a function of Î»? We then employ these results in a new, reliable algorithm for solving LASSO based on approximate message passing (AMP).",2018,
Phylogenetic diversity of nonmarine picocyanobacteria.,"We studied the phylogenetic diversity of nonmarine picocyanobacteria broadening the sequence data set with 43 new sequences of the 16S rRNA gene. The sequences were derived from monoclonal strains isolated from four volcanic high-altitude athalassohaline lakes in Mexico, five glacial ultraoligotrophic North Patagonian lakes and six Italian lakes of glacial, volcanic and morenic origin. The new sequences fall into a number of both novel and previously described clades within the phylogenetic tree of 16S rRNA gene. The new cluster of Lake Nahuel Huapi (North Patagonia) forms a sister clade to the subalpine cluster II and the marine Synechococcus subcluster 5.2. Our finding of the novel clade of 'halotolerants' close to the marine subcluster 5.3 (Synechococcus RCC307) constitutes an important demonstration that euryhaline and marine strains affiliate closely. The intriguing results obtained shed new light on the importance of the nonmarine halotolerants in the phylogenesis of picocyanobacteria.",2013,FEMS microbiology ecology
"Multivariate Signal Processing for Quantitative and Qualitative Analysis of Ion Mobility Spectrometry data, applied to Biomedical Applications and Food Related Applications","There are several applications where the measurement of VOC results to be useful, such as: toxic leaks, air quality measurements, explosive detection, monitoring of food and beverages quality, diagnosis of diseases, etc. Some of this applications claim for fast responses or even real time responses. In this context, there are few analytical techniques for performing gas phase analysis, among of them Ion Mobility Spectrometry (IMS). IMS is a fast analytical device based on the time of flight of ions in a drift tube. The response of IMS lasts typically few seconds, but it can be even less than a second. This fast response has drifted its use towards novel applications, such as biomedical and food applications (bio-related applications). Nonetheless, it has also brought the need to analyze complex spectra with hundreds of compounds. In fact, tackling this disadvantage is the main focus of this thesis, where new algorithms for enhancing the IMS performance are investigated when are applied to bio-related applications. Nonlinear behavior and charge competitions of IMS responses are important issues that need to be addressed. Both effects have a direct impact in the IMS spectra interpretation â€”especially when real dataset are studied. Additionally, the use of univariate spectra analysis, where peaks information is extracted manually, becomes unfeasible in bio-related applications. In this context, this work introduces multivariate methodologies focused on quantitative and qualitative analysis. In the case of quantitative analysis, calibration models were built using univariate methodology, Partial Leas Squares (PLS) and Multivariate Curve Resolution techniques (MCR). The quantitative analysis aims tackling the main issues of IMS such as non linearities and mixture effect. Definitely, univariate techniques provides poor or overoptimistic results that minimize the impact of the IMS use. The results show a really improvement on the performance when multivariate techniques were used. Regarding the results between MCR and PLS, the main difference is the interpretability that offers MCR. In the case of qualitative analysis, two different approaches were planned for building models for classes' discrimination. The first approach consisted on building a model through principal component analysis and linear discriminant analysis, besides of using robust cross validation methodology for obtaining reliable results. This methodology were implemented in samples of wine, where main motivation was found discrimination regarding to their origin. The results were fully satisfactory because the model was able to separate four groups with a high accuracy rate. The second approach involves the use of Multivarite Curve Resolution â€” Lasso algorithm for extracting pure components of samples from rats' breath and then use a feature selection technique for obtaining the most representative features subset. In this case, the objective of the application was to find a model that discriminate rats with sepsis from control rats. The results shows there were few pure components of IMS that generate a discriminatory model that means there are specific compounds in the breath linked with the disease. Summarizing, the following proposal has as main objective resolving open issues in stand-alone IMS that are applied to the analysis of bio-related applications. Two major investigation lines were proposed in this thesis: (i) qualitative analysis and (ii) quantitative analysis. The qualitative analysis covers pre-processing algorithms and the developing of new methodologies for building models in bio-related applications. The quantitative analysis are focused on highlighting the importance of the use of multivariate techniques instead of univariate techniques. In order to reach the objectives of this thesis, a set of datasets were created, which are detailed on the content of this thesis. The results and main conclusions are deeply explained in the extended proposal.",2015,
Group spikeâ€andâ€slab lasso generalized linear models for disease prediction and associated genes detection by incorporating pathway information,"Motivation: Largeâ€scale molecular data have been increasingly used as an important resource for prognostic prediction of diseases and detection of associated genes. However, standard approaches for omics data analysis ignore the group structure among genes encoded in functional relationships or pathway information. Results: We propose new Bayesian hierarchical generalized linear models, called group spikeâ€andâ€slab lasso GLMs, for predicting disease outcomes and detecting associated genes by incorporating largeâ€scale molecular data and group structures. The proposed model employs a mixture doubleâ€exponential prior for coefficients that induces selfâ€adaptive shrinkage amount on different coefficients. The group information is incorporated into the model by setting groupâ€specific parameters. We have developed a fast and stable deterministic algorithm to fit the proposed hierarchal GLMs, which can perform variable selection within groups. We assess the performance of the proposed method on several simulated scenarios, by varying the overlap among groups, group size, number of nonâ€null groups, and the correlation within group. Compared with existing methods, the proposed method provides not only more accurate estimates of the parameters but also better prediction. We further demonstrate the application of the proposed procedure on three cancer datasets by utilizing pathway structures of genes. Our results show that the proposed method generates powerful models for predicting disease outcomes and detecting associated genes. Availability and implementation: The methods have been implemented in a freely available R package BhGLM (http://www.ssg.uab.edu/bhglm/). Contact: nyi@uab.edu Supplementary information: Supplementary data are available at Bioinformatics online.",2018,Bioinformatics
Applications of the lasso and grouped lasso to the estimation of sparse graphical models,"We propose several methods for estimating edge-sparse and nodesparse graphical models based on lasso and grouped lasso penalties. We develop ecien t algorithms for tting these models when the numbers of nodes and potential edges are large. We compare them to competing methods including the graphical lasso and SPACE (Peng, Wang, Zhou & Zhu 2008). Surprisingly, we nd that for edge selection, a simple method based on univariate screening of the elements of the empirical correlation matrix usually performs as well or better than all of the more complex methods proposed here and elsewhere. Running title: Applications of the lasso and grouped lasso",2010,
New rotary reciprocating compressor: its development and potential,"A rotary positive-displacement mechanism, that was conceived by Mr. Marek J. Lassota in an engine configuration has been found advantageous also for compressor applications. The mechanism has potential for versatility in applications covering both internal and external combustion engines as well as lubricated and nonlubricated compressors and expanders. Motivated mainly by energy conservation needs, development as an efficient heat pump compressor is being pursued. The objective of this program is to develop a residential and light commercial size, lightweight, highly reliable heat pump compressor surpassing the most modern machines in efficiency and cost. The performance and life-cycle testing results achieved so far indicate that these objectives will be met.",1979,
The thirty year birthday of the Italian Associazione per l'Economia della Cultura,"This article celebrates the 30 years birthday of the Italian Associazione per l'Economia della Cultura/AEC - inspired by the American ACEI - created in Rome in 1986 under the auspices of the then Secretary of State for Cultural Heritage, Giuseppe Galasso. In a country where cultural policies have been historically mainly dominated by heritage issues, ACEI was, in fact, the first Italian research organization to deal with cultural economics and cultural policy as a whole - from the heritage and the visual arts to the performing arts, as well as to the audiovisual and publishing industries - and to appeal to some of the main experts of these disciplines. It describes the ups and downs of ACEI's life, often threatened in the years 2000s, along with other cultural organizations, by the shortages in public financing of culture - and its main three axes of activity: 1) national and international conferences; 2) research (including the publication of two extensive Reports on Cultural Economics in Italy (for the eighties and nineties decades respectively); 3) the publication, since 1992, of the quarterly journal Economia della Cultura, the only existing journal of the like in continental Europe. The article finally deals with the progressive shift of ACEI's focus - since the years 2000s - from the mainly economic, administrative and statistical issues to the outset on culture as one of the main axes of sustainable development, thus dealing as well on its growing role of common good and as a factor of social inclusion in a changing society.",2017,
Combined l1 and greedy l0 penalized least squares for linear model selection,"We introduce a computationally effective algorithm for a linear model selection consisting of three steps: screening-ordering-selection (SOS). Screening of predictors is based on the thresholded Lasso that is l1 penalized least squares. The screened predictors are then fitted using least squares (LS) and ordered with respect to their |t| statistics. Finally, a model is selected using greedy generalized information criterion (GIC) that is l0 penalized LS in a nested family induced by the ordering. We give non-asymptotic upper bounds on error probability of each step of the SOS algorithm in terms of both penalties. Then we obtain selection consistency for different (n, p) scenarios under conditions which are needed for screening consistency of the Lasso. Our error bounds and numerical experiments show that SOS is worth considering alternative for multi-stage convex relaxation, the latest quasiconvex penalized LS. For the traditional setting (n > p) we give Sanov-type bounds on the error probabilities of the ordering-selection algorithm. It is surprising consequence of our bounds that the selection error of greedy GIC is asymptotically not larger than of exhaustive GIC.",2015,J. Mach. Learn. Res.
