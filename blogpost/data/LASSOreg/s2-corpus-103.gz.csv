title,abstract,year,journal
Challenges in High-throughput Data Analysis: Proteomic Data Pre-processing and Network Methods for Integrating Multiple Data Types,"Author(s): Liao, Eileen | Advisor(s): Elashoff, Robert M | Abstract: 1) Proteomic Data Pre-processing: Quantification and Normalization of Luminex Assay System High through-put genomic and proteomic technologies allow rapid analysis of molecular targets of thousands of genes at a time, either at the DNA, RNA or protein level. In these type of experiments variations in expression measurements can occur from a variety of sources. Our goal was to examine measurement and normalization techniques to reduce the experimental variation in data derived from a bead-based multiplex Luminex assay system which allows simultaneous measurements of proteins. Normalization for the Luminex assay system requires a fundamentally different approach than the case of traditional microarrays. In the Luminex assay system, each experimental unit is a plate and each plate has results for multiple subjects and analytes. We quantified performance among different measurement systems (fluorescent intensity, background in fluorescent intensity, and observed concentration) in both high and standard scanning systems. Various normalization techniques (scale normalization, quantile normalization, lowess curve normalization) were adapted to the Luminex data scenario and their performance was compared in two datasets. We used the coefficient of variation across plates to compare the performance of normalization methods. Median and Lowess normalizations appeared to result in reducing plate- to-plate variation the most. Quantile normalization does not appear to work well for these datasets. Our results suggest that simple normalizations such as scale and lowess curve normalizations perform better than complex methods such as quantile normalization. Complex methods may add noise and bias to the normalized adjustment when the assumptions are not met. 2) Integration of microRNA and mRNA by Weighted Gene Co-expression Network AnalysisWe focus on the step-by-step network construction and module detection of mRNAs by weighted gene co-expression network analysis (WGCNA), followed by identifying the strong correlation between miRNA and module eigengenes. We then evaluate whether the predicted mRNA targets are differentially present between a given module and other modules by using the Fisher's exact test. We retained miRNAs who are significant in the fisher exact test, and are strongly correlated with eigengenes in a module.Next we relate modules to disease status by using eigengene network methodology, we found that 11 out of 13 modules are significantly related with disease status. Enrichment analyses by DAVID software are implemented for the 11 modules. We also run step-by-step network construction and module detection of miRNAs and found 6 modules. We used LASSO regression to explore the relationship between miRNA and mRNAs. The predictors are module eigengene of miRNA and the outcome is the eigengene from each mRNA module. We found that 1 miRNA ""hsa_miR_25"" is significantly anti-correlated with mRNA Magenta module. ""hsa_miR_25"" belongs to the miRNA module ""blue"" that is also predictive to Magenta mRNA module through LASSO regression. Its putative mRNA targets are found and integrated from the renal dataset. In conclusion, the weighted co-expression network analysis provides a novel integrative view of miRNA and their putative genes. It also greatly alleviates the multiple testing problems that plague standard gene-centric methods.",2012,
Brain Regions Involved in Arousal and Reward Processing are Associated with Apathy in Alzheimer's Disease and Frontotemporal Dementia.,"BACKGROUND
Apathy is a common and problematic symptom of several neurodegenerative illnesses, but its neuroanatomical bases are not understood.


OBJECTIVE
To determine the regions associated with apathy in subjects with mild Alzheimer's disease (AD) using a method that accounts for the significant co-linearity of regional atrophy and neuropsychiatric symptoms.


METHODS
We identified 57 subjects with mild AD (CDRâ€Š=â€Š1) and neuropsychiatric symptoms in the Alzheimer's Disease Neuroimaging Initiative (ADNI) database. We performed a multivariate multiple regression with LASSO regularization on all symptom subscales of the Neuropsychiatric Inventory and the whole-brain ROI volumes calculated from their baseline MRIs with FreeSurfer. We compared our results to those from a previous study using the same method in patients with frontotemporal dementia (FTD) and corticobasal syndrome (CBS).


RESULTS
Of neuropsychiatric symptoms, apathy showed the most robust neuroanatomical associations in the AD subjects. Atrophy of the following regions were independently associated with apathy: the ventromedial prefrontal cortex; ventrolateral prefrontal cortex; posterior cingulate cortex and adjacent lateral cortex; and the bank of the superior temporal sulcus. These results replicate previous studies using FTD and CBS patients, mostly agree with the previous literature on apathy in AD, and correspond to the Medial and Orbital Prefrontal Cortex networks identified in non-human primates.


CONCLUSION
The current study, previous studies from our laboratory, and the previous literature suggest that impairment of the same brain networks involved in arousal, threat response, and reward processing are associated with apathy in AD and FTD.",2017,Journal of Alzheimer's disease : JAD
Feature selection and survival modeling in The Cancer Genome Atlas,"PURPOSE
Personalized medicine is predicated on the concept of identifying subgroups of a common disease for better treatment. Identifying biomarkers that predict disease subtypes has been a major focus of biomedical science. In the era of genome-wide profiling, there is controversy as to the optimal number of genes as an input of a feature selection algorithm for survival modeling.


PATIENTS AND METHODS
The expression profiles and outcomes of 544 patients were retrieved from The Cancer Genome Atlas. We compared four different survival prediction methods: (1) 1-nearest neighbor (1-NN) survival prediction method; (2) random patient selection method and a Cox-based regression method with nested cross-validation; (3) least absolute shrinkage and selection operator (LASSO) optimization using whole-genome gene expression profiles; or (4) gene expression profiles of cancer pathway genes.


RESULTS
The 1-NN method performed better than the random patient selection method in terms of survival predictions, although it does not include a feature selection step. The Cox-based regression method with LASSO optimization using whole-genome gene expression data demonstrated higher survival prediction power than the 1-NN method, but was outperformed by the same method when using gene expression profiles of cancer pathway genes alone.


CONCLUSION
The 1-NN survival prediction method may require more patients for better performance, even when omitting censored data. Using preexisting biological knowledge for survival prediction is reasonable as a means to understand the biological system of a cancer, unless the analysis goal is to identify completely unknown genes relevant to cancer biology.",2013,International Journal of Nanomedicine
A data-driven approach to prostate cancer detection from dynamic contrast enhanced MRI,"Magnetic resonance imaging (MRI), particularly dynamic contrast enhanced (DCE) imaging, has shown great potential in prostate cancer diagnosis and staging. In the current practice of DCE-MRI, diagnosis is based on quantitative parameters extracted from the series of T1-weighted images acquired after the injection of a contrast agent. To calculate these parameters, a pharmacokinetic model is fitted to the T1-weighted intensities. Most models make simplistic assumptions about the perfusion process. Moreover, these models require accurate estimation of the arterial input function, which is challenging. In this work we propose a data-driven approach to characterization of the prostate tissue that uses the time series of DCE T1-weighted images without pharmacokinetic modeling. This approach uses a number of model-free empirical parameters and also the principal component analysis (PCA) of the normalized T1-weighted intensities, as features for cancer detection from DCE MRI. The optimal set of principal components is extracted with sparse regularized regression through least absolute shrinkage and selection operator (LASSO). A support vector machine classifier was used with leave-one-patient-out cross validation to determine the ability of this set of features in cancer detection. Our data is obtained from patients prior to radical prostatectomy and the results are validated based on histological evaluation of the extracted specimens. Our results, obtained on 449 tissue regions from 16 patients, show that the proposed data-driven features outperform the traditional pharmacokinetic parameters with an area under ROC of 0.86 for LASSO-isolated PCA parameters, compared to 0.78 for pharmacokinetic parameters. This shows that our novel approach to the analysis of DCE data has the potential to improve the multiparametric MRI protocol for prostate cancer detection.",2015,Computerized medical imaging and graphics : the official journal of the Computerized Medical Imaging Society
The LASSO for generic design matrices as a function of the relaxation parameter,"The LASSO is a variable subset selection procedure in statistical linear regression based on $\ell_1$ penalization of the least-squares operator. Its behavior crucially depends, both in practice and in theory, on the ratio between the fidelity term and the penalty term. We provide a detailed analysis of the fidelity vs. penalty ratio as a function of the relaxation parameter. Our study is based on a general position condition on the design matrix which holds with probability one for most experimental models. Along the way, the proofs of some well known basic properties of the LASSO are provided from this new generic point of view.",2011,arXiv: Statistics Theory
Nonconvex selection in nonparametric additive models,"High-dimensional data offers researchers increased ability to find useful factors in predicting a response. However, determination of the most important factors requires careful selection of the explanatory variables. In order to tackle this challenge, much work has been done on single or grouped variable selection under the penalized regression framework. Although the topic of variable selection has been extensively studied under the parametric framework, its extensions to more flexible nonparametric models are yet to be explored. In order to implement the variable selection in nonparametric additive models, I introduce and study two nonconvex selection methods under the penalized regression framework, namely the group MCP and the adaptive group LASSO, aiming at improvements on the selection performances of the more widely known group LASSO method in such models. One major part of the dissertation focuses on the theoretical properties of the group MCP and the adaptive group LASSO. I derive their selection and estimation properties. The application of the presently proposed methods to nonparametric additive models are further examined using simulation. Their applications to areas such as the economics and genomics are presented as well. Under both the simulation studies and data applications, the group MCP and the adaptive group LASSO have shown their advantages over the more traditionally used group LASSO method. For the proposed adaptive group LASSO that uses the newly proposed weights,",2014,
Analyzing Risk Factors for Morbidity and Mortality after Lung Resection for Lung Cancer Using the NSQIP Database.,"BACKGROUND
Our goal was to develop a predictive model that identifies how preoperative risk factors and perioperative complications lead to mortality after anatomic pulmonary resections.


STUDY DESIGN
This was a retrospective cohort study. The American College of Surgeons NSQIP database was examined for all patients undergoing elective lobectomies for cancer from 2005 through 2012. Fifty-eight pre- and intraoperative risk factors and 13 complications were considered for their impact on perioperative mortality within 30 days of surgery. Multivariate logistic regression and a logistic regression model using least absolute shrinkage and selection operator (LASSO) selection methods were used to identify preoperative risk factors that were significant for predicting mortality, either through or independent of complications. Only factors that were significant under both the multivariate logistic regression and LASSO-selected models were considered to be validated for the final model.


RESULTS
There were 6,435 lobectomies identified. After multivariate logistic regression modeling, 28 risk factors and 5 complications were found to be predictors for mortality. This was then tested against the LASSO method. There were 7 factors shared between the LASSO and multivariate logistic regressions that predicted mortality based on comorbidity: age (pÂ = 0.007), male sex (pÂ = 0.011), open lobectomy (pÂ = 0.001), preoperative dyspnea at rest (p < 0.001), preoperative dyspnea on exertion (pÂ = 0.003), preoperative dysnatremia (serum sodium <135 mEq/L or >145 mEq/L) (pÂ = 0.011), and preoperative anemia (pÂ = 0.002). Of these, 3 variables predicted mortality independent of any complications: dyspnea at rest, dyspnea on exertion, and dysnatremia.


CONCLUSIONS
The clinical factors that predict postoperative complications and mortality are multiple and not necessarily aligned. Efforts to improve quality after anatomic pulmonary resections should focus on mechanisms to address both types of adverse outcomes.",2016,Journal of the American College of Surgeons
Data Science for Delamination Prognosis and Online Batch Learning in Semiconductor Assembly Process,"The transformation of wafers into chips is a complex manufacturing process involving literally thousands of equipment parameters. Delamination, a leading cause of defective products, can occur between die and epoxy molding compound (EMC), epoxy and substrate, lead frame and EMC, and so on. Troubleshooting is generally on a case-by-case basis and is both time-consuming and labor-intensive. We propose a three-phase data science (DS) framework for process prognosis and prediction. The first phase is for data preprocessing. The second phase uses least absolute shrinkage and selection operator (LASSO) regression and stepwise regression to identify the key variables affecting delamination. The third phase develops a backpropagation neural network (BPNN), support vector regression (SVR), partial least squares (PLS), and gradient boosting machine (GBM) to predict the ratio of the delamination area in a die. We also investigate the imbalance between a false positive rate and a false negative rate after quality classification with BPN and GBM models to improve the tradeoff between the two types of risks. We conducted an empirical study of a semiconductor manufacturer, and the results show that the proposed framework provides an effective delamination prediction supporting the troubleshooting. In addition, for online prediction, it is necessary to determine the batch size for the timing of retraining the model, and we suggest the cost-oriented method to solve the issue.",2020,"IEEE Transactions on Components, Packaging and Manufacturing Technology"
Assumptionless consistency of the Lasso,"The Lasso is a popular statistical tool invented by Robert Tibshirani for linear regression when the number of covariates is greater than or comparable to the number of observations. The purpose of this note is to highlight the simple fact (noted in a number of earlier papers in various guises) that for the loss function considered in Tibshirani's original paper, the Lasso is consistent under almost no assumptions at all.",2013,arXiv: Statistics Theory
Protein disulfide isomerases are promising targets for predicting the survival and tumor progression in glioma patients,"The present study focused on the expression patterns, prognostic values and potential mechanism of the PDI family in gliomas. Most PDI family members' mRNA expressions were observed significantly different between gliomas classified by clinical features. Construction of the PDI signature, cluster and risk score models of glioma was done using GSVA, consensus clustering analysis, and LASSO Cox regression analysis respectively. High values of PDI signature/ risk score and cluster 1 in gliomas were associated with malignant clinicopathological characteristics and poor prognosis. Analysis of the distinctive genomic alterations in gliomas revealed that many cases having high PDI signature and risk score were associated with genomic aberrations of driver oncogenes. GSVA analysis showed that PDI family was involved in many signaling pathways in ERAD, apoptosis, and MHC class I among many more. Prognostic nomogram revealed that the risk score was a good prognosis indicator for gliomas. The qRT-PCR and immunohistochemistry confirmed that P4HB, PDIA4 and PDIA5 were overexpressed in gliomas. In summary, this research highlighted the clinical importance of PDI family in tumorigenesis and progression in gliomas.",2020,Aging (Albany NY)
Integration Analysis of Three Omics Data Using Penalized Regression Methods: An Application to Bladder Cancer,"Omics data integration is becoming necessary to investigate the genomic mechanisms involved in complex diseases. During the integration process, many challenges arise such as data heterogeneity, the smaller number of individuals in comparison to the number of parameters, multicollinearity, and interpretation and validation of results due to their complexity and lack of knowledge about biological processes. To overcome some of these issues, innovative statistical approaches are being developed. In this work, we propose a permutation-based method to concomitantly assess significance and correct by multiple testing with the MaxT algorithm. This was applied with penalized regression methods (LASSO and ENET) when exploring relationships between common genetic variants, DNA methylation and gene expression measured in bladder tumor samples. The overall analysis flow consisted of three steps: (1) SNPs/CpGs were selected per each gene probe within 1Mb window upstream and downstream the gene; (2) LASSO and ENET were applied to assess the association between each expression probe and the selected SNPs/CpGs in three multivariable models (SNP, CPG, and Global models, the latter integrating SNPs and CPGs); and (3) the significance of each model was assessed using the permutation-based MaxT method. We identified 48 genes whose expression levels were significantly associated with both SNPs and CPGs. Importantly, 36 (75%) of them were replicated in an independent data set (TCGA) and the performance of the proposed method was checked with a simulation study. We further support our results with a biological interpretation based on an enrichment analysis. The approach we propose allows reducing computational time and is flexible and easy to implement when analyzing several types of omics data. Our results highlight the importance of integrating omics data by applying appropriate statistical strategies to discover new insights into the complex genetic mechanisms involved in disease conditions.",2015,PLoS Genetics
Prognostic value of long non-coding RNA signatures in bladder cancer,"Bladder cancer (BLCA) is a devastating cancer whose early diagnosis can ensure better prognosis. Aim of this study was to evaluate the potential utility of lncRNAs in constructing lncRNA-based classifiers of BLCA prognosis and recurrence. Based on the data concerning BLCA retrieved from TCGA, lncRNA-based classifiers for OS and RFS were built using the least absolute shrinkage and selection operation (LASSO) Cox regression model in the training cohorts. More specifically, a 14-lncRNA-based classifier for OS and a 12-lncRNA-based classifier for RFS were constructed using the LASSO Cox regression. According to the prediction value, patients were divided into high/low-risk groups based on the cut-off of the median risk-score. The log-rank test showed significant differences in OS and RFS between low- and high-risk groups in the training, validation and whole cohorts. In the time-dependent ROC curve analysis, the AUCs for OS in the first, third, and fifth year were 0.734, 0.78, and 0.78 respectively, whereas the prediction capability of the 14-lncRNA classifier was superior to a previously published lncRNA classifier. As for the RFS, the AUCs in the first, third, and fifth year were 0.755, 0.715, and 0.740 respectively. In summary, the two-lncRNA-based classifiers could serve as novel and independent prognostic factors for OS and RFS individually.",2019,Aging (Albany NY)
REMAS: a new regression model to identify alternative splicing events from exon array data,"BackgroundAlternative splicing (AS) is an important regulatory mechanism for gene expression and protein diversity in eukaryotes. Previous studies have demonstrated that it can be causative for, or specific to splicing-related diseases. Understanding the regulation of AS will be helpful for diagnostic efforts and drug discoveries on those splicing-related diseases. As a novel exon-centric microarray platform, exon array enables a comprehensive analysis of AS by investigating the expression of known and predicted exons. Identifying of AS events from exon array has raised much attention, however, new and powerful algorithms for exon array data analysis are still absent till now.ResultsHere, we considered identifying of AS events in the framework of variable selection and developed a regression method for AS detection (REMAS). Firstly, features of alternatively spliced exons were scaled by reasonably defined variables. Secondly, we designed a hierarchical model which can represent gene structure and transcriptional influence to exons, and the lasso type penalties were introduced in calculation because of huge variable size. Thirdly, an iterative two-step algorithm was developed to select alternatively spliced genes and exons. To avoid negative effects introduced by small sample size, we ranked genes as parameters indicating their AS capabilities in an iterative manner. After that, both simulation and real data evaluation showed that REMAS could efficiently identify potential AS events, some of which had been validated by RT-PCR or supported by literature evidence.ConclusionAs a new lasso regression algorithm based on hierarchical model, REMAS has been demonstrated as a reliable and effective method to identify AS events from exon array data.",2009,BMC Bioinformatics
Decoding force from deep brain electrodes in Parkinsonian patients.,"Limitations of many Brain Machine Interface (BMI) systems using invasive electrodes include reliance on single neurons and decoding limited to kinematics only. This study investigates whether force-related information is present in the local field potential (LFP) recorded with deep brain electrodes using data from 14 patients with Parkinson's disease. A classifier based on logistic regression (LR) is developed to classify various force stages, using 10-fold cross validation. Least Absolute and Shrinkage Operator (Lasso) is then employed in order to identify the features with the most predictivity. The results show that force-related information is present in the LFP, and it is possible to distinguish between various force stages using certain frequency-domain (delta, beta, gamma) and time-domain (mobility) features in real-time.",2016,Conference proceedings : ... Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual Conference
Sharp thresholds for high-dimensional and noisy recovery of sparsity,"The problem of consistently estimating the sparsity pattern of a vector $\betastar \in \real^\mdim$ based on observations contaminated by noise arises in various contexts, including subset selection in regression, structure estimation in graphical models, sparse approximation, and signal denoising. We analyze the behavior of $\ell_1$-constrained quadratic programming (QP), also referred to as the Lasso, for recovering the sparsity pattern. Our main result is to establish a sharp relation between the problem dimension $\mdim$, the number $\spindex$ of non-zero elements in $\betastar$, and the number of observations $\numobs$ that are required for reliable recovery. For a broad class of Gaussian ensembles satisfying mutual incoherence conditions, we establish existence and compute explicit values of thresholds $\ThreshLow$ and $\ThreshUp$ with the following properties: for any $\epsilon > 0$, if $\numobs > 2 (\ThreshUp + \epsilon) \log (\mdim - \spindex) + \spindex + 1$, then the Lasso succeeds in recovering the sparsity pattern with probability converging to one for large problems, whereas for $\numobs < 2 (\ThreshLow - \epsilon) \log (\mdim - \spindex) + \spindex + 1$, then the probability of successful recovery converges to zero. For the special case of the uniform Gaussian ensemble, we show that $\ThreshLow = \ThreshUp = 1$, so that the threshold is sharp and exactly determined.",2006,ArXiv
Solar Flare Forecasting from Magnetic Feature Properties Generated by the Solar Monitor Active Region Tracker,"We study the predictive capabilities of magnetic-feature properties (MF) generated by the Solar Monitor Active Region Tracker (SMART: Higgins etÂ al. in Adv. Space Res.47, 2105, 2011) for solar-flare forecasting from two datasets: the full dataset of SMART detections from 1996 to 2010 which has been previously studied by Ahmed etÂ al. (Solar Phys.283, 157, 2013) and a subset of that dataset that only includes detections that are NOAA active regions (ARs). The main contributions of this work are: we use marginal relevance as a filter feature selection method to identify the most useful SMART MF properties for separating flaring from non-flaring detections and logistic regression to derive classification rules to predict future observations. For comparison, we employ a Random Forest, Support Vector Machine, and a set of Deep Neural Network models, as well as lasso for feature selection. Using the linear model with three features we obtain significantly better results (True Skill Score: TSS = 0.84) than those reported by Ahmed etÂ al. (Solar Phys.283, 157, 2013) for the full dataset of SMART detections. The same model produced competitive results (TSS = 0.67) for the dataset of SMART detections that are NOAA ARs, which can be compared to a broader section of flare-forecasting literature. We show that more complex models are not required for this data.",2018,Solar Physics
A Bayesian mixture of lasso regressions with t-errors,"The following article considers a mixture of regressions with variable selection problem. In many real-data scenarios, one is faced with data which possess outliers, skewness and, simultaneously, one would like to be able to construct clusters with specific predictors that are fairly sparse. A Bayesian mixture of lasso regressions with t-errors to reflect these specific demands is developed. The resulting model is necessarily complex and to fit the model to real data, a state-of-the-art Particle Markov chain Monte Carlo (PMCMC) algorithm based upon sequential Monte Carlo (SMC) methods is developed. The model and algorithm are investigated on both simulated and real data.",2014,Comput. Stat. Data Anal.
Uncertainty Quantification for Modern High-Dimensional Regression via Scalable Bayesian Methods,"ABSTRACTTremendous progress has been made in the last two decades in the area of high-dimensional regression, especially in the â€œlarge p, small nâ€ setting. Such sample starved settings inevitably lead to models which are potentially very unstable and hence quite unreliable. To this end, Bayesian shrinkage methods have generated a lot of recent interest in the modern high-dimensional regression and model selection context. Such methods span the wide spectrum of modern regression approaches and include among others, spike-and-slab priors, the Bayesian lasso, ridge regression, and global-local shrinkage priors such as the Horseshoe prior and the Dirichletâ€“Laplace prior. These methods naturally facilitate tractable uncertainty quantification and have thus been used extensively across diverse applications. A common unifying feature of these models is that the corresponding priors on the regression coefficients can be expressed as a scale mixture of normals. This property has been leveraged extensively to devel...",2019,Journal of Computational and Graphical Statistics
Shrinkage and absolute penalty estimation in linear regression models,"In predicting a response variable using multiple linear regression model, several candidate models may be available which are subsets of the full model. Shrinkage estimators borrow information from the full model and provides a hybrid estimate of the regression parameters by shrinking the full model estimates toward the candidate submodel. The process introduces bias in the estimation but reduces the overall prediction error that offsets the bias. In this article, we give an overview of shrinkage estimators and their asymptotic properties. A real data example is given and a Monte Carlo simulation study is carried out to evaluate the performance of shrinkage estimators compared to the absolute penalty estimators such as least absolute shrinkage and selection operator (LASSO), adaptive LASSO and smoothly clipped absolute deviation (SCAD) based on prediction errors criterion in a multiple linear regression setup. WIREs Comput Stat 2012, 4:541â€“553. DOI: 10.1002/wics.1232",2012,Wiley Interdisciplinary Reviews: Computational Statistics
Adaptive LASSO model selection in a multiphase quantile regression,"We propose a general adaptive LASSO method for a quantile regression model. Our method is very interesting when we know nothing about the first two moments of the model error. We first prove that the obtained estimators satisfy the oracle properties, which involves the relevant variable selection without using hypothesis test. Next, we study the proposed method when the (multiphase) model changes to unknown observations called change-points. Convergence rates of the change-points and of the regression parameter estimators in each phase are found. The sparsity of the adaptive LASSO quantile estimators of the regression parameters is not affected by the change-points estimation. If the number of phases is unknown, a consistent criterion is proposed. Numerical studies by Monte Carlo simulations show the performance of the proposed method, compared to other existing methods in the literature, for models with a single phase or for multiphase models. The adaptive LASSO quantile method performs better than known...",2016,Statistics
Enhancing the prediction of acute kidney injury risk after percutaneous coronary intervention using machine learning techniques: A retrospective cohort study,"BACKGROUND
The current acute kidney injury (AKI) risk prediction model for patients undergoing percutaneous coronary intervention (PCI) from the American College of Cardiology (ACC) National Cardiovascular Data Registry (NCDR) employed regression techniques. This study aimed to evaluate whether models using machine learning techniques could significantly improve AKI risk prediction after PCI.


METHODS AND FINDINGS
We used the same cohort and candidate variables used to develop the current NCDR CathPCI Registry AKI model, including 947,091 patients who underwent PCI procedures between June 1, 2009, and June 30, 2011. The mean age of these patients was 64.8 years, and 32.8% were women, with a total of 69,826 (7.4%) AKI events. We replicated the current AKI model as the baseline model and compared it with a series of new models. Temporal validation was performed using data from 970,869 patients undergoing PCIs between July 1, 2016, and March 31, 2017, with a mean age of 65.7 years; 31.9% were women, and 72,954 (7.5%) had AKI events. Each model was derived by implementing one of two strategies for preprocessing candidate variables (preselecting and transforming candidate variables or using all candidate variables in their original forms), one of three variable-selection methods (stepwise backward selection, lasso regularization, or permutation-based selection), and one of two methods to model the relationship between variables and outcome (logistic regression or gradient descent boosting). The cohort was divided into different training (70%) and test (30%) sets using 100 different random splits, and the performance of the models was evaluated internally in the test sets. The best model, according to the internal evaluation, was derived by using all available candidate variables in their original form, permutation-based variable selection, and gradient descent boosting. Compared with the baseline model that uses 11 variables, the best model used 13 variables and achieved a significantly better area under the receiver operating characteristic curve (AUC) of 0.752 (95% confidence interval [CI] 0.749-0.754) versus 0.711 (95% CI 0.708-0.714), a significantly better Brier score of 0.0617 (95% CI 0.0615-0.0618) versus 0.0636 (95% CI 0.0634-0.0638), and a better calibration slope of observed versus predicted rate of 1.008 (95% CI 0.988-1.028) versus 1.036 (95% CI 1.015-1.056). The best model also had a significantly wider predictive range (25.3% versus 21.6%, p < 0.001) and was more accurate in stratifying AKI risk for patients. Evaluated on a more contemporary CathPCI cohort (July 1, 2015-March 31, 2017), the best model consistently achieved significantly better performance than the baseline model in AUC (0.785 versus 0.753), Brier score (0.0610 versus 0.0627), calibration slope (1.003 versus 1.062), and predictive range (29.4% versus 26.2%). The current study does not address implementation for risk calculation at the point of care, and potential challenges include the availability and accessibility of the predictors.


CONCLUSIONS
Machine learning techniques and data-driven approaches resulted in improved prediction of AKI risk after PCI. The results support the potential of these techniques for improving risk prediction models and identification of patients who may benefit from risk-mitigation strategies.",2018,PLoS Medicine
"DNA methylation-driven genes for constructing diagnostic, prognostic, and recurrence models for hepatocellular carcinoma","In this study, we performed a comprehensively analysis of gene expression and DNA methylation data to establish diagnostic, prognostic, and recurrence models for hepatocellular carcinoma (HCC). Methods: We collected gene expression and DNA methylation datasets for over 1,200 clinical samples. Integrated analyses of RNA-sequencing and DNA methylation data were performed to identify DNA methylation-driven genes. These genes were utilized in univariate, least absolute shrinkage and selection operator (LASSO), and multivariate Cox regression analyses to build a prognostic model. Recurrence and diagnostic models for HCC were also constructed using the same genes. Results: A total of 123 DNA methylation-driven genes were identified. Two of these genes (SPP1 and LCAT) were chosen to construct the prognostic model. The high-risk group showed a markedly unfavorable prognosis compared to the low-risk group in both training (HR = 2.81; P < 0.001) and validation (HR = 3.06; P < 0.001) datasets. Multivariate Cox regression analysis indicated the prognostic model to be an independent predictor of prognosis (P < 0.05). Also, the recurrence model successfully distinguished the HCC recurrence rate between the high-risk and low-risk groups in both training (HR = 2.22; P < 0.001) and validation (HR = 2; P < 0.01) datasets. The two diagnostic models provided high accuracy for distinguishing HCC from normal samples and dysplastic nodules in the training and validation datasets, respectively. Conclusions: We identified and validated prognostic, recurrence, and diagnostic models that were constructed using two DNA methylation-driven genes in HCC. The results obtained by integrating multidimensional genomic data offer novel research directions for HCC biomarkers and new possibilities for individualized treatment of patients with HCC.",2019,Theranostics
The Study on Impact Factors of Foreign Direct Investment Based on Lasso,"Among the many factors affecting foreign direct investment,the market size,infrastructure conditions,tariffs,trade openness and labor productivity are the five main impact factors of foreign direct investment,and the influence of market size is far greater than the influence of other factors.In the meantime,Lasso method,least squares method and stepwise regression method are compared.From the results we can see that the Lasso method is better than the other two methods in terms of variable selection.",2014,Journal of Hunan University
Probability genotype imputation method and integrated weighted lasso for QTL identification,"BackgroundMany QTL studies have two common features: (1) often there is missing marker information, (2) among many markers involved in the biological process only a few are causal. In statistics, the second issue falls under the headings â€œsparsityâ€ and â€œcausal inferenceâ€. The goal of this work is to develop a two-step statistical methodology for QTL mapping for markers with binary genotypes. The first step introduces a novel imputation method for missing genotypes. Outcomes of the proposed imputation method are probabilities which serve as weights to the second step, namely in weighted lasso. The sparse phenotype inference is employed to select a set of predictive markers for the trait of interest.ResultsSimulation studies validate the proposed methodology under a wide range of realistic settings. Furthermore, the methodology outperforms alternative imputation and variable selection methods in such studies. The methodology was applied to an Arabidopsis experiment, containing 69 markers for 165 recombinant inbred lines of a F8 generation. The results confirm previously identified regions, however several new markers are also found. On the basis of the inferred ROC behavior these markers show good potential for being real, especially for the germination trait Gmax.ConclusionsOur imputation method shows higher accuracy in terms of sensitivity and specificity compared to alternative imputation method. Also, the proposed weighted lasso outperforms commonly practiced multiple regression as well as the traditional lasso and adaptive lasso with three weighting schemes. This means that under realistic missing data settings this methodology can be used for QTL identification.",2013,BMC Genetics
"Significant predictors of mathematical literacy for top-tiered countries/economies, Canada, and the United States on PISA 2012: Case for the sparse regression model.","BACKGROUND
National ranking from the triennial Programme of International Student Assessment (PISA) often serves as a barometer of national performance and human capital. Though excessive student- and school-level covariates (nÂ >Â 700) may prove intractable for traditional least-squares estimate procedures, shrinkage methods may be more suitable for subset selection.


AIMS
With a focus on the United States, this paper proposes sparse regression for PISA 2012 to discover salient student- and school-level predictor variables for mathematical literacy achievement.


SAMPLE
The sparse regression analysis was conducted on 10 top-tiered OECD countries/economies, Canada, and the United States in mathematical literacy on the 2012 PISA. Two- and three-level hierarchical regression analyses were performed on Canadian and US students (NÂ =Â 26,522) along with five of the ten top-tiered countries/economies (NÂ =Â 58,385).


METHODS
Using the 'least absolute shrinkage and selection operator' (LASSO) technique, the study (1) identified salient predictor variables of mathematical literacy performance for the top-tiered countries/economies, Canada, and the United States and (2) used these salient variables to perform two- and three-level hierarchical regression on data from Canada and the United States along with five top-tiered countries/economies. Weights and replicates were used to account for complex sample design. A weighted, two-level confirmatory factor analysis was performed to identify latent constructs. Missing data were handled through multiple imputation.


RESULTS
Separate two-level hierarchical models accounted for 32-35% student-level and 58-70% school-level variance in Canada and the United States, respectively; three-level models accounted for 33% of level-one variance, 62-65% level-two variance, and 13-44% of level-three variance for the US/Canada and US/Canada/top-tiered students, respectively. Following top-tiered countries/economies, Canadian students had high levels of self-efficacy, were more likely to encounter advanced concepts in class, were less activity/small group-centred, and were more likely to consider truancy a learning hindrance. Factor analyses revealed a positive relation with rigour and class organization (teacher-centred) for top-tiered countries and Canada, though not for the United States. For all countries, there was a strong relation between rigour and self-beliefs.


CONCLUSION
Compared to top performers, a less rigorous curriculum, coupled with class and school factors, may explain lag in US performance.",2018,The British journal of educational psychology
Regularized Laplacian Estimation and Fast Eigenvector Approximation,"Recently, Mahoney and Orecchia demonstrated that popular diffusion-based procedures to compute a quick approximation to the first nontrivial eigenvector of a data graph Laplacian exactly solve certain regularized Semi-Definite Programs (SDPs). In this paper, we extend that result by providing a statistical interpretation of their approximation procedure. Our interpretation will be analogous to the manner in which l2-regularized or l1-regularized l2-regression (often called Ridge regression and Lasso regression, respectively) can be interpreted in terms of a Gaussian prior or a Laplace prior, respectively, on the coefficient vector of the regression problem. Our framework will imply that the solutions to the Mahoney-Orecchia regularized SDP can be interpreted as regularized estimates of the pseudoinverse of the graph Laplacian. Conversely, it will imply that the solution to this regularized estimation problem can be computed very quickly by running, e.g., the fast diffusion-based PageRank procedure for computing an approximation to the first nontrivial eigenvector of the graph Laplacian. Empirical results are also provided to illustrate the manner in which approximate eigenvector computation implicitly performs statistical regularization, relative to running the corresponding exact algorithm.",2011,
Incorporating group correlations in genome-wide association studies using smoothed group Lasso.,"In genome-wide association studies, penalization is an important approach for identifying genetic markers associated with disease. Motivated by the fact that there exists natural grouping structure in single nucleotide polymorphisms and, more importantly, such groups are correlated, we propose a new penalization method for group variable selection which can properly accommodate the correlation between adjacent groups. This method is based on a combination of the group Lasso penalty and a quadratic penalty on the difference of regression coefficients of adjacent groups. The new method is referred to as smoothed group Lasso (SGL). It encourages group sparsity and smoothes regression coefficients for adjacent groups. Canonical correlations are applied to the weights between groups in the quadratic difference penalty. We first derive a GCD algorithm for computing the solution path with linear regression model. The SGL method is further extended to logistic regression for binary response. With the assistance of the majorize-minimization algorithm, the SGL penalized logistic regression turns out to be an iteratively penalized least-square problem. We also suggest conducting principal component analysis to reduce the dimensionality within groups. Simulation studies are used to evaluate the finite sample performance. Comparison with group Lasso shows that SGL is more effective in selecting true positives. Two datasets are analyzed using the SGL method.",2013,Biostatistics
Novel Immune-Related Gene Signature for Risk Stratification and Prognosis of Survival in Lower-Grade Glioma,"Objective Despite several clinicopathological factors being integrated as prognostic biomarkers, the individual variants and risk stratification have not been fully elucidated in lower grade glioma (LGG). With the prevalence of gene expression profiling in LGG, and based on the critical role of the immune microenvironment, the aim of our study was to develop an immune-related signature for risk stratification and prognosis prediction in LGG. Methods RNA-sequencing data from The Cancer Genome Atlas (TCGA), Genome Tissue Expression (GTEx), and Chinese Glioma Genome Atlas (CGGA) were used. Immune-related genes were obtained from the Immunology Database and Analysis Portal (ImmPort). Univariate, multivariate cox regression, and Lasso regression were employed to identify differentially expressed immune-related genes (DEGs) and establish the signature. A nomogram was constructed, and its performance was evaluated by Harrellâ€™s concordance index (C-index), receiver operating characteristic (ROC), and calibration curves. Relationships between the risk score and tumor-infiltrating immune cell abundances were evaluated using CIBERSORTx and TIMER. Results Noted, 277 immune-related DEGs were identified. Consecutively, 6 immune genes (CANX, HSPA1B, KLRC2, PSMC6, RFXAP, and TAP1) were identified as risk signature and Kaplanâ€“Meier curve, ROC curve, and risk plot verified its performance in TCGA and CGGA datasets. Univariate and multivariate Cox regression indicated that the risk group was an independent predictor in primary LGG. The prognostic signature showed fair accuracy for 3- and 5-year overall survival in both internal (TCGA) and external (CGGA) validation cohorts. However, predictive performance was poor in the recurrent LGG cohort. The CIBERSORTx algorithm revealed that naÃ¯ve CD4+ T cells were significant higher in low-risk group. Conversely, the infiltration levels of M1-type macrophages, M2-type macrophages, and CD8+T cells were significant higher in high-risk group in both TCGA and CGGA cohorts. Conclusion The present study constructed a robust six immune-related gene signature and established a prognostic nomogram effective in risk stratification and prediction of overall survival in primary LGG.",2020,Frontiers in Genetics
Size matters: network inference tackles the genome scale,"The growing importance of microarray data challenges biologists, and especially the systems biology community, to come up with genome-scale analysis methods that can convert the large quantity of available high-throughput data into high-quality systems-level insights. One area of systems-level analysis that has received considerable attention in recent years is that of inferring molecular-level regulation, with frequent focus on transcriptional regulatory networks (Kholodenko et al, 1997; Tavazoie et al, 1999; Gardner et al, 2003; Segal et al, 2003; Beer and Tavazoie, 2004; Yu et al, 2004; di Bernardo et al, 2005; Gardner and Faith, 2005; Woolf et al, 2005; Margolin et al, 2006; Faith et al, 2007). As microarrays provide a tool for measuring transcript levels of the whole genome, recent interest has shifted to inferring networks on a genome scale. The less-studied organisms are a natural starting point for such mapping, as it is for these organisms that the rapid, genome-scale identification of regulatory structure is most needed. 
 
In a recent study, Bonneau et al (2006) apply the Inferelator, their elegant new algorithm, for inferring gene networks, to precisely such a little-studied but important organism. Specifically, the authors focus on Halobacterium NRC-I, a model archaeon (DasSarma et al, 2006), to show that, at least for a small genome, it is possible to determine a sizeable portion of the transcriptional regulatory network from microarrays without much prior knowledge. This choice of an organism has two practical advantages. First, the salt-loving NRC-I is one of a handful of Halobacteria for which transformation techniques have been well studied, allowing in vivo validation of network predictions. Second, NRC-I's genome is relatively small and thus, its regulation ought to be comparatively easy to reconstruct. Small genome or not, putting high-throughput profiling technologies to work on the genome scale requires a confluence of robust algorithms, biologically plausible simplifying assumptions, and a robust verification strategy. The work of Bonneau et al (2006) is a good example, using multiple tools in the bioinformatics toolbox to build a credible blueprint of a transcriptional-regulatory network involving thousands of genes and more than 100 transcription factors. 
 
In order to appreciate the need for a well-structured approach to regulatory mapping, consider the mathematical and biological scope of this cross-disciplinary problem. The tiny archaeon Halobacterium NRC-I contains about 2400 genes. For each one of these, the goal is to understand the transcriptional regulatory apparatusâ€”that is about 2400 question marks, each with thousands of possible answers in the form of a set of transcriptional regulators. Put that against a typical compendium size of several hundred chips for a given organism, and you get what is known as a â€˜small n, large p' problem, where the number of possible parameters (regulators), p, dwarfs the number of data points (microarrays), n, available to define them. This problem gets considerably worse for complex organisms, where a larger number of available microarrays are more than offset by the vast complexity of large genomes, alternate splice variants, and multiple layers of regulation. For network inference algorithms, â€˜small n, large p' means dearth of data and very high computational demands. 
 
As if this computational complexity were not bad enough, there is the inherent high dimensionality in the biological realm. Regulation happens in the domains of mRNA, proteins, metabolites, kinases, acetylases, and so on, and through a variety of pleiotropic perturbations and influences, such as salinity, temperature, and cell-wall permeability. As the best high-throughput data capture only mRNA, one must make simplifying assumptions and skip many important parameters. Bonneau and colleagues' best simplifying assumption is to focus on predicting the targets of transcription factors in the network, along with some key environmental influences. When only transcription factors are allowed to regulate other genes, the â€˜p' in the â€˜small n, large p' problem is no longer so big. In fact, at 120, it is smaller than the number of chips (268) used in this study. 
 
To further constrain the network learning problem, the Inferelator performs a pre-processing step of bi-clusteringâ€”organizing experimental data by both genes and conditions. This algorithm, the cMonkey (Reiss et al, 2006), allows further reduction of dimensionality by collapsing genes into conditionally coexpressed modules. cMonkey identified 300 such bi-clusters, and 159 individual genes that could not be grouped, a nearly six-fold reduction in dimensionality. Crucially, as the composition of the culture medium used for the microarray-profiled experiments is known, each bi-cluster's grouping of genes by experimental condition suggests plausible metabolic or environmental effectors of regulation. The authors exploit this benefit of their approach in one of their verifying experiments. Bi-clustering, therefore, serves two ends: it limits the number of genes, and thus variables to reconstruct, to fewer than 500 (including only 80 TFs and metabolites), and places each predicted regulatory interaction into an experiment-specific context. 
 
The problem now becomes mathematically well-posed, and the authors solve it using LASSO regression, a sparse regression method designed just for such computationally difficult problems (Tibshirani, 1996). LASSO works by selecting a small set of the most likely regulators of a given gene, and simultaneously determines a quantitative influence function relating regulator expression to target expression (Figure 1). In addition, the authors extend the LASSO algorithm beyond its typical linear domain by including piecewise and nonlinear terms in the regression to model saturation effects and pairwise combinatorial regulation. With this approach, the authors construct a model of transcription regulation in Halobacterium that matches 80 transcription factors to 500 predicted gene targets and captures the putative metabolic controllers of these pathways. This is an impressive result, both in size and regulatory complexity, particularly in light of the relatively modest size of the experimental data set (i.e., 268 microarrays). Moreover, this represents a dramatic leap in our understanding of this little-studied organism. 
 
 
 
Figure 1 
 
(A) Schematic diagram of a hypothetical bacterial operon, represented by a single gene Y, which is regulated by a protein X1 and a protein complex X2X3. (B) Within its dynamic range, the level of the transcript y may be modeled as a function of transcripts ... 
 
 
 
Having obtained the first-pass transcriptional blueprint, Bonneau and colleagues ask the obligatory next question: how much do we trust this network? In network inference, three broad types of verification are possible: computational verification through cross-validation, in vivo verification, and literature-driven curation. To be effective, the last approach should leverage a large data set documenting connectivity known in the literature, such as TransFac (Matys et al, 2003) or RegulonDB (Salgado et al, 2006). This type of verification not being available for Halobacterium, the authors vigorously pursue the former two, including knockout experimentation and ChIP-chip analysis, demonstrating that their network can serve as a reliable and useful blueprint of Halobacterium NRC-I's transcriptional regulation. 
 
Bonneau et al (2006) show the feasibility of mapping a genome-scale regulatory network from a modestly sized compendium of microarrays, an important success for the systems biology community. As microarray technology continues to improve and costs drop, growing databases of microarrays present an opportunity to infer ever more complex regulatory networks in both microbes and higher organisms. Abundance of data fuels the need for a network inference case study that would clearly map the boundaries of what is possible with today's network mapping algorithms. To this end, we believe that the once and future model organisms like Escherichia coli and Saccharomyces cerevisiae, buoyed by extensive bodies of literature and large databases such as RegulonDB, SGD (Christie et al, 2004), and TransFac, may represent attractive short-term targets for network inference studies. In addition to the use of curated data sets, it may be possible to seed organisms with small synthetic in vivo networks, the connectivity of which is known by design, and to measure the success of network reconstruction on the whole by success or failure to reconstruct the seed. We are aware of at least one lab doing such work (Cantone et al, 2006). Biological yardsticks in general will gain in importance, as they supplement in silico testing and usher in algorithms' transition from design to practical use, and from simple organisms to higher eukaryotes. 
 
Challenges remain, but we see the immediate future of network inference as promising and bright. Molecular biologists have long been looking for ways to generate more oomph from their microarrays. Systems biology may have some answers, and we laud Bonneau and colleagues for providing an illuminating step in that direction.",2007,Molecular Systems Biology
