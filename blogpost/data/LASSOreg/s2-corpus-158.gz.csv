title,abstract,year,journal
A novel DNA damage response signature of IDH-mutant grade II and grade III astrocytoma at transcriptional level,"The WHO classification for IDH-mutant grade II and grade III astrocytoma may not be as prognostically meaningful as expected. We aimed to develop a novel classification system based on the DNA damage response signature. We developed the gene signature of DNA damage response with 115 samples from The Cancer Genome Atlas (TCGA) database. The dataset from Chinese Glioma Genome Atlas (CGGA) database with 41 samples was used as the validation set. Lasso Cox regression model was applied for selection of the best signature. Gene set enrichment analysis (GSEA) and gene ontology (GO) analysis were implemented to reveal its biological phenotype. A two-gene DNA damage response signature (RAD18, MSH2) was developed using the lasso Cox regression model based on the TCGA dataset. Its prognostic efficiency was validated in the CGGA cohort. The result of Cox regression analysis showed that the signature has a better predictive accuracy than the WHO grade. The risk score was an independent prognostic factor for the overall survival of the IDH-mutant grade II and grade III astrocytoma. GSEA and GO analysis confirmed enhanced processes related to DNA damage response in high-risk group. We developed a two-gene signature which can effectively predict the prognosis of patients with IDH-mutant grade II and grade III astrocytoma. It suggests a novel classification of astrocytoma with better prognostic accuracy based on the expression of DNA damage response genes.",2020,Journal of Cancer Research and Clinical Oncology
Adaptive Estimation of Multivariate Regression with Hidden Variables.,"This paper studies the estimation of the coefficient matrix $\Ttheta$ in multivariate regression with hidden variables, $Y = (\Ttheta)^TX + (B^*)^TZ + E$, where $Y$ is a $m$-dimensional response vector, $X$ is a $p$-dimensional vector of observable features, $Z$ represents a $K$-dimensional vector of unobserved hidden variables, possibly correlated with $X$, and $E$ is an independent error. The number of hidden variables $K$ is unknown and both $m$ and $p$ are allowed but not required to grow with the sample size $n$. Since only $Y$ and $X$ are observable, we provide necessary conditions for the identifiability of $\Ttheta$. The same set of conditions are shown to be sufficient when the error $E$ is homoscedastic. Our identifiability proof is constructive and leads to a novel and computationally efficient estimation algorithm, called HIVE. The first step of the algorithm is to estimate the best linear prediction of $Y$ given $X$ in which the unknown coefficient matrix exhibits an additive decomposition of $\Ttheta$ and a dense matrix originated from the correlation between $X$ and the hidden variable $Z$. Under the row sparsity assumption on $\Ttheta$, we propose to minimize a penalized least squares loss by regularizing $\Ttheta$ via a group-lasso penalty and regularizing the dense matrix via a multivariate ridge penalty. Non-asymptotic deviation bounds of the in-sample prediction error are established. Our second step is to estimate the row space of $B^*$ by leveraging the covariance structure of the residual vector from the first step. In the last step, we remove the effect of hidden variable by projecting $Y$ onto the complement of the estimated row space of $B^*$. Non-asymptotic error bounds of our final estimator are established. The model identifiability, parameter estimation and statistical guarantees are further extended to the setting with heteroscedastic errors.",2020,arXiv: Statistics Theory
Variable selection in rank regression for analyzing longitudinal data,"In this paper, we consider variable selection in rank regression models for longitudinal data. To obtain both robustness and effective selection of important covariates, we propose incorporating shrinkage by adaptive lasso or SCAD in the Wilcoxon dispersion function and establishing the oracle properties of the new method. The new method can be conveniently implemented with the statistical software R. The performance of the proposed method is demonstrated via simulation studies. Finally, two datasets are analyzed for illustration. Some interesting findings are reported and discussed.",2018,Statistical Methods in Medical Research
Evaluation of machine learning algorithms and structural features for optimal MRI-based diagnostic prediction in psychosis,"A relatively large number of studies have investigated the power of structural magnetic resonance imaging (sMRI) data to discriminate patients with schizophrenia from healthy controls. However, very few of them have also included patients with bipolar disorder, allowing the clinically relevant discrimination between both psychotic diagnostics. To assess the efficacy of sMRI data for diagnostic prediction in psychosis we objectively evaluated the discriminative power of a wide range of commonly used machine learning algorithms (ridge, lasso, elastic net and L0 norm regularized logistic regressions, a support vector classifier, regularized discriminant analysis, random forests and a Gaussian process classifier) on main sMRI features including grey and white matter voxel-based morphometry (VBM), vertex-based cortical thickness and volume, region of interest volumetric measures and wavelet-based morphometry (WBM) maps. All possible combinations of algorithms and data features were considered in pairwise classifications of matched samples of healthy controls (N = 127), patients with schizophrenia (N = 128) and patients with bipolar disorder (N = 128). Results show that the selection of feature type is important, with grey matter VBM (without data reduction) delivering the best diagnostic prediction rates (averaging over classifiers: schizophrenia vs. healthy 75%, bipolar disorder vs. healthy 63% and schizophrenia vs. bipolar disorder 62%) whereas algorithms usually yielded very similar results. Indeed, those grey matter VBM accuracy rates were not even improved by combining all feature types in a single prediction model. Further multi-class classifications considering the three groups simultaneously made evident a lack of predictive power for the bipolar group, probably due to its intermediate anatomical features, located between those observed in healthy controls and those found in patients with schizophrenia. Finally, we provide MRIPredict (https://www.nitrc.org/projects/mripredict/), a free tool for SPM, FSL and R, to easily carry out voxelwise predictions based on VBM images.",2017,PLoS ONE
On the Q-linear Convergence of a Majorized Proximal ADMM for Convex Composite Programming and Its Applications to Regularized Logistic Regression,"This paper aims to study the convergence rate of a majorized alternating direction method of multiplier with indefinite proximal terms (iPADMM) for solving linearly constrained convex composite optimization problems. We establish the Q-linear rate convergence theorem for 2-block majorized iPADMM under mild conditions. Based on this result, the convergence rate analysis of symmetric Gaussian-Seidel based majorized ADMM, which is designed for solving multi-block composite convex optimization problems, are given. We apply the majorized iPADMM to solve three types of regularized logistic regression problems: constrained regression, fused lasso and overlapping group lasso. The efficiency of majorized iPADMM are demonstrated on both simulation experiments and real data sets.",2017,
Comparison of Regularized Regression Methods for ~Omics Data,"Background: In this study, we compare methods that can be used to relate a phenotypic trait of interest to an ~omics data set, where the number or variables outnumbers by far the number of samples. Methods: We apply univariate regression and different regularized multiple regression methods: ridge regression (RR), LASSO, elastic net (EN), principal components regression (PCR), partial least squares regression (PLS), sparse partial least squares regression (SPLS), support vector regression (SVR) and random forest regression (RF). These regression methods were applied to a data set from a potato mapping population, where we predict potato flesh colour from a metabolomics data set. Results: We compare the methods in terms of the mean square error of prediction of the trait, goodness of fit of the models, and the selection and ranking of the metabolites. In terms of the prediction error, elastic net performed better than the other methods. Different numbers of variables are selected by the methods that allow variable selection but seven variables were in common between LASSO, EN and SPLS. SPLS performed better than EN with respect to the selection of grouped correlated variables. Conclusions: Four out of these seven variables selected by LASSO, EN, SPLS were putatively identified as carotenoid derived compounds; since the carotenoid pathway is important for flesh colour of potato, this indicates that meaningful compounds are selected. We developed a web application that can perform all the described methods, and that includes a double cross validation for optimization of the methods and for proper estimation of the prediction error.",2012,Metabolomics
Nonparametric regression using needlet kernels for spherical data,"Needlets have been recognized as state-of-the-art tools to tackle spherical data, due to their excellent localization properties in both spacial and frequency domains. 
This paper considers developing kernel methods associated with the needlet kernel for nonparametric regression problems whose predictor variables are defined on a sphere. Due to the localization property in the frequency domain, we prove that the regularization parameter of the kernel ridge regression associated with the needlet kernel can decrease arbitrarily fast. A natural consequence is that the regularization term for the kernel ridge regression is not necessary in the sense of rate optimality. Based on the excellent localization property in the spacial domain further, we also prove that all the $l^{q}$ $(01\leq q < \infty)$ kernel regularization estimates associated with the needlet kernel, including the kernel lasso estimate and the kernel bridge estimate, possess almost the same generalization capability for a large range of regularization parameters in the sense of rate optimality. 
This finding tentatively reveals that, if the needlet kernel is utilized, then the choice of $q$ might not have a strong impact in terms of the generalization capability in some modeling contexts. From this perspective, $q$ can be arbitrarily specified, or specified merely by other no generalization criteria like smoothness, computational complexity, sparsity, etc..",2019,ArXiv
Identification of recurrence marker associated with immune infiltration in prostate cancer with radical resection and build prognostic nomogram,"BACKGROUND
Some historic breakthroughs have been made in immunotherapy of advanced cancer. However, there is still little research on immunotherapy in prostate cancer. We explored the relationship between immune cell infiltration and prostate cancer recurrence and tried to provide new ideas for the treatment of prostate cancer.


METHODS
Prostate cancer RNA-seq data and clinical information were downloaded from the TCGA database and GEO database. The infiltration of 24 immune cells in tissues was quantified by ssGSEA. Univariate Cox regression analysis was used to screen for immune cell types associated with tumor recurrence, weighted gene co-expression network analysis (WGCNA) and LASSO were used to identify hub genes which regulate prognosis in patients through immune infiltration. Then, the nomogram was constructed based on the hub gene to predict the recurrence of prostate cancer, and the decision curve analysis (DCA) was used to compare the accuracy with the PSA and Gleason prediction models.


RESULT
Analysis showed that Th2 cells and Tcm related to prostate cancer recurrence after radical prostatectomy, and they are independent protective factors for recurrence. Through WGCNA and Lasso, we identified that NDUFA13, UQCR11, and USP34 involved in the infiltration of Th2 cells and Tcm in tumor tissues, and the expression of genes is related to the recurrence of patients. Based on the above findings, we constructed a clinical prediction model and mapped a nomogram, which has better sensitivity and specificity for prostate cancer recurrence prediction, and performed better in comparison with PSA and Gleason's predictions.


CONCLUSION
The immune cells Th2 cells and Tcm are associated with recurrence of PCa. Moreover, the genes NDUFA13, UQCR11, and USP34 may affect the recurrence of PCa by affecting the infiltration of Th2 cells and Tcm. Moreover, nomogram can make prediction effectively.",2019,BMC Cancer
Tell Me What You Like and I'll Tell You What You Are: Discriminating Visual Preferences on Flickr Data,"The John Ruskin's 19th century adage suggests that personal taste is not merely an absolute set of aesthetic principles valid for everyone: actually, it is a process of interpretation which have also roots in one's life experiences. This aspect represents nowadays a major problem for inferring automatically the quality of a picture. In this paper, instead of trying to solve this age-old problem, we consider an intriguing, orthogonal direction, aimed at discovering how different are the personal tastes. Given a set of preferred images of a user, obtained from Flickr, we extract a pool of low- and high-level features; LASSO regression is then exploited to learn the most discriminative ones, considering a group of 200 random Flickr users. Such aspects can be easily recovered, allowing to understand what is the ""what we like"" which distinguish us from the others. We then perform multi-class classification, where a test sample is a set of preferred pictures of an unknown user, and the classes are all the users. The results are surprising: given only 1 image as test, we can match the user preferences definitely more than the chance, and with 20 images we reach an nAUC of 91%, considering the cumulative matching characteristic curve. Extensive experiments promote our approach, suggesting new intriguing perspectives in the study of computational aesthetics.",2012,
Confidence Sets Based on the Lasso Estimator,"In a linear regression model with fixed dimension, we construct confidence sets for the unknown parameter vector based on the Lasso estimator in finite samples as well as in an asymptotic setup, thereby quantifying estimation uncertainty of this estimator. In finite samples with Gaussian errors and asymptotically in the case where the Lasso estimator is tuned to perform conservative model-selection, we derive formulas for computing the minimal coverage probability over the entire parameter space for a large class of shapes for the confidence sets, thus enabling the construction of valid confidence sets based on the Lasso estimator in these settings. The choice of shape for the confidence sets and comparison with the confidence ellipse based on the least-squares estimator is also discussed. Moreover, in the case where the Lasso estimator is tuned to enable consistent model-selection, we give a simple confidence set with minimal coverage probability converging to one.",2015,arXiv: Statistics Theory
ë‹¤ì¤‘ì„ í˜•íšŒê·€ëª¨í˜•ì—ì„œì˜ ë³€ìˆ˜ì„ íƒê¸°ë²• í‰ê°€,"The purpose of variable selection techniques is to select a subset of relevant variables for a particular learning algorithm in order to improve the accuracy of prediction model and improve the efficiency of the model. We conduct an empirical analysis to evaluate and compare seven well-known variable selection techniques for multiple linear regression model, which is one of the most commonly used regression model in practice. The variable selection techniques we apply are forward selection, backward elimination, stepwise selection, genetic algorithm (GA), ridge regression, lasso (Least Absolute Shrinkage and Selection Operator) and elastic net. Based on the experiment with 49 regression data sets, it is found that GA resulted in the lowest error rates while lasso most significantly reduces the number of variables. In terms of computational efficiency, forward/backward elimination and lasso requires less time than the other techniques.",2016,
A landscape-level analysis of yellow-cedar decline in coastal British Columbia,"Yellow-cedar (Chamaecyparis nootkatensis D. Don (Spach)) is currently undergoing a dramatic decline in western North America. Recent research suggests that site factors combined with a shift in climate have predisposed yellow-cedar trees to decline. We conducted the first landscape-level analysis of the decline in coastal British Columbia to assess relations between the decline and topographic variables. We used lasso-penalized logistic regression to model yellow-cedar decline presence and absence with topographic variables derived from a digital elevation model. Model results indicated that low el- evation sites close to the coast, which are more exposed and have more variation in elevation, are more likely to show evi- dence of decline. The logistic model fit the data well (Nagelkerke R 2 = 0.846) and had high predictive accuracy (AUC = 0.98). The topographic variables identified by the model influence degree of soil saturation, temperatures, and snowpack presence in a forest stand, supporting the proposed associations in the current decline hypothesis. The analysis also high- lighted the utility of the lasso logistic model for selecting significant variables and mapping areas at high risk for decline. Knowledge of the determinants of the spatial pattern of decline will improve predictability and provide critical information for conservation and management of yellow-cedar. Resume : Un deperissement spectaculaire frappe presentement le faux-cypres de Nootka (Chamaecyparis nootkatensis D. Don (Spach)) dans l'ouest de l'Amerique du Nord. Des travaux de recherche recents indiquent que des facteurs station- nels combines a une modification du climat auraient predispose le faux-cypres de Nootka au deperissement. Nous avons rea- lise la premiere analyse du deperissement a l'echelle du paysage dans la region cotiere de la Colombie-Britannique pour evaluer les relations entre le deperissement et les variables topographiques. Nous avons utilise la regression logistique pena- lisee de type Lasso pour modeliser la presence et l'absence de deperissement du faux-cypres de Nootka en fonction des va- riables topographiques derivees d'un modele digital d'altitude. Les resultats du modele indiquent que les stations situees a faible altitude pres de la cote, qui sont plus exposees et dont l'altitude est plus variable, ont plus de chance de montrer des signes de deperissement. Le modele logistique epouse bien les donnees (R 2 de Nagelkerke = 0,846) et genere des predic- tions d'une grande precision (ASC = 0,98). Les variables topographiques identifiees par le modele influencent le degre de saturation du sol, la temperature ainsi que la presence de la couverture nivale dans un peuplement forestier; ce qui supporte l'hypothese actuelle au sujet de la combinaison de facteurs associes au deperissement. L'analyse a egalement fait ressortir l'utilite du modele logistique de type Lasso pour choisir les variables importantes et cartographier les zones ou le risque de deperissement est eleve. La connaissance des facteurs qui determinent le patron spatial du deperissement va ameliorer la previsibilite et fournir une information cruciale pour la conservation et l'amenagement du faux-cypres de Nootka. (Traduit par la Redaction)",2011,Canadian Journal of Forest Research
Sparse Modeling-Based Sequential Ensemble Learning for Effective Outlier Detection in High-Dimensional Numeric Data,"The large proportion of irrelevant or noisy features in reallife high-dimensional data presents a significant challenge to subspace/feature selection-based high-dimensional outlier detection (a.k.a. outlier scoring) methods. These methods often perform the two dependent tasks: relevant feature subset search and outlier scoring independently, consequently retaining features/subspaces irrelevant to the scoring method and downgrading the detection performance. This paper introduces a novel sequential ensemble-based framework SEMSE and its instance CINFO to address this issue. SEMSE learns the sequential ensembles to mutually refine feature selection and outlier scoring by iterative sparse modeling with outlier scores as the pseudo target feature. CINFO instantiates SEMSE by using three successive recurrent components to build such sequential ensembles. Given outlier scores output by an existing outlier scoring method on a feature subset, CINFO first defines a Cantelliâ€™s inequality-based outlier thresholding function to select outlier candidates with a false positive upper bound. It then performs lasso-based sparse regression by treating the outlier scores as the target feature and the original features as predictors on the outlier candidate set to obtain a feature subset that is tailored for the outlier scoring method. Our experiments show that two different outlier scoring methods enabled by CINFO (i) perform significantly better on 11 real-life high-dimensional data sets, and (ii) have much better resilience to noisy features, compared to their bare versions and three state-of-theart competitors. The source code of CINFO is available at https://sites.google.com/site/gspangsite/sourcecode.",2018,
Fast and Effective Approximations for Summarization and Categorization of Very Large Text Corpora,"Author(s): Godbehere, Andrew B. | Advisor(s): El Ghaoui, Laurent | Abstract: Given the overwhelming quantities of data generated every day, there is a pressing need for tools that can extract valuable and timely information. Vast reams of text data are now published daily, containing information of interest to those in social science, marketing, finance, and public policy, to name a few. Consider the case of the micro-blogging website Twitter, which in May 2013 was estimated to contain 58 million messages per day: in a single day, Twitter generates a greater volume of words than the Encyclopedia Brittanica. The magnitude of the data being analyzed, even over short time-spans, is out of reach of unassisted human comprehension. This thesis explores scalable computational methodologies that can assist human analysts and researchers in understanding very large text corpora. Existing methods for sparse and interpretable text classification, regression, and topic modeling, such as the Lasso, Sparse PCA, and probabilistic Latent Semantic Indexing, provide the foundation for this work. While these methods are either linear algebraic or probabilistic in nature, this thesis contributes a hybrid approach wherein simple probability models provide dramatic dimensionality reduction to linear algebraic problems, resulting in computationally efficient solutions suitable for real-time human interaction. Specifically, minimizing the probability of large deviations of a linear regression model while assuming a $k$-class probabilistic text model yields a $k$-dimensional optimization problem, where $k$ can be much smaller than either the number of documents or features. Further, a simple non-negativity constraint on the problem yields a sparse result without the need of an $\ell_1$ regularization. The problem is also considered and analyzed in the case of uncertainty in the model parameters. Towards the problem of estimating such probabilistic text models, a fast implementation of Sparse Principal Component Analysis is investigated and compared with Latent Dirichlet Allocation. Methods of fitting topic models to a dataset are discussed. Specific examples on a variety of text datasets are provided to demonstrate the efficacy of the proposed methods.",2015,
Network Elastic Net for Identifying Smoking specific gene expression for lung cancer,"Survival month for non-small lung cancer patients depend upon which stage of lung cancer is present. Our aim is to identify smoking specific gene expression biomarkers in prognosis of lung cancer patients. In this paper, we introduce the network elastic net, a generalization of network lasso that allows for simultaneous clustering and regression on graphs. In network elastic net, we consider similar patients based on smoking cigarettes per year to form the network. We then further find the suitable cluster among patients based on coefficients of genes having different survival month structures and showed the efficacy of the clusters using stage enrichment. This can be used to identify the stage of cancer using gene expression and smoking behavior of patients without doing any tests.",2019,2019 New York Scientific Data Summit (NYSDS)
Modified Cross-Validation for Penalized High-Dimensional Linear Regression Models,"In this article, for Lasso penalized linear regression models in high-dimensional settings, we propose a modified cross-validation (CV) method for selecting the penalty parameter. The methodology is extended to other penalties, such as Elastic Net. We conduct extensive simulation studies and real data analysis to compare the performance of the modified CV method with other methods. It is shown that the popular K-fold CV method includes many noise variables in the selected model, while the modified CV works well in a wide range of coefficient and correlation settings. Supplementary materials containing the computer code are available online.",2013,Journal of Computational and Graphical Statistics
Pretreatment risk management of a novel nomogram model for prediction of thoracoabdominal extrahepatic metastasis in primary hepatic carcinoma,"BackgroundExtrahepatic metastasis is the independent risk factor of poor survival of primary hepatic carcinoma (PHC), and most occurs in the chest and abdomen. Currently, there is still no available method to predict thoracoabdominal extrahepatic metastasis in PHC. In this study, a novel nomogram model was developed and validated for prediction of thoracoabdominal extrahepatic metastasis in PHC, thereby conducted individualized risk management for pretreatment different risk population.MethodsThe nomogram model was developed in a primary study that consisted of 330 consecutive pretreatment patients with PHC. Large-scale datasets were extracted from clinical practice. The nomogram was based on the predictors optimized by data dimension reduction through Lasso regression. The prediction performance was measured by the area under the receiver operating characteristic (AUROC), and calibrated to decrease the overfit bias. Individualized risk management was conducted by weighing the net benefit of different risk population via decision curve analysis. The prediction performance was internally and independently validated, respectively. An independent-validation study using a separate set of 107 consecutive patients.ResultsFour predictors from 55 high-dimensional clinical datasets, including size, portal vein tumor thrombus, infection, and carbohydrate antigen 125, were incorporated to develop a nomogram model. The nomogram demonstrated valuable prediction performance with AUROC of 0.830 (0.803 in internal-validation, and 0.773 in independent-validation, respectively), and fine calibration. Individual risk probability was visually scored. Weighing the net benefit, threshold probability was classified for three-independent risk population, which wasâ€‰<â€‰19.9%, 19.9â€“71.8% andâ€‰>â€‰71.8%, respectively. According to this classification, pretreatment risk management was based on a treatment-flowchart for individualized clinical decision-making.ConclusionsThe proposed nomogram is a useful tool for pretreatment risk management of thoracoabdominal extrahepatic metastasis in PHC for the first time, and may handily facilitate timely individualized clinical decision-making for different risk population.",2019,Journal of Translational Medicine
Essays on Structural Microeconometrics,"Author(s): Su, Jiun-Hua | Advisor(s): Powell, James | Abstract: This dissertation consists of three chapters studying microeconometric methods. The first two chapters focus on models with unobserved heterogeneity, and topics include testing shape restrictions imposed by economic theory and estimating counterfactual policy effects in duration analysis. In the last chapter, predictive methods in machine learning are adapted to study model selection within the framework of utility-maximizing binary decision-making. These proposed methods are described in greater detail below.Causal inference on the individual treatment effect is fundamental in econometric analysis. In Chapter 1, I develop the concept of structural monotonicity, that is, monotonicity of a structural function in a treatment given any observable covariates and unobserved heterogeneity. Different from regression monotonicity, in which heterogeneous factors average out, structural monotonicity emphasizes the sign of ceteris paribus individual treatment effect. Since economic theory may neither detail enough potential heterogeneous factors nor elaborate on parametric structures, I consider a two-period panel data model with nonseparable time-invariant heterogeneity, and avoid imposing restrictions on the dimensionality of heterogeneity and functional form of the structural function. Structural monotonicity in this setup implies shape constraints on the joint cumulative distribution function (CDF) of outcome variables conditional on the observable treatments and covariates over some regions. These regions are parameterized by a nuisance parameter, which can be consistently estimated. According to the shape constraints on the conditional joint CDF over the estimated regions, I propose a test for structural monotonicity and validate the empirical bootstrap method. Some Monte Carlo experiments show that the proposed test can detect departures from structural monotonicity, which are not revealed by some existing tests for regression monotonicity.The presence of unobserved heterogeneity is also essential for policy effects especially in duration analysis. In Chapter 2, I propose a counterfactual Kaplan-Meier estimator that incorporates time-invariant exogenous covariates and nonseparable heterogeneity in duration models with random censoring. The over-parameterization in traditional duration analysis can be avoided because distributional features of unobserved heterogeneity are unspecified. I establish the joint weak convergence of the proposed counterfactual Kaplan-Meier estimator and the traditional Kaplan-Meier estimator under some regularity conditions. Therefore, by comparing the estimated counterfactual and original unconditional distribution of the duration variable, we can evaluate the policy effects, for example the change of duration dependence in response to an exogenous manipulation of covariates.In addition to counterfactual analysis in policy research, a better prediction may improve policy-making. In Chapter 3, I show that in a model of binary decision-making based on the prediction of a binary outcome variable, the semiparametric maximum utility estimation can be viewed as cost-sensitive binary classification. Its in-sample overfitting issue is thus similar to that of perceptron learning in the machine learning literature. To alleviate the in-sample overfitting, I apply techniques in structural risk minimization to construct a utility-maximizing prediction rule. This proposed prediction rule, in comparison to the common machine learning Lasso-logit predictor, has larger relative expected utility in some simulation results when the conditional probability of the binary outcome is misspecified. The results show that a better prediction arising from the combination of machine learning techniques and economic theory can improve policy-making.",2018,
A Generally Efficient Targeted Minimum Loss Based Estimator,"Suppose we observe n independent and identically distributed observations of a finite dimensional bounded random variable. This article is concerned with the construction of an efficient targeted minimum loss-based estimator (TMLE) of a pathwise differentiable target parameter based on a realistic statistical model. The canonical gradient of the target parameter at a particular data distribution will depend on the data distribution through an infinite dimensional nuisance parameter which can be defined as the minimizer of the expectation of a loss function (e.g., log-likelihood loss). For many models and target parameters the nuisance parameter can be split up in two components, one required for evaluation of the target parameter and one real nuisance parameter. The only smoothness condition we will enforce on the statistical model is that these nuisance parameters are multivariate real valued cadlag functions and have a finite supremum and variation norm. We propose a general one-step targeted minimum loss-based estimator (TMLE) based on an initial estimator of the nuisance parameters defined by a loss-based super-learner that uses cross-validation to combine a library of candidate estimators. We enforce this library to contain minimum loss based estimators minimizing the empirical risk over the parameter space under the additional constraint that the variation norm is bounded by a set constant, across a set of constants for which the maximal constant converges to infinity with sample size. We show that this super-learner is not only asymptotically equivalent with the best performing algorithm in the library, but also that it always converges to the true nuisance parameter values at a rate faster than $n{-1/4}$. This minimal rate applies to each dimension of the data and even to nonparametric statistical models. We also demonstrate that the implementation of these constant-specific minimum loss-based estimators can be carried out by minimizing the empirical risk over linear combinations of basis functions under the constraint that the sum of the absolute value of the coefficients is smaller than the constant (e.g., Lasso regression), making our proposed estimators practically feasible. Based on this rate of the super-learner of the nuisance parameter, we can establish that this one-step TMLE is asymptotically efficient at any data generating distribution in the model, under very weak structural conditions on the target parameter mapping and model. We demonstrate our general theorems by constructing such a one-step TMLE of the average causal effect in a nonparametric model, and presenting the corresponding efficiency theorem.",2015,
Design and optimization of wireless sensor networks for localization and tracking,"Knowledge of the position of nodes in a WSN is crucial in most wireless sensor network (WSN) applications. The gathered information needs to be associated with a particular location in a specific time instant in order to appropiately control de surveillance area. Moreover, WSNs may be used for tracking certain objects in monitoring applications, which also requires the incorporation of location information of the sensor nodes into the tracking algorithms. These requisites make localizacion and tracking two of the most important tasks of WSN. Despite of the large research efforts that have been made in this field, considerable technical challenges continue existing in subjects areas like data processing or communications. This thesis is mainly concerned with some of these technical problems. Specifically, we study three different challenges: sensor deployment, model independent localization and sensor selection. The first part of the work is focused on the task of sensor deployement. This is considered critical since it affects cost, detection, and localization accuracy of a WSN. There have been significant research efforts on deploying sensors from different points of view, e.g. connectivity or target detection. However, in the context of target localization, we believe it is more convenient to deploy the sensors in views of obtaining the best estimation possible on the target positioning. Therefore, in this work we suggest an analysis of the deployment from the standpoint of the error in the position estimation. To this end, we suggest the application of the modified CramÂ´er-Rao bound (MCRB) in a sensor network to perform a prior analysis of the system operation in the localization task. This analysis provides knowledge about the system behavior without a complete deployment. It also provides essential information to select fundamental parameters properly, like the number of sensors. To do so, a complete formulation of the modified information matrix (MFIM) and MCRB is developed for the most common measurement models, such as received signal strength (RSS), time-of-arrival (ToA) and angle-of-arrival (AoA). In addition, this formulation is extended for heterogeneous models that combine different measurement models. Simulation results demonstrate the utility of the proposed analysis and point out the similarity between MCRB and CRB. Secondly, we address the problem of target localization which encompasses many of the challenging issues which commonly arise in WSN. Consequently, many localization algorithms have been proposed in the literature each one oriented towards solving these issues. Nevertheless, it have seen tahta the localization performance of above methods usually relies heavily on the availability of accurate knowledge regarding the observation model. When errors in the measurement model are present, their target localization accuracy is degraded significantly. To overcome this problem, we proposed a novel localization algorithm to be used in applications where the measurement model is not accurate or incomplete. The independence of the algorithm from the model provides robustness and versatility. In order to do so, we apply radial basis functions (RBFs) interpolation to evaluate the measurement function in the entire surveillance area, and estimate the target position. In addition, we also propose the application of LASSO regression to compute the weigths of the RBFs and improve the generalization of the interpolated function. Simulation results have demonstrated the good performance of the proposed algorithm in the localization of single or multiples targets. Finally, we study the sensor selection problem. In order to prolong the network lifetime, sensors alternate their state between active and idle. The decision of which sensor should be activated is based on a variety of factors depending on the algorithm or the sensor application. Therefore, here we investigate the centralized selection of sensors in target-tracking applications over huge networks where a large number of randomly placed sensors are available for taking measurements. Specifically, we focus on the application of optimization algorithms for the selection of sensors using a variant of the CRB, the Posterior CRB (PCRB), as the performance-based optimization criteria. This bound provides the performance limit on the mean square error (MSE) for any unbiased estimator of a random parameter, and is iteratively computed by a particle filter (in our case, by a Rao-Blackwellized Particle Filter). In this work we analyze, and compare, three optimization algorithms: a genetic algorithm (GA), the particle swarm optimization (PSO), and a new discrete-variant of the cuckoo search (CS) algorithm. In addition, we propose a local-search versions of the previous optimization algorithms that provide a significant reduction of the computation time. Lastly, simulation results demonstrate the utility of these optmization algorithm to solve a sensor selection problem and point out the reduction of the computation time when local search is applied. ---------------------------------------------------",2014,
Folded concave penalized learning in identifying multimodal MRI marker for Parkinsonâ€™s disease,"BACKGROUND
Brain MRI holds promise to gauge different aspects of Parkinson's disease (PD)-related pathological changes. Its analysis, however, is hindered by the high-dimensional nature of the data.


NEW METHOD
This study introduces folded concave penalized (FCP) sparse logistic regression to identify biomarkers for PD from a large number of potential factors. The proposed statistical procedures target the challenges of high-dimensionality with limited data samples acquired. The maximization problem associated with the sparse logistic regression model is solved by local linear approximation. The proposed procedures then are applied to the empirical analysis of multimodal MRI data.


RESULTS
From 45 features, the proposed approach identified 15 MRI markers and the UPSIT, which are known to be clinically relevant to PD. By combining the MRI and clinical markers, we can enhance substantially the specificity and sensitivity of the model, as indicated by the ROC curves.


COMPARISON TO EXISTING METHODS
We compare the folded concave penalized learning scheme with both the Lasso penalized scheme and the principle component analysis-based feature selection (PCA) in the Parkinson's biomarker identification problem that takes into account both the clinical features and MRI markers. The folded concave penalty method demonstrates a substantially better clinical potential than both the Lasso and PCA in terms of specificity and sensitivity.


CONCLUSIONS
For the first time, we applied the FCP learning method to MRI biomarker discovery in PD. The proposed approach successfully identified MRI markers that are clinically relevant. Combining these biomarkers with clinical features can substantially enhance performance.",2016,Journal of Neuroscience Methods
Spatio-Temporal Spectrum Reuse based on Channel Gain Cartography,"During the last decade, the perceived scarcity of spectrum resources along with the proliferation of new wireless technologies have motivated a substantial research effort on dynamic spectrum management. Although a fixed frequency assignment policy has guiltily led to an alarming spectrum crowding belief, a noticeable underutilization of the allocated frequency bands has been revealed by extensive spectrum occupancy measurements. Therefore, a dynamic re-utilization of the licensed frequencies would be a breakthrough toward a mitigation of the troublesome inefficiency in the spectrum management, aggressively answering to the unceasing demand of resources for new wireless services. 
 
Prominent in this context is the hierarchical spectrum access, an emerging model that envisages secondary users (a.k.a. cognitive radios) aiming to access to the frequency bands of the licensed systems (a.k.a. primary users) in a dynamical and nonintrusive manner. Envisioned as autonomous entities endowed with learning and decisional capabilities, secondary devices accomplish spectrum sensing and dynamical radio resource allocation tasks, thus enabling an opportunistic access to portions of the spectrum under the primary-secondary hierarchy. 
 
The consequent continuous need for a concrete situational awareness required by the cognitive radios demands for innovative signal processing algorithms for high-resolution primary users' activity monitoring, efficient transmission opportunity exploitation and, most importantly, accurate characterization of the surrounding RF propagation environment. Due to the lack of explicit coordination between the two networks, as envisaged in the cognitive radio paradigm, learning the features of the propagation environment is conceivably critical for adaptation of operational system parameters and obligatory protection of the licensed primary system. 
 
To strike the foregoing sensing and control objectives reliably, a significant departure from a one-dimensional view of the RF environment, conventionally attained by point-to-point feedback strategies to acquire channel coefficients as well as interference levels on a per-link basis, is advocated. Toward this direction the present Thesis introduces the concept of channel gain cartography, a groundbreaking geostatistics-inspired application that enables a portrayal of the RF environment impinging upon arbitrary locations in space. The most appealing feature of the proposed tool consists in the non-trivial capability of inferring the channel gain between arbitrary transmitter-receiver locations, based on the only measurements taken among collaborating cognitive radios. Such ability in estimating any-to-any channel gains may open the door to aggressive resource allocation techniques, thus leading to markedly higher spectral efficiency - and finds well-motivated applications not only in the cognitive radio context. 
 
With an accurate RF environment description close at hand, the Thesis presents a primary system's state tracker based on a parsimonious model accounting for the reasonable sparse activity of the primary sources - due to well-known mutual interference concerns - in the monitored geographical area. Motivated by recent advances in sparse linear regression, where the $\ell_1$-norm places itself as a cornerstone for lassoing the non-zero support of the estimand, a sparsity-cognizant state tracker is developed in both centralized and distributed formats. As a byproduct ensued from the parsimonious model, the tracker possesses localization and primary transmission power estimation abilities, which lead to a capability of estimating the actual power spectral density map of the primary system, a continuously-updated portrayal of the aggregate primary power impinging upon the whole monitored geographical region. Detection of the so called spectrum spatial holes is efficiently attainable, thus enhancing the spatial re-use of the primary frequency bands. 
 
Due to the aforementioned lack of explicit support from the primary system, sensing algorithms often face difficulty in acquiring secondary-to-primary users channels. Moreover, the sensing algorithms cannot detect silent licensed receivers, which nevertheless have to be obligatorily protected. 
Based on primary coverage map and channel gain cartography, the approach pursued here is to exploit statistical knowledge of the secondary-to-primary channels, where the combined effect of shadow fading as well as small-scale fading is accounted for, to maximize a given secondary network utility function under chance constraints that ensure protection to any potential licensed user. Albeit a non-convex interference-constrained network utility maximization problem is derived, Karush-Kuhn-Tucker solutions are provably obtained by the proposed algorithms. 
 
Error-corrupted measurements and missing and/or outdated channel gain estimates may undoubtedly compromise the accomplishment of the power control task. To overcome this issue, a novel probabilistic approach encompassing channel knowledge uncertainty on both secondary-to-secondary and secondary-to-primary links is also presented. 
 
The foregoing technical findings are fully corroborated by numerical tests.",2011,
DEXA-Measured VAT Robustly Predicts Impaired Glucose Tolerance and Metabolic Syndrome in Obese Women,"Abdominal visceral adiposity (VAT) has been shown to be an independent risk factor for metabolic and cardiovascular disease. Using enCORE analysis version 13.6 on a GE Lunar iDXA, a new fully automated analysis software to measure VAT, we determined the strength of associations between DEXA-derived VAT and other known indicators for diabetes and cardiovascular disease risk in Caucasian and African American obese women. We collected anthropometrics, vital signs, lipid profile, and DXA whole body composition scan for 229 subjects with BMI 30.0 â€“ 49.9 kg/m2 & age 21 to 69 y. We then performed the non-parametric Spearman correlation analysis and found that in subjects overall, DEXA-VAT is positively associated with triglyceride, fasting glucose, fasting insulin, and HOMA-IR, and negatively associated with HDL. Among all anthropometric, body composition and cardiometabolic variables, DEXA-VAT was the most robust predictor of impaired glucose tolerance (IGT) and metabolic syndrome (MetSx) in binary regression analysis, even after adjusting for race. LASSO regression after adjusting for covariates that best predicted IGT and MetSx showed that HOMAIR and DEXA-VAT most significantly predicted IGT (p<0.001, p<0.001, respectively), and DEXA-VAT most significantly predicted MetSx (p<0.001). These observations have implications for VAT associated risk in diabetes and cardiovascular disease. INTRODUCTION â€¢  Abdominal obesity, especially the visceral component of adipose tissue (VAT), is strongly associated with metabolic and cardiovascular risk in humans (1-2). â€¢  The differences in sex and race with regard to body composition and metabolic risk have also been demonstrated with VAT associated risk. â€¢  Although CT and MRI are considered the â€œgold standardsâ€ in the measurement of type and distribution of body fat, dual energy X-ray absorptiometry (DEXA) can accurately measure body composition with high-precision, low X-ray exposure, and short-scanning time (3). â€¢  We previously showed strong correlations between DEXA and MRI whole body composition, with coefficients of variation of â‰¤ 2% for DEXAderived adiposity measures (4). â€¢  In addition to whole body composition, we now have a newly available software to estimate VAT area (cm3) and mass (g) using enCORE analysis version 13.6 (5) on a GE Lunar iDXA. METHODS Study: Cross-sectional design of subjects previously recruited for studies at the Vanderbilt Clinical Research Center. Subjects: 229 subjects with BMI 30.0 â€“ 49.9 kg/m2 & age 21 to 69 y. All records de-identified. Measures Anthropometrics Height, weight, BMI, Waist & hip circumference (WC & HC), waist-to-hip ratio (WHR), waist-toheight ratio (WHtR) Lipid profile Total cholesterol, HDL, LDL, triglyceride (TG) Fasting glucose, insulin, HOMA-IR DEXA whole body composition scan Metabolic disease states â€¢  Impaired Glucose tolerance (IGT): fasting glucose â‰¥100  mg/dL â€¢  Metabolic Syndrome defined as â‰¥ 3 of the following: 1. WC (>102 cm for , >88 cm for ); 2. TG (â‰¥150 mg/dl); 3. HDL (<40 mg/dl in , <50 mg/dl in ); 4. hypertension (â‰¥130/â‰¥85 mmHg); 5. impaired fasting glucose (â‰¥100 mg/dl). Analysis: R version 3.0.1 analyzed with nonparametric distribution. RESULTS DEXA-VAT Associations with Metabolic Risk Factors Overall, DEXA-VAT was positively associated with SBP, DBP, TG, fasting glucose & insulin, HOMA-IR, and negatively associated with HDL. DEXA-VAT was still associated with SBP, DBP, insulin, and HOMA-IR after adjusting for race, and associated with hsCRP after adjusting for the intâ€™n with race. RESULTS Multivariate LASSO Regression for IGT A: combination of anthropometric, body composition, and cardiometabolic variables guided by previous binary regressions presented with adjusted odds ratios (OR); B: ANOVA analysis of A adjusted for race; C: ANOVA analysis of A adjusted for interaction with race. Multivariate LASSO Regression for MetSx A: combination of anthropometric, body composition, and cardiometabolic variables guided by previous binary regressions presented with adjusted odds ratios (OR); B: ANOVA analysis of A adjusted for race; C: ANOVA analysis of A adjusted for interaction with race. CONCLUSION â€¢  DEXA-VAT was positively associated with TG, fasting glucose & insulin, and HOMA-IR, and negatively associated with HDL-C â€¢  In binary regression analysis, DEXA-VAT was a more robust predictor of IGT and MetSx than other anthropometric and body composition variables â€¢  In Mutivariate LASSO regression, the odds ratio for having IGT was most robustly predicted by HOMAIR and DEXA-VAT; the odds ratio for having MetSx was most robustly predicted by DEXA-VAT REFERENCES 1.  Canoy D. Distribution of body fat and risk of coronary heart disease in men and women. Curr Opin Cardiol. 2008 Nov;23(6):591-8. 2.  Shuster A, Patlas M, Pinthus JH, Mourtzakis M. The clinical importance of visceral adiposity: a critical review of methods for visceral adipose tissue analysis. Br J Radiol. 2012 Jan;85(1009):1-10. 3.  Direk K et al. The relationship between DXA-based and anthropometric measures of visceral fat and morbidity in women. BMC Cardiovasc Disord. 2013 Apr 3;13:25. 4.  Silver HJ et al. Comparison of gross body fat-water magnetic resonance imaging at 3 Tesla to dual-energy X-ray absorptiometry in obese women. Obesity. 2013 Apr;21(4): 765-74. 5.  Kaul S, et al. Dual-energy X-ray absorptiometry for quantification of visceral fat. Obesity (Silver Spring). 2012 Jun;20(6):1313-8. ACKNOWLEDGEMENT Research supported by multiple sources obtained by Silver, Buchowski, Shibao & NIH T35DK007383 DEXA-Measured VAT Robustly Predicts Impaired Glucose Tolerance and Metabolic Syndrome in Obese Women 1Bi X., 2Keil C.D., 2Seabolt L., 2Tyree R., 2Buchowski M., 2Kang H., 2Shibao C., 2Silver H.J. 1Jefferson Medical College of Thomas Jefferson University, Philadelphia, PA 2Department of Medicine, School of Medicine of Vanderbilt University, Nashville, TN",2018,
Regularized extreme learning machine for regression problems,"Extreme learning machine (ELM) is a new learning algorithm for single-hidden layer feedforward networks (SLFNs) proposed by Huang et al. [1]. Its main advantage is the lower computational cost, which is especially relevant when dealing with many patterns defined in a high-dimensional space. This paper proposes an algorithm for pruning ELM networks by using regularized regression methods, thus obtaining a suitable number of the hidden nodes in the network architecture. Beginning from an initial large number of hidden nodes, irrelevant nodes are then pruned using ridge regression, elastic net and lasso methods; hence, the architectural design of ELM network can be automated. Empirical studies on several commonly used regression benchmark problems show that the proposed approach leads to compact networks that generate competitive results compared with the ELM algorithm.",2011,Neurocomputing
Marker-assisted prediction of non-additive genetic values,"It has become increasingly clear from systems biology arguments that interaction and non-linearity play an important role in genetic regulation of phenotypic variation for complex traits. Marker-assisted prediction of genetic values assuming additive gene action has been widely investigated because of its relevance in artificial selection. On the other hand, it has been less well-studied when non-additive effects hold. Here, we explored a nonparametric model, radial basis function (RBF) regression, for predicting quantitative traits under different gene action modes (additivity, dominance and epistasis). Using simulation, it was found that RBF had better ability (higher predictive correlations and lower predictive mean square errors) of predicting merit of individuals in future generations in the presence of non-additive effects than a linear additive model, the Bayesian Lasso. This was true for populations undergoing either directional or random selection over several generations. Under additive gene action, RBF was slightly worse than the Bayesian Lasso. While prediction of genetic values under additive gene action is well handled by a variety of parametric models, nonparametric RBF regression is a useful counterpart for dealing with situations where non-additive gene action is suspected, and it is robust irrespective of mode of gene action.",2011,Genetica
Incorporating grouping information in bayesian variable selection with applications in genomics,"In many applications it is of interest to determine a limited number of important explanatory factors (representing groups of potentially overlapping predictors) rather than original predictor variables. The often imposed require-ment that the clustered predictors should enter the model simultaneously may be limiting as not all the variables within a group need to be associated with the out-come. Within-group sparsity is often desirable as well. Here we propose a Bayesian variable selection method, which uses the grouping information as a means of in-troducing more equal competition to enter the model within the groups rather than as a source of strict regularization constraints. This is achieved within the context of Bayesian LASSO (least absolute shrinkage and selection operator) by allowing each regression coefficient to be penalized differentially and by considering an additional regression layer to relate individual penalty parameters to a group identification matrix. The proposed hierarchical model therefore enables inference simultaneously on two levels: (1) the regression layer for the continuous outcome in relation to the predictors and (2) the regression layer for the penalty param-eters in relation to the grouping information. Both situations with overlapping and non-overlapping groups are applicable. The method does not assume within-group homogeneity across the regression coefficients, which is implicit in many structured penalized likelihood approaches. The smoothness here is enforced at the penalty level rather than within the regression coefficients. To enhance the potential of the proposed method we develop two rapid computational procedures based on the expectation maximization (EM) algorithm, which offer substantial time savings in applications where the high-dimensionality renders Markov chain Monte Carlo (MCMC) approaches less practical. We demonstrate the usefulness of our method in predicting time to death in glioblastoma patients using pathways of genes.",2014,Bayesian Analysis
Classification of suspicious lesions on prostate multiparametric MRI using machine learning,"Abstract. We present a radiomics-based approach developed for the SPIE-AAPM-NCI PROSTATEx challenge. The task was to classify clinically significant prostate cancer in multiparametric (mp) MRI. Data consisted of a â€œtraining datasetâ€ (330 suspected lesions from 204 patients) and a â€œtest datasetâ€ (208 lesions/140 patients). All studies included T2-weighted (T2-W), proton density-weighted, dynamic contrast enhanced, and diffusion-weighted imaging. Analysis of the images was performed using the MIM imaging platform (MIM Software, Cleveland, Ohio). Prostate and peripheral zone contours were manually outlined on the T2-W images. A workflow for rigid fusion of the aforementioned images to T2-W was created in MIM. The suspicious lesion was outlined using the high b-value image. Intensity and texture features were extracted on four imaging modalities and characterized using nine histogram descriptors: 10%, 25%, 50%, 75%, 90%, mean, standard deviation, kurtosis, and skewness (216 features). Three classification methods were used: classification and regression trees (CART), random forests, and adaptive least absolute shrinkage and selection operator (LASSO). In the held out by the organizers test dataset, the areas under the curve (AUCs) were: 0.82 (random forests), 0.76 (CART), and 0.76 (adaptive LASSO). AUC of 0.82 was the fourth-highest score of 71 entries (32 teams) and the highest for feature-based methods.",2018,Journal of Medical Imaging
Fused-MCP With Application to Signal Processing,"ABSTRACTFriedman etÂ al. proposed the fused lasso signal approximator (FLSA) to denoise piecewise constant signals by penalizing the l1 differences between adjacent signal points. In this article, we propose a new method, referred to as the fused-MCP, by combining the minimax concave penalty (MCP) with the fusion penalty. The fused-MCP performs better than the FLSA in maintaining the profile of the original signal and preserving the edge structure. We show that, with a high probability, the fused-MCP selects the right change-points and has the oracle property, unlike the FLSA. We further show that the fused-MCP achieves the same l2 error rate as the FLSA. We develop algorithms to solve fused-MCP problems, either by transforming them into MCP regression problems or by using an adjusted majorization-minimization algorithm. Simulation and experimental results show the effectiveness of our method. Supplementary material for this article is available online.",2018,Journal of Computational and Graphical Statistics
