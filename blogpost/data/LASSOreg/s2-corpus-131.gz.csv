title,abstract,year,journal
Blood parameters score predicts long-term outcomes in stage II-III gastric cancer patients,"BACKGROUND
Increasing numbers of laboratory blood parameters (BPM) have been reported to greatly affect the long-term outcomes of gastric cancer (GC) patients. However, the existing prognostic models do not comprehensively analyze these predictors.


AIM
To construct a new prognostic tool, based on all the prognostic BPM, to achieve more accurate prognosis prediction for GC.


METHODS
We retrospectively assessed 850 consecutive patients who underwent curative resection for stage II-III GC from January 2010 to April 2013. The patients were classified into developing (n = 567) and validation (n = 283) cohorts using computer-generated random numbers. A scoring system, namely BPM score, was then constructed using least absolute shrinkage and selection operator (LASSO) Cox regression model in the developing cohort, and validated in the validation cohort. A nomogram consisting of BPM score and tumor-lymph node-metastasis (TNM) stage was further created. The discrimination and calibration of the nomogram were evaluated via Harrell's C-statistic and the Hosmer-Lemeshow test.


RESULTS
Using the LASSO model, we established the BPM score based on five BPM: Albumin, lymphocyte-to-monocyte ratio, neutrophil-to-lymphocyte ratio, carcinoembryonic antigen, and carbohydrate antigen 19-9. The BPM scores were divided into high- and low-BPM groups based on a cut-off value of -0.93. High-BPM patients were significantly older and had more advanced, larger tumors. In the developing cohort, significant differences were found in 5-year overall survival (OS) and 5-year disease-specific survival between the high-BPM and low-BPM patients. Similar results were found in the validation group. Multivariable analysis showed that the BPM score was an independent predictor of OS. High-BPM patients had a poorer 5-year OS for each subgroup. Furthermore, a nomogram that combined the BPM score and TNM stage had significantly better prognostic value compared with TNM stage alone.


CONCLUSION
The BPM score provides more accurate prognosis prediction in stage II-III GC patients and is an effective complement to the TNM staging system.",2019,World Journal of Gastroenterology
"Exploring the use of learning techniques for relating the site index of radiata pine stands with climate, soil and physiography","Abstract The environmental uncertainty derived from climate change suggests that some commonly used empirical indicators of forest productivity, such as site index, may not be suitable for future growth prediction in the following decades. As a consequence, the development of statistical models that relate these indicators with environmental variables may be a crucial support resource for practical forest management. In this research, we tested seven different statistical learning techniques for estimating site index of radiata pine (Pinus radiata D. Don) stands in the northwest of Spain. The predictors used for this task were a set of 43 physiographic, soil, and climatic variables obtained from available raster maps for this region, whereas the site index data came from a network of 489 plot-inventory combinations set by the Sustainable Forest Management Unit of the University of Santiago de Compostela. The proposed learning techniques produced models of easy interpretation in comparison to other â€œmachine learningâ€ approaches, and accounted for up to 50% of the responseâ€™s variability. The stepwise, Elastic Net, Least Angle Regression and Infinitesimal Forward Stagewise techniques provided models with high performance but at the expense of including a large number of predictors. By contrast, the Lasso and the Partial Least Squares techniques produced more parsimonious alternatives. However, these models showed noticeable heteroscedastic residuals and a certain regression to the mean. The Multivariate Adaptive Regression Splines seemed to be the most suitable technique, as it explained 50% of the site index variability with a reduced amount of predictors, and did not show undesirable patterns in the residuals and predicted values. Besides, the growth-environment relationships represented by this model seemed to be ecologically coherent.",2020,Forest Ecology and Management
A Sparse Generative Model and its EM Algorithm for Variable Selection in High-Dimensional Regression,"We address the problem of Bayesian variable selection for high-dimensional linear regression. We consider a generative model that uses a spike-and-slab like prior distribution obtained by multiplying a deterministic binary vector, which traduces the sparsity of the problem, with a random Gaussian parameter vector. Such a model allows an expectation-maximization algorithm, optimizing a type-II log-likelihood, to be derived. This marginal log-likelihood involves an Occam's razor term, automatically penalizing the complexity, which is used for model selection. Albeit NP-hard, the algorithm we propose can be relaxed in order to infer a family of models. Model selection is eventually performed afterwards based on Occam's razor. We report numerical comparisons between our method, called spinyReg, and the most recent variable selection algorithms, including lasso, adaptive lasso and stability selection. SpinyReg turns out to perform well compared to those algorithms, especially regarding false detection rates.",2014,
Exploring the Performance of Methods to Deal Multicollinearity: Simulation and Real Data in Radiation Epidemiology Area,"The issue of multicollinearity has long been acknowledged in statistical modelling; however, it is often untreated in the most of published papers. Indeed, the use of methods for multicollinearity correction is still scarce. One important reason is that despite many proposed methods, little is known about their strength or performance. We compare the statistical properties and performance of four main techniques to correct multicollinearity, i.e., Ridge Regression (R-R), Principal Components Regression (PC-R), Partial Least Squares Regression (PLS-R), and Lasso Regression (L-R), in both a simulation study and two real data examples used for modelling volumes of heart and Thyroid as a function of clinical and anthropometric parameters. We find that when the statistical approaches were used to address different levels of collinearity, we observed that R-R, PC-R and PLS-R appeared to have a somewhat similar behavior, with a slight advantage for the PLS-R. Indeed, in all implemented cases, the PLS-R always provided the smallest value of root mean square error (RMSE). When the degree of collinearity was moderate, low or very low, the L-R method had also somewhat similar performance to other methods. Furthermore, correction methods allowed us to provide stable and trustworthy parameter estimates for predictors in the modelling of heart and Thyroid volumes. Therefore, this work will contribute to highlighting performances of methods used only for situations ranging from low to very high multicollinearity.",2018,International journal of statistics in medical research
Multi-level modeling and computational approaches to investigate long-term diabetes complications,"Diabetes mellitus is a lifelong, incapacitating disease affecting multiple organs. Worldwide prevalence figures estimate that there are 250 million diabetic patients today and that this number will increase by 50% by 2025. The disease is associated with devastating chronic complications including coronary heart disease, stroke and peripheral vascular disease (macrovascular disease) as well as microvascular disorders, leading to damage of kidneys (nephropathy) and eyes (retinopathy). These complications impose an immense burden on the quality of life of the patients and account for more than 10% of health care costs in Europe. Therefore, novel means to prevent the onset and the progression of these devastating diabetic complications are needed. 
The aim of the work presented in this thesis is to propose novel computational methods to study diabetes complications with a multi-level approach. 
Diabetes mellitus is a strongly multifactorial disease, and several risks factors (such as genetic, and environmental factors) are combined together in a complex trait, leading to the onset of the disease. 
Physiological mechanisms that underlie the disease and the onset and progression of the different complications are still mostly unknown. 
Given the complex nature of diabetes, the study of the complications can be faced with a multi-level modeling approach. In the general scheme for complex disease, such as diabetes, 3 key elements act together to determine the disease status (outcome) of a patient: i) the phenotype, i.e. the set of all metabolic, anthropometric and clinical variables characterizing the patient, ii) the genotype, i.e. the DNA sequence of the patient, iii) the set of interventions on the patient, i.e. therapies and treatments with drugs. All these 3 variables are connected each other through interactions and have a joint effect on the final outcome of the patient. 
The multi-level approach allows to disjoint the full problem into sub-problems, focusing only on a set of variables and interaction (reflecting a specific level of information) according to available data. 
In the present work, 3 main levels of study of diabetes complications are considered, and, for each approach, novel methodologies developed during my PhD are proposed. 
The 3 levels of study considered in the present work are: i) modeling the effect of genotype on the outcome, ii) modeling the effect of phenotype and treatment on the progression of the outcome, iii) modeling the effect of treatment on the phenotype. 
In the first level of study, diabetes complications are studied from a static point of view, i.e. without considering their progression over time, and the main objective is to identify the genetic biomarkers that allow to predict the disease state of the patients with the final goal to stratify patients according to the risk of developing the disease. Genome Wide Associations Studies (GWAs) are statistical studies aiming at identify those SNPs able to explain the differences observed for a certain outcome (the disease status) between cases (diseased subjects) and controls (healthy subjects) in a study population. Several methods performing univariate and/or multivariate selection have been used in literature for the identification of genetic markers from GWAs data. In this thesis, a novel algorithm for genetic biomarker selection and subjects classification from genome-wide SNP data has been developed. The algorithm is based on the Naive Bayes classification framework, enriched by three main features: i) bootstrap aggregating of an ensemble of Naive Bayes classifiers, ii) a novel strategy for ranking and selecting the attributes used by each classifier in the ensemble, iii) a permutation-based procedure for selecting significant biomarkers, based on their marginal utility in the classification process. The algorithm has been validated on the Wellcome Trust Case-Control Consortium on Type 1 Diabetes and its performance compared with the ones of both a standard Naive Bayes algorithm and HyperLASSO, a penalized logistic regression algorithm from the state-of-the-art in simultaneous genome-wide data analysis. 
The second level of study is represented by the dynamic analysis of diabetes complications, where the variable â€œtimeâ€ plays a major role. In particular, the objective is to model the onset and the progression of diabetes complications over time, using phenotypic and therapeutic information, with the final goal to estimate a probability for the diabetic patient to develop a certain complication, thus optimizing clinical trials and avoiding invasive and expensive tests. So far, several models of diabetes complications are present in literature, but none is able to flexibly integrate accumulating â€“omics knowledge (i.e. proteomics, metabolomics, genomics) into a clinical macro-level. The most interesting complication models, in fact, are based on Markov Models (also called state transition model) and use phenotypic information to describe the cohort of interest without the possibility to easily integrate additional information. A new in-silico model for simulating the progression of cardiovascular and kidney complications in diabetic patients is presented. The model proposes, as innovative feature, the use of Dynamic Bayesian Networks (DBNs) for modeling the interactions between variables. Compared to Markov Models, which require as many nodes as the number of combinations of variablesâ€™ values, DBNs are more advantageous in handling both the structure and possible additional information, since each variable is simply represented by a node in the network. The model was built relying on data from the Diabetes Control and Complications Trial, a multicenter randomized clinical trial designed to compare intensive with conventional therapy with regard to their effects on the development and progression of the early vascular and neurologic. The developed model is able to predict the progression of the main diabetes complications with an accuracy greater than 95% at a population level. The model is suitable to be used as a decision support tool to help clinicians in the therapy design through cost-effectiveness analysis: exploiting the simulations generated through the model, it is possible, for example, to choose the best strategy between two different therapies for treating a specific cohort of patients. To this aim, a user-interface based on the present model is currently under development. The flexible structure of the model will allow to easily add genotypic information in the next feature as a potential mean to improve predictions. 
The last level of study focuses on the action of a specific drug on a target phenotype, with the final aim to develop rational means to personalize drug therapy and to ensure maximum efficacy with minimal adverse effects. Focusing on cardiovascular diseases as a direct complication of diabetes, aspirin therapy is an important component of cardiovascular prevention for high risk patients. Aspirin performs its preventive action by inhibiting a key enzyme (the prostaglandin-endoperoxide synthase PTGS-1, also known as cyclooxygenase COX-1) in the cascade leading to the production of thromboxane B2 (TxB2), the major factor involved in the platelets aggregation with consequent formation of thrombi. It is known, from literature, that diabetic patients exhibit a different response to aspirin therapy in comparison to healthy subjects, showing a reduced effectiveness of the drug, which is often referred to as â€˜aspirin resistanceâ€™. Given the lack of a mathematical characterization of these phenomena, the problem was faced using a pharmacodynamics modeling approach, with an explorative intent. Relaying on biological knowledge retrieved from literature, a partially lumped and partially distributed compartmental model was developed, able to describe: i) the kinetics of COX-1 enzyme, from its production within megakaryocytes in bone-marrow to circulating platelets in blood, ii) the pharmacokinetics and pharmacodynamics of aspirin, i.e. its distribution in the body tissues and its interaction with COX-1. The model was tested using data of serum thromboxane TxB2 recovery levels after aspirin withdrawal in healthy subjects. Possible mechanisms to explain the so-called â€˜aspirin resistanceâ€™ have been finally discussed.",2014,
Enhancer identification in mouse embryonic stem cells using integrative modeling of chromatin and genomic features,"BackgroundEpigenetic modifications, transcription factor (TF) availability and differences in chromatin folding influence how the genome is interpreted by the transcriptional machinery responsible for gene expression. Enhancers buried in non-coding regions are found to be associated with significant differences in histone marks between different cell types. In contrast, gene promoters show more uniform modifications across cell types. Here we used histone modification and chromatin-associated protein ChIP-Seq data sets in mouse embryonic stem (ES) cells as well as genomic features to identify functional enhancer regions. Using co-bound sites of OCT4, SOX2 and NANOG (co-OSN, validated enhancers) and co-bound sites of MYC and MYCN (limited enhancer activity) as enhancer positive and negative training sets, we performed multinomial logistic regression with LASSO regularization to identify key features.ResultsCross validations reveal that a combination of p300, H3K4me1, MED12 and NIPBL features to be top signatures of co-OSN regions. Using a model from 10 signatures, 83% of top 1277 putative 1â€‰kb enhancer regions (probability greater than or equal to 0.8) overlapped with at least one TF peak from 7 mouse ES cell ChIP-Seq data sets. These putative enhancers are associated with increased gene expression of neighbouring genes and significantly enriched in multiple TF bound loci in agreement with combinatorial models of TF binding. Furthermore, we identified several motifs of known TFs significantly enriched in putative enhancer regions compared to random promoter regions and background. Comparison with an active H3K27ac mark in various cell types confirmed cell type-specificity of these enhancers.ConclusionsThe top enhancer signatures we identified (p300, H3K4me1, MED12 and NIPBL) will allow for the identification of cell type-specific enhancer regions in diverse cell types.",2011,BMC Genomics
Identifying predictive features in drug response using machine learning: opportunities and challenges.,"This article reviews several techniques from machine learning that can be used to study the problem of identifying a small number of features, from among tens of thousands of measured features, that can accurately predict a drug response. Prediction problems are divided into two categories: sparse classification and sparse regression. In classification, the clinical parameter to be predicted is binary, whereas in regression, the parameter is a real number. Well-known methods for both classes of problems are briefly discussed. These include the SVM (support vector machine) for classification and various algorithms such as ridge regression, LASSO (least absolute shrinkage and selection operator), and EN (elastic net) for regression. In addition, several well-established methods that do not directly fall into machine learning theory are also reviewed, including neural networks, PAM (pattern analysis for microarrays), SAM (significance analysis for microarrays), GSEA (gene set enrichment analysis), and k-means clustering. Several references indicative of the application of these methods to cancer biology are discussed.",2015,Annual review of pharmacology and toxicology
Different Role of Left Atrial Size and Function in Patients with Paroxysmal versus Persistent Atrial Fibrillation,"Purpose: Pulmonary vein isolation (PVI) is established as interventional treatment in atrial fibrillation (AF). Recent studies suggest that atrial reservoir function (preload) is involved in initiation of paroxysmal AF (AFparox) and conduit function (after-load) may be crucial in persistent AF (AFpers). Methods: We enrolled 336 consecutive patients scheduled for PVI (age 60Â±10 years). Point-to-point radiofrequency ablation was used in the majority of cases. Success was defined as a minimum of 3 months (median 12 months) follow-up as absence of recurrence of AF in any Holter ECG since PVI. Results: Patients with recurrent AFparox demonstrated a significantly better outcome (64% success) than patients with AFpers (41% success). AFparox patients were 5 years younger and had smaller atria, a better emptying index, higher systolic pulmonary venous velocity, and only borderline impaired diastolic left ventricular function. LA size per se was not related to success in AFparox. Left ventricular filling times (Tfill) and the ratio of systolic to diastolic tissue Doppler (TDI) velocity (Sâ€™/Eâ€™) were found to be potential risk predictors. Increased LA-size, reduced LA-emptying and impaired diastolic left ventricular function were found in AFpers patients. Sâ€™/Eâ€™ and isovolumic relaxation time stratify risk in AFpers patients. Conclusions: In AFparox long term failure of primary PVI is associated with restrictive reservoir function but not LA enlargement. Whereas in AFpers a combination of severely reduced LA emptying, increased LA size and severely impaired diastolic LV function result in a high rate of recurrent AF. TDI (Tfill, Sâ€™/Eâ€™) may help to improve risk stratification. in particular pulmonary vein isolation (PVI), can be considered as an early option in the management of patients with AF [5]. Success rates of the initial ablation procedure depend on the definition of success; they vary between 60 and 85% for AFparox and are lower for persistent AF (AFpers) [3,6-9]. The initiation of AF requires both a trigger and a susceptible substrate. In recurrent AFparox ectopic focal activity is thought to be local and limited to one or a few sites. On the contrary in AFpers there tend to be multiple ectopic sites throughout the atria and advanced damage of atrial myocardium (more susceptible substrate) [10-13]. Large left atrial (LA) size was found to be a predictor of recurrence of AF after electrical or pharmacological cardio-version as well as after ablative treatment [14-17]. The prognostic value of volume seems to be absent in AFparox, however [18]. LA function comprises three phases: reservoir, conduit and active contraction phase [19]. LAactive function increases with slight volume load as long as LA structural dilatation is limited [19,20]. Thus, in early LA dysfunction as expected in AFparox systolic LA dimensions are ambiguous, because an increased systolic volume may reflect increased loading and/or size (history of dilatation). Recent studies suggest that preserved atrial reservoir function related to preload is a predictor of success [21]. Impairment of atrial reservoir function and poor long-term success of PVI in small left atria was related to enhanced fibrosis [22]. Whereas conduit function related to after-load may become increasingly important for PVI success in large atria with poor emptying. Thus, we tested the hypothesis that in AFparox atrial reservoir function and in AFpers residual conduit function is crucial for success of PVI by an analysis in the function-size plane in a retrospective study in our PVI data-base. As second end points we wanted to identify echocardiographic parameters potentially predicting recurrence of AF after PVI and elucidate the impact of body mass index (BMI) in this context. Methods Study design The study is a retrospective analysis in the data-base of an Michael GrÃ¤fe, Charalampos Kriatselis, Vesna Furundzija, Usan Thanabalasingam, Jin-Hong Gerds-Li, Eckart Fleck and Ernst Wellnhofer* Deutsches Herzzentrum Berlin, Augustenburger Platz 1, 13353 Berlin, Germany *Address for Correspondence Dr. med Ernst Wellnhofer, Deutsches Herzzentrum Berlin, Augustenburger Platz 1, 13353 Berlin, Germany, Tel: +493045932498; Fax: +493034901861; E-mail: wellnhofer@dhzb.de Submission: 03 October 2013 Accepted: 10 November 2013 Published: 15 November 2013 Reviewed & Approved by: Dr. Krasniqi Nazmi Division of Cardiology, University Hospital of Zurich, Switzerland Research Article Open Access Journal of Cardiobiology Avens Publishing Group Inviting Innovations Avens Publishing Group Inviting Innovations Introduction Atrial fibrillation (AF) is the most common cardiac arrhythmia, particularly among the elderly. AF is responsible for an increased risk of stroke, heart failure and all-cause mortality [1]. Haissaguerre et al. demonstrated that ectopic activity in pulmonary veins (PV) may be responsible for triggering AF, particularly in paroxysmal AF (AFparox) [2]. Thus, ablation of tissue where ectopic activity arises is an interventional treatment strategy. Recent improvements in the technique of catheter ablation provide favorable results compared with pharmacological treatment [3,4]. Therefore, catheter ablation, Copyright: Â© 2013 GrÃ¤fe M, et al. This is an open access article distributed under the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. Citation: GrÃ¤fe M, Kriatselis C, Furundzija V, Thanabalasingam U, Gerds-Li JH, et al. Different Role of Left Atrial Size and Function in Patients with Paroxysmal versus Persistent Atrial Fibrillation. J Cardiobiol. 2013;1(2): 10. J Cardiobiol 1(2): 10 (2013) Page 02 observational prospective cohort of consecutive patients scheduled for PVI to treat AF. Inclusion criteria were symptomatic, medically refractory AF eligible for ablative treatment and feasibility of a TTE study. Patients with LAdiameter >60 mm, valve prosthesis, pacemaker or LV-EF<40% were excluded. The study complies with the declaration of Helsinki. The research protocol was approved by the internal review board and all patients gave written informed consent. Study protocol One day prior to the intervention a transthoracic (TTE) and a transoesophageal (TEE) echocardiogram were obtained with an IE33 Philips machine. All patients underwent a comprehensive evaluation with M-mode, B-mode, pulsed Doppler and tissue Doppler imaging. Each measurement was repeated at least seven times in AF and at least thrice if SR was present. Peak early filling velocity (E) and deceleration time (EDT) were determined in pulsed Doppler measurements with a sample volume at the tips of the mitral leaflet. For determination of isovolumic relaxation time (IVRT) the sample volume was adjusted between inflow and outflow jet and time resolution was maximized. LA diameter (LA), area (LAA) and volume (LAV) were evaluated at end systole (maximum) and at end diastole (minimum). LA volumes were calculated from apical LAA by a modified biplane area length method [23]. Emptying index was calculated as 1 LAVdiast/ LAVsys. 4D volumes were additionally acquired by 4D echocardiography (Philips IE33) before PVI since end of 2010. An example is displayed in Figure 1. Pulmonary vein systolic (PVsyst) and diastolic (PVdiast) peak velocities were derived from pulsed Doppler measurements, and a non-dimensional ratio (S/D ratio) was calculated. Systolic (Sâ€™) and early diastolic (Eâ€™) tissue Doppler (TDI) of left ventricular base/ mitral ring velocities were measured in a four chamber (septal and lateral) and two chamber view (anterior, inferior). Median Sâ€™, were calculated. TDI data were analyzed offline with QLabTM (PHILIPS B.V Eindhoven The Netherlands). In view of the limitations of LA size and emptying index in predicting PVI success in AFparox we evaluated a high resolution monitoring of the time course left ventricular contraction and filling by measurement of time intervals between R-wave and maximal negative strain and between maximal negative strain and return to 70% of the start value (Tfill). User interaction is minimized (selection of ROI in QLABTM and visual quality check) to warrant standardized and reproducible assessment of these times. Exported spread sheets are automatically post processed by a simple generic algorithm. Definition of success Follow-up included repeated 24-hour Holter ECG at 1, 3, 6, 9, 12, and 18 months after PVI. Anti-arrhythmic drugs were withdrawn 4 weeks and 3 months after ablative treatment of AFparox and AFpers respectively. Success was defined and assessed independently in patients with a minimum of 3 months follow-up as absence of recurrence of AF in any Holter ECG since PVI. Ablation procedure Oral anticoagulation was stopped 3-7 days before the scheduled ablation. Point-to-point radiofrequency ablation was used in the majority of cases in the study sample. Three dimensional anatomy of left atrial pulmonary vein ostia was reconstructed from rotational angiography [24,25]. The end-point of the procedure was the electrical isolation of all pulmonary veins defined as elimination of PV potentials at the Lasso catheter that was positioned as closely as possible to the PV ostium. Statistics IBMÂ® SPSSÂ® Statistics version 20, IBM Corporation) was used for statistics. Means Â± standard deviations and rates (percentages) and p-values for significant group differences (t-test, Mann Whitney U test) are given. The subset of 4D volumes additionally acquired by 4D echocardiography serves as cross-validation sample. As general validation strategy bootstrapping (100 samples) was performed in larger groups. Correlation and regression analysis and scatter and box plots were employed for pattern analysis regarding group differences. Predictive value was assessed by area under the curve (AUC), crosstable analysis (odds ratio) and binary logistic modeling with HosmerLemeshow test for calibration and bootst",2013,
Sparsity Regularization for classification of large dimensional data,"Feature selection has evolved to be a very important step in several machine learning paradigms. Especially in the domains of bio-informatics and text classification which involve data of high dimensions, feature selection can help in drastically reducing the feature space. In cases where it is difficult or infeasible to obtain sufficient training examples, feature selection helps overcome the curse of dimensionality which in turn helps improve performance of the classification algorithm. The focus of our research are five embedded feature selection methods which use the ridge regression, or use Lasso regression, and those which combine the two with the goal of simultaneously performing variable selection and grouping correlated variables.",2017,ArXiv
"Diagnostic, progressive and prognostic performance of m6A methylation RNA regulators in lung adenocarcinoma","Background: N6-methyladenosine (m6A) RNA methylation is dynamically and reversibly regulated by methyl-transferases (""writers""), binding proteins (""readers""), and demethylases (""erasers""). The m6A is restored to adenosine and thus to achieve demethylation modification. The abnormality of m6A epigenetic modification in cancer has been increasingly attended. However, we are rarely aware of its diagnostic, progressive and prognostic performance in lung adenocarcinoma (LUAD). Methods and Results: The expression of 13 widely reported m6A RNA regulators in LUAD and normal samples were systematically analyzed. There were 12 m6A RNA methylation genes displaying aberrant expressions, and an 11-gene diagnostic score model was finally built (Diagnostic score =0.033*KIAA1429+0.116*HNRNPC+0.115*RBM15-0.067* METTL3-0.048*ZC3H13-0.221*WTAP+ 0.213*YTHDF1-0.132*YTHDC1-0.135* FTO+0.078*YTHDF2+0.014*ALKBH5). Receiver operating characteristic (ROC) analysis was performed to demonstrate superiority of the diagnostic score model (Area under the curve (AUC) was 0.996 of training cohort, P<0.0001; AUC was 0.971 of one validation cohort-GSE75037, P<0.0001; AUC was 0.878 of another validation cohort-GSE63459, P<0.0001). In both training and validation cohorts, YTHDC2 was associated with tumor stage (P<0.01), while HNRNPC was up expressed in progressed tumor (P<0.05). Besides, WTAP, RBM15, KIAA1429, YTHDF1, and YTHDF2 were all up expressed for TP53 mutation. Furthermore, using least absolute shrinkage and selection operator (lasso) regression analysis, a ten-gene risk score model was built. Risk score=0.169*ALKBH5-0.159*FTO+0.581*HNRNPC-0.348* YTHDF2-0.265*YTHDF1-0.123*YTHDC2 +0.434*RBM15+0.143*KIAA1429-0.200*WTAP-0.310*METTL3. There existed correlation between the risk score and TNM stage (P<0.01), lymph node stage (P<0.05), gender (P<0.05), living status (P<0.001). Univariate and multivariate Cox regression analyses of relevant clinicopathological characters and the risk score revealed risk score was an independent risk factor of lung adenocarcinoma (HR: 2.181, 95%CI (1.594-2.984), P<0.001). Finally, a nomogram was built to facilitate clinicians to predict outcome. Conclusions: m6A epigenetic modification took part in the progression, and provided auxiliary diagnosis and prognosis of LUAD.",2020,International Journal of Biological Sciences
A Significance Test for the Lasso.,"In the sparse linear regression setting, we consider testing the significance of the predictor variable that enters the current lasso model, in the sequence of models visited along the lasso solution path. We propose a simple test statistic based on lasso fitted values, called the covariance test statistic, and show that when the true model is linear, this statistic has an Exp(1) asymptotic distribution under the null hypothesis (the null being that all truly active variables are contained in the current lasso model). Our proof of this result for the special case of the first predictor to enter the model (i.e., testing for a single significant predictor variable against the global null) requires only weak assumptions on the predictor matrix X. On the other hand, our proof for a general step in the lasso path places further technical assumptions on X and the generative model, but still allows for the important high-dimensional case p > n, and does not necessarily require that the current lasso model achieves perfect recovery of the truly active variables. Of course, for testing the significance of an additional variable between two nested linear models, one typically uses the chi-squared test, comparing the drop in residual sum of squares (RSS) to a [Formula: see text] distribution. But when this additional variable is not fixed, and has been chosen adaptively or greedily, this test is no longer appropriate: adaptivity makes the drop in RSS stochastically much larger than [Formula: see text] under the null hypothesis. Our analysis explicitly accounts for adaptivity, as it must, since the lasso builds an adaptive sequence of linear models as the tuning parameter Î» decreases. In this analysis, shrinkage plays a key role: though additional variables are chosen adaptively, the coefficients of lasso active variables are shrunken due to the [Formula: see text] penalty. Therefore, the test statistic (which is based on lasso fitted values) is in a sense balanced by these two opposing properties-adaptivity and shrinkage-and its null distribution is tractable and asymptotically Exp(1).",2014,Annals of statistics
Can Automatic Classification Help to Increase Accuracy in Data Collection?,"Abstract Purpose The authors aim at testing the performance of a set of machine learning algorithms that could improve the process of data cleaning when building datasets. Design/methodology/approach The paper is centered on cleaning datasets gathered from publishers and online resources by the use of specific keywords. In this case, we analyzed data from the Web of Science. The accuracy of various forms of automatic classification was tested here in comparison with manual coding in order to determine their usefulness for data collection and cleaning. We assessed the performance of seven supervised classification algorithms (Support Vector Machine (SVM), Scaled Linear Discriminant Analysis, Lasso and elastic-net regularized generalized linear models, Maximum Entropy, Regression Tree, Boosting, and Random Forest) and analyzed two properties: accuracy and recall. We assessed not only each algorithm individually, but also their combinations through a voting scheme. We also tested the performance of these algorithms with different sizes of training data. When assessing the performance of different combinations, we used an indicator of coverage to account for the agreement and disagreement on classification between algorithms. Findings We found that the performance of the algorithms used vary with the size of the sample for training. However, for the classification exercise in this paper the best performing algorithms were SVM and Boosting. The combination of these two algorithms achieved a high agreement on coverage and was highly accurate. This combination performs well with a small training dataset (10%), which may reduce the manual work needed for classification tasks. Research limitations The dataset gathered has significantly more records related to the topic of interest compared to unrelated topics. This may affect the performance of some algorithms, especially in their identification of unrelated papers. Practical implications Although the classification achieved by this means is not completely accurate, the amount of manual coding needed can be greatly reduced by using classification algorithms. This can be of great help when the dataset is big. With the help of accuracy, recall, and coverage measures, it is possible to have an estimation of the error involved in this classification, which could open the possibility of incorporating the use of these algorithms in software specifically designed for data cleaning and classification. Originality/value We analyzed the performance of seven algorithms and whether combinations of these algorithms improve accuracy in data collection. Use of these algorithms could reduce time needed for manual data cleaning.",2016,Journal of Data and Information Science
"A Network and Machine Learning Approach to Factor, Asset, and Blended Allocation","The main idea of this article is to approach and compare factor and asset allocation portfolios using both traditional and alternative allocation techniques: inverse variance optimization, minimum-variance optimization, and centrality-based techniques from network science. Analysis of the interconnectedness between assets and factors shows that their relationship is strong. The authors compare the allocation techniques, considering centrality and hierarchal-based networks. They demonstrate the advantages of graph theory to explain the advantages to portfolio management and the dynamic nature of assets and factors with their â€œimportance score.â€ They find that asset allocation can be efficiently derived using directed networks, dynamically driven by both US Treasuries and currency returns with significant centrality scores. Alternatively, the inverse variance weight estimation and correlation-based networks generate factor allocation with favorable riskâ€“return parameters. Furthermore, factor allocation is driven mostly by the importance scores of the Famaâ€“Frenchâ€“Carhart factors: SMB, HML, CMA, RMW, and MOM. The authors confirm previous results and argue that both factors and assets are interconnected with different value and momentum factors. Therefore, a blended strategy comprising factors and assets can be defensible for investors. As argued in previous research, factors are much more overcrowded than assets. Therefore, the centrality scores help to identify the crowded exposure and build diversified allocation. The authors run LASSO regressions and show how the network-based allocation can be implemented using machine learning. TOPICS:Factor-based models, portfolio theory, portfolio construction Key Findings â€¢ The authors compare network-based asset and factor allocations and blended strategies and argue that network analysis can be used to derive allocation, monitor interactions, and provide additional layers of risk control mechanisms. â€¢ The authors find that factors and assets are strongly interconnected. Investors should pay close attention to currencies and the Famaâ€“Frenchâ€“Carhart factors (RMW, HML, CMA) because they have large centrality scores. â€¢ Using machine learning and predictive models, investors can find asset and factor allocation solutions. The authors argue that factor exposure is desired within asset allocation.",2020,The Journal of Portfolio Management
Comments on: l1-penalization for mixture regression models,"I would like to start my discussion by congratulating the authors on this fine piece of work. The model they have considered exhibits significant practical interest, as they nicely illustrate through their real-data example. At the same time it poses some interesting theoretical challenges, brilliantly addressed by the authors. I do not plan to discuss all of them, but rather will focus on the oracle properties of the proposed estimators. A very attractive feature of lasso-type estimators is that they allow one to do estimation and model selection in a single step, with a typically low computational cost. Trying to be a bit more precise, let us consider the usual linear regression model with Gaussian errors yi = si + ÏƒÎµi, i = 1, . . . , n, (1) with Îµi i.i.d. standard normal. We may have covariates xi âˆˆ R and try to select a model among the collection",2010,TEST
Enhanced classical dysphonia measures and sparse regression for telemonitoring of Parkinson's disease progression,"Dysphonia measures are signal processing algorithms that offer an objective method for characterizing voice disorders from recorded speech signals. In this paper, we study disordered voices of people with Parkinson's disease (PD). Here, we demonstrate that a simple logarithmic transformation of these dysphonia measures can significantly enhance their potential for identifying subtle changes in PD symptoms. The superiority of the log-transformed measures is reflected in feature selection results using Bayesian Least Absolute Shrinkage and Selection Operator (LASSO) linear regression. We demonstrate the effectiveness of this enhancement in the emerging application of automated characterization of PD symptom progression from voice signals, rated on the Unified Parkinson's Disease Rating Scale (UPDRS), the gold standard clinical metric for PD. Using least squares regression, we show that UPDRS can be accurately predicted to within six points of the clinicians' observations.",2010,"2010 IEEE International Conference on Acoustics, Speech and Signal Processing"
Rectal Microbiome Alterations Associated With Oral Human Immunodeficiency Virus Pre-Exposure Prophylaxis,"Background
Oral daily tenofovir (TFV) disoproxil fumarate/emtricitabine (TDF/FTC) for human immunodeficiency virus (HIV) pre-exposure prophylaxis (PrEP) is highly effective for HIVprevention, yet long-term effects are not fully understood. We investigated the effects of PrEP on the rectal microbiome in a cohort of men who have sex with men (MSM).


Methods
This cross-sectional analysis included HIV-negative MSM either on PrEP (n = 37) or not (n = 37) selected from an ongoing cohort using propensity score matching. Rectal swabs were used to examine microbiome composition using 16S ribosomal ribonucleic acid gene sequencing, and associations between PrEP use and microbiota abundance were examined. Hair specimens were used to quantify TFV and FTC exposure over the past 6 weeks on a subset of participants (n = 15).


Results
Pre-exposure prophylaxis use was associated with a significant increase in Streptococcus abundance (adjusted P = .015). Similar associations were identified using least absolute shrinkage and selection operator (LASSO) regression, confirming the increase in Streptococcus and also showing increased Mitsuokella, Fusobacterium, and decreased Escherichia/Shigella. Increased Fusobacterium was significantly associated with increasing TFV exposure.


Conclusions
Oral TDF/FTC for PrEP is associated with rectal microbiome changes compared to well matched controls, specifically increased Streptococcus and Fusobacterium abundance. This study highlights the need for future investigations of the role of microbiome changes on HIV susceptibility and effectiveness of PrEP.",2019,Open Forum Infectious Diseases
GeneExpressScore Signature: a robust prognostic and predictive classifier in gastric cancer,"Although several prognostic signatures have been developed for gastric cancer (GC), the utility of these tools is limited in clinical practice due to lack of validation with large and multiple independent cohorts, or lack of a statistical test to determine the robustness of the predictive models. Here, a prognostic signature was constructed using a least absolute shrinkage and selection operator (LASSO) Cox regression model and a training dataset with 300 GC patients. The signature was verified in three independent datasets with a total of 658 tumors across multiplatforms. A nomogram based on the signature was built to predict disease-free survival (DFS). Based on the LASSO model, we created a GeneExpressScore signature (GESGC ) classifier comprised of eight mRNA. With this classifier patients could be divided into two subgroups with distinctive prognoses [hazard ratio (HR)Â =Â 4.00, 95% confidence interval (CI)Â =Â 2.41-6.66, PÂ <Â 0.0001]. The prognostic value was consistently validated in three independent datasets. Interestingly, the high-GESGC group was associated with invasion, microsatellite stable/epithelial-mesenchymal transition (MSS/EMT), and genomically stable (GS) subtypes. The predictive accuracy of GESGC also outperformed five previously published signatures. Finally, a well-performed nomogram integrating the GESGC and four clinicopathological factors was generated to predict 3- and 5-year DFS. In summary, we describe an eight-mRNA-based signature, GESGC , as a predictive model for disease progression in GC. The robustness of this signature was validated across patient series, populations, and multiplatform datasets.",2018,Molecular Oncology
Conditional Granger Causality and Genetic Algorithms in VAR Model Selection,"Overcoming symmetry in combinatorial evolutionary algorithms is a challenge for existing niching methods. This research presents a genetic algorithm designed for the shrinkage of the coefficient matrix in vector autoregression (VAR) models, constructed on two pillars: conditional Granger causality and Lasso regression. Departing from a recent information theory proof that Granger causality and transfer entropy are equivalent, we propose a heuristic method for the identification of true structural dependencies in multivariate economic time series. Through rigorous testing, both empirically and through simulations, the present paper proves that genetic algorithms initialized with classical solutions are able to easily break the symmetry of random search and progress towards specific modeling.",2019,Symmetry
Predicting ship fuel consumption based on LASSO regression,"Abstract During the voyage, predicting fuel consumption of ships under different sea-states and weather conditions has been a challenging and far-reaching topic, because there are a great number of feature variables affecting the fuel consumption, including main-engine status, cargo weight, ship draft, sea-states and weather conditions, etc. Data driven statistical models have been employed to model the relationship between fuel consumption rate and these voyage parameters. However, some of the feature variables are highly correlated, e.g. wind speed and wave height, air pressure and wind force, cargo weight and draft etc., thus a typical multiple collinearity problem arises so that the fuel consumption cannot be accurately calculated by using the traditional multiple linear regression. In this study, the LASSO (Least Absolute Shrinkage and Selection Operator) regression algorithm is employed to implement the variable selection for these feature variables, additionally, it guides the trained predictor towards a generalizable solution, thereby improving the interpretability and accuracy of the model. On the basis of the LASSO, a novel ship fuel consumption prediction model is proposed. Experimentally, the superiority of the proposed method was confirmed by comparing it with some existing methods on predicting the fuel consumption. The proposed method is a promising development that improves the calculation of the fuel consumption.",2017,Transportation Research Part D-transport and Environment
A multimodal approach to distinguish MCI-C from MCI-NC subjects,"Alzheimerâ€™s Disease (AD) is one of the most common neurodegenerative diseases, affecting 60-80% from all dementia cases. Unfortunately, the cure for AD is still not known and only some treatments can be done in its early stages to slow up the symptoms and cognitive decline, avoiding worst patientsâ€™ living conditions. As most of the AD diagnoses are late, it increases the difficulty of applying the strategies and treatments available. Therefore, current studies aim at detecting AD at an early stage. For this purpose, they are studying mild cognitive impairment (MCI) subjects, as this is normally the first condition before developing AD. Nonetheless, not all MCI patients convert to AD, some remain stable or even may reverse the cognitive decline. In this sense, being able to distinguish between MCI-converters (MCI-C) and MCI-non converters (MCI-NC) reveals a quite important task. In order to distinguish between these and other groups of subjects many classifiers can be used. Classifiers are machine learning algorithms which apply artificial intelligence. These are extremely useful to identify patterns in, for example, medical brain images, to find disease related patterns and try to achieve an early and reliable diagnosis. The Support Vector Machine (SVM) is a widely used classifier for AD studies and is very appealing as it deals well with high-dimensional problems, which is present when using neuroimages because of the high number of voxels in each image. Nonetheless, SVM is a non-probabilistic classifier and only provides the class predicted for a given test. In a clinical perspective, it would be advantageous to also have a confidence level about the prediction made, to avoid diagnosis being hampered by overconfidence. Hence, of late the interest in probabilistic classifiers is rising. The Logistic Regression (LR) and the Gaussian Process (GP) are examples of probabilistic classifiers, but few studies used these methods to present results for AD classification, additionally the analysis of the posterior probability given by these classifiers is also still not well explored. In this context, this thesis proposes the comparison of the performance of probabilistic (LR and GP) and non-probabilistic (SVM) classifiers for AD context with special interest in reaching good results for MCI-C vs MCI-NC. These tests were done using two neuroimaging modalities: the deoxyglucose Positron Emission Tomography (FDG-PET) and structural Magnetic Resonance Imaging (sMRI), in single modal and multimodal approach. A whole-brain approach was chosen, to avoid restringing the model just for certain brain regions. For feature selection methods, the LASSO and group LASSO with L1/L2 regularization, for both single and multimodality cases, were used respectively. Four different binary classification tests involving AD, MCI and elderly cognitive normal (CN) subjects from the Alzheimerâ€™s Disease Neuroimaging Initiative (ADNI) database,",2016,
Variable Selection Properties of L1 Penalized Regression in Generalized Linear Models,"Title of dissertation: VARIABLE SELECTION PROPERTIES OF L1 PENALIZED REGRESSION IN GENERALIZED LINEAR MODELS Chon Sam, Doctor of Philosophy, 2008 Dissertation directed by: Professor Paul J. Smith A hierarchical Bayesian formulation in Generalized Linear Models (GLMs) is proposed in this dissertation. Under this Bayesian framework, empirical and fully Bayes variable selection procedures related to Least Absolute Selection and Shrinkage Operator (LASSO) are developed. By specifying a double exponential prior for the covariate coefficients and prior probabilities for each candidate model, the posterior distribution of candidate model given data is closely related to LASSO, which shrinks some coefficient estimates to zero, thereby performing variable selection. Various variable selection criteria, empirical Bayes (CML) and fully Bayes under the conjugate prior (FBC Conj), with flat prior (FBC Flat) a special case, are given explicitly for linear, logistic and Poisson models. Our priors are data dependent, so we are performing a version of objective Bayes analysis. Consistency of Lp penalized estimators in GLMs is established under regularity conditions. We also derive the limiting distribution of âˆš n times the estimation error for Lp penalized estimators in GLMs. Simulation studies and data analysis results of the Bayesian criteria mentioned above are carried out. They are also compared to the popular information criteria, Cp, AIC and BIC. The simulations yield the following findings. The Bayesian criteria behave very differently in linear, Poisson and logistic models. For logistic models, the performance of CML is very impressive, but it seldom does any variable selection in Poisson cases. The CML performance in the linear case is somewhere in between. In the presence of a predictor coefficient nearly zero and some significant predictors, CML picks out the significant predictors most of the time in the logistic case and fairly often in the linear case, while FBC Conj tends to select the significant predictors equally well in all linear, Poisson and logistic models. The behavior of fully Bayes criteria depends strongly on their chosen priors for the Poisson and logistic cases, but not in the linear case. From the simulation studies, the Bayesian criteria are generally more likely than Cp and AIC to choose correct predictors.",2008,
AUTALASSO: an automatic adaptive LASSO for genome-wide prediction,"BackgroundGenome-wide prediction has become the method of choice in animal and plant breeding. Prediction of breeding values and phenotypes are routinely performed using large genomic data sets with number of markers on the order of several thousands to millions. The number of evaluated individuals is usually smaller which results in problems where model sparsity is of major concern. The LASSO technique has proven to be very well-suited for sparse problems often providing excellent prediction accuracy. Several computationally efficient LASSO algorithms have been developed, but optimization of hyper-parameters can be demanding.ResultsWe have developed a novel automatic adaptive LASSO (AUTALASSO) based on the alternating direction method of multipliers (ADMM) optimization algorithm. The two major hyper-parameters of ADMM are the learning rate and the regularization factor. The learning rate is automatically tuned with line search and the regularization factor optimized using Golden section search. Results show that AUTALASSO provides superior prediction accuracy when evaluated on simulated and real bull data compared to the adaptive LASSO, LASSO and ridge regression implemented in the popular glmnet software.ConclusionsThe AUTALASSO provides a very flexible and computationally efficient approach to GWP, especially when it is important to obtain high prediction accuracy and genetic gain. The AUTALASSO also has the capability to perform GWAS of both additive and dominance effects with smaller prediction error than the ordinary LASSO.",2019,BMC Bioinformatics
Characterization of the equivalence of robustification and regularization in linear and matrix regression,"Sparsity is a key driver in modern statistical problems, from linear regression via the Lasso to matrix regression with nuclear norm penalties in matrix completion and beyond. In stark contrast to sparsity motivations for such problems, it is known in the field of robust optimization that a variety of vector regression problems, such as Lasso which appears as a loss function plus a regularization penalty, can arise by simply immunizing a nominal problem (with only a loss function) to uncertainty in the data. Such a robustification offers an explanation for why some linear regression methods perform well in the face of noise, even when these methods do not produce reliably sparse solutions. In this paper we deepen and extend the understanding of the connection between robustification and regularization in regression problems. Specifically, (a) in the context of linear regression, we characterize under which conditions on the model of uncertainty used and on the loss function penalties robustification and regularization are equivalent; (b) we show how to tractably robustify median regression problems; and (c) we extend the characterization of robustification and regularization to matrix regression problems (matrix completion and Principal Component Analysis).",2018,Eur. J. Oper. Res.
Penalized robust regression in high-dimension,We discuss the behavior of penalized robust regression estimators in high-dimension and compare our theoretical predictions to simulations. Our results show the importance of the geometry of the dataset and shed light on the theoretical behavior of LASSO and much more involved methods.,2011,
Effects of univariate and multivariate regression on the accuracy of hydrogen quantification with laser-induced breakdown spectroscopy,"Abstract Hydrogen (H) is a critical element to measure on the surface of Mars because its presence in mineral structures is indicative of past hydrous conditions. The Curiosity rover uses the laser-induced breakdown spectrometer (LIBS) on the ChemCam instrument to analyze rocks for their H emission signal at 656.6Â nm, from which H can be quantified. Previous LIBS calibrations for H used small data sets measured on standards and/or manufactured mixtures of hydrous minerals and rocks and applied univariate regression to spectra normalized in a variety of ways. However, matrix effects common to LIBS make these calibrations of limited usefulness when applied to the broad range of compositions on the Martian surface. In this study, 198 naturally-occurring hydrous geological samples covering a broad range of bulk compositions with directly-measured H content are used to create more robust prediction models for measuring H in LIBS data acquired under Mars conditions. Both univariate and multivariate prediction models, including partial least square (PLS) and the least absolute shrinkage and selection operator (Lasso), are compared using several different methods for normalization of H peak intensities. Data from the ChemLIBS Mars-analog spectrometer at Mount Holyoke College are compared against spectra from the same samples acquired using a ChemCam-like instrument at Los Alamos National Laboratory and the ChemCam instrument on Mars. Results show that all current normalization and data preprocessing variations for quantifying H result in models with statistically indistinguishable prediction errors (accuracies) ca. Â±Â 1.5Â weight percent (wt%) H2O, limiting the applications of LIBS in these implementations for geological studies. This error is too large to allow distinctions among the most common hydrous phases (basalts, amphiboles, micas) to be made, though some clays (e.g., chlorites with â‰ˆÂ 12Â wt% H2O, smectites with 15â€“20Â wt% H2O) and hydrated phases (e.g., gypsum with â‰ˆÂ 20Â wt% H2O) may be differentiated from lower-H phases within the known errors. Analyses of the H emission peak in Curiosity calibration targets and rock and soil targets on the Martian surface suggest that shot-to-shot variations of the ChemCam laser on Mars lead to variations in intensity that are comparable to those represented by the breadth of H standards tested in this study.",2018,Spectrochimica Acta Part B: Atomic Spectroscopy
Application of Penalized Regression Techniques in Modelling Insulin Sensitivity by Correlated Metabolic Parameters,"This paper aims to introduce penalized estimation techniques in clinical investigations of diabetes, as well as to assess their possible advantages and limitations. Data from a previous study was used to carry out the simulations to assess: a) which procedure results in the lowest prediction error of the final model in the setting of a large number of predictor variables with high multicollinearity (of importance if insulin sensitivity should be predicted) and b) which procedure achieves the most accurate estimate of regression coefficients in the setting of fewer predictors with small unidirectional effects and moderate correlation between explanatory variables (of importance if the specific relation between an independent variable and insulin sensitivity should be examined). Moreover a special focus is on the correct direction of estimated parameter effects, a non-negligible source of error and misinterpretation of study results. The simulations were performed for varying sample size to evaluate the performance of LASSO, Ridge as well as different algorithms for Elastic Net. These methods were also compared with automatic variable selection procedures (i.e. optimizing AIC or BIC).We were not able to identify one method achieving superior performance in all situations. However, the improved accuracy of estimated effects underlines the importance of using penalized regression techniques in our example (e.g. if a researcher aims to compare relations of several correlated parameters with insulin sensitivity). However, the decision which procedure should be used depends on the specific context of a study (accuracy versus complexity) and moreover should involve clinical prior knowledge.",2015,PLoS ONE
High-Sensitivity Determination of Nutrient Elements in Panax notoginseng by Laser-induced Breakdown Spectroscopy and Chemometric Methods,"High-accuracy and fast detection of nutritive elements in traditional Chinese medicine Panax notoginseng (PN) is beneficial for providing useful assessment of the healthy alimentation and pharmaceutical value of PN herbs. Laser-induced breakdown spectroscopy (LIBS) was applied for high-accuracy and fast quantitative detection of six nutritive elements in PN samples from eight producing areas. More than 20,000 LIBS spectral variables were obtained to show elemental differences in PN samples. Univariate and multivariate calibrations were used to analyze the quantitative relationship between spectral variables and elements. Multivariate calibration based on full spectra and selected variables by the least absolute shrinkage and selection operator (Lasso) weights was used to compare the prediction ability of the partial least-squares regression (PLS), least-squares support vector machines (LS-SVM), and Lasso models. More than 90 emission lines for elements in PN were found and located. Univariate analysis was negatively interfered by matrix effects. For potassium, calcium, magnesium, zinc, and boron, LS-SVM models based on the selected variables obtained the best prediction performance with Rp values of 0.9546, 0.9176, 0.9412, 0.9665, and 0.9569 and root mean squared error of prediction (RMSEP) of 0.7704 mg/g, 0.0712 mg/g, 0.1000 mg/g, 0.0012 mg/g, and 0.0008 mg/g, respectively. For iron, the Lasso model based on full spectra obtained the best result with an Rp value of 0.9348 and RMSEP of 0.0726 mg/g. The results indicated that the LIBS technique coupled with proper multivariate chemometrics could be an accurate and fast method in the determination of PN nutritive elements for traditional Chinese medicine management and pharmaceutical analysis.",2019,Molecules
Exploratory Determined Correlates of Physical Activity in Children and Adolescents: The MoMo Study,"Background: Physical activity is an important contributor to reducing the risk for a variety of diseases. Understanding why people are physically active contributes to evidence-based planning of public health interventions because successful actions will target factors known to be related to physical activity (PA). Therefore the aim of this study is to identify the most meaningful correlates of PA in children and adolescents using a large, representative data set. Methods: Among n = 3539 (1801 boys) 6 to 17-year-old participants of the German representative Motorik-Modul baseline study (2003â»2006) a total of 1154 different demographic, psychological, behavioral, biological, social and environmental factors were ranked according to their power of predicting PA using least absolute shrinkage and selection operator (LASSO) regressions. Results: A total of 18 (in girls) and 19 (in boys) important PA predictors from different, personal, social and environmental factors have been identified and ranked by LASSO. Peer modeling and physical self-concept were identified as the strongest correlates of PA in both boys and girls. Conclusions: The results confirm that PA interventions must target changes in different categories of PA correlates, but we suggest to focus particularly on the social environment and physical self-concept for interventions targeting children and adolescents in Germany nowadays. We also strongly recommend to repeatedly track correlates of PA, at least every 10 years, from representative samples in order to tailor contemporary PA interventions.",2019,International Journal of Environmental Research and Public Health
The Multifaceted Impact of Statistical Methodology and Theory in Data Science,"The vast amount of recorded data and the exponential growth of computational power in the last two decades have enabled the extraction and processing of information in unprecedented ways. This has led to the emergence of data science as a bridge between data mining, algorithm design, modeling, machine learning, visualization, and artificial intelligence, in an effort to understand and gain deeper insights from data in various forms. In turn, this has caused a resurgence of statistical science, due to the prominent role of statistical methodology and statistical theory in data science. This special issue of Mathematics for Applications features a compilation of contributions dealing with the multifaceted impact of statistical methodology and theory in data science. The works featured in this special issue have initially been reported at the conferences UP-STAT 2016 â€œData Science, Statistical Practice, and Educationâ€ and UP-STAT 2017 â€œData Science, Statistics, and the Environment.â€ After a careful selection and a rigorous review process, seven substantially extended papers out of over 100 works presented at both conferences have been selected for publication in this issue. These works contribute to the theoretical foundations of data science and explore mathematical models and methods for solving problems in applied fields. One of the most prominent ways in which statistical science has enriched data science is through the introduction of regularization methods, such as ridge regression and LASSO, which are used to handle ultra-high dimensional or multicollinear datasets. In the first paper of this special issue, the authors Y. Zhang, J. Thakar, D.J. Topham, A.R. Falsey, D. Zeng and X. Qiu construct two useful equivalence relationships for regularized regression â€“ one for efficiently fitting the concurrent functional regression model, and the second for efficiently solving weighted principal component regression. The development of ensemble learning methods has complemented traditional model selection and regularization approaches, giving experimenters and end-users a rich arsenal of powerful statistical learning methods. Spearheaded by techniques such as random forest, bagging, adaptive boosting, gradient boosting, and random subspace learning, ensemble learning methods have proven valuable in the study and analysis of increasingly larger and more complex data sets. In the second paper, M. Elshrif and E. FokouÃ© present a novel adaptation of the random subspace learning approach to regression analysis and classification of high-dimension, lowsample-size data.",2018,
A risk signature with four autophagyâ€related genes for predicting survival of glioblastoma multiforme,"Glioblastoma multiforme (GBM) is a devastating brain tumour without effective treatment. Recent studies have shown that autophagy is a promising therapeutic strategy for GBM. Therefore, it is necessary to identify novel biomarkers associated with autophagy in GBM. In this study, we downloaded autophagy-related genes from Human Autophagy Database (HADb) and Gene Set Enrichment Analysis (GSEA) website. Least absolute shrinkage and selection operator (LASSO) regression and multivariate Cox regression analysis were performed to identify genes for constructing a risk signature. A nomogram was developed by integrating the risk signature with clinicopathological factors. Time-dependent receiver operating characteristic (ROC) curve and calibration plot were used to evaluate the efficiency of the prognostic model. Finally, four autophagy-related genes (DIRAS3, LGALS8, MAPK8 and STAM) were identified and were used for constructing a risk signature, which proved to be an independent risk factor for GBM patients. Furthermore, a nomogram was developed based on the risk signature and clinicopathological factors (IDH1 status, age and history of radiotherapy or chemotherapy). ROC curve and calibration plot suggested the nomogram could accurately predict 1-, 3- and 5-year survival rate of GBM patients. For function analysis, the risk signature was associated with apoptosis, necrosis, immunity, inflammation response and MAPK signalling pathway. In conclusion, the risk signature with 4 autophagy-related genes could serve as an independent prognostic factor for GBM patients. Moreover, we developed a nomogram based on the risk signature and clinical traits which was validated to perform better for predicting 1-, 3- and 5-year survival rate of GBM.",2020,Journal of Cellular and Molecular Medicine
Robust Bayesian Lasso Regression,"Choosing the best subset is one of the key issues in formulating regression models. The purpose of these methods is to determine the predictor variables and to distinguish them from tolerable predictor variables so that the accuracy of the estimates and hence the accuracy of the future observations increases. When the number of the variables of the regression model is more than the number of the observations, or if there is a linear correlation between the predictors, choosing a variable with frequentist methods is usually inefficient and therefore the importance of Bayesian methods becomes more obvious. Among these methods, the Bayesian Lasso regression method is an appropriate choice, because by using this method we can compute the uncertainty level in any chosen model, however it is not the case in standard Lasso. Considering the fact that this method is sensitive to outliers, the convergence of the parameter estimation algorithm will decrease, therefore, it is better to use the robust methods. In this article, the Robust Bayesian Lasso regression is presented and its priority over the Bayesian Lasso regression will be mentioned.",2016,Journal of Fundamental and Applied Sciences
` 1-Penalised Ordinal Polytomous Regression Estimators with Application to Gene Expression Studies,"Qualitative but ordered random variables, such as severity of a pathology, are of paramount importance in biostatistics and medicine. Understanding the conditional distribution of such qualitative variables as a function of other explanatory variables can be performed using a specific regression model known as ordinal polytomous regression. Variable selection in the ordinal polytomous regression model is a computationally difficult combinatorial optimisation problem which is however crucial when practitioners need to understand which covariates are physically related to the output and which covariates are not. One easy way to circumvent the computational hardness of variable selection is to introduce a penalised maximum likelihood estimator based on some well chosen non-smooth penalisation function such as, e.g., the `1-norm. In the case of the Gaussian linear model, the `1-penalised least-squares estimator, also known as LASSO estimator, has attracted a lot of attention in the last decade, both from the theoretical and algorithmic viewpoints. However, even in the Gaussian linear model, accurate calibration of the relaxation parameter, i.e., the relative weight of the penalisation term in the estimation cost function is still considered a difficult problem that has to be addressed with caution. In the present paper, we apply `1-penalisation to the ordinal polytomous regression model and compare several hyper-parameter calibration strategies. Our main contributions are: (a) a useful and simple `1 penalised estimator for ordinal polytomous regression and a thorough description of how to apply Nesterovâ€™s accelerated gradient and the online Frank-Wolfe methods to the problem of computing this estimator, (b) a new hyper-parameter calibration method for the proposed model, based on the QUT idea of Giacobino et al. and (c) a code which can be freely used that implements the proposed estimation procedure. 2012 ACM Subject Classification Mathematics of computing â†’ Regression analysis",2018,
Twin Boosting: improved feature selection and prediction,"We propose Twin Boosting which has much better feature selection behavior than boosting, particularly with respect to reducing the number of false positives (falsely selected features). In addition, for cases with a few important effective and many noise features, Twin Boosting also substantially improves the predictive accuracy of boosting. Twin Boosting is as general and generic as (gradient-based) boosting. It can be used with general weak learners and in a wide variety of situations, including generalized regression, classification or survival modeling. Furthermore, it is computationally feasible for large problems with potentially many more features than observed samples. Finally, for the special case of orthonormal linear models, we prove equivalence of Twin Boosting to the adaptive Lasso which provides some theoretical aspects on feature selection with Twin Boosting.",2010,Statistics and Computing
