title,abstract,year,journal
Optimal and sequential design for bridge regression with application in organic chemistry,"This thesis presents and applies methods for the design and analysis of experiments for a family of coefficient shrinkage methods, known collectively as bridge regression, with emphasis on the two special cases of ridge regression and the lasso. The application is the problem of understanding and predicting the melting point of small molecule organic compounds using chemical descriptors. Experiments typically have a large number of predictors compared to the number of observations, and high correlations between pairs of predictors. In this thesis, bridge regression is used to select linear models which are then compared to models selected by more commonly used methods of variable selection, such as subset selection and stepwise selection. Models including two-way product, or interaction, terms are also considered. A general method is developed for the selection of an optimal design when accurate estimates of the model coefficients are required. The method exploits a relationship between bridge regression and Bayesian methods which is used to develop a class of D-optimal designs. A necessary approximation to the variance-covariance matrix of coefficient estimators is derived. Designs are found using algorithmic search for ridge regression and the lasso, for experiments with (a) two-level factors and (b) the motivating chemistry problem. Comparisons are made with alternative designs. A sequential design criterion is developed to enhance an existing design. The criterion selects additional design points, from a finite set of candidate points, that exhibit the highest estimated prediction variance obtained from bootstrapping. The method is applied to the Bayesian D-optimal designs and is shown to be capable of improving design performance through the addition of only a small number of runs",2011,
The diagnostic nomogram of platelet-based score models for hepatic alveolar echinococcosis and atypical liver cancer,"Hepatic alveolar echinococcosis (HAE) and liver cancer had similarities in imaging results, clinical characteristics, and so on. And it is difficult for clinicians to distinguish them before operation. The aim of our study was to build a differential diagnosis nomogram based on platelet (PLT) score model and use internal validation to check the model. The predicting model was constructed by the retrospective database that included in 153 patients with HAE (66 cases) or liver cancer (87 cases), and all cases was confirmed by clinicopathology and collected from November 2011 to December 2018. Lasso regression analysis model was used to construct data dimensionality reduction, elements selection, and building prediction model based on the 9 PLT-based scores. A multi-factor regression analysis was performed to construct a simplified prediction model, and we added the selected PLT-based scores and relevant clinicopathologic features into the nomogram. Identification capability, calibration, and clinical serviceability of the simplified model were evaluated by the Harrellâ€™s concordance index (C-index), calibration plot, receiver operating characteristic curve (ROC), and decision curve. An internal validation was also evaluated by the bootstrap resampling. The simplified model, including in 4 selected factors, was significantly associated with differential diagnosis of HAE and liver cancer. Predictors of the simplified diagnosis nomogram consisted of the API index, the FIB-4 index, fibro-quotent (FibroQ), and fibrosis index constructed by Kingâ€™s College Hospital (Kingâ€™s score). The model presented a perfect identification capability, with a high C-index of 0.929 (0.919 through internal validation), and good calibration. The area under the curve (AUC) values of this simplified prediction nomogram was 0.929, and the result of ROC indicated that this nomogram had a good predictive value. Decision curve analysis showed that our differential diagnosis nomogram had clinically identification capability. In conclusion, the differential diagnosis nomogram could be feasibly performed to verify the preoperative individualized diagnosis of HAE and liver cancer.",2019,Scientific Reports
Pulse Wave Velocity and Machine Learning to Predict Cardiovascular Outcomes in Prediabetic and Diabetic Populations,"Few studies have addressed the predictive value of arterial stiffness determined by pulse wave velocity (PWV) in a high-risk population with no prevalent cardiovascular disease and with obesity, hypertension, hyperglycemia, and preserved kidney function. This longitudinal, retrospective study enrolled 88 high-risk patients and had a follow-up time of 12.4 years. We collected clinical and laboratory data, as well as information on arterial stiffness parameters using arterial tonometry and measurements from ambulatory blood pressure monitoring. We considered nonfatal, incident cardiovascular events as the primary outcome. Given the small size of our dataset, we used survival analysis (i.e., Cox proportional hazards model) combined with a machine learning-based algorithm/penalization method to evaluate the data. Our predictive model, calculated with Cox regression and least absolute shrinkage and selection operator (LASSO), included body mass index, diabetes mellitus, gender (male), and PWV. We recorded 16 nonfatal cardiovascular events (5 myocardial infarctions, 5 episodes of heart failure, and 6 strokes). The adjusted hazard ratio for PWV was 1.199 (95% confidence interval: 1.09â€“1.37, pâ€‰<â€‰0.001). Arterial stiffness was a predictor of cardiovascular disease development, as determined by PWV in a high-risk population. Thus, in obese, hypertensive, hyperglycemic patients with preserved kidney function, PWV can serve as a prognostic factor for major adverse cardiac events.",2019,Journal of Medical Systems
Experimental assessment of the accuracy of genomic selection in sugarcane,"Sugarcane cultivars are interspecific hybrids with an aneuploid, highly heterozygous polyploid genome. The complexity of the sugarcane genome is the main obstacle to the use of marker-assisted selection in sugarcane breeding. Given the promising results of recent studies of plant genomic selection, we explored the feasibility of genomic selection in this complex polyploid crop. Genetic values were predicted in two independent panels, each composed of 167 accessions representing sugarcane genetic diversity worldwide. Accessions were genotyped with 1,499 DArT markers. One panel was phenotyped in Reunion Island and the other in Guadeloupe. Ten traits concerning sugar and bagasse contents, digestibility and composition of the bagasse, plant morphology, and disease resistance were used. We used four statistical predictive models: bayesian LASSO, ridge regression, reproducing kernel Hilbert space, and partial least square regression. The accuracy of the predictions was assessed through the correlation between observed and predicted genetic values by cross validation within each panel and between the two panels. We observed equivalent accuracy among the four predictive models for a given trait, and marked differences were observed among traits. Depending on the trait concerned, within-panel cross validation yielded median correlations ranging from 0.29 to 0.62 in the Reunion Island panel and from 0.11 to 0.5 in the Guadeloupe panel. Cross validation between panels yielded correlations ranging from 0.13 for smut resistance to 0.55 for brix. This level of correlations is promising for future implementations. Our results provide the first validation of genomic selection in sugarcane.",2013,Theoretical and Applied Genetics
An Alpha-Defensin Gene Single Nucleotide Polymorphism Modulates the Gut Microbiota and May Alter the Risk of Acute Graft-Versus-Host Disease,"Î±â€defensinâ€5 (HD5) accounts for 70% of the bactericidal peptide activity of Paneth cells and regulates gut microbial homeostasis. We previously reported that the G allele of rs4415345, a single nucleotide polymorphism (SNP) in the gene for HD5, was associated with approximately 40% lower risk of grade II-IV aGVHD (PMID: 30004111). Considering the role of microbiota disruptions in aGVHD, we hypothesized that rs4415345G may have a specific gut microbiota signature. First, we analyzed linked host-microbiota genetic data from 248 subjects in the Human Microbiome Project to test our hypothesis in healthy individuals. With the assumption that many microbial taxa have no association with a given SNP, we used 10-fold cross-validated Lasso regression as a test of association between rs4415345 and taxon relative abundance. A single taxon (Odoribacter splanchnicus) was retained in 99% of the simulations, always with a positive association with the G allele. Next, we evaluated whether O. splanchnicus is associated with less aGVHD in 226 patients with a 16S rRNA gene-sequenced research stool sample collected between days -30 and -6 of allo-HCT. O. splanchnicus was present in 65 (29%) patients. The univariate hazard ratio (HR) of O. splanchnicus detectability for grade II-IV aGVHD was 0.70, with a 95% confidence interval (95%CI) of 0.44-1.10 (Pâ€¯=â€¯0.12). Multivariable analysis, adjusted for anti-anaerobic antibiotic exposure between days -7 and +14 yielded a similar result (HR: 0.70, 95%CI: 0.44-1.11, Pâ€¯=â€¯0.13). In summary, the G allele of rs4610776 is associated with the higher relative abundance of O. splanchnicus in healthy individuals and less aGVHD in allo-HCT recipients. In an independent dataset, O. splanchnicus presence was associated with a 30% reduction in risk for aGVHD, though this association did not reach statistical significance. O. splanchnicus is a weakly butyrogenic bacterium also capable of producing indole, and depleted in patients with IBD. Both butyrate and indole can limit aGVHD. In addition, O. splanchnicus correlates with less TNFÎ± production in response to inflammatory stimuli. We propose that O. splanchnicus may reduce aGVHD risk in rs4415345G individuals.",2020,Biology of Blood and Marrow Transplantation
Preselection bias in high dimensional regression,"In this thesis, we have studied the preselection bias that can occur when the number of covariates in a high dimensional regression problem is reduced prior to a high dimensional regression analysis like the lasso. Datasets in genomics often include tenor hundred thousands, or even millions, of covariates and a few hundred or less patients. To reduce computations or to make the problem tractable, practitioners often rank the covariates according to univariate importance for the response, and preselect some thousand covariates from the top of the list for multivariate analysis via penalized regression. If the preselection of covariates is not done in a controlled way, this leads to preselection bias. We have studied the effect of preselection on estimation and prediction and the bias this might induce. With a small preselected dataset, the lasso in combination with cross validation tends to select many covariates, which together are able to explain the data at hand very well. However, for a new independent dataset, these covariates predict rather poorly. This is preselection bias. We have visualized the preselection bias through boxplots in several different datasets from genomics and in simulated data. We have also demonstrated that the problem of preselection bias is most evident in datasets where there is a lot of noise, and where there are heavy dependencies between covariates, as the univariate ranking will not be able to capture the structure of the complex relations in this case. To be able to trust predictions made from penalized regression on preselected covariates, the preselection should be coupled with some algorithm that controls how many covariates that should be included in order to avoid the bias. We have studied methods like â€œSAFEâ€, â€œstrongâ€ and â€œfreezingâ€ that all make preselection more safe, the word safe meaning that the lasso analysis for the preselected set of covariates should conclude with the same result as if all covariates were included.",2014,
Group sparse Lasso for cognitive network sensing robust to model uncertainties and outliers,"a b s t r a c t To account for variations in the frequency, time, and space dimensions, dynamic re-use of licensed bands under the cognitive radio (CR) paradigm calls for innovative network-level sensing algorithms for multi-dimensional spectrum opportunity awareness. Toward this direction, the present paper develops a collaborative scheme whereby CRs cooperate to localize active primary user (PU) transmitters and reconstruct a power spectral density (PSD) map portraying the spatial distribution of power across the monitored area per frequency band and channel coherence interval. The sensing scheme is based on a parsimonious model that accounts for two forms of sparsity: one due to the narrowband nature of transmit-PSDs compared to the large portion of spectrum that a CR can sense, and another one emerging when adopting a spatial grid of candidate PU locations. Capitalizing on this dual sparsity, an estimator of the model coefficients is obtained based on the group sparse least-absolute-shrinkage-and-selection operator (GS-Lasso). A novel reduced-complexity GS-Lasso solver is developed by resorting to the alternating direction method of multipliers (ADMoM). Robust versions of this GS-Lasso estimator are also introduced using a GS total least-squares (TLS) approach to cope with both uncertainty in the regression matrices, arising due to inaccurate channel estimation and grid-mismatch effects, and unexpected model outliers. In spite of the non-convexity of the GS-TLS criterion, the novel robust algorithm has guaranteed convergence to (at least) a local optimum. The analytical findings are corroborated by numerical tests. Published by Elsevier B.V.",2012,Phys. Commun.
Predicting age at loss of ambulation in Duchenne muscular dystrophy with deep phenotypic measures,"Although Duchenne muscular dystrophy (DMD), the most common single-gene lethal disorder, is caused by a homogeneous biochemical defect in all patients, substantial patient-patient variety in disease progression is observed. The loss of ambulation (LoA) is a functional milestone of DMD progression and the age at LoA is often used as an indication of disease severity. But as age at LoA is not always available, such as when patients remain ambulant at study end, its use has been limited. In this paper, we report machine learning approaches to predict age at LoA based on clinical measures of muscular strength and motor function, and validate the algorithms using the CINRG dataset. With extensive experiments and rigorous statistical analysis, we found that (1) the utilization of multiple clinical features yields better prediction than using any of the single measures, and (2) the prediction based on Lasso is more accurate than other multivariate analytical approaches such as ordinary least squares and ridge regression. To our knowledge, we are the first to provide point predictions for age at LoA in DMD using clinical phenotypic measures. Importantly, we find that not all clinical measures contribute to the prediction. Age at the last visit (before LoA), velocity of walking 10 meters, and velocity of climbing 4 steps are selected as important predictors by Lasso. The usefulness of the prediction model is illustrated with evidence that the association between a well-known modifier of DMD severity and age at LoA has better power when the predicted values are utilized.",2014,2014 IEEE Global Conference on Signal and Information Processing (GlobalSIP)
QSAR Investigation on Quinolizidinyl Derivatives in Alzheimerâ€™s Disease,"Sets of quinolizidinyl derivatives of bi- and tri-cyclic (hetero) aromatic systems were studied as selective inhibitors. On the pattern, quantitative structure-activity relationship (QSAR) study has been done on quinolizidinyl derivatives as potent inhibitors of acetylcholinesterase in alzheimerâ€™s disease (AD). Multiple linear regression (MLR), partial least squares (PLSs), principal component regression (PCR), and least absolute shrinkage and selection operator (LASSO) were used to create QSAR models. Geometry optimization of compounds was carried out by B3LYP method employing 6â€“31â€‰G basis set. HyperChem, Gaussian 98â€‰W, and Dragon software programs were used for geometry optimization of the molecules and calculation of the quantum chemical descriptors. Finally, Unscrambler program was used for the analysis of data. In the present study, the root mean square error of the calibration and R2 using MLR method were obtained as 0.1434 and 0.95, respectively. Also, the R and R2 values were obtained as 0.79, 0.62 from stepwise MLR model. The R2 and mean square values using LASSO method were obtained as 0.766 and 3.226, respectively. The root mean square error of the calibration and R2 using PLS method were obtained as 0.3726 and 0.62, respectively. According to the obtained results, it was found that MLR model is the most favorable method in comparison with other statistical methods and is suitable for use in QSAR models.",2013,
Using Genetic Risk Score Approaches to Infer Whether an Environmental Factor Attenuates or Exacerbates the Adverse Influence of a Candidate Gene,"Some candidate genes have been robustly reported to be associated with complex traits, such as the fat mass and obesity-associated (FTO) gene on body mass index (BMI), and the fibroblast growth factor 5 (FGF5) gene on blood pressure levels. It is of interest to know whether an environmental factor (E) can attenuate or exacerbate the adverse influence of a candidate gene. To this end, we here evaluate the performance of â€œgenetic risk scoreâ€ (GRS) approaches to detect â€œgene-environment interactionsâ€ (G Ã— E). In the first stage, a GRS is calculated according to the genotypes of variants in a candidate gene. In the second stage, we test whether E can significantly modify this GRS effect. This two-stage procedure can not only provide a p-value for a G Ã— E test but also guide inferences on how E modifies the adverse effect of a gene. With systematic simulations, we compared several ways to construct a GRS. If E exacerbates the adverse influence of a gene, GRS formed by the elastic net (ENET) or the least absolute shrinkage and selection operator (LASSO) is recommended. However, the performance of ENET or LASSO will be compromised if E attenuates the adverse influence of a gene, and using the ridge regression (RIDGE) can be more powerful in this situation. Applying RIDGE to 18,424 subjects in the Taiwan Biobank, we showed that performing regular exercise can attenuate the adverse influence of the FTO gene on four obesity measures: BMI (p = 0.0009), body fat percentage (p = 0.0031), waist circumference (p = 0.0052), and hip circumference (p = 0.0001). As another example, we used RIDGE and found the FGF5 gene has a stronger effect on blood pressure in Han Chinese with a higher waist-to-hip ratio [p = 0.0013 for diastolic blood pressure (DBP) and p = 0.0027 for systolic blood pressure (SBP)]. This study provides an evaluation on the GRS approaches, which is important to infer whether E attenuates or exacerbates the adverse influence of a candidate gene.",2020,
Variable Selection for High Dimensional Multivariate Outcomes.,"We consider variable selection for high-dimensional multivariate regression using penalized likelihoods when the number of outcomes and the number of covariates might be large. To account for within-subject correlation, we consider variable selection when a working precision matrix is used and when the precision matrix is jointly estimated using a two-stage procedure. We show that under suitable regularity conditions, penalized regression coefficient estimators are consistent for model selection for an arbitrary working precision matrix, and have the oracle properties and are efficient when the true precision matrix is used or when it is consistently estimated using sparse regression. We develop an efficient computation procedure for estimating regression coefficients using the coordinate descent algorithm in conjunction with sparse precision matrix estimation using the graphical LASSO (GLASSO) algorithm. We develop the Bayesian Information Criterion (BIC) for estimating the tuning parameter and show that BIC is consistent for model selection. We evaluate finite sample performance for the proposed method using simulation studies and illustrate its application using the type II diabetes gene expression pathway data.",2014,Statistica Sinica
"Computational Sufficiency, Reflection Groups, and Generalized Lasso Penalties","We study estimators with generalized lasso penalties within the computational sufficiency framework introduced by Vu (2018, arXiv:1807.05985). By representing these penalties as support functions of zonotopes and more generally Minkowski sums of line segments and rays, we show that there is a natural reflection group associated with the underlying optimization problem. A consequence of this point of view is that for large classes of estimators sharing the same penalty, the penalized least squares estimator is computationally minimal sufficient. This means that all such estimators can be computed by refining the output of any algorithm for the least squares case. An interesting technical component is our analysis of coordinate descent on the dual problem. A key insight is that the iterates are obtained by reflecting and averaging, so they converge to an element of the dual feasible set that is minimal with respect to a ordering induced by the group associated with the penalty. Our main application is fused lasso/total variation denoising and isotonic regression on arbitrary graphs. In those cases the associated group is a permutation group.",2018,arXiv: Statistics Theory
"Toward the International Classification of Functioning, Disability and Health (ICF) Rehabilitation Set: A Minimal Generic Set of Domains for Rehabilitation as a Health Strategy.","OBJECTIVE
To develop a comprehensive set of the International Classification of Functioning, Disability and Health (ICF) categories as a minimal standard for reporting and assessing functioning and disability in clinical populations along the continuum of care. The specific aims were to specify the domains of functioning recommended for an ICF Rehabilitation Set and to identify a minimal set of environmental factors (EFs) to be used alongside the ICF Rehabilitation Set when describing disability across individuals and populations with various health conditions.


DESIGN
Secondary analysis of existing data sets using regression methods (Random Forests and Group Lasso regression) and expert consultations.


SETTING
Along the continuum of care, including acute, early postacute, and long-term and community rehabilitation settings.


PARTICIPANTS
Persons (N=9863) with various health conditions participated in primary studies. The number of respondents for whom the dependent variable data were available and used in this analysis wasÂ 9264.


INTERVENTIONS
Not applicable.


MAIN OUTCOME MEASURES
For regression analyses, self-reported general health was used as a dependent variable. The ICF categories from the functioning component and the EF component were used as independent variables for the development of the ICF Rehabilitation Set and the minimal set of EFs, respectively.


RESULTS
Thirty ICF categories to be complemented with 12 EFs were identified as relevant to the identified ICF sets. The ICF Rehabilitation Set constitutes of 9 ICF categories from the component body functions and 21 from the component activities and participation. The minimal set of EFs contains 12 categories spanning all chapters of the EF component of the ICF.


CONCLUSIONS
The identified sets proposed serve as minimal generic sets of aspects of functioning in clinical populations for reporting data within and across heath conditions, time, clinical settings including rehabilitation, and countries. These sets present a reference framework for harmonizing existing information on disability across general and clinical populations.",2016,Archives of physical medicine and rehabilitation
Search for early pancreatic cancer blood biomarkers in five European prospective population biobanks using metabolomics,"Background and aim Most patients with pancreatic cancer present with advanced disease and die within the first year after diagnosis. Predictive biomarkers that signal the presence of pancreatic cancer in an early stage are desperately needed. We aimed to identify new and validate previously found plasma metabolomic biomarkers associated with early stages of pancreatic cancer. Methods The low incidence rate complicates prospective biomarker studies. Here, we took advantage of the availability of biobanked samples from five large population cohorts (HUNT2, HUNT3, FINRISK, Estonian biobank, Rotterdam Study) and identified prediagnostic blood samples from individuals who were to receive a diagnosis of pancreatic cancer between one month and seventeen years after blood sampling, and compared these with age- and gender-matched controls from the same cohorts. We applied 1H-NMR-based metabolomics on the Nightingale platform on these samples and applied logistic regression to assess the predictive value of individual metabolite concentrations, with gender, age, body mass index, smoking status, type 2 diabetes mellitus status, fasting status, and cohort as covariates. Results After quality assessment, we retained 356 cases and 887 controls. We identified two interesting hits, glutamine (p=0.011) and histidine (p=0.012), and obtained Westfall-Young family-wise error rate adjusted p-values of 0.43 for both. Stratification in quintiles showed a 1.5x elevated risk for the lowest 20% of glutamine and a 2.2x increased risk for the lowest 20% of histidine. Stratification by time to diagnosis (<2 years, 2-5 years, >5 years) suggested glutamine to be involved in an earlier process, tapering out closer to onset, and histidine in a process closer to the actual onset. Lasso-penalized logistic regression showed a slight improvement of the area under the Receiver Operator Curves when including glutamine and histidine in the model. Finally, our data did not support the earlier identified branched-chain amino acids as potential biomarkers for pancreatic cancer in several American cohorts. Conclusion While identifying glutamine and histidine as early biomarkers of potential biological interest, our results imply that a study at this scale does not yield metabolomic biomarkers with sufficient predictive value to be clinically useful per se as prognostic biomarkers.",2019,bioRxiv
Regularized Regression Incorporating Network Information: Simultaneous Estimation of Covariate Coefficients and Connection Signs,"We develop an algorithm that incorporates network information into regression settings. It simultaneously estimates the covariate coefficients and the signs of the network connections (i.e. whether the connections are of an activating or of a repressing type). For the coefficient estimation steps an additional penalty is set on top of the lasso penalty, similarly to Li and Li (2008). We develop a fast implementation for the new method based on coordinate descent. Furthermore, we show how the new methods can be applied to time-to-event data. The new method yields good results in simulation studies concerning sensitivity and specificity of non-zero covariate coefficients, estimation of network connection signs, and prediction performance. We also apply the new method to two microarray time-to-event data sets from patients with ovarian cancer and diffuse large B-cell lymphoma. The new method performs very well in both cases. The main application of this new method is of biomedical nature, but it may also be useful in other fields where network data is available.",2014,
Recent update on COVID-19 in India: Is locking down the country enough?,"Abstract Background: India is the second-largest population in the world, and it is not well equipped, hitherto, in the scenario of the global pandemic, SARS-CoV-2 could impart a devastating impact on the Indian population. Only way to respond against this critical condition is by practicing large-scale social distancing. India lock down for 21 days, however, till 7 April 2020, SARS-CoV-2 positive cases were growing exponentially, which raises the concerns if the number of reported and actual cases are similar. Methods: We use Lasso Regression with Î±=0.12 and Polynomial features of degree 2 to predict the growth factor. Also, we predicted Logistic curve using the Prophet Python. Further, using the growth rate to logistic, and carrying capacity is 20000 allowed us to calculate the maximum cases and new cases per day. Results: We found the predicted growth factor with a standard deviation of 0.3443 for the upcoming days. When the growth factor becomes 1.0, which is known as Inflection point, it will be safe to state that the rate is no longer exponential. The estimated time to reach the inflection point is between 15-20 April. At that time, the estimated number of total positive cases will be over 12500, if lockdown remains continue. Conclusions: Our analysis suggests that there is an urgent need to take action to extend the period of lockdown and allocate enough resources, including personnel, beds, and intensive care facilities, to manage the situation in the next few days and weeks. Otherwise, the outbreak in India can reach the level of the USA or Italy or could be worse than these countries within a few days or weeks, given the size of the population and lack of resources.",2020,medRxiv
Screen and clean: a tool for identifying interactions in genome-wide association studies.,"Epistasis could be an important source of risk for disease. How interacting loci might be discovered is an open question for genome-wide association studies (GWAS). Most researchers limit their statistical analyses to testing individual pairwise interactions (i.e., marginal tests for association). A more effective means of identifying important predictors is to fit models that include many predictors simultaneously (i.e., higher-dimensional models). We explore a procedure called screen and clean (SC) for identifying liability loci, including interactions, by using the lasso procedure, which is a model selection tool for high-dimensional regression. We approach the problem by using a varying dictionary consisting of terms to include in the model. In the first step the lasso dictionary includes only main effects. The most promising single-nucleotide polymorphisms (SNPs) are identified using a screening procedure. Next the lasso dictionary is adjusted to include these main effects and the corresponding interaction terms. Again, promising terms are identified using lasso screening. Then significant terms are identified through the cleaning process. Implementation of SC for GWAS requires algorithms to explore the complex model space induced by the many SNPs genotyped and their interactions. We propose and explore a set of algorithms and find that SC successfully controls Type I error while yielding good power to identify risk loci and their interactions. When the method is applied to data obtained from the Wellcome Trust Case Control Consortium study of Type 1 Diabetes it uncovers evidence supporting interaction within the HLA class II region as well as within Chromosome 12q24.",2010,Genetic epidemiology
International Comparison of Analytical Methods of Determining the Soil Organic Matter Content of Lithuanian Eutric Albeluvisols,"Abstract Several soil organic matter (SOM) methodologies have been employed to analyze a suite of subsampled soils, and their results have been correlated. This will permit future comparison of the large archive of SOM databases, which widely exist in Lithuania and other Central and Eastern European countries, with those of other international countries. Samples were collected (n=92) from topsoil and subsoil horizons of Eutric Glassoboralfs (Eutric Albeluvisols) at five longâ€term monitoring sites (three sites with 8 years' duration and two sites with 20 years' duration) containing a total of 46 experimental field plots. Each soil sample was subsampled and SOM determined by several analytical approaches (namely, dry combustion, Walkleyâ€“Black, Tyurin photometric, Tyurin titrimetric, and lossâ€onâ€ignition methods). Correlation coefficients between multiple sets of results varied between r=0.831 and r=0.965 (n=92, P<0.001). Based on the strength and significance of these relationships, we propose that simple linear regression equations can be confidently employed to recalculate SOM data among various analytical methodologies and thus help resolve the issue of international data comparison.",2006,Communications in Soil Science and Plant Analysis
Making room for the modeler in data mining,"I would like to use this perspective to advocate further research in data mining toward methods that allow and formalize greater input from the modeler. By â€˜modelerâ€™, I mean the domain expert, scientist, business analyst, or researcher who has questions to answer using data. Soliciting input from such users is atypical of most data mining algorithms (e.g., see the introductory discussion in ref. [1]). Although excluding pesky users enables one to enforce protections against, say, overfitting, those same users may come to resent the exclusion. Many fields remain deeply suspicious of black-box models that generate predictions without asking for help or supplying explanations. Handpicked regression models remain the model of choice in the social sciences, for instance, because these allow the user to pick the features and specify the relationships. Regression allows the model to express a theory about how the world works, and the success of the model depends on this specification. To this community, the fact that we can automate the process of selecting features or supply black-box predictions that are more accurate than those of regression is secondary to the absence of interpretative hooks. Allowing the user to customize a statistical model introduces any number of problems. With practice and a bit of collinearity in the data, one can mold a regression model to support a variety of theories by iteratively adding and removing variables. We know how to keep an automated search for features from overfitting; the challenge is to involve users in this process. Modelers who invest years gathering data are not eager to lay down their favorite tools unless we provide them with alternatives that respect their knowledge of the context and their desire to influence the analysis. A modeler who has a better theory and understanding of the data should be able to get a better model than one who lacks these insights. Black-box models remove the competitive advantage: both modelers pour the data into the model and pop out the same predictions. The methodologies that I want to encourage reward the modeler who has useful insights. I have a couple of illustrative examples in mind. First, consider the grouped LASSO [2] in the classical regression setting: for a sample of n observations, we observe a response y and k explanatory features in the vector x. The goal is a precise estimate of E y = x â€²Î². The original LASSO provides a method for identifying useful features. Rather than relying on stepwise or heuristic searches, LASSO selects the coefficient vector Î²Ì‚ that solves",2012,Statistical Analysis and Data Mining
LASSO model adaptation for automatic speech recognition,"ï€  Inspired by the success of least absolute shrinkage and selection operator (LASSO) in statistical learning, we propose an regularized maximum likelihood linear regression (MLLR) to estimate models with only a limited set of adaptation data to improve accuracy for automatic speech recognition, by regularizing the standard MLLR objective function with an constraint. The so-called LASSO MLLR is a natural solution to the data insufficiency problem because the constraint regularizes some parameters to exactly 0 and reduces the number of free parameters to estimate. Tested on the 5kWSJ0 task, the proposed LASSO MLLR gives significant word error rate reduction from the errors obtained with the standard MLLR in an utterance-by-utterance unsupervised adaptation scenario.",2011,
Use of electronic health record data and machine learning to identify candidates for HIV pre-exposure prophylaxis: a modelling study.,"BACKGROUND
The limitations of existing HIV risk prediction tools are a barrier to implementation of pre-exposure prophylaxis (PrEP). We developed and validated an HIV prediction model to identify potential PrEP candidates in a large health-care system.


METHODS
Our study population was HIV-uninfected adult members of Kaiser Permanente Northern California, a large integrated health-care system, who were not yet using PrEP and had at least 2 years of previous health plan enrolment with at least one outpatient visit from Jan 1, 2007, to Dec 31, 2017. Using 81 electronic health record (EHR) variables, we applied least absolute shrinkage and selection operator (LASSO) regression to predict incident HIV diagnosis within 3 years on a subset of patients who entered the cohort in 2007-14 (development dataset), assessing ten-fold cross-validated area under the receiver operating characteristic curve (AUC) and 95% CIs. We compared the full model to simpler models including only men who have sex with men (MSM) status and sexually transmitted infection (STI) positivity, testing, and treatment. Models were validated prospectively with data from an independent set of patients who entered the cohort in 2015-17. We computed predicted probabilities of incident HIV diagnosis within 3 years (risk scores), categorised as low risk (<0Â·05%), moderate risk (0Â·05% to <0Â·20%), high risk (0Â·20% to <1Â·0%), and very high risk (â‰¥1Â·0%), for all patients in the validation dataset.


FINDINGS
Of 3â€ˆ750â€ˆ664 patients in 2007-17 (3â€ˆ143â€ˆ963 in the development dataset and 606â€ˆ701 in the validation dataset), there were 784 incident HIV cases within 3 years of baseline. The LASSO procedure retained 44 predictors in the full model, with an AUC of 0Â·86 (95% CI 0Â·85-0Â·87) for incident HIV cases in 2007-14. Model performance remained high in the validation dataset (AUC 0Â·84, 0Â·80-0Â·89). The full model outperformed simpler models including only MSM status and STI positivity. For the full model, flagging 13â€ˆ463 (2Â·2%) patients with high or very high HIV risk scores in the validation dataset identified 32 (38Â·6%) of the 83 incident HIV cases, including 32 (46Â·4%) of 69 male cases and none of the 14 female cases. The full model had equivalent sensitivity by race whereas simpler models identified fewer black than white HIV cases.


INTERPRETATION
Prediction models using EHR data can identify patients at high risk of HIV acquisition who could benefit from PrEP. Future studies should optimise EHR-based HIV risk prediction tools and evaluate their effect on prescription of PrEP.


FUNDING
Kaiser Permanente Community Benefit Research Program and the US National Institutes of Health.",2019,The lancet. HIV
An additive Cox model for coronary heart disease study,"ABSTRACT Existing models for coronary heart disease study use a set of common risk factors to predict the survival time of the disease, via the standard Cox regression model. For complex relationships between the survival time and risk factors, the linear regression specification in the existing Cox model is not flexible enough to accounts for such relationships. Also, the risk factors are actually risky only when they fall in some risk ranges. For more flexibility in modelling and characterize the risk factors more accurately, we study a semi-parametric additive Cox model, using basis splines and LASSO technique. The proposed model is evaluated by simulation studies and is used for the analysis of a real data in the Strong Heart Study.",2018,Journal of Applied Statistics
The Bigraphical Lasso,"The i.i.d. assumption in machine learning is endemic, but often flawed. Complex data sets exhibit partial correlations between both instances and features. A model specifying both types of correlation can have a number of parameters that scales quadratically with the number of features and data points. We introduce the bigraphical lasso, an estimator for precision matrices of matrix-normals based on the Cartesian product of graphs. A prominent product in spectral graph theory, this structure has appealing properties for regression, enhanced sparsity and interpretability. To deal with the parameter explosion we introduce l1 penalties and fit the model through a flip-flop algorithm that results in a linear number of lasso regressions. We demonstrate the performance of our approach with simulations and an example from the COIL image data set.",2013,
Wavelet-based LASSO in functional linear regression.,"In linear regression with functional predictors and scalar responses, it may be advantageous, particularly if the function is thought to contain features at many scales, to restrict the coefficient function to the span of a wavelet basis, thereby converting the problem into one of variable selection. If the coefficient function is sparsely represented in the wavelet domain, we may employ the well-known LASSO to select a relatively small number of nonzero wavelet coefficients. This is a natural approach to take but to date, the properties of such an estimator have not been studied. In this paper we describe the wavelet-based LASSO approach to regressing scalars on functions and investigate both its asymptotic convergence and its finite-sample performance through both simulation and real-data application. We compare the performance of this approach with existing methods and find that the wavelet-based LASSO performs relatively well, particularly when the true coefficient function is spiky. Source code to implement the method and data sets used in the study are provided as supplemental materials available online.",2012,"Journal of computational and graphical statistics : a joint publication of American Statistical Association, Institute of Mathematical Statistics, Interface Foundation of North America"
Marginal false discovery rate approaches to inference on penalized regression models,"Penalized regression methods, most notably the lasso, are a popular analysis tool for data sets containing large numbers of a variables due to their ability to naturally perform variable selection. However, performing statistical inference on lasso models is difficult, making the topic an active area of research. Much of the literature on inferential methods for penalized regression models is focused exclusively on the linear regression setting, with few options available for models with categorical or survival outcomes. This dissertation proposes novel false discovery rate methods for a broad class of penalized likelihood modeling approaches that includes not only linear regression, but also generalized linear models, such as logistic regression, and survival models, such as Cox regression. The methods we propose are designed to be suitable next steps after fitting a penalized likelihood model, requiring minimal added computation or inputs other than the fitted model. The methods proposed in this dissertation deal with two broad categories of false discovery rates that exist in the large-scale hypothesis testing literature. In the first category are tail-area based approaches, which pertain to the overall false discovery rate for an entire set of rejected hypotheses. In the case of penalized regression, this is the entire set of variables active in the model. In the second category are local approaches, which refer to the specific probabilities of single variables being false discoveries, in the regression setting local approaches describe each active variable individually. Both types of false discovery rates are valuable tools depending upon the inferential question, and this dissertation proposes methods falling into both categories. In addition to making these proposals and investigating their properties using simulation, we also look at applying the methods to several case studies involving",2018,
Trace regression model with simultaneously low rank and row(column) sparse parameter,"Abstract In this paper, we consider the trace regression model with matrix covariates, where the parameter is a matrix of simultaneously low rank and row(column) sparse. To estimate the parameter, we formulate a convex optimization problem with the nuclear norm and group Lasso penalties, and propose an alternating direction method of multipliers (ADMM) algorithm. The asymptotic properties of the estimator are established. Simulation results confirm the effectiveness of our method.",2017,Comput. Stat. Data Anal.
Online heterogeneous feature fusion machines for visual recognition,"Heterogeneous Feature Fusion Machines (HFFM) is a kernel based logistic regression model that effectively fuses multiple features for visual recognition tasks. However, the batch mode solution for HFFM, 'Block Coordinate Gradient Descent' (BCGD) has the same low efficiency and poor scalability as the most batch algorithms do. In this paper, we describe a newly developed online learning algorithm in multiple Reproducing Kernel Hilbert Spaces for solving HFFM model. This new algorithm is called OLHFFM, i.e. Online HFFM. OLHFFM is novel combination of kernel-based learning technique with dual averaging gradient descent methods. In addition, group LASSO regularization technique is used in OLHFFM for finding important explanatory coefficients that are related to support samples in group manner. The effectiveness of OLHFFM has been demonstrated by a number of experiments that were conducted on public event, object dataset, as well as on large scale handwritten digital dataset. Using the OLHFFM approach, we have achieved almost equivalent recognition performance to that using batch-mode approach. Experiments conducted on both MIT Caltech-6 and challenging VOC2011 TrainVal object datasets show that OLHFFM is superior in performance to kernel based online learning approaches such as ILK or NORMA. In addition, the classification performance of OLHFFM approach as demonstrated by the experiments conducted on large scale MNIST dataset is comparable to or better than that of the current state-of-the-art online multiple kernel learning approaches such as OM-2, UFO-MKL, OMCL and OMKL. Extensive experiments on visual data classification demonstrate the effectiveness and robustness of the new OLHFFM approach.",2014,Neurocomputing
Penalized multimarker vs. single-marker regression methods for genome-wide association studies of quantitative traits.,"The data from genome-wide association studies (GWAS) in humans are still predominantly analyzed using single-marker association methods. As an alternative to single-marker analysis (SMA), all or subsets of markers can be tested simultaneously. This approach requires a form of penalized regression (PR) as the number of SNPs is much larger than the sample size. Here we review PR methods in the context of GWAS, extend them to perform penalty parameter and SNP selection by false discovery rate (FDR) control, and assess their performance in comparison with SMA. PR methods were compared with SMA, using realistically simulated GWAS data with a continuous phenotype and real data. Based on these comparisons our analytic FDR criterion may currently be the best approach to SNP selection using PR for GWAS. We found that PR with FDR control provides substantially more power than SMA with genome-wide type-I error control but somewhat less power than SMA with Benjamini-Hochberg FDR control (SMA-BH). PR with FDR-based penalty parameter selection controlled the FDR somewhat conservatively while SMA-BH may not achieve FDR control in all situations. Differences among PR methods seem quite small when the focus is on SNP selection with FDR control. Incorporating linkage disequilibrium into the penalization by adapting penalties developed for covariates measured on graphs can improve power but also generate more false positives or wider regions for follow-up. We recommend the elastic net with a mixing weight for the Lasso penalty near 0.5 as the best method.",2015,Genetics
A 17-gene expression-based prognostic signature associated with the prognosis of patients with breast cancer: A STROBE-compliant study.,"Identification of reliable predictive biomarkers for patients with breast cancer (BC).Univariate Cox proportional hazards regression model was conducted to identify genes correlated with the overall survival (OS) of patients in the TCGA-BRCA cohort. Functional enrichment analysis was conducted to investigate the biological meaning of these survival related genes. Then, patients in TCGA-BCRA were randomly divided into training set and test. Least absolute shrinkage and selection operator (LASSO) penalized Cox regression model was performed and the risk score of BC patients in this model was used to build a prognostic signature. The prognostic performance of the signature was evaluated in the training set, test set, and an independent validation set GSE7390.2519 genes were demonstrated to be significantly associated with the OS of BC patients. Functional annotation of the 2519 genes suggested that these genes were associated with immune response and protein synthesis related gene ontology terms and pathways. 17 genes were identified in the LASSO Cox regression model and used to construct a 17-gene signature. Patients in the 17-gene signature low risk group have better OS and event-free survival compared with those in the 17-gene signature high risk group in the TCGA-BRCA cohort. The prognostic role of the 17-gene signature has been confirmed in the validation cohort. Multivariable Cox proportional hazards regression model suggested the 17-gene signature was an independent prognostic factor in BC.The 17-gene signature we developed could successfully classify patients into high- and low-risk groups, indicating that it might serve as candidate biomarker in BC.",2020,Medicine
Feature Constrained Multi-Task Learning Models for Spatiotemporal Event Forecasting,"Spatial event forecasting from social media is potentially extremely useful but suffers from critical challenges, such as the dynamic patterns of features (keywords) and geographic heterogeneity (e.g., spatial correlations, imbalanced samples, and different populations in different locations). Most existing approaches (e.g., LASSO regression, dynamic query expansion, and burst detection) address some, but not all, of these challenges. Here, we propose a novel multi-task learning framework that aims to concurrently address all the challenges involved. Specifically, given a collection of locations (e.g., cities), forecasting models are built for all the locations simultaneously by extracting and utilizing appropriate shared information that effectively increases the sample size for each location, thus improving the forecasting performance. The new model combines both static features derived from a predefined vocabulary by domain experts and dynamic features generated from dynamic query expansion in a multi-task feature learning framework. Different strategies to balance homogeneity and diversity between static and dynamic terms are also investigated. And, efficient algorithms based on Iterative Group Hard Thresholding are developed to achieve efficient and effective model training and prediction. Extensive experimental evaluations on Twitter data from civil unrest and influenza outbreak datasets demonstrate the effectiveness and efficiency of our proposed approach.",2017,IEEE Transactions on Knowledge and Data Engineering
Relating Brain Functional Connectivity to Anatomical Connections: Model Selection,"We aim to learn across several subjects a mapping from brain anatomical connectivity to functional connectivity. Following [1], we formulate this problem as estimating a multivariate autoregressive (MAR) model with sparse linear regression. We introduce a model selection framework based on cross-validation. We select the appropriate sparsity of the connectivity matrices and demonstrate that choosing an ordering for the MAR that lends to sparser models is more appropriate than a random. Finally, we suggest randomized Least Absolute Shrinkage and Selective Operator (LASSO) in order to identify relevant anatomo-functional links with better recovery of ground truth.",2011,
An anthropometry-based nomogram for predicting metabolic syndrome in the working population,"Background: Early detection of metabolic syndrome is highly desirable for the prevention and treatment of various diseases. Therefore, this study aimed to develop and validate an anthropometry-based nomogram for predicting metabolic syndrome in a working population. Methods: The present study was a secondary analysis of a cross-sectional study. A total of 60,799 workers in Spain were enrolled between 2012 and 2016, of which 50% were randomly assigned to the derivation cohort and the remainder to the validation cohort. Participantsâ€™ demographics and anthropometric variables were entered into least absolute shrinkage and selection operator (LASSO) regression for the selection of variables. Subsequently, multivariable logistic regression was performed to develop the predictive model and a nomogram. The discrimination ability, calibration curve analysis and decision curve analysis of the nomogram was evaluated. Internal validation of the model was also performed. Results: There were 2725 (9.0%) participants diagnosed with metabolic syndrome in the derivation cohort and 2762 (9.1%) participants in the validation cohort. Six variables (age, smoking, body fat percentage, waist circumference, systolic blood pressure and diastolic blood pressure were included in the nomogram. The area under the curve was 0.901 (95% confidence interval (CI) 0.895â€“0.906) and 0.899 (95% CI 0.894â€“0.905) for the predictive and internal validation, respectively. Furthermore, decision curve analysis showed that if the threshold probability of metabolic syndrome is less than 72.0%, application of this nomogram can benefit more than either the treat-all or treat-none strategies. Conclusions: An anthropometry-based nomogram for predicting metabolic syndrome in a working population was developed that incorporates reliable non-invasive anthropometric features to facilitate health counselling and self-risk assessment of developing metabolic syndrome.",2019,European Journal of Cardiovascular Nursing
On the properties of simulation-based estimators in high dimensions,"Considering the increasing size of available data, the need for statistical methods that control the finite sample bias is growing. This is mainly due to the frequent settings where the number of variables is large and allowed to increase with the sample size bringing standard inferential procedures to incur significant loss in terms of performance. Moreover, the complexity of statistical models is also increasing thereby entailing important computational challenges in constructing new estimators or in implementing classical ones. A trade-off between numerical complexity and statistical properties is often accepted. However, numerically efficient estimators that are altogether unbiased, consistent and asymptotically normal in high dimensional problems would generally be ideal. In this paper, we set a general framework from which such estimators can easily be derived for wide classes of models. This framework is based on the concepts that underlie simulation-based estimation methods such as indirect inference. The approach allows various extensions compared to previous results as it is adapted to possibly inconsistent estimators and is applicable to discrete models and/or models with a large number of parameters. We consider an algorithm, namely the Iterative Bootstrap (IB), to efficiently compute simulation-based estimators by showing its convergence properties. Within this framework we also prove the properties of simulation-based estimators, more specifically the unbiasedness, consistency and asymptotic normality when the number of parameters is allowed to increase with the sample size. Therefore, an important implication of the proposed approach is that it allows to obtain unbiased estimators in finite samples. Finally, we study this approach when applied to three common models, namely logistic regression, negative binomial regression and lasso regression.",2018,arXiv: Statistics Theory
Online TTC Estimation Using Nonparametric Analytics Considering Wind Power Integration,"Total transfer capability (TTC) is an effective indicator to evaluate the transmission limit of the interconnected systems. However, due to the large-scale wind power integration, operation conditions of a power system may change rapidly, yielding time-varying characteristics of the TTC. As a result, the traditional time-consuming transient stability constrained TTC model is unable to assess the online transmission margin. In this paper, we propose an online measurement-based TTC estimator using the nonparametric analytics. It consists of three major components: the probabilistic data generation, the composite feature selection, and the group Lasso regression-based training scheme. Specifically, we present a probabilistic data generation approach to take into account the uncertainties of the day-ahead generation scheduling and to reduce the number of redundant or infeasible data. Then, the composite feature selection is used to reduce the dimension of the generated data and identify the features which are highly correlated with TTC. The features are determined by the maximal information coefficients and nonparametric independence screening approach. Finally, these selected features are trained by the group Lasso regression to learn the correlation between the TTC and the online measurements. Once real-time measurements are available, the TTC can be assessed immediately through the learned correlation relationship. Extensive numerical results carried out on the modified New England 39-bus test system demonstrate the feasibility of the proposed TTC estimator for online applications.",2019,IEEE Transactions on Power Systems
Segmentation and Classification of Cine-MR Images Using Fully Convolutional Networks and Handcrafted Features,"Three-dimensional cine-MRI is of crucial importance for assessing the cardiac function. Features that describe the anatomy and function of cardiac structures (e.g. Left Ventricle (LV), Right Ventricle (RV), and Myocardium(MC)) are known to have significant diagnostic value and can be computed from 3D cine-MR images. However, these features require precise segmentation of cardiac structures. Among the fully automated segmentation methods, Fully Convolutional Networks (FCN) with Skip Connections have shown robustness in medical segmentation problems. In this study, we develop a complete pipeline for classification of subjects with cardiac conditions based on 3D cine-MRI. For the segmentation task, we develop a 2D FCN and introduce Parallel Paths (PP) as a way to exploit the 3D information of the cine-MR image. For the classification task, 125 features were extracted from the segmented structures, describing their anatomy and function. Next, a two-stage pipeline for feature selection using the LASSO method is developed. A subset of 20 features is selected for classification. Each subject is classified using an ensemble of Logistic Regression, Multi-Layer Perceptron, and Support Vector Machine classifiers through majority voting. The Dice Coefficient for segmentation was 0.95+-0.03, 0.89+-0.13, and 0.90+-0.03 for LV, RV, and MC respectively. The 8-fold cross validation accuracy for the classification task was 95.05% and 92.77% based on ground truth and the proposed methods segmentations respectively. The results show that the PPs increase the segmentation accuracy, by exploiting the spatial relations. Moreover, the classification algorithm and the features showed discriminability while keeping the sensitivity to segmentation error as low as possible.",2017,ArXiv
Extended Bayesian Information Criteria for Gaussian Graphical Models,"Gaussian graphical models with sparsity in the inverse covariance matrix are of significant interest in many modern applications. For the problem of recovering the graphical structure, information criteria provide useful optimization objectives for algorithms searching through sets of graphs or for selection of tuning parameters of other methods such as the graphical lasso, which is a likelihood penalization technique. In this paper we establish the consistency of an extended Bayesian information criterion for Gaussian graphical models in a scenario where both the number of variables p and the sample size n grow. Compared to earlier work on the regression case, our treatment allows for growth in the number of non-zero parameters in the true model, which is necessary in order to cover connected graphs. We demonstrate the performance of this criterion on simulated data when used in conjunction with the graphical lasso, and verify that the criterion indeed performs better than either cross-validation or the ordinary Bayesian information criterion when p and the number of non-zero parameters q both scale with n.",2010,
Detecting stock market bubbles based on the cross-sectional dispersion of stock prices,"A statistical method is proposed for detecting stock market bubbles that occur when speculative funds concentrate on a small set of stocks. The bubble is defined by stock price diverging from the fundamentals. A firmâ€™s financial standing is certainly a key fundamental attribute of that firm. The law of one price would dictate that firms of similar financial standing share similar fundamentals. We investigate the variation in market capitalization normalized by fundamentals that is estimated by Lasso regression of a firmâ€™s financial standing. The market capitalization distribution has a substantially heavier upper tail during bubble periods, namely, the market capitalization gap opens up in a small subset of firms with similar fundamentals. This phenomenon suggests that speculative funds concentrate in this subset. We demonstrated that this phenomenon could have been used to detect the dot-com bubble of 1998â€“2000 in different stock exchanges.",2019,
Grundlage fÃ¼r eine Beurteilung des Transmissionsrisikos von Fasciola hepatica,"Die Fasciolose, ausgelost durch eine Infestation mit dem Grosen Leberegel, Fasciola hepatica, gilt weltweit als Ursache erheblicher wirtschaftlicher Verluste durch die meist chronisch verlaufende Erkrankung bei Rindern. Um Regionen, in denen ein erhohtes Risiko fur die Ubertragung von Fasciola hepatica besteht, fruhzeitig erkennen zu konnen, bedarf es eines geeigneten raumlichen Modelles, welches alle Schritte der Transmission berucksichtigt. 
Gegenstand der vorliegenden Arbeit war es, ein geeignetes Regressionsmodell zu entwickeln, das zur Vorhersage des Vorkommens von Galba truncatula, der Zwergschlammschnecke, befahigen sollte. Diese ist in vielen Landern, so auch in der Schweiz, Zwischenwirt fur den Grosen Leberegel. 
Insgesamt wurde der potenzielle Einfluss von 70 Kovariablen auf die Wahrscheinlichkeit Ï€_i, Galba truncatula an einem Ort vorzufinden, untersucht. Als Datenbasis dienten dabei n=242 Gelandepunkte, die in den Jahren 1999-2000, 2004 und 2010 von G. Knubben-Schweizer sowie R. Baggenstos auf das Vorhandensein von Zwergschlammschnecken uberpruft worden waren. 
Der binare Response (Schnecken ja/ nein) erforderte die Verwendung Generalisierter Linearer (Gemischter) Regressionsmodelle (GL(M)Ms). Im Speziellen kamen zwei Regressionsverfahren in Form binarer Logitmodelle zur Anwendung: Das grplasso-Verfahren und ein glmmLasso-Modell, die beide als gleichnamige Pakete im Statistikprogramm R implementiert sind. Das Ausmas der Penalisierung einzelner Koeffizienten und die Sparsamkeit der Modelle wurde durch 10-fache Kreuzvalidierung beziehungsweise das sogenannte Bayessche Informationskriterium gesteuert. 
Es ergaben sich zwei Modelle, die eine Vorhersage des Auftretens von Galba truncatula ermoglichen sollten. 
Im grplasso-Modell wurden 19 Kovariablen mit unterschiedlich starken Einflussen auf das Schneckenvorkommen ausgewahlt, wahrend via glmmLasso-Modell nur 13 Kovariablen als relevant erkannt wurden, seine Verwendung im Gegenzug aber die Berucksichtigung standortspezifischer Effekte, sogenannter random effects, erlaubte. Beide Modelle stimmten in neun Kovariablen uberein, denen damit die ausgepragtesten Effekte auf die Schneckenpopulation beigemessen werden mussen. Riete beziehungsweise Feuchtflachen, Hangwasseraustritte, Gewasser im Umkreis von 100 Metern und Baumbestand stellten ebenso positive Pradiktoren dar wie eine mittlere im Vergleich zu tiefer Grundigkeit des Bodens. Negative Pradiktoren waren insbesondere gehaufte Uberschreitungen hoher Temperaturen von 30 Â°C und Unterschreitungen von 0 Â°C. Bodennah gemessene Temperaturen wurden bevorzugt selektiert und schienen die entscheidenden Temperaturverhaltnisse besser widerzuspiegeln als makroklimatische Parameter. Niederschlagsabhangigen Kovariablen wurden weder im grplasso- noch im glmmLasso-Modell relevante Einflusse zugebilligt. Sie durften auf Grund ausreichend starker Niederschlage in den betrachteten Klimaregionen der nordostlichen Schweiz keinen limitierenden Faktor fur das Auftreten von Galba truncatula darstellen. 
Jedes der beiden Regressionsverfahren kann als geeignete Methode zur Vorhersage des Vorkommens von Galba truncatula dienen, wobei die Schatzungen des glmmLasso-Modelles stabiler gegenuber datenstrukturbedingten Einflussen zu sein scheinen. 
Eine Validierung der Schatzgenauigkeit beider Modelle anhand neuer, unbekannter Stichproben steht aus. Doch lassen sich aus der vorliegenden Arbeit Empfehlungen fur die Methodik zukunftiger Transmissionsmodelle zur Fasciolose in der Schweiz ableiten.",2016,
Sound Field Reproduction using the Lasso,"Reproducing a sampled sound field using an array of loudspeakers is a problem with well-appreciated applications to acoustics and ultrasound treatment. Loudspeaker signal design has traditionally relied on (possibly regularized) least-squares (LS) criteria. In many cases however, the desired sound field can be reproduced using only a few loudspeakers, which are sparsely distributed in space. To exploit this feature, the fresh look advocated here permeates benefits from advances in variable selection and compressive sampling to sound field synthesis by formulating a sparse linear regression problem that is solved using the least-absolute shrinkage and selection operator (Lasso). An efficient implementation of the Lasso for the problem at hand is developed based on a coordinate descent iteration. Analysis and simulations demonstrate that Lasso-based sound field reproduction yields better performance than LS especially at high frequencies and for reproduction of under-sampled sound fields. In addition, Lasso-based synthesis enables judicious placement of loudspeaker arrays.",2010,"IEEE Transactions on Audio, Speech, and Language Processing"
