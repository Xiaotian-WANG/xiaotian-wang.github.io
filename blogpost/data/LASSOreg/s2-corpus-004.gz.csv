title,abstract,year,journal
Asymptotics and Optimal Designs of SLOPE for Sparse Linear Regression,"In sparse linear regression, the SLOPE estimator generalizes LASSO by assigning magnitude-dependent regularizations to different coordinates of the estimate. In this paper, we present an asymptotically exact characterization of the performance of SLOPE in the high-dimensional regime where the number of unknown parameters grows in proportion to the number of observations. Our asymptotic characterization enables us to derive optimal regularization sequences to either minimize the MSE or to maximize the power in variable selection under any given level of Type-I error. In both cases, we show that the optimal design can be recast as certain infinite-dimensional convex optimization problems, which have efficient and accurate finite-dimensional approximations. Numerical simulations verify our asymptotic predictions. They also demonstrate the superiority of our optimal design over LASSO and a regularization sequence previously proposed in the literature.",2019,2019 IEEE International Symposium on Information Theory (ISIT)
Post-selection adaptive inference for Least Angle Regression and the Lasso,"We propose inference tools for least angle regression and the lasso, from the joint distribution of suitably normalized spacings of the LARS algorithm. From this we extend the results of the asymptotic null distribution of the â€œcovariance testâ€ of Lockhart et al. (2013). But we go much further, deriving exact finite sample results for a new asymptotically equivalent procedure called the â€œspacing testâ€. This provides exact conditional tests at any step of the LAR algorithm as well as â€œselection intervalsâ€ for the appropriate true underlying regression parameter. Remarkably, these tests and intervals account correctly for the adaptive selection done by LARS.",2014,
Variable Selection for Varying-coefficient Model with Autoregressive Error,"The local linear regression technique is applied to estimate parameters for varying-coefficient regression model with autoregressive error.This paper discusses how to use the estimated residuals and the LASSO method to determine both the order of the error and estimator of unknown parameters.Finally,Monte Carlo simulation and comparative study show that the proposed method is effective.",2012,Journal of Jiaxing University
DL-PDE: Deep-learning based data-driven discovery of partial differential equations from discrete and noisy data,"In recent years, data-driven methods have been utilized to learn dynamical systems and partial differential equations (PDE). However, major challenges remain to be resolved, including learning PDE under noisy data and limited discrete data. To overcome these challenges, in this work, a deep-learning based data-driven method, called DL-PDE, is developed to discover the governing PDEs of underlying physical processes. The DL-PDE method combines deep learning via neural networks and data-driven discovery of PDEs via sparse regressions, such as the least absolute shrinkage and selection operator (Lasso) and sequential threshold ridge regression (STRidge). In this method, derivatives are calculated by automatic differentiation from the deep neural network, and equation form and coefficients are obtained with sparse regressions. The DL-PDE is tested with physical processes, governed by groundwater flow equation, contaminant transport equation, Burgers equation and Korteweg-de Vries (KdV) equation, for proof-of-concept and applications in real-world engineering settings. The proposed DL-PDE achieves satisfactory results when data are discrete and noisy.",2019,ArXiv
High-dimensional Linear Regression for Dependent Data with Applications to Nowcasting,"In recent years, extensive research has focused on the `1 penalized least squares (Lasso) estimators of high-dimensional linear regression when the number of covariates p is considerably larger than the sample size n. However, there is limited attention paid to the properties of the estimators when the errors and/or the covariates are serially dependent. In this paper, we investigate the theoretical properties of the Lasso estimators for linear regression with random design and weak sparsity under serially dependent and/or non-sub-Gaussian errors and covariates. In contrast to the traditional case in which the errors are i.i.d and have finite exponential moments, we show that p can be at most a power of n if the errors have only finite polynomial moments. In addition, the rate of convergence becomes slower due to the serial dependence in errors and the covariates. We also consider sign consistency for model selection via Lasso when there are serial correlations in the errors or the covariates or both. Adopting the framework of functional dependence measure, we provide a detailed description on 1 Statistica Sinica: Newly accepted Paper (accepted author-version subject to English editing)",2020,Statistica Sinica
Improved CBR for Endpoint Carbon Content Prediction of BOF Steelmaking,"Traditional basic oxygen furnace (BOF) prediction is usually dependent on experts and experiences which will result in deviation. Aiming at increasing the accuracy of endpoint carbon content prediction in BOF steelmaking, this paper proposes a data driven way called case-based reasoning (CBR) method to predict the endpoint carbon content of BOF steelmaking. In CBR, case representation is the basic, the selected attributes in case representation play an important role but dependent on expert experiences so that this paper uses lasso regression algorithm to get the attributes. Case retrieval makes a significant impact on reasoning result. Therefore, we apply affinity propagation (AP) clustering algorithm and water-filling algorithm to enhance the case retrieval so as to improve the accuracy and stability of endpoint carbon content prediction. Through the simulation experiment, the results show that the improved CBR can obviously improve the accuracy of endpoint carbon content prediction.",2018,2018 Ninth International Conference on Intelligent Control and Information Processing (ICICIP)
Efficient and accurate methods for updating generalized linear models with multiple feature additions,"In this paper, we propose an approach for learning regression models efficiently in an environment where multiple features and data-points are added incrementally in a multistep process. At each step, any finite number of features maybe added and hence, the setting is not amenable to low rank updates. We show that our approach is not only efficient and optimal for ordinary least squares, weighted least squares, generalized least squares and ridge regression, but also more generally for generalized linear models and lasso regression that use iterated re-weighted least squares for maximum likelihood estimation. Our approach instantiated to linear settings has close relations to the partitioned matrix inversion mechanism based on Schur's complement. For arbitrary regression methods, even a relaxation of the approach is no worse than using the model from the previous step or using a model that learns on the additional features and optimizes the residual of the model at the previous step. Such problems are commonplace in complex manufacturing operations consisting of hundreds of steps, where multiple measurements are taken at each step to monitor the quality of the final product. Accurately predicting if the finished product will meet specifications at each or, at least, important intermediate steps can be extremely useful in enhancing productivity. We further validate our claims through experiments on synthetic and real industrial data sets.",2014,J. Mach. Learn. Res.
Identification of genes in lipid metabolism associated with white matter features in preterm infants,"Abstract Background We have previously identified an association between lipids and features of diffusion tensor imaging in preterm infants, highlighting the KEGG peroxisome proliferator-activated receptor pathway (PPAR) as the most highly ranked in association with a white matter phenotype. Here we applied a penalised linear regression modelâ€”the graph-guided group lasso (GGGL)â€”to select single nucleotide polymorphisms within functionally related genes associated with the imaging trait. The GGGL model utilises network-based prior knowledge to improve the selection of variables of interest. Methods Images acquired with a 3T MR scanner were collected along with saliva for 72 preterm infants (mean gestational age 28 weeks [+4 days], mean postmenstrual age at scan 40 weeks [+3 days]). Salivary DNA was extracted and genotyped with HumanOmniExpress-12 arrays (Illumina, San Diego, CA, USA). Fractional anisotropy maps were constructed from diffusion tensor imaging, and tract-based spatial statistics were used to obtain a group white matter skeleton varying with degree of prematurity, adjusting for age at scan. GGGL was applied to the genes in the PPAR pathway. Findings The GGGL method selected five of the 69 genes in the PPAR pathway, and these were functionally related in terms of Gene Ontology Biological Process and linearly correlated with white matter fractional anisotropy ( AQP7, ME1, PLIN1, SLC27A1 , and ACAA1 ). Analysis of transcriptional regulation with the PASTAA algorithm indicated that ACAA1, AQP7, ME1 , and SLC27A1 were jointly regulated by the EGR4 transcription factor (adjusted p=7Â·7Ã—10 âˆ’4 ). Interpretation Our GGGL analysis of genes in the PPAR pathway identified five highly ranked genes involved in neuronal growth, myelinogenesis, and nervous system response to inflammation, and jointly transcriptionally regulated. Since preterm infants are at increased risk of mental illness and cardiovascular morbidity in later life, this work uses an integrative data-driven strategy to propose a unifying mechanism for these systemic effects, suggesting a promising avenue for intervention. Funding Medical Research Council, National Institute for Health Research Biomedical Research Centre.",2016,The Lancet
Information criteria for variable selection under sparsity,"The optimization of an information criterion in a variable selection procedure leads to an additional bias, which can be substantial for sparse, high-dimensional data. One can compensate for the bias by applying shrinkage while estimating within the selected models. This paper presents modified information criteria for use in variable selection and estimation without shrinkage. The analysis motivating the modified criteria follows two routes. The first, which we explore for signal-plus-noise observations only, proceeds by comparing estimators with and without shrinkage. The second, discussed for general regression models, describes the optimization or selection bias as a double-sided effect, which we call a mirror effect: among the numerous insignificant variables, those with large, noisy values appear more valuable than an arbitrary variable, while in fact they carry more noise than an arbitrary variable. The mirror effect is investigated for Akaikeâ€™s information criterion and for Mallowsâ€™ Cp, with special attention paid to the latter criterion as a stopping rule in a least-angle regression routine. The result is a new stopping rule, which focuses not on the quality of a lasso shrinkage selection but on the least-squares estimator without shrinkage within the same selection.",2014,Biometrika
A gene-expression-based signature predicts survival in adults with T-cell lymphoblastic lymphoma: a multicenter study,"We aimed to establish a discriminative gene-expression-based classifier to predict survival outcomes of T-cell lymphoblastic lymphoma (T-LBL) patients. After exploring global gene-expression profiles of progressive (nâ€‰=â€‰22) vs. progression-free (nâ€‰=â€‰28) T-LBL patients, 43 differentially expressed mRNAs were identified. Then an eleven-gene-based classifier was established using LASSO Cox regression based on NanoString quantification. In the training cohort (nâ€‰=â€‰169), high-risk patients stratified using the classifier had significantly lower progression-free survival (PFS: hazards ratio 4.123, 95% CI 2.565â€“6.628; pâ€‰<â€‰0.001), disease-free survival (DFS: HR 3.148, 95% CI 1.857â€“5.339; pâ€‰<â€‰0.001), and overall survival (OS: HR 3.790, 95% CI 2.237â€“6.423; pâ€‰<â€‰0.001) compared with low-risk patients. The prognostic accuracy of the classifier was validated in the internal testing (nâ€‰=â€‰84) and independent validation cohorts (nâ€‰=â€‰360). A prognostic nomogram consisting of five independent variables including the classifier, lactate dehydrogenase levels, ECOG-PS, central nervous system involvement, and NOTCH1/FBXW7 status showed significantly greater prognostic accuracy than each single variable alone. The addition of a five-miRNA-based signature further enhanced the accuracy of this nomogram. Furthermore, patients with a nomogram score â‰¥154.2 significantly benefited from the BFM protocol. In conclusion, our nomogram comprising the 11-gene-based classifier may make contributions to individual prognosis prediction and treatment decision-making.",2020,Leukemia
Variablenselektion in Problemen der linearen Regression,"In dieser Arbeit werden unterschiedliche Methoden zur Variablenselektion betrachtet und theoretisch beschrieben, sowie an einem praktischen Beispiel miteinander verglichen. Ausgehend vom kleinste Quadrate Schatzer werden beginnend mit der Ridge Regression auch moderne Verfahren wie LASSO, LARS und Forward Stagewise besprochen. Diese Methoden werden schlieslich an einem Datensatz uber RNA-Strange zur Vorhersage des SCI verwendet.",2008,
Cox-LASSO Analysis Reveals a Ten-lncRNA Signature to Predict Outcomes in Patients with High-Grade Serous Ovarian Cancer.,"High-grade serous ovarian cancer (HGSOC) is one of the most common and lethal gynecological cancers. Long noncoding RNAs (lncRNAs) play important roles and act as prognostic biomarkers of ovarian cancer. However, few studies have focused on the prognostic prediction of lncRNAs solely in HGSOC. In this study, we identified candidate lncRNAs for a prognostic evaluation by examining reannotated lncRNA expression profiles and clinical data of 343 patients with HGSOC from The Cancer Genome Atlas. We built a 10-lncRNA signature using Cox-LASSO regression to predict the prognosis of patients with HGSOC. Trichotomized by the 10-lncRNA signature, high-risk patients experienced significantly shorter disease-free survival and overall survival (OS). Our novel 10-lncRNA signature showed superior predictive capacity compared to the other 2 published lncRNA signature models and clinicopathological parameters. We developed a nomogram for clinical use by integrating the 10-lncRNA signature and two clinicopathological risk factors to predict 1-, 3-, and 5-year OS. In addition, gene set enrichment analysis suggested that the group of high-risk patients was associated with mitotic spindle pathways. This model was also compatible with patients with or without BRCA1/2 mutations and had the potential to predict the response to platinum-based adjuvant chemotherapy. Our findings provide a novel 10-lncRNA prognostic signature for further clinical application in patients with HGSOC and indicate the underlying mechanisms of HGSOC progression.",2019,DNA and cell biology
Learning 2D Gabor Filters by Infinite Kernel Learning Regression,"Gabor functions have wide-spread applications in image processing and computer vision. In this paper, we prove that 2D Gabor functions are translation-invariant positive-definite kernels and propose a novel formulation for the problem of image representation with Gabor functions based on infinite kernel learning regression. Using this formulation, we obtain a support vector expansion of an image based on a mixture of Gabor functions. The problem with this representation is that all Gabor functions are present at all support vector pixels. Applying LASSO to this support vector expansion, we obtain a sparse representation in which each Gabor function is positioned at a very small set of pixels. As an application, we introduce a method for learning a dataset-specific set of Gabor filters that can be used subsequently for feature extraction. Our experiments show that use of the learned Gabor filters improves the recognition accuracy of a recently introduced face recognition algorithm.",2017,ArXiv
A pr 2 00 7 Multi-Stage Variable Selection : Screen and Clean,"This paper explores the following question: what kind of sta ti ical guarantees can be given when doing variable variable in high dimensional mo dels? In particular, we look at the error rates and power of some multi-stage regress ion methods. In the first stage we fit a set of candidate models. In the second stage we se lect one model by cross-validation. In the third stage we use hypothesis test ing to eliminate some variables. We refer to the first two stages as â€œscreeningâ€ and the l ast stage as â€œcleaning.â€ We consider three screening methods: the lasso, marginal re gression, and forward stepwise regression. Our method also gives consistent vari able selection under weak conditions.",2007,
Variable Selection via Nonconcave Penalized Likelihood in High Dimensional Medical Problems,"Variable selection is fundamental to high-dimensional statistical modelling in diverse fields of sciences. Specially in health studies, many potential factors are introduced to determine an outcome variable. In our study, different statistical methods are applied to analyze trauma annual data, collected by 30 General Hospitals in Greece. The dataset consists of 6334 observations and at most 131 factors that include demographic, transport and intrahospital data. The statistical methods employed in this work were the nonconcave penalized likelihood methods, SCAD, LASSO, and Hard, the generalized linear logistic regression, and the best subset variable selection, used to detect possible risk factors of death. A variety of different statistical models are considered, with respect to the combinations of factors and the number of observations. A comparative survey reveals differences between results and execution times of each method. The performed analysis reveals several distinct advantages of the nonconcave penalized likelihood methods over the traditional model selection techniques.",2009,International journal of applied mathematics and statistics
Development and validation of a 5-year mortality prediction model using regularized regression and Medicare data.,"PURPOSE
De-implementation of low-value services among patients with limited life expectancy is challenging. Robust mortality prediction models using routinely collected health care data can enhance health care stakeholders' ability to identify populations with limited life expectancy. We developed and validated a claims-based prediction model for 5-year mortality using regularized regression methods.


METHODS
Medicare beneficiaries age 66 or older with an office visit and at least 12Â months of pre-visit continuous Medicare A/B enrollment were identified in 2008. Five-year mortality was assessed through 2013. Secondary outcomes included 30-, 90-, and 180-day and 1-year mortality. Claims-based predictors, including comorbidities and indicators of disability, frailty, and functional impairment, were selected using regularized logistic regression, applying the least absolute shrinkage and selection operator (LASSO) in a random 80% training sample. Model performance was assessed and compared with the Gagne comorbidity score in the 20% validation sample.


RESULTS
Overall, 183Â 204 (24%) individuals died. In addition to demographics, 161 indicators of comorbidity and function were included in the final model. In the validation sample, the c-statistic was 0.825 (0.823-0.828). Median-predicted probability of 5-year mortality was 14%; almost 4% of the cohort had a predicted probability greater than 80%. Compared with the Gagne score, the LASSO model led to improved 5-year mortality classification (net reclassification indexÂ =Â 9.9%; integrated discrimination indexÂ =Â 5.2%).


CONCLUSIONS
Our claims-based model predicting 5-year mortality showed excellent discrimination and calibration, similar to the Gagne score model, but resulted in improved mortality classification. Regularized regression is a feasible approach for developing prediction tools that could enhance health care research and evaluation of care quality.",2019,Pharmacoepidemiology and drug safety
L2-Boosting for Economic Applications,"In the recent years more and more highdimensional data sets, where the number of parameters p is high compared to the number of observations n or even larger, are available for applied researchers. Boosting algorithms represent one of the major advances in machine learning and statistics in recent years and are suitable for the analysis of such data sets. While Lasso has been applied very successfully for highdimensional data sets in Economics, boosting has been underutilized in this field, although it has been proven very powerful in fields like Biostatistics and Pattern Recognition. We attribute this to missing theoretical results for boosting. The goal of this paper is to fill this gap and show that boosting is a competitive method for inference of a treatment effect or instrumental variable (IV) estimation in a high-dimensional setting. First, we present the L2Boosting with componentwise least squares algorithm and variants which are tailored for regression problems which are the workhorse for most Econometric problems. Then we show how L2Boosting can be used for estimation of treatment effects and IV estimation. We highlight the methods and illustrate them with simulations and empirical examples. For further results and technical details we refer to (?) and (?) and to the online supplement of the paper.",2017,
Car-Specific Metro Train Crowding Prediction Based on Real-Time Load Data,"The paper formulates the car-specific metro train crowding prediction problem based on real-time load data and evaluates the performance of several prediction methods (stepwise regression, lasso, and boosted tree ensembles). The problem is studied for multiple stations along a metro line in Stockholm, Sweden. Prediction accuracy is evaluated with respect to absolute passenger loads and predefined discrete crowding levels. When available, predictions with real-time load data significantly outperform historical averages, with accuracy improvements varying in magnitude across target stations and prediction horizons.",2018,2018 21st International Conference on Intelligent Transportation Systems (ITSC)
Risk factors for anastomotic leakage after anterior resection for rectal cancer.,"BACKGROUND
The risk factors for anastomotic leak (AL) after anterior resection have been evaluated in several studies and remain controversial as the findings are often inconsistent or inconclusive.


OBJECTIVE
To analyze the risk factors for AL after anterior resection in patients with rectal cancer.


DESIGN
Retrospective analysis.


SETTING
The Nationwide Inpatient Sample 2006 to 2009.


PATIENTS
A total of 72 055 patients with rectal cancer who underwent elective anterior resection.


MAIN OUTCOME MEASURES
To build a predictive model for AL using demographic characteristics and preadmission comorbidities, the lasso algorithm for logistic regression was used to select variables most predictive of AL.


RESULTS
The AL rate was 13.68%. The AL group had higher mortality vs the non-AL group (1.78% vs 0.74%). Hospital length of stay and cost were significantly higher in the AL group. Laparoscopic and open resections with a diverting stoma had a higher incidence of AL than those without a stoma (15.97% vs 13.25%). Multivariate analysis revealed that weight loss and malnutrition, fluid and electrolyte disorders, male sex, and stoma placement were associated with a higher risk of AL. The use of laparoscopy was associated with a lower risk of AL. Postoperative ileus, wound infection, respiratory/renal failure, urinary tract infection, pneumonia, deep vein thrombosis, and myocardial infarction were independently associated with AL.


CONCLUSIONS
Anastomotic leak after anterior resection increased mortality rates and health care costs. Weight loss and malnutrition, fluid and electrolyte disorders, male sex, and stoma placement independently increased the risk of leak. Laparoscopy independently decreased the risk of leak. Further studies are needed to delineate the significance of these findings.",2013,JAMA surgery
Regularized Taylor Echo State Networks for Predictive Control of Partially Observed Systems,"The existing neural networks suffer from partial observation while modeling and controlling dynamic systems. In this paper, a new linearized recurrent neural network, the Taylor expanded echo state network (TESN), is proposed for predictive control of partially observed dynamic systems. Two schemes of regularization, ridge regression and sparse regression, are imposed on TESNs to tackle the issue of ill-conditioned estimation. Furthermore, two estimators, lasso and elastic net, are investigated for sparse regression. Regularized learning is found to improve the estimation consistency of readout coefficients and, at the same time, suppress the accumulation of linearization residues in a prediction horizon. A series of experiments was carried out, and the results verified that regularized learning is contributive to TESNs in predictive control of partially observed dynamic systems.",2016,IEEE Access
Efficient Empirical Bayes Variable Selection and Estimation in Linear Models,"We propose an empirical Bayes method for variable selection and coefficient estimation in linear regression models. The method is based on a particular hierarchical Bayes formulation, and the empirical Bayes estimator is shown to be closely related to the LASSO estimator. Such a connection allows us to take advantage of the recently developed quick LASSO algorithm to compute the empirical Bayes estimate, and provides a new way to select the tuning parameter in the LASSO method. Unlike previous empirical Bayes variable selection methods, which in most practical situations can be implemented only through a greedy stepwise algorithm, our method gives a global solution efficiently. Simulations and real examples show that the proposed method is very competitive in terms of variable selection, estimation accuracy, and computation speed compared with other variable selection and estimation methods.",2005,Journal of the American Statistical Association
A computational model for biosonar echoes from foliage,"Since many bat species thrive in densely vegetated habitats, echoes from foliage are likely to be of prime importance to the animals' sensory ecology, be it as clutter that masks prey echoes or as sources of information about the environment. To better understand the characteristics of foliage echoes, a new model for the process that generates these signals has been developed. This model takes leaf size and orientation into account by representing the leaves as circular disks of varying diameter. The two added leaf parameters are of potential importance to the sensory ecology of bats, e.g., with respect to landmark recognition and flight guidance along vegetation contours. The full model is specified by a total of three parameters: leaf density, average leaf size, and average leaf orientation. It assumes that all leaf parameters are independently and identically distributed. Leaf positions were drawn from a uniform probability density function, sizes and orientations each from a Gaussian probability function. The model was found to reproduce the first-order amplitude statistics of measured example echoes and showed time-variant echo properties that depended on foliage parameters. Parameter estimation experiments using lasso regression have demonstrated that a single foliage parameter can be estimated with high accuracy if the other two parameters are known a priori. If only one parameter is known a priori, the other two can still be estimated, but with a reduced accuracy. Lasso regression did not support simultaneous estimation of all three parameters. Nevertheless, these results demonstrate that foliage echoes contain accessible information on foliage type and orientation that could play a role in supporting sensory tasks such as landmark identification and contour following in echolocating bats.",2017,PLoS ONE
Structure identification and variable selection in geographically weighted regression models,"ABSTRACT Geographically weighted regression (GWR) is an important tool for exploring spatial non-stationarity of a regression relationship, in which whether a regression coefficient really varies over space is especially important in drawing valid conclusions on the spatial variation characteristics of the regression relationship. This paper proposes a so-called GWGlasso method for structure identification and variable selection in GWR models. This method penalizes the loss function of the local-linear estimation of the GWR model by the coefficients and their partial derivatives in the way of the adaptive group lasso and can simultaneously identify spatially varying coefficients, nonzero constant coefficients and zero coefficients. Simulation experiments are further conducted to assess the performance of the proposed method and the Dublin voter turnout data set is analysed to demonstrate its application.",2017,Journal of Statistical Computation and Simulation
