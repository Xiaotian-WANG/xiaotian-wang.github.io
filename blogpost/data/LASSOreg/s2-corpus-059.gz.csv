title,abstract,year,journal
The ternary complex factor protein ELK1 is an independent prognosticator of disease recurrence in prostate cancer.,"BACKGROUND
Both hormone-sensitive and castration- and enzalutamide-resistant prostate cancers (PCa) depend on the ternary complex factor (TCF) protein ELK1 to serve as a tethering protein for the androgen receptor (AR) to activate a critical set of growth genes. The two sites in ELK1 required for AR binding are conserved in other members of the TCF subfamily, ELK3 and ELK4. Here we examine the potential utility of the three proteins as prognosticators of disease recurrence in PCa.


METHODS
Transcriptional activity assays; Retrospective analysis of PCa recurrence using data on 501 patients in The Cancer Genome Atlas (TCGA) database; Unpaired Wilcoxon rank-sum test and multiple comparison correction using the Holm's method; Spearman's correlations; Kaplan-Meier methods; Univariable and multivariable Cox regression analyses; LASSO-based penalized Cox regression models; Time-dependent area under the receiver operating characteristic (ROC) curve.


RESULTS
ELK4 but not ELK3 was coactivated by AR similar to ELK1. Tumor expression of neither ELK3 nor ELK4 was associated with disease-free survival (DFS). ELK1 was associated with higher clinical T-stage, pathology T-stage, Gleason score, prognostic grade, and positive lymph node status. ELK1 was a negative prognosticator of DFS, independent of ELK3, ELK4, clinical T-stage, pathology T-stage, prognostic grade, lymph node status, age, and race. Inclusion of ELK1 increased the abilities of the Oncotype DX and Prolaris gene panels to predict disease recurrence, correctly predicting disease recurrence in a unique subset of patients.


CONCLUSIONS
ELK1 is a strong, independent prognosticator of disease recurrence in PCa, underscoring its unique role in PCa growth. Inclusion of ELK1 may enhance the utility of currently used prognosticators for clinical decision making in prostate cancer.",2019,The Prostate
Quantitative metabolomics reveals HFmrEF as a distinct phenotype of heart failure,"Abstract Background Heart Failure with middle-range ejection fraction (HFmrEF) has been recently acknowledged as a separate phenotype but metabolomics evaluation of this subtype remains largely untouched. Methods A quantitative metabolomics study on amino acids and acylcarnitines was performed to characterize different states of HF in 628 participants. Both multivariate orthogonal partial least squares- discriminant analysis and univariate Mann-Whitney U test were used to explore reliable metabolic profiles associated with different HF states. The resulting metabolites were further refined for obtaining diagnostic metabolite score (DMS) with ordinal logistic regression. Lasso-penalized regression was applied to produce a survival-associated prognostic metabolite score (PMS). The Cox proportional hazards model, Kaplan-Meier curve, and time-dependent ROC were used for a comprehensive assessment of prognostic value using PMS versus traditional clinical biomarkers. Results The optimized models identified a panel of fifteen differential metabolites that are shared across different HF states while some metabolites are associated with a specific state. PMS consisting of nine metabolites demonstrated an appreciably better prognostic value (hazard ratio, HR 1.62 [1.25-2.1] 95% CI, p Conclusions Targeted metabolomics has provided a novel understanding of the molecular mechanism underlying HF. Both DMS and PMS clearly demonstrated HFmrEF as a distinct phenotype between a mild HFpEF state and a severe HFrEF state. PMS exhibited superior prognostic value than Ln(NT-proBNP). Further investigation is needed with independent large-scale validation.",2020,Canadian Journal of Cardiology
A Risk Model for Prediction of 1-Year Mortality in Patients Undergoing MitraClip Implantation.,"There is a lack of specific tools for risk stratification in patients who undergo MitraClip implantation. We aimed at combining preprocedural variables with prognostic impact into a specific risk model for the prediction of 1-year mortality in patients undergoing MitraClip implantation. A total of 311 consecutive patients who underwent MitraClip implantation were included. A lasso-penalized Cox-proportional hazard regression model was used to identify independent predictors of 1-year all-cause mortality. A nomogram (GRASP [Getting Reduction of mitrAl inSufficiency by Percutaneous clip implantation] nomogram) was obtained from the Cox model. Validation was performed using internal bootstrap resampling. Forty-two deaths occurred at 1-year follow-up. The Kaplan-Meier estimate of 1-year survival was 0.845 (95% confidence interval, 0.802 to 0.895). Four independent predictors of mortality (mean arterial blood pressure, hemoglobin natural log-transformed pro-brain natriuretic peptide levels, New York Heart Association class IV at presentation) were identified. At internal bootstrap resampling validation, the GRASP nomogram had good discrimination (area under receiver operating characteristic curve of 0.78, Somers' Dxy statistic of 0.53) and calibration (le Cessie-van Houwelingen-Copas-Hosmer p value of 0.780). Conversely, the discriminative ability of the EuroSCORE II (the European System for Cardiac Operative Risk Evaluation II) and the STS-PROM (the Society of Thoracic Surgeons Predicted Risk of Mortality score) was fairly modest with area under the curve values of 0.61 and 0.55, respectively. A treatment-specific risk model in patients who undergo MitraClip implantation may be useful for the stratification of mortality at 1Â year. Further studies are needed to provide external validation and support the generalizability ofÂ the GRASP nomogram.",2017,The American journal of cardiology
Hyperspectral image spectral denoising using pure and wavelet with sparse restoration,"Hyper Spectral Image (HSI) denoising are usually classified as signal dependent and signal independent methods of denoising. In an HSI data, both signal dependent and signal independent noises co-exist and it is said as a mixed noise condition. The proposed method in this paper denoises an HSI data, in which a signal dependent Poisson noise is degraded by a signal independent AWGN. This method uses lasso regression along with Poisson based Stein Unbaised Risk Estimation to remove the Poisson degraded by AWGN. The regularization parameter is updated for every band of data and it uses wavelet for image denoising. This method is evaluated in terms SNR and IEF and compared with the other existing methods of denoising.",2015,2015 International Conference on Communications and Signal Processing (ICCSP)
Comparison of Cox Model Methods in A Low-dimensional Setting with Few Events,"Prognostic models based on survival data frequently make use of the Cox proportional hazards model. Developing reliable Cox models with few events relative to the number of predictors can be challenging, even in low-dimensional datasets, with a much larger number of observations than variables. In such a setting we examined the performance of methods used to estimate a Cox model, including (i) full model using all available predictors and estimated by standard techniques, (ii) backward elimination (BE), (iii) ridge regression, (iv) least absolute shrinkage and selection operator (lasso), and (v) elastic net. Based on a prospective cohort of patients with manifest coronary artery disease (CAD), we performed a simulation study to compare the predictive accuracy, calibration, and discrimination of these approaches. Candidate predictors for incident cardiovascular events we used included clinical variables, biomarkers, and a selection of genetic variants associated with CAD. The penalized methods, i.e., ridge, lasso, and elastic net, showed a comparable performance, in terms of predictive accuracy, calibration, and discrimination, and outperformed BE and the full model. Excessive shrinkage was observed in some cases for the penalized methods, mostly on the simulation scenarios having the lowest ratio of a number of events to the number of variables. We conclude that in similar settings, these three penalized methods can be used interchangeably. The full model and backward elimination are not recommended in rare event scenarios.",2016,"Genomics, Proteomics & Bioinformatics"
Variable selection in the general linear model for censored data,"Variable selection is a popular topic in statistics today. However, for right censored data, only a few methods are available. The principle method assumes that the data comes from a Cox proportional hazards model. In 1997, Tibshirani proposed a variation of the LASSO method that minimizes the log partial likelihood subject to the sum of the absolute values of the parameters being bounded by a constant in the Cox proportional hazards model. Due to the nature of this constraint, it shrinks coefficients and produces some coefficients that are exactly zero. The resulting prediction error is smaller than that of subset selection methods. However, the proportional hazard assumption isnâ€™t always appropriate for real data. Therefore, we apply this method to the class of models (linear regression models) in which the response variable is right censored and the error is symmetric at zero, but is otherwise distribution free. The method also uses a sieve-likelihood to calculate a variation of the LASSO criterion and uses generalized cross-validation to choose the tuning parameter. Simulation shows that this method gives smaller prediction error than the method that depends on the proportional hazard assumption in some scenarios, especially for larger sample sizes. The performance of the proposed method is also examined via a data set from a study of the ganglioside content of primary brain tumors and a data set from a study of bone marrow transplants in Chronic Myelogenous Leukemia patients.",2007,
MO-FG-202-09: Virtual IMRT QA Using Machine Learning: A Multi-Institutional Validation.,"PURPOSE
To validate a machine learning approach to Virtual IMRT QA for accurately predicting gamma passing rates using different QA devices at different institutions.


METHODS
A Virtual IMRT QA was constructed using a machine learning algorithm based on 416 IMRT plans, in which QA measurements were performed using diode-array detectors and a 3%local/3mm with 10% threshold. An independent set of 139 IMRT measurements from a different institution, with QA data based on portal dosimetry using the same gamma index and 10% threshold, was used to further test the algorithm. Plans were characterized by 90 different complexity metrics. A weighted poison regression with Lasso regularization was trained to predict passing rates using the complexity metrics as input.


RESULTS
In addition to predicting passing rates with 3% accuracy for all composite plans using diode-array detectors, passing rates for portal dosimetry on per-beam basis were predicted with an error <3.5% for 120 IMRT measurements. The remaining measurements (19) had large areas of low CU, where portal dosimetry has larger disagreement with the calculated dose and, as such, large errors were expected. These beams need to be further modeled to correct the under-response in low dose regions. Important features selected by Lasso to predict gamma passing rates were: complete irradiated area outline (CIAO) area, jaw position, fraction of MLC leafs with gaps smaller than 20 mm or 5mm, fraction of area receiving less than 50% of the total CU, fraction of the area receiving dose from penumbra, weighted Average Irregularity Factor, duty cycle among others.


CONCLUSION
We have demonstrated that the Virtual IMRT QA can predict passing rates using different QA devices and across multiple institutions. Prediction of QA passing rates could have profound implications on the current IMRT process.",2016,Medical physics
Some Analysis of the Knockoff Filter and its Variants,"In many applications, we need to study a linear regression model that consists of a response variable and a large number of potential explanatory variables and determine which variables are truly associated with the response. In 2015, Barber and Candes introduced a new variable selection procedure called the knockoff filter to control the false discovery rate (FDR) and proved that this method achieves exact FDR control. In this paper, we provide some analysis of the knockoff filter and its variants. Based on our analysis, we propose a PCA prototype group selection filter that has exact group FDR control and several advantages over existing group selection methods for strongly correlated features. Another contribution is that we propose a new noise estimator that can be incorporated into the knockoff statistic from a penalized method without violating the exchangeability property. Our analysis also reveals that some knockoff statistics, including the Lasso path and the marginal correlation statistics, suffer from the alternating sign effect. To overcome this deficiency, we introduce the notion of a good statistic and propose several alternative statistics that take advantage of the good statistic property. Finally, we present a number of numerical experiments to demonstrate the effectiveness of our methods and confirm our analysis.",2017,arXiv: Methodology
Robust causal dependence mining in big data network and its application to traffic flow predictions,"Abstract In this paper, we focus on a special problem in transportation studies that concerns the so called â€œBig Dataâ€ challenge, which is: how to build concise yet accurate traffic flow prediction models based on the massive data collected by different sensors ? The size of the data, the hidden causal dependence and the complexity of traffic time series are some of the obstacles that affect making reliable forecast at a reasonable cost, both time-wise and computation-wise. To better prepare the data for traffic modeling, we introduce a multiple-step strategy to process the raw â€œBig Dataâ€ into compact time series that are better suited for regression and causality analysis. First, we use the Granger causality to define and determine the potential dependence among data, and produce a much condensed set of times series who are also highly dependent. Next, we deploy a decomposition algorithm to separate daily-similar trend and nonstationary bursts components from the traffic flow time series yielded by the Granger test. The decomposition results are then treated by two rounds of Lasso regression: the standard Lasso method is first used to quickly filter out most of the irrelevant data, followed by a robust Lasso method to further remove the disturbance caused by bursts components and recover the strongest dependence among the remaining data. Test results show that the proposed method significantly reduces the costs of building prediction models. Moreover, the obtained causal dependence graph reveals the relationship between the structure of road networks and the correlations among traffic time series. All these findings are useful for building better traffic flow prediction models.",2015,Transportation Research Part C-emerging Technologies
Sparse methods for wind energy prediction,"In this work we will analyze and apply to the prediction of wind energy some of the best known regularized linear regression algorithms, such as Ordinary Least Squares, Ridge Regression and, particularly, Lasso, Group Lasso and Elastic-Net that also seek to impose a certain degree of sparseness on the final models. To achieve this goal, some of them introduce a non-differentiable regularization term that requires special techniques to solve the corresponding optimization problem that will yield the final model. Proximal Algorithms have been recently introduced precisely to handle this kind of optimization problems, and so we will briefly review how to apply them in regularized linear regression. Moreover, the proximal method FISTA will be used when applying the non-differentiable models to the problem of predicting the global wind energy production in Spain, using as inputs numerical weather forecasts for the entire Iberian peninsula. Our results show how some of the studied sparsity-inducing models are able to produce a coherent selection of features, attaining similar performance to a baseline model using expert information, while making use of less data features.",2012,The 2012 International Joint Conference on Neural Networks (IJCNN)
Dimensionality reduction via variables selection â€“ Linear and nonlinear approaches with application to vibration-based condition monitoring of planetary gearbox,"Abstract Feature extraction and variable selection are two important issues in monitoring and diagnosing a planetary gearbox. The preparation of data sets for final classification and decision making is usually a multi-stage process. We consider data from two gearboxes, one in a healthy and the other in a faulty state. First, the gathered raw vibration data in time domain have been segmented and transformed to frequency domain using power spectral density. Next, 15 variables denoting amplitudes of calculated power spectra were extracted; these variables were further examined with respect to their diagnostic ability. We have applied here a novel hybrid approach: all subset search by using multivariate linear regression (MLR) and variables shrinkage by the least absolute selection and shrinkage operator (Lasso) performing a non-linear approach. Both methods gave consistent results and yielded subsets with healthy or faulty diagnostic properties.",2014,Applied Acoustics
The effects of co-morbidity in defining major depression subtypes associated with long-term course and severity.,"BACKGROUND
Although variation in the long-term course of major depressive disorder (MDD) is not strongly predicted by existing symptom subtype distinctions, recent research suggests that prediction can be improved by using machine learning methods. However, it is not known whether these distinctions can be refined by added information about co-morbid conditions. The current report presents results on this question.


METHOD
Data came from 8261 respondents with lifetime DSM-IV MDD in the World Health Organization (WHO) World Mental Health (WMH) Surveys. Outcomes included four retrospectively reported measures of persistence/severity of course (years in episode; years in chronic episodes; hospitalization for MDD; disability due to MDD). Machine learning methods (regression tree analysis; lasso, ridge and elastic net penalized regression) followed by k-means cluster analysis were used to augment previously detected subtypes with information about prior co-morbidity to predict these outcomes.


RESULTS
Predicted values were strongly correlated across outcomes. Cluster analysis of predicted values found three clusters with consistently high, intermediate or low values. The high-risk cluster (32.4% of cases) accounted for 56.6-72.9% of high persistence, high chronicity, hospitalization and disability. This high-risk cluster had both higher sensitivity and likelihood ratio positive (LR+; relative proportions of cases in the high-risk cluster versus other clusters having the adverse outcomes) than in a parallel analysis that excluded measures of co-morbidity as predictors.


CONCLUSIONS
Although the results using the retrospective data reported here suggest that useful MDD subtyping distinctions can be made with machine learning and clustering across multiple indicators of illness persistence/severity, replication with prospective data is needed to confirm this preliminary conclusion.",2014,Psychological medicine
Variable selection via Lasso with high-dimensional proteomic data,"Variable Selection via Lasso with High-dimensional Proteomic Data by Hongxuan Zhai A.M. in Statistics, Washington University in St. Louis, 2018. Professor Todd Kuffner, Chair Multiclass classification with high-dimensional data is an applied topic both in statistics and machine learning. The classification procedure could be done in various ways. In this thesis, we review the theory of the Lasso procedure which provides a parameter estimator while simultaneously achieving dimension reduction due to a property of the `1 norm. Lasso with elastic net penalty and sparse group lasso are also reviewed. Our data is high-dimensional proteomic data (iTRAQ ratios) of breast cancer patients with four subtypes of breast cancer. We use the multinomial logistic regression to train our classifier and use the false classification rates obtained from cross validation to compare models.",2018,
Solution paths for the generalized lasso with applications to spatially varying coefficients regression,"Abstract Penalized regression can improve prediction accuracy and reduce dimension. The generalized lasso problem is used in many applications in various fields. The generalized lasso penalizes a linear transformation of the coefficients rather than the coefficients themselves. The proposed algorithm solves the generalized lasso problem and provides the full solution path. A confidence set can then be constructed on the generalized lasso parameters based on the modified residual bootstrap lasso. The approach is demonstrated using spatially varying coefficients regression, and it is shown to be both accurate and efficient compared to previous work.",2020,Comput. Stat. Data Anal.
Knowledge guided multi-level network inference,"Constantly decreasing costs of high-throughput profiling on many molecular levels generate vast amounts of so-called multi-omics data. Studying one biomedical question on two or more omic levels provides deeper insights into underlying molecular processes or disease pathophysiology. For the majority of multi-omics data projects, the data analysis is performed level-wise, followed by a combined interpretation of results. Few exceptions exist, for example the pairwise integration for quantitative trait analysis. However, the full potential of integrated data analysis is not leveraged yet, presumably due to the complexity of the data and the lacking toolsets. Here we propose a versatile approach, to perform a multi-level integrated analysis: The Knowledge guIded Multi-Omics Network inference approach, KiMONo. KiMONo performs network inference using statistical modeling on top of a powerful knowledge-guided strategy exploiting prior information from biological sources. Within the resulting network, nodes represent features of all input types and edges refer to associations between them, e.g. underlying a disease. Our method infers the network by combining sparse grouped-LASSO regression with a genomic position-confined Biogrid protein-protein interaction prior. In a comprehensive evaluation, we demonstrate that our method is robust to noise and still performs on low-sample size data. Applied to the five-level data set of the publicly available Pan-cancer collection, KiMONO integrated mutation, epigenetics, transcriptomics, proteomics and clinical information, detecting cancer specific omic features. Moreover, we analysed a four-level data set from a major depressive disorder cohort, including genetic, epigenetic, transcriptional and clinical data. Here we demonstrated KiMONoâ€™s analytical power to identify expression quantitative trait methylation sites and loci and show itâ€™s advantage to state-of-the-art methods. Our results show the general applicability to the full spectrum multi-omics data and demonstrating that KiMONo is a powerful approach towards leveraging the full potential of data sets. The method is freely available as an R package (https://github.com/cellmapslab/kimono).",2020,bioRxiv
Kernelized Elastic Net Regularization based on Markov selective sampling,"Abstract This paper extends Kernelized Elastic Net Regularization (KENReg) algorithm from the assumption of independent and identically distributed (i.i.d.) samples to the case of non-i.i.d. samples. We first establish the generalization bounds of KENReg algorithm with uniformly ergodic Markov chain samples, then we prove that the KENReg algorithm with uniformly ergodic Markov chain samples is consistent and obtain the fast learning rate of KENReg algorithm with uniformly ergodic Markov chain samples. We also introduce the KENReg algorithm based on Markov selective sampling. Based on Gaussian kernels, the advantages of KENReg algorithm against the traditional one with i.i.d. samples are demonstrated on various real-world datasets. Compared to randomly independent sampling, experimental results show that the KENReg algorithm based on Markov selective sampling not only has much higher prediction accuracy in terms of mean square errors and generates simpler models in terms of the number of non-zero regression coefficients, but also has shorter total time of sampling and training. We compare the algorithm proposed in this paper with these known regularization algorithms, like kernelized Ridge regression and kernelized Least absolute shrinkage and selection operator (Lasso).",2019,Knowl. Based Syst.
Variable Inclusion and Shrinkage Algorithms,"The Lasso is a popular and computationally efficient procedu re for automatically performing both variable selection and coefficient shrinka ge on linear regression models. One limitation of the Lasso is that the same tuning param eter is used for both variable selection and shrinkage. As a result, it typically ends up selecting a model with too many variables to prevent over shrinkage of the regr ession coefficients. We suggest an improved class of methods called â€Variable Inclu sion and Shrinkage Algorithmsâ€ (VISA). Our approach is capable of selecting sparse models while avoiding over shrinkage problems and uses a path algorithm so is also c omputationally efficient. We show through extensive simulations that VISA sign ificantly outperforms the Lasso and also provides improvements over more recent pr ocedures, such as the Dantzig selector, Relaxed Lasso and Adaptive Lasso. In addi tion, we provide theoretical justification for VISA in terms of non-asymptotic bound s on the estimation error that suggest it should exhibit good performance even for lar ge numbers of predictors. Finally, we extend the VISA methodology, path algorithm, an d theoretical bounds to the Generalized Linear Models framework. Some key words : Variable Selection; Lasso; Generalized Linear Models; Da ntzig Selector.",2008,
Sharp Threshold for Multivariate Multi-Response Linear Regression via Block Regularized Lasso,"In this paper, we investigate a multivariate multi-response (MVMR) linear regression problem, which contains multiple linear regression models with differently distributed design matrices, and different regression and output vectors. The goal is to recover the support union of all regression vectors using $l_1/l_2$-regularized Lasso. We characterize sufficient and necessary conditions on sample complexity \emph{as a sharp threshold} to guarantee successful recovery of the support union. Namely, if the sample size is above the threshold, then $l_1/l_2$-regularized Lasso correctly recovers the support union; and if the sample size is below the threshold, $l_1/l_2$-regularized Lasso fails to recover the support union. In particular, the threshold precisely captures the impact of the sparsity of regression vectors and the statistical properties of the design matrices on sample complexity. Therefore, the threshold function also captures the advantages of joint support union recovery using multi-task Lasso over individual support recovery using single-task Lasso.",2013,ArXiv
Adaptive Bayesian SLOPE -- High-dimensional Model Selection with Missing Values,"The selection of variables with high-dimensional and missing data is a major challenge and very few methods are available to solve this problem. Here we propose a new method -- adaptive Bayesian SLOPE -- which is an extension of the SLOPE method of sorted $l_1$ regularization within a Bayesian framework and which allows to simultaneously estimate the parameters and select variables for large data despite missing values. The method follows the idea of the Spike and Slab LASSO, but replaces the Laplace mixture prior with the frequentist motivated ""SLOPE"" prior, which targets control of the False Discovery Rate. The regression parameters and the noise variance are estimated using stochastic approximation EM algorithm, which allows to incorporate missing values as well as latent model parameters, like the signal magnitude and its sparsity. Extensive simulations highlight the good behavior in terms of power, FDR and estimation bias under a wide range of simulation scenarios. Finally, we consider an application of severely traumatized patients from Paris hospitals to predict the level of platelet, and demonstrate, beyond the advantage of selecting relevant variables, which is crucial for interpretation, excellent predictive capabilities. The methodology is implemented in the R package ABSLOPE, which incorporates C++ code to improve the efficiency of the proposed method.",2019,arXiv: Methodology
"The Impact of customers' relation quality and sensing on the marketing performance for micro industries development in Banten Province, Indonesia","This research was intended to evaluate the marketing performance as one of the food and beverage Micro Industries (MI's) constraints in Banten Province. We built a relation model between the marketing performance and the customer's relation quality and sensing by which they determined some empowerment development strategies for independence and capable MI. Data for the model building was taken by interview based on a set of questionnaire to the 130 chosen respondents by simple random sampling. They were split into training and test sets. Then three regression models were applied namely best-subset, ridge, and lasso regression model to the training set. The result showed that the ridge regression model was the best model having the smallest MSE (Mean Squared Error). When it applied to the test set, its standardized coefficients showed the customers' relation quality needed to be improved for the MI development strategies. There were keeping, offering, and maintaining strategies as the main strategies to develop the MI. The keeping strategy related to trust and agreement, while the offering strategy was supplying products and information. The maintaining strategy, on the other hand, was preserving emotional closeness.",2014,Review of applied socio-economic research
Regularized multivariate stochastic regression,"In many high dimensional problems, the dependence structure among the variables can be quite complex. An appropriate use of the regularization techniques coupled with other classical statistical methods can often improve estimation and prediction accuracy and facilitate model interpretation, by seeking a parsimonious model representation that involves only the subset of revelent variables. We propose two regularized stochastic regression approaches, for efficiently estimating certain sparse dependence structure in the data. We first consider a multivariate regression setting, in which the large number of responses and predictors may be associated through only a few channels/pathways and each of these associations may only involve a few responses and predictors. We propose a regularized reduced-rank regression approach, in which the model estimation and rank determination are conducted simultaneously and the resulting regularized estimator of the coefficient matrix admits a sparse singular value decomposition (SVD). Secondly, we consider model selection of subset autoregressive moving-average (ARMA) modelling, for which automatic selection methods do not directly apply because the innovation process is latent. We propose to identify the optimal subset ARMA model by fitting a penalized regression, e.g. adaptive Lasso, of the time series on its lags and the lags of the residuals from a long autoregression fitted to the time-series data, where the residuals serve as proxies for the innovations. Computation algorithms and regularization parameter selection methods for both proposed approaches are developed, and their properties are ex-",2011,
Bayesian lasso regression,"The lasso estimate for linear regression corresponds to a posterior mode when independent, double-exponential prior distributions are placed on the regression coefficients. This paper introduces new aspects of the broader Bayesian treatment of lasso regression. A direct characterization of the regression coefficients' posterior distribution is provided, and computation and inference under this characterization is shown to be straightforward. Emphasis is placed on point estimation using the posterior mean, which facilitates prediction of future observations via the posterior predictive distribution. It is shown that the standard lasso prediction method does not necessarily agree with model-based, Bayesian predictions. A new Gibbs sampler for Bayesian lasso regression is introduced. Copyright 2009, Oxford University Press.",2009,Biometrika
Comparisons of single-stage and two-stage approaches to genomic selection,"Genomic selection (GS) is a method for predicting breeding values of plants or animals using many molecular markers that is commonly implemented in two stages. In plant breeding the first stage usually involves computation of adjusted means for genotypes which are then used to predict genomic breeding values in the second stage. We compared two classical stage-wise approaches, which either ignore or approximate correlations among the means by a diagonal matrix, and a new method, to a single-stage analysis for GS using ridge regression best linear unbiased prediction (RR-BLUP). The new stage-wise method rotates (orthogonalizes) the adjusted means from the first stage before submitting them to the second stage. This makes the errors approximately independently and identically normally distributed, which is a prerequisite for many procedures that are potentially useful for GS such as machine learning methods (e.g. boosting) and regularized regression methods (e.g. lasso). This is illustrated in this paper using componentwise boosting. The componentwise boosting method minimizes squared error loss using least squares and iteratively and automatically selects markers that are most predictive of genomic breeding values. Results are compared with those of RR-BLUP using fivefold cross-validation. The new stage-wise approach with rotated means was slightly more similar to the single-stage analysis than the classical two-stage approaches based on non-rotated means for two unbalanced datasets. This suggests that rotation is a worthwhile pre-processing step in GS for the two-stage approaches for unbalanced datasets. Moreover, the predictive accuracy of stage-wise RR-BLUP was higher (5.0â€“6.1Â %) than that of componentwise boosting.",2012,Theoretical and Applied Genetics
A Feature Sampling Strategy for Analysis of High Dimensional Genomic Data,"With the development of high throughput technology, it has become feasible and common to profile tens of thousands of gene activities simultaneously. These genomic data typically have sample size of hundreds or fewer, which is much less than the feature size (number of genes). In addition, the genes, in particular the ones from the same pathway, are often highly correlated. These issues impose a great challenge for selecting meaningful genes from a large number of (correlated) candidates in many genomic studies. Quite a few methods have been proposed to attack this challenge. Among them, regularization-based techniques, e.g., lasso, become much more appealing, because they can do model fitting and variable selection at the same time. However, the lasso regression has its known limitations. One is that the number of genes selected by the lasso couldnâ€™t exceed the number of samples. Another limitation is that, if causal genes are highly correlated, the lasso tends to select only one or few genes from them. Biologists, however, desire to identify them all. To overcome these limitations, we present here a novel, robust, and stable variable selection method. Through simulation studies and a real application to the transcriptome data, we demonstrate the superiority of the proposed method in selecting highly correlated causal genes. We also provide some theoretical justifications for this feature sampling strategy based on the mean and variance analyses.",2019,IEEE/ACM Transactions on Computational Biology and Bioinformatics
Regularising the Factor Zoo with OWL Chuanping Sun,"Hundreds of anomaly variables have been proposed, claiming explanatory power to the cross-section of average returns in equity market. Cochrane (2011) dubs this phenomenon the â€factor zooâ€ and further argues that the characteristic related factors to explain the average returns are in disarray. This paper introduces a newly developed machine learning tool, ordered and weighted L1 norm regularisation (OWL) to â€regulariseâ€ this chaotic â€factor zooâ€. The innovation of OWL is that high correlations among explanatory variables are permitted. Highly correlated variables will be identified simultaneously and grouped together. This is important because factor correlation prevails in high dimensionality and biases standard estimators (Fama-MacBeth regression, LASSO, etc.) yet it has not been discussed extensively in the literature. Empirical evidence suggests that â€™liquidityâ€™ related factors play an important role in explaining the cross-section of average returns. Further robustness check shows that OWL selected factors have superior performance in a broad range of criteria. Out-of-sample Sharpe ratio of hedge portfolio, formed using OWL selected factors as predictors in the past two decades, is around 3.5 (annualised) considering all stocks, and above 2.2 when excluding micro stocks. JEL classification: C33 C49 C52 C55 C58 G12",2018,
Frank-Wolfe with Subsampling Oracle,"We analyze two novel randomized variants of the Frank-Wolfe (FW) or conditional gradient algorithm. While classical FW algorithms require solving a linear minimization problem over the domain at each iteration, the proposed method only requires to solve a linear minimization problem over a small \emph{subset} of the original domain. The first algorithm that we propose is a randomized variant of the original FW algorithm and achieves a $\mathcal{O}(1/t)$ sublinear convergence rate as in the deterministic counterpart. The second algorithm is a randomized variant of the Away-step FW algorithm, and again as its deterministic counterpart, reaches linear (i.e., exponential) convergence rate making it the first provably convergent randomized variant of Away-step FW. In both cases, while subsampling reduces the convergence rate by a constant factor, the linear minimization step can be a fraction of the cost of that of the deterministic versions, especially when the data is streamed. We illustrate computational gains of the algorithms on regression problems, involving both $\ell_1$ and latent group lasso penalties.",2018,
High-dimensional Linear Regression Analysis by using Genetic Algorithm,"The research objective is to study the effectiveness of parameter estimation and variable selection by using genetic algorithm in the high-dimensional linear regression analysis. The results of the proposed method from the simulation are compared with the other three well-known methods: lasso, elastic net, and stepwise regression. The comparison criteria are the percentage of the number of correct fitting models, the percentage of the number of over-fitting models, the percentage of the number of under-fitting models, the percentage of the number of incorrect fitting models including mean squared error and the accuracy of the parameter estimates. It can be concluded that the direct selection by genetic algorithm yields the best results when compared with the other three methods in nearly all cases. Keywords : variable selection, genetic algorithm, high-dimensional data, linear regression analysis",2018,
Identification of a novel four-lncRNA signature as a prognostic indicator in cirrhotic hepatocellular carcinoma,"Background
Many studies have shown that long noncoding RNAs (lncRNA) are closely associated with the occurrence and development of various tumors and have the potential to be prognostic markers. Moreover, cirrhosis is an important prognostic risk factors in patients with liver cancer. Some studies have reported that lncRNA-related prognostic models have been used to predict overall survival (OS) and recurrence-free survival (RFS) in patients with hepatocellular carcinoma (HCC). However, no one has constructed a prognostic lncRNA model only in patients with cirrhotic HCC. Thus, it is necessary to screen novel potential lncRNA markers for improve the prognosis of cirrhotic HCC patients.


Methods
The probe expression profile dataset (GSE14520-GPL3921) from the Gene Expression Omnibus (GEO), which included 204 cirrhotic HCC samples, was reannotated and the lncRNA and mRNA expression dataset was obtained. The patients were randomly assigned to either the training set (nÂ =Â 103) and testing set (nÂ =Â 100). Univariate cox regression and the least absolute shrinkage and selection operator (LASSO) model were applied to screen lncRNAs linked to the OS of cirrhotic HCC in the training set. The lncRNAs having significant correlation with OS were then selected and the multivariate Cox regression model was implemented to construct the prognostic score model. Whether or not this model was related to RFS in the training set was simultaneously determined. The testing set was used to validate the lncRNA risk score model. A risk score based on the lncRNA signature was used for stratified analysis of different clinical features to test their prognostic performance. The prognostic lncRNA-related protein genes were identified by the co-expression matrix of lncRNA-mRNA, and the function of these lncRNAs was predicted through the enrichment of these co-expression genes.


Results
The signature consisted of four lncRNAs:AC093797.1,POLR2J4,AL121748.1 and AL162231.4. The risk model was closely correlated with the OS of cirrhotic HCC in the training cohort, with a hazard ratio (HR) of 3.650 (95% CI [1.761-7.566]) and log-rank P value of 0.0002. Moreover, this model also showed favorable prognostic significance for RFS in the training set (HR: 2.392, 95% CI [1.374-4.164], log-rank PÂ =Â 0.0015). The predictive performance of the four-lncRNA model for OS and RFS was verified in the testing set. Furthermore, the results of stratified analysis revealed that the four-lncRNA model was an independent factor in the prediction of OS and RFS of patients with clinical characteristics such as TNM (Tumor, Node, Metastasis system) stages I-II, Barcelona Clinic Liver Cancer (BCLC) stages 0-A, and solitary tumors in both the training set and testing set. The results of functional prediction showed that four lncRNAs may be potentially involve in multiple metabolic processes, such as amino acid, lipid, and glucose metabolism in cirrhotic HCC.",2019,PeerJ
HIV among Female Sex Workers in Five Cities in Burkina Faso: A Cross-Sectional Baseline Survey to Inform HIV/AIDS Programs,"Background
Female sex workers (FSWs) are considered a vulnerable population for HIV infection and a priority for HIV/AIDS response programs. This study aimed to determine HIV prevalence among FSWs in five cities in Burkina Faso.


Methods
FSWs aged 18 and older were recruited using respondent driven sampling (RDS) in five cities (Ouagadougou, Bobo-Dioulasso, Koudougou, Ouahigouya, and Tenkodogo) in Burkina Faso from 2013 to 2014. HIV testing was performed using the HIV testing national algorithm. We conducted bivariate and multivariate logistic regression analysis to assess correlates of HIV in all cities combined (not RDS-adjusted).


Results
Among Ouagadougou, Koudougou, and Ouahigouya FSWs, RDS-adjusted HIV prevalence was 13.5% (95% Confidence Interval [CI]: 9.6-18.7), 13.3% (95% CI: 7.6-22.4), and 13.0% (95% CI: 7.6-21.3), respectively, compared to 30.1% (95% CI: 25.5-35.1) among Bobo-Dioulasso FSWs. Factors associated with HIV infection were age (adjusted odds ratio [aOR] = 7.84 95% CI: 3.78-16.20), being married or cohabitating (aOR = 2.43, 95% CI: 1.31-4.49), and history of pregnancy (aOR = 5.24, 95% CI: 1.44-18.97).


Conclusion
These results highlight the need to strengthen HIV prevention among FSWs, through behavior change strategies, and improve access to sexual and reproductive health services.",2017,AIDS Research and Treatment
"POSTER SESSION 2: Thursday, 1 May 2008, 13:30â€“18:00 Location: Poster Area","R Mitsutake; S Miura; B Zhang; K Saku Fukuoka University Hospital Cardiology, Fukuoka, Japan Background: A relationship has been noted between lower levels of HDL-associated phospholipid and atherosclerotic coronary artery disease (CAD). The aim of this study was to assess the relationship between numerous lipidemic factors and severity of CAD as evaluated by multi-detector row computed tomography (MDCT). Methods and Results: Subjects included 195 consecutive patients who underwent coronary angiography using MDCT because of suspected CAD. Coronary artery calcification (CAC) score and Gensiniâ€™s score (GS) as evaluated by MDCT, platelet-activating factor acetylhydrolase (PAF-AH), free cholesterol (FC), phospholipid (PL), remnant-like lipoprotein particle-cholesterol (RLP-C), apolipoprotein (apo)-C, apo-B, apo-C3, apo-E, highly-sensitive CRP were determined. The number of the significant coronary vessel disease (VD) was determined using MDCT. The number of VD was significantly associated with HDL-C, HDL-associated HDL-C, HDL-associated PL, HDL-associated FC and RLP-C. Logistic regression analysis revealed that the number of VD was most closely correlated with HDL-C levels (p=0.02). GS was significantly associated with age, HDL cholesterol, hemoglobin A1c, HDLassociated PAF-AH, HDL-associated FC and HDL-associated phospholipid. Multivariate analysis revealed that GS was most closely correlated with HDL-associated phospholipid (p=0.02). Conclusion: Lower levels of HDL-associated phospholipid may be an indicator and provide additional information regarding the severity of CAD compared with other lipidemic factors.",2008,European Journal of Cardiovascular Prevention & Rehabilitation
Prediction of maize phenotype based on whole-genome single nucleotide polymorphisms using deep belief networks,"Selection in plant breeding could be more effective and more efficient if it is based on genomic data. Genomic selection (GS) is a new approach for plant-breeding selection that exploits genomic data through a mechanism called genomic prediction (GP). Most of GP models used linear methods that ignore effects of interaction among genes and effects of higher order nonlinearities. Deep belief network (DBN), one of the architectural in deep learning methods, is able to model data in high level of abstraction that involves nonlinearities effects of the data. This study implemented DBN for developing a GP model utilizing whole-genome Single Nucleotide Polymorphisms (SNPs) as data for training and testing. The case study was a set of traits in maize. The maize dataset was acquisitioned from CIMMYT's (International Maize and Wheat Improvement Center) Global Maize program. Based on Pearson correlation, DBN is outperformed than other methods, kernel Hilbert space (RKHS) regression, Bayesian LASSO (BL), best linear unbiased predictor (BLUP), in case allegedly non-additive traits. DBN achieves correlation of 0.579 within -1 to 1 range.",2017,
Error estimation for surrogate models of dynamical systems using machine learning,"This work proposes a machine-learning-based framework for estimating the error introduced by surrogate models of parameterized dynamical systems. The framework applies high-dimensional regression techniques (e.g., random forests, LASSO) to map a large set of inexpensively-computed 'error indicators' (i.e., features) produced by the surrogate model at a given time instance to a prediction of the surrogate-model error in a quantity of interest (QoI). The methodology requires a training set of parameter instances at which the time-dependent surrogate-model error is computed by simulating both the high-fidelity and surrogate models. Using these training data, the method first performs feature-space partitioning (via classification or clustering), and subsequently constructs a 'local' regression model to predict the time-instantaneous error within each identified region of feature space. We consider two uses for the resulting error model: (1) as a correction to the surrogate-model QoI prediction at each time instance, and (2) as a way to statistically model arbitrary functions of the time-dependent surrogate-model error (e.g., time-integrated errors). We apply the proposed framework to a nonlinear oil-water problem with time-varying well-control (bottom-hole pressure) parameters, and consider proper orthogonal decomposition applied with trajectory piecewise linearization (POD-TPWL) as the surrogate model. When the first use of the method is considered, numerical experiments demonstrate that the method consistently improves accuracy in the time-instantaneous QoI prediction relative to the original surrogate model across a large number of test cases. When the second use is considered, numerical experiments show that the proposed method generates accurate, unbiased statistical predictions of the time- and well-averaged errors.",2017,ArXiv
Predicting qualitative phenotypes from microarray data â€“ the Eadgene pig data set,"BackgroundThe aim of this work was to study the performances of 2 predictive statistical tools on a data set that was given to all participants of the Eadgene-SABRE Post Analyses Working Group, namely the Pig data set of Hazard et al. (2008). The data consisted of 3686 gene expressions measured on 24 animals partitioned in 2 genotypes and 2 treatments. The objective was to find biomarkers that characterized the genotypes and the treatments in the whole set of genes.MethodsWe first considered the Random Forest approach that enables the selection of predictive variables. We then compared the classical Partial Least Squares regression (PLS) with a novel approach called sparse PLS, a variant of PLS that adapts lasso penalization and allows for the selection of a subset of variables.ResultsAll methods performed well on this data set. The sparse PLS outperformed the PLS in terms of prediction performance and improved the interpretability of the results.ConclusionWe recommend the use of machine learning methods such as Random Forest and multivariate methods such as sparse PLS for prediction purposes. Both approaches are well adapted to transcriptomic data where the number of features is much greater than the number of individuals.",2009,BMC Proceedings
High-dimensional Cox models: the choice of penalty as part of the model building process.,"The Cox proportional hazards regression model is the most popular approach to model covariate information for survival times. In this context, the development of high-dimensional models where the number of covariates is much larger than the number of observations (p>>n) is an ongoing challenge. A practicable approach is to use ridge penalized Cox regression in such situations. Beside focussing on finding the best prediction rule, one is often interested in determining a subset of covariates that are the most important ones for prognosis. This could be a gene set in the biostatistical analysis of microarray data. Covariate selection can then, for example, be done by L(1)-penalized Cox regression using the lasso (Tibshirani (1997). Statistics in Medicine 16, 385-395). Several approaches beyond the lasso, that incorporate covariate selection, have been developed in recent years. This includes modifications of the lasso as well as nonconvex variants such as smoothly clipped absolute deviation (SCAD) (Fan and Li (2001). Journal of the American Statistical Association 96, 1348-1360; Fan and Li (2002). The Annals of Statistics 30, 74-99). The purpose of this article is to implement them practically into the model building process when analyzing high-dimensional data with the Cox proportional hazards model. To evaluate penalized regression models beyond the lasso, we included SCAD variants and the adaptive lasso (Zou (2006). Journal of the American Statistical Association 101, 1418-1429). We compare them with ""standard"" applications such as ridge regression, the lasso, and the elastic net. Predictive accuracy, features of variable selection, and estimation bias will be studied to assess the practical use of these methods. We observed that the performance of SCAD and adaptive lasso is highly dependent on nontrivial preselection procedures. A practical solution to this problem does not yet exist. Since there is high risk of missing relevant covariates when using SCAD or adaptive lasso applied after an inappropriate initial selection step, we recommend to stay with lasso or the elastic net in actual data applications. But with respect to the promising results for truly sparse models, we see some advantage of SCAD and adaptive lasso, if better preselection procedures would be available. This requires further methodological research.",2010,Biometrical journal. Biometrische Zeitschrift
EM Adaptive LASSOâ€”A Multilocus Modeling Strategy for Detecting SNPs Associated with Zero-inflated Count Phenotypes,"Count data are increasingly ubiquitous in genetic association studies, where it is possible to observe excess zero counts as compared to what is expected based on standard assumptions. For instance, in rheumatology, data are usually collected in multiple joints within a person or multiple sub-regions of a joint, and it is not uncommon that the phenotypes contain enormous number of zeroes due to the presence of excessive zero counts in majority of patients. Most existing statistical methods assume that the count phenotypes follow one of these four distributions with appropriate dispersion-handling mechanisms: Poisson, Zero-inflated Poisson (ZIP), Negative Binomial, and Zero-inflated Negative Binomial (ZINB). However, little is known about their implications in genetic association studies. Also, there is a relative paucity of literature on their usefulness with respect to model misspecification and variable selection. In this article, we have investigated the performance of several state-of-the-art approaches for handling zero-inflated count data along with a novel penalized regression approach with an adaptive LASSO penalty, by simulating data under a variety of disease models and linkage disequilibrium patterns. By taking into account data-adaptive weights in the estimation procedure, the proposed method provides greater flexibility in multi-SNP modeling of zero-inflated count phenotypes. A fast coordinate descent algorithm nested within an EM (expectation-maximization) algorithm is implemented for estimating the model parameters and conducting variable selection simultaneously. Results show that the proposed method has optimal performance in the presence of multicollinearity, as measured by both prediction accuracy and empirical power, which is especially apparent as the sample size increases. Moreover, the Type I error rates become more or less uncontrollable for the competing methods when a model is misspecified, a phenomenon routinely encountered in practice.",2016,Frontiers in Genetics
