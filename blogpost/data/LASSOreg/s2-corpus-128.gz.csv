title,abstract,year,journal
Modelling interactions in high-dimensional data with backtracking,We study the problem of high-dimensional regression when there may be interacting variables. Approaches using sparsity-inducing penalty functions such as the Lasso can be useful for producing inter...,2016,Journal of Machine Learning Research
Statistical Inference for Structured High-dimensional Models 593,"High-dimensional statistical inference is a newly emerged direction of statistical science in the 21 century. Its importance is due to the increasing dimensionality and complexity of models needed to process and understand the modern real world data. The main idea making possible meaningful inference about such models is to assume suitable lower dimensional underlying structure or low-dimensional approximations, for which the error can be reasonably controlled. Several types of such structures have been recently introduced including sparse high-dimensional regression, sparse and/or low rank matrix models, matrix completion models, dictionary learning, network models (stochastic block model, mixed membership models) and more. The workshop focused on recent developments in structured sequence and regression models, matrix and tensor estimation, robustness, statistical learning in complex settings, network data, and topic models. Mathematics Subject Classification (2010): 62Gxx (in particular, 62G05, 62G08, 62G10). Introduction by the Organisers The workshop Statistical Inference for Structured High-Dimensional Models, organized by Anatoli Juditsky (UniversitÃ© Grenoble-Alpes), Alexandre Tsybakov (CREST, ENSAE), and Cun-Hui Zhang (Rutgers University), was held March 11th â€“ March 17th, 2018. The workshop aimed to highlight recent achievements in high-dimensional inference for structured statistical models based on the interplay of techniques from mathematical statistics, optimization theory and highdimensional probability, and to bring together researchers to exchange the ideas and to explore open mathematical problems. These goals were largely achieved. 592 Oberwolfach Report 12/2018 The workshop was well attended by 52 participants with broad geographic representation from three continents. Twenty five talks were presented, and seven PhD students shortly presented their work in a â€Young researcherâ€™s seriesâ€ on Tuesday evening. The talks can be roughly chategorized into the following topics, which the workshop was focused on. Estimation and inference in structured sequence and high-dimensional regression models: Pierre Bellec reports recent advances on the noise-barrier and signal bias of the Lasso and other convex estimators; Emmanuel Candes presents an asymptotic theory in logistic regression in the regime where the number of data points is of the same order as the number of unknown parameters; Richard Samworth studies the least square estimation in isotonic regression in general dimensions; Bin Yu studies local identifiability analysis of dictionary learning. Matrix and tensor estimation: Vladimir Koltchinskii discusses asymptotically efficient estimation of functionals of high-dimensional covariance; Zongming Ma reports recent developments in local asymptotic normality in spiked random matrix models; Vladimir Spokoiny studies large ball probability with applications to inference for spectral projectors; Martin Wahl presents relative perturbation bounds with applications to empirical covariance operators; Dong Xia studies noisy low rank tensor completion; Anru Zhang discusses singular value decomposition for high-dimensional tensor data. Robust inference: Rina Foygel Barber studies robust inference with the knockoff filter; Olivier Collier presents recent work on sparse functional estimation and robust variance estimation; Arnak Dalalyan studies statistically and computationally efficient estimation of multidimensional linear functionals; Stanislav Minsker considers robust modifications of U-statistics and their applications. Statistical leaning in complex settings and computational issues: Chao Gao presents convergence rates of variational posteriors; Alexandra Carpentier studies hypothesis testing with Gaussian mixture models; Andrea Montanari studies feasibility in weak recovery of high-dimensional signals; Boaz Nadler develops an asymptotic theory of projection pursuit in high dimensions; Richard Nickl studies information operators and statistical inverse problems; Johannes Schmidt-Hieber presents a statistical theory for deep neural networks; Yihong Wu studies optimal estimation of Gaussian mixtures via denoised method of moments. Network data, topic models and other applications: Mladen Kolar studies estimation and inference for differential networks; Jing Lei discusses nonparametric network representation and estimation using graph root distribution; Florentina Bunea presents optimal estimation of structured loading matrices with applications to overlapping clustering and topic models; Zheng Ke develops a spectral approach to optimal topic estimation. Statistical Inference for Structured High-dimensional Models 593 Acknowledgement: The MFO and the workshop organizers would like to thank the National Science Foundation for supporting the participation of junior researchers in the workshop by the grant DMS-1641185, â€œUS Junior Oberwolfach Fellowsâ€. Moreover, the MFO and the workshop organizers would like to thank the Simons Foundation for supporting YihongWu in the â€œSimons Visiting Professorsâ€ program at the MFO. Statistical Inference for Structured High-dimensional Models 595 Workshop: Statistical Inference for Structured High-dimensional Models",2019,
Nomogram based on shear-wave elastography radiomics can improve preoperative cervical lymph node staging for papillary thyroid carcinoma.,"Background Accurate preoperative prediction of cervical lymph node (LN) metastasis in patients with papillary thyroid carcinoma (PTC) provide a basis for surgical decision making and the extent of tumor resection. This study aimed to develop and validate an ultrasound radiomics nomogram for the preoperative assessment of LN status. Methods Data from 147 PTC patients at the Wuhan Tongji Hospital and 90 cases at the Hunan Provincial Tumor Hospital between January 2017 and September 2019 were included in our study. They were grouped as training and external validation set. Radiomics features were extracted from shear-wave elastography (SWE) images and corresponding B-mode ultrasound (BMUS) images. Then, the minimum redundancy maximum relevance (mRMR) algorithm and the least absolute shrinkage and selection operator (LASSO) regression were used to select LN status-related features and construct the SWE and BMUS radiomics score (Rad-score). Multivariate logistic regression was performed using the two radiomics scores together with clinical data, and a nomogram was subsequently developed. The performance of the nomogram was assessed with respect to discrimination, calibration, and clinical usefulness in the training and external validation set. Results Both the SWE and BMUS Rad-scores were signiï¬cantly higher in patients with cervical LN metastasis. Multivariate analysis indicated that SWE Rad-score, multifocality and US-reported LN status were independent risk factors associated with LN status. The radiomics nomogram, which incorporated the three variables showed good calibration and discrimination in the training set (AUC 0.851; 95% CI, 0.791-0.912) and the validation set (AUC 0.832; 95% CI, 0.749-0.916). The signiï¬cantly improved net reclassiï¬cation index (NRI) and integrated discriminatory improvement (IDI) demonstrated that SWE radiomics signature was a very useful marker to predict the LN metastasis in PTC. Decision curve analysis indicated that the SWE radiomics nomogram was clinically useful. Furthermore, the nomogram also showed favorable discriminatory efficacy in the US-reported LN negative (cN0) subgroup (AUC 0.812; 95% CI, 0.745-0.860). Conclusion The presented radiomics nomogram, which is based on the SWE radiomics signature, shows favorable predictive value for LN staging in patients with PTC.",2020,Thyroid : official journal of the American Thyroid Association
Variable inclusion and shrinkage algorithms,"The Lasso is a popular and computationally efficient procedure for automatically performing both variable selection and coefficient shrinkage on linear regression models. One limitation of the Lasso is that the same tuning parameter is used for both variable selection and shrinkage. As a result, it typically ends up selecting a model with too many variables to prevent overshrinkage of the regression coefficients. We suggest an improved class of methods called variable inclusion and shrinkage algorithms (VISA). Our approach is capable of selecting sparse models while avoiding overshrinkage problems and uses a path algorithm, and so also is computationally efficient. We show through extensive simulations that VISA significantly outperforms the Lasso and also provides improvements over more recent procedures, such as the Dantzig selector, relaxed Lasso, and adaptive Lasso. In addition, we provide theoretical justification for VISA in terms of nonasymptotic bounds on the estimation error that suggest it shoul...",2008,Journal of the American Statistical Association
Robust high dimensional learning for Lipschitz and convex losses,"We establish risk bounds for Regularized Empirical Risk Minimizers (RERM) when the loss is Lipschitz and convex and the regularization function is a norm. We obtain these results in the i.i.d. setup under subgaussian assumptions on the design. In a second part, a more general framework where the design might have heavier tails and data may be corrupted by outliers both in the design and the response variables is considered. In this situation, RERM performs poorly in general. We analyse an alternative procedure based on median-of-means principles and called ""minmax MOM"". We show optimal subgaussian deviation rates for these estimators in the relaxed setting. The main results are meta-theorems allowing a wide-range of applications to various problems in learning theory. To show a non-exhaustive sample of these potential applications, it is applied to classification problems with logistic loss functions regularized by LASSO and SLOPE, to regression problems with Huber loss regularized by Group LASSO, Total Variation and Fused LASSO and to matrix completion problems with quantile loss regularized by the nuclear norm. A short simulation study concludes the paper, illustrating in particular robustness properties of regularized minmax MOM procedures.",2019,arXiv: Statistics Theory
Adaptive logistic group Lasso method for predicting the no-reflow among the multiple types of high-dimensional variables with missing data,"The prediction of no-reflow phenomenon aroused much attention, because of its independent association with increased in-hospital mortality, malignant arrhythmias, and cardiac failure. Many studies on prediction of no-reflow were carried out focusing on only few predictors. As big data era has been coming, high-dimensional predictors are available for prediction. However, as a common problem, big data analytics in healthcare from the electronic medical record (EMR) system is faced with many challenges, such as missing data processing, multiple types of variables processing and the high-dimensional data prediction. A general method based on improved weighted K-nearest neighbors and adaptive logistic group Lasso was proposed for predicting the no-reflow after cardiac surgery among the multiple types of variables with missing data. Compared with logistic regression, Lasso method, and artificial neural network method, our method has lower misclassification error rate and less complex model for no-reflow prediction, especially when predicting among multiple types of variables with missing data.",2016,2016 7th IEEE International Conference on Software Engineering and Service Science (ICSESS)
Beyond â„“1 norm minimization â€” High quality recovery of non-sparse compressible signals,"We propose a novel algorithm for the recovery of non-sparse, but compressible signals from linear undersampled measurements. The algorithm proposed in this paper consists of two steps. The first step recovers the signal by the â„“1 minimization. Then, the second step decomposes the â„“1 reconstruction into major and minor components. By using the major components, measurements for the minor components of the target signal are estimated. Error evaluation of the estimate leads to the standard ridge regression for the recovery of the minor components with the regularization parameter determined using the error bound. After a slight modification to the major components, the final estimate is obtained by combining the two estimates. Computational cost of the proposed algorithm is mostly the same as the â„“1 minimization. Simulation results show the effectiveness of the proposed algorithm over not only â„“1 minimization but also the Lasso estimator.",2014,"Signal and Information Processing Association Annual Summit and Conference (APSIPA), 2014 Asia-Pacific"
A generic coordinate descent solver for nonsmooth convex optimization,"We present a generic coordinate descent solver for the minimization of a nonsmooth convex objective with structure. The method can deal in particular with problems with linear constraints. The implementation makes use of efficient residual updates and automatically determines which dual variables should be duplicated. A list of basic functional atoms is pre-compiled for efficiency and a modelling language in Python allows the user to combine them at run time. So, the algorithm can be used to solve a large variety of problems including Lasso, sparse multinomial logistic regression, linear and quadratic programs.",2018,arXiv: Optimization and Control
Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization,"We introduce a proximal version of the stochastic dual coordinate ascent method and show how to accelerate the method using an inner-outer iteration procedure. We analyze the runtime of the framework and obtain rates that improve state-of-the-art results for various key machine learning optimization problems including SVM, logistic regression, ridge regression, Lasso, and multiclass SVM. Experiments validate our theoretical findings.",2016,Mathematical Programming
Two Tales of Variable Selection for High Dimensional Data: Screening and Model Building,"Variable selection plays an important role in high dimensional data analysis where a large number of variables are given as potential predictors of a response of interest. Typically, it arises at two stages of statistical modeling, namely screening and formal model building, with different goals. Screening aims at filtering out irrelevant variables prior to model building where a formal description of a functional relation between the variables screened for relevance and the response is sought. Accordingly, proper comparison of variable selection methods calls for evaluation criteria that reflect the differential goals: accuracy in ranking order of variables for screening and prediction accuracy for formal modeling. Without delineating the difference in the two aspects, confounding comparisons of various screening and selection methods have often been made in the literature, which may lead to misleading conclusions. In this dissertation, we present comprehensive numerical studies for comparison of three commonly used screening and selection procedures: correlation screening (a.k.a. sure independence screening), forward selection and LASSO in regression setting. By clearly differentiating these two aspects of variable selection, we highlight the situations where the performance of the three approaches differs, offering a guideline for proper choice of a method in practice. Furthermore, we discuss connections to relevant comparisons performed in the recent literature to clarify the different findings and conclusions. We also conduct similar types of studies for comparison of two corresponding screening and selection procedures of LASSO and correlation screening in classification setting, i.e., L1 ii penalized logistic regression and two-sample t-test. Initial results of exploratory analysis are presented to provide some insights on the preferred scenarios of the two methods respectively. Discussions are made on possible extensions, future works and difference between regression and classification setting.",2012,
Robust Inference on Average Treatment Effects with Possibly More Covariates than Observations,"This paper concerns robust inference on average treatment effects following model selection. In the selection on observables framework, we show how to construct confidence intervals based on a doubly-robust estimator that are robust to model selection errors and prove that they are valid uniformly over a large class of treatment effect models. The class allows for multivalued treatments with heterogeneous effects (in observables), general heteroskedasticity, and selection amongst (possibly) more covariates than observations. Our estimator attains the semiparametric efficiency bound under appropriate conditions. Precise conditions are given for any model selector to yield these results, and we show how to combine data-driven selection with economic theory. For implementation, we give a specific proposal for selection based on the group lasso and derive new technical results for high-dimensional, sparse multinomial logistic regression. A simulation study shows our estimator performs very well in finite samples over a wide range of models. Revisiting the National Supported Work demonstration data, our method yields accurate estimates and tight confidence intervals.",2013,
Quality optimization of H.264/AVC video transmission over noisy environments using a sparse regression framework,"We propose the use of the Least Absolute Shrinkage and Selection Operator (LASSO) regression method in order to predict the Cumulative Mean Squared Error (CMSE), incurred by the loss of individual slices in video transmission. We extract a number of quality-relevant features from the H.264/AVC video sequences, which are given as input to the LASSO. This method has the benefit of not only keeping a subset of the features that have the strongest effects towards video quality, but also produces accurate CMSE predictions. Particularly, we study the LASSO regression through two different architectures; the Global LASSO (G.LASSO) and Local LASSO (L.LASSO). In G.LASSO, a single regression model is trained for all slice types together, while in L.LASSO, motivated by the fact that the values for some features are closely dependent on the considered slice type, each slice type has its own regression model, in an e ort to improve LASSO's prediction capability. Based on the predicted CMSE values, we group the video slices into four priority classes. Additionally, we consider a video transmission scenario over a noisy channel, where Unequal Error Protection (UEP) is applied to all prioritized slices. The provided results demonstrate the efficiency of LASSO in estimating CMSE with high accuracy, using only a few features. les that typically contain high-entropy data, producing a footprint that is far less conspicuous than existing methods. The system uses a local web server to provide a le system, user interface and applications through an web architecture.",2015,
Non-Linear Returns to Schooling Among Filipino Men,"The issue of possible non-linearities in the relationship between log wages and schooling has received a good deal of attention in the literature on the United States, as well as in Less Developed Countries (LDCs). In this paper, I use data from a recent household survey for the Philippines, the 1998 Annual Poverty Indicator Survey (APIS), to test the fit of the log-linear specification for Filipino men. I present results based on a number of estimation strategies, including discontinuous spline regressions, semi-parametric regressions with a large number of dummies for years of schooling and experience, and kernel regressions. The basic conclusions of the paper are two. First, there appear to be large differences between the rates of return to education across levels in the Philippines. In particular, the wage premia to both primary and secondary education are lower than those for tertiary education. Second, within a given level, the last year of schooling is disproportionately rewarded in terms of higher wages. That is, there are clear sheepskin effects associated with graduation from primary school, secondary school, and university. * Economist, East Asia and the Pacific, Poverty Reduction and Economic Management, The World Bank. 1 I would like to thank Harold Alderman, Ruperto Alonzo, Benu Bidani, Gaurav Datt, Olivier Deschenes, Emanuela Galasso, Hans Hoogeveen, Emmanuel Jimenez, Homi Kharas, Erzo Luttmer, Tamar Manuelyan Atinc, and Lant Pritchett for many useful comments and conversations. Please address correspondence to the author at nschady@worldbank.org. The views and interpretations expressed in this paper are those of the author and do not necessarily represent the views and interpretations of the World Bank, its Executive Directors, or the countries they represent.",2000,
A LASSO penalized regression approach for genome-wide association analyses using related individuals: application to the Genetic Analysis Workshop 19 simulated data,"We propose a novel LASSO (least absolute shrinkage and selection operator) penalized regression method used to analyze samples consisting of (potentially) related individuals. Developed in the context of linear mixed models, our method models the relatedness of individuals in the sample through a random effect whose covariance structure is a linear function of known matrices with elements combinations of the condensed coefficients of identity between the individuals in the sample. We implement our method to analyze the simulated family data provided by the 19th Genetic Analysis Workshop in an effort to identify loci regulating the simulated trait of systolic blood pressure. The analyses were performed with full knowledge of the simulation model. Our findings demonstrate that we can significantly reduce the rate of false positive signals by incorporating the relatedness of the study participants.",2016,BMC Proceedings
Comparison of Different Regularized and Shrinkage Regression Methods to Predict Daily Tropospheric Ozone Concentration in the Grand Casablanca Area,"Tropospheric ozone (O3) is one of 
the pollutants that have a significant impact on human health. It can increase 
the rate of asthma crises, cause permanent lung infections and death. 
Predicting its concentration levels is therefore important for planning 
atmospheric protection strategies. The aim of this study is to predict the 
daily mean O3 concentration one day ahead in the Grand Casablanca area of 
Morocco using primary pollutants and meteorological variables. Since the 
available explanatory variables are multicollinear, multiple linear regressions 
are likely to lead to unstable models. To counteract the multicollinearity problem, we compared several 
alternative regression methods: 1) Continuum Regression; 2) Ridge & Lasso Regressions; 3) Principal component regression (PCR); 4) Partial least Square regression & sparse PLS and; 5) Biased Power Regression. The aim is to set up a good prediction model 
of the daily ozone in the Grand Casablanca area. These models are fitted on a 
training data set (from the years 2013 and 2014), tested on a data set (from 
2015) and validated on yet another data set data (from 2015). The Lasso model 
showed a better performance for the prediction of ozone concentrations compared 
to multiple linear regression and its other alternative methods.",2018,Advances in Pure Mathematics
Spindle thermal error robust modeling using LASSO and LS-SVM,"To improve the spindle thermal error prediction accuracy, the least absolute shrinkage and selection operator (LASSO) is used to directly select the temperature-sensitive point subset to guarantee the prediction performance of the thermal error model built by least squares support vector machines (LS-SVM). Taking a horizontal machining center as a test stand, the thermal error experiments with different spindle speed states are carried out. Then the temperature-sensitive points are selected using LASSO. The number of temperature-sensitive points is reduced from 20 to 7. Afterward, the thermal error model is designed by LS-SVM. The prediction performance and generalization performance of the thermal error model are compared with another two thermal error models using gray model (GM) and multiple linear regression (MLR), respectively. The comparison results indicate that the thermal error model derived from LS-SVM shows better prediction performance and generalization performance than those derived from GM and MLR with the highest prediction accuracy increasing about 74.6 and 54.3%, respectively. Thus, the feasibility and effectiveness of the proposed spindle thermal error robust modeling method are validated.",2018,The International Journal of Advanced Manufacturing Technology
Block splitting for distributed optimization,"This paper describes a general purpose method for solving convex optimization problems in a distributed computing environment. In particular, if the problem data includes a large linear operator or matrix $$A$$A, the method allows for handling each sub-block of $$A$$A on a separate machine. The approach works as follows. First, we define a canonical problem form called graph form, in which we have two sets of variables related by a linear operator $$A$$A, such that the objective function is separable across these two sets of variables. Many types of problems are easily expressed in graph form, including cone programs and a wide variety of regularized loss minimization problems from statistics, like logistic regression, the support vector machine, and the lasso. Next, we describe graph projection splitting, a form of Douglasâ€“Rachford splitting or the alternating direction method of multipliers, to solve graph form problems serially. Finally, we derive a distributed block splitting algorithm based on graph projection splitting. In a statistical or machine learning context, this allows for training models exactly with a huge number of both training examples and features, such that each processor handles only a subset of both. To the best of our knowledge, this is the only general purpose method with this property. We present several numerical experiments in both the serial and distributed settings.",2014,Mathematical Programming Computation
Fatal course in severe meningococcemia: clinical predictors and effect of transfusion therapy.,"OBJECTIVE
To investigate whether the administration of fresh-frozen plasma to patients with systemic meningococcal disease is associated with an increased mortality rate compared with the administration of plasma substitutes.


DESIGN
Seventeen-year case-control study.


SETTING
Intensive care units and departments of internal medicine and pediatrics of one university hospital and one local hospital.


PATIENTS
A total of 336 patients with culture-proven meningococcemia or symptoms characteristic of meningococcemia who were admitted to two hospitals in northern Norway between 1974 and 1991.


MEASUREMENTS AND MAIN RESULTS
High-risk patients were selected on the basis of two different scoring systems (Niklasson's score and clinical score) and classified according to the type of intravenous fluid regimen (fresh-frozen plasma, blood, or colloids). For comparison between groups, analysis of variance and chi-square tests were used. Assessments of adjusted effects on mortality rate were done by multiple logistic regression. Administration of blood or plasma was significantly associated with a fatal course, both in the total patient population (p < .01) and in the high-risk group (p = .02), while using colloids alone was negatively associated with death, although not reaching statistical significance. A significantly lower mortality rate was found in one of the hospitals where colloids were used instead of plasma or blood in the last part of the period studied (p < .05).


CONCLUSION
The results support our hypothesis that the use of fresh-frozen plasma may negatively influence outcome in systemic meningococcal disease.",1993,Critical care medicine
Parallel Coordinate Descent for L1-Regularized Loss Minimization,"We propose Shotgun, a parallel coordinate descent algorithm for minimizing L1-regularized losses. Though coordinate descent seems inherently sequential, we prove convergence bounds for Shotgun which predict linear speedups, up to a problem-dependent limit. We present a comprehensive empirical study of Shotgun for Lasso and sparse logistic regression. Our theoretical predictions on the potential for parallelism closely match behavior on real data. Shotgun outperforms other published solvers on a range of large problems, proving to be one of the most scalable algorithms for L1.",2011,ArXiv
Comparative Study on Variable Selection Approaches in Establishment of Remote Sensing Model for Forest Biomass Estimation,"In the field of quantitative remote sensing of forest biomass, a prominent phenomenon is the increasing number of explanatory variables. Then how to effectively select explanatory variables has become an important issue. Linear regression model is one of the commonly used remote sensing models. In the process of establishing the linear regression model, a vital step is to select explanatory variables. Focusing on variable selection and model stability, this paper conducts a comparative study on the performance of eight linear regression parameter estimation methods (Stepwise Regression Method (SR), Criterions Based on The Bayes Method (BIC), Criterions Based on The Bayes Method (AIC), Criterions Based on Prediction Error (Cp), Least Absolute Shrinkage and Selection Operator (Lasso), Adaptive Lasso, Smoothly Clipped Absolute Deviation (SCAD), Non-negative garrote (NNG)) in the subtropical forest biomass remote sensing model development. For the purpose of comparison, OLS and RR, are commonly used as methods with no variable selection ability, and are also compared and discussed. The performance of five aspects are evaluated in this paper: (i) Determination coefficient, prediction error, model error, etc., (ii) significance test about the difference between determination coefficients, (iii) parameter stability, (iv) variable selection stability and (v) variable selection ability of the methods. All the results are obtained through a five ten-fold CV. Some evaluation indexes are calculated with or without degrees of freedom. The results show that BIC performs best in comprehensive evaluation, while NNG, Cp and AIC perform poorly as a whole. Other methods show a great difference in the performance on each index. SR has a strong capability in variable selection, although it is poor in commonly used indexes. The short-wave infrared band and the texture features derived from it are selected most frequently by various methods, indicating that these variables play an important role in forest biomass estimation. Some of the conclusions in this paper are likely to change as the study object changes. The ultimate goal of this paper is to introduce various model establishment methods with variable selection capability, so that we can have more choices when establishing similar models, and we can know how to select the most appropriate and effective method for specific problems.",2019,Remote Sensing
Application of hyperspectral data for assessing peatland forest condition with spectral and texture classification,"Peatland in tropical region is a major CO2 emission source because of peat decomposition and forest fire by human induced activities. Remote sensing is effective tool to monitor environmental condition of peatland and forest ecosystem in peatland. A pixel-based approach is one of the most attractive choices for forest type classification or biomass prediction. The traditional method, however, is not sufficient for using spatial information. The spatial information, such as image texture, is an important factor for identifying objects or types, because a pixel is not independent of its neighbors and its dependence can be useful for classification and biomass prediction in forest regions. In this paper, we used combined data of spectral and spatial information from hyperspectral data (Hymap) to develop a more accurate classification or biomass prediction model. The spatial information was texture data by using Grey Level Co-occurrence Matrix (GLCM) texture measures. Sparse discrimination analysis (SDA) was applied for the classification model, and LASSO regression was applied for the biomass prediction model. The results were compared to find out how the spatial information enhances the classification and biomass prediction. According to the accuracy assessment, both classification and biomass prediction model derived from the combined data performed high accuracy.",2013,2013 IEEE International Geoscience and Remote Sensing Symposium - IGARSS
Assessing precision of lasso conditional logistic estimates in the case-crossover design,"The case-crossover design introduced by Maclure (1991) is an observational epidemiological study for analyzing the effects of transient exposures on the risk of acute-onset events. This design is based on exclusively selecting subjects that have experienced the event under investigation. The aim is to identify the short-term, transient triggers of event, rather than to identify who is at highest risk of disease. The association between event onset and risk factors is then estimated by comparing exposure during the period of time just prior to the event onset (case period) to the same subject's exposure during one or more control periods. The standard tool for this analysis is conditional logistic regression (Breslow and Day, 1980; Maclure, 1991). Until present, variable selection issues have not received much attention when modeling the risk of an event of interest under the case-crossover design. On the one hand, time-invariant potential confounders are controlled by design. On the other hand, studies in the literature using the case-crossover design are often concerned by few main risk factors. Nevertheless, advances in technology for data collection lead to complex situations such as those involving many predictors. This may also be the case in studies based on the case-crossover design. The lasso procedure (Tibshirani, 1996), which relies on an L1 penalty, has been widely used for accommodating high-dimensional predictors. This approach has also been adapted to conditional logistic regression in the context of matched case-control studies (Avalos, 2009). However, a drawback of LASSO regression is that no measure of uncertainty is provided. In this work we study by simulations the behavior of nonparametric bootstrap percentile confidence intervals in assessing significance of lasso point estimates. We also illustrate the use of this method with some examples under the case-crossover design. 1. Avalos M. (2009) Model selection via the lasso in conditional logistic regression. In: Conference abstracts. Second International Biometric Society Channel Network Conference, Belgium (2009). 2. Breslow, N. E. and Day, N. E. (1980). Statistical methods in cancer research. Volume 1: The analysis of case-control studies. IARC Scientific Publications, Lyon. 3. Maclure, M. (1991). The case-crossover design: a method for studying transient effects on the risk of acute event. Am J Epidemiol. 133, 144-153. 4. Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. J. Royal. Statist. Soc B. 58, 267-288.",2011,
A study of machine learning regression methods for major elemental analysis of rocks using laser-induced breakdown spectroscopy,"Abstract The ChemCam instrument on the Mars Curiosity rover is generating thousands of LIBS spectra and bringing interest in this technique to public attention. The key to interpreting Mars or any other types of LIBS data are calibrations that relate laboratory standards to unknowns examined in other settings and enable predictions of chemical composition. Here, LIBS spectral data are analyzed using linear regression methods including partial least squares (PLS-1 and PLS-2), principal component regression (PCR), least absolute shrinkage and selection operator (lasso), elastic net, and linear support vector regression (SVR-Lin). These were compared against results from nonlinear regression methods including kernel principal component regression (K-PCR), polynomial kernel support vector regression (SVR-Py) and k -nearest neighbor ( k NN) regression to discern the most effective models for interpreting chemical abundances from LIBS spectra of geological samples. The results were evaluated for 100 samples analyzed with 50 laser pulses at each of five locations averaged together. Wilcoxon signed-rank tests were employed to evaluate the statistical significance of differences among the nine models using their predicted residual sum of squares (PRESS) to make comparisons. For MgO, SiO 2 , Fe 2 O 3 , CaO, and MnO, the sparse models outperform all the others except for linear SVR, while for Na 2 O, K 2 O, TiO 2 , and P 2 O 5 , the sparse methods produce inferior results, likely because their emission lines in this energy range have lower transition probabilities. The strong performance of the sparse methods in this study suggests that use of dimensionality-reduction techniques as a preprocessing step may improve the performance of the linear models. Nonlinear methods tend to overfit the data and predict less accurately, while the linear methods proved to be more generalizable with better predictive performance. These results are attributed to the high dimensionality of the data (6144 channels) relative to the small number of samples studied. The best-performing models were SVR-Lin for SiO 2 , MgO, Fe 2 O 3 , and Na 2 O, lasso for Al 2 O 3 , elastic net for MnO, and PLS-1 for CaO, TiO 2 , and K 2 O. Although these differences in model performance between methods were identified, most of the models produce comparable results when p Â â‰¤Â 0.05 and all techniques except kNN produced statistically-indistinguishable results. It is likely that a combination of models could be used together to yield a lower total error of prediction, depending on the requirements of the user.",2015,Spectrochimica Acta Part B: Atomic Spectroscopy
Asymptotic normality and optimalities in estimation of large Gaussian graphical models,"The Gaussian graphical model, a popular paradigm for studying relationship among variables in a wide range of applications, has attracted great attention in recent years. This paper considers a fundamental question: When is it possible to estimate low-dimensional parameters at parametric square-root rate in a large Gaussian graphical model? A novel regression approach is proposed to obtain asymptotically efficient estimation of each entry of a precision matrix under a sparseness condition relative to the sample size. When the precision matrix is not sufficiently sparse, or equivalently the sample size is not sufficiently large, a lower bound is established to show that it is no longer possible to achieve the parametric rate in the estimation of each entry. This lower bound result, which provides an answer to the delicate sample size question, is established with a novel construction of a subset of sparse precision matrices in an application of Le Cam's lemma. Moreover, the proposed estimator is proven to have optimal convergence rate when the parametric rate cannot be achieved, under a minimal sample requirement. The proposed estimator is applied to test the presence of an edge in the Gaussian graphical model or to recover the support of the entire model, to obtain adaptive rate-optimal estimation of the entire precision matrix as measured by the matrix $\ell_q$ operator norm and to make inference in latent variables in the graphical model. All of this is achieved under a sparsity condition on the precision matrix and a side condition on the range of its spectrum. This significantly relaxes the commonly imposed uniform signal strength condition on the precision matrix, irrepresentability condition on the Hessian tensor operator of the covariance matrix or the $\ell_1$ constraint on the precision matrix. Numerical results confirm our theoretical findings. The ROC curve of the proposed algorithm, Asymptotic Normal Thresholding (ANT), for support recovery significantly outperforms that of the popular GLasso algorithm.",2015,Annals of Statistics
Machine Learning-Based Prediction Models for 30-Day Readmission after Hospitalization for Chronic Obstructive Pulmonary Disease.,"While machine learning approaches can enhance prediction ability, little is known about their ability to predict 30-day readmission after hospitalization for Chronic Obstructive Pulmonary Disease (COPD). We identified patients aged â‰¥40â€‰years with unplanned hospitalization due to COPD in the Diagnosis Procedure Combination database, an administrative claims database in Japan, from 2011 through 2016 (index hospitalizations). COPD was defined by ICD-10-CM diagnostic codes, according to Centers for Medicare and Medicaid Services (CMS) readmission measures. The primary outcome was any readmission within 30â€‰days after index hospitalization. In the training set (randomly-selected 70% of sample), patient characteristics and inpatient care data were used as predictors to derive a conventional logistic regression model and two machine learning models (lasso regression and deep neural network). In the test set (remaining 30% of sample), the prediction performances of the machine learning models were examined by comparison with the reference model based on CMS readmission measures. Among 44,929 index hospitalizations for COPD, 3413 (7%) were readmitted within 30â€‰days after discharge. The reference model had the lowest discrimination ability (C-statistic: 0.57 [95% confidence interval (CI) 0.56-0.59]). The two machine learning models had moderate, significantly higher discrimination ability (C-statistic: lasso regression, 0.61 [95% CI 0.59-0.61], pâ€‰=â€‰0.004; deep neural network, 0.61 [95% CI 0.59-0.63], pâ€‰=â€‰0.007). Tube feeding duration, blood transfusion, thoracentesis use, and male sex were important predictors. In this study using nationwide administrative data in Japan, machine learning models improved the prediction of 30-day readmission after COPD hospitalization compared with a conventional model.AbbreviationsADLactivities of daily livingCOPDchronic obstructive pulmonary diseaseCMSCenters for Medicare and Medicaid ServicesDPCDiagnosis Procedure CombinationICD-10-CMInternational Classification of Diseases, Tenth Revision, Clinical ModificationROCreceiver-operating-characteristic.",2019,COPD
Adaptive Penalized Estimation of Directed Acyclic Graphs From Categorical Data,"We develop in this article a penalized likelihood method to estimate sparse Bayesian networks from categorical data. The structure of a Bayesian network is represented by a directed acyclic graph (DAG). We model the conditional distribution of a node given its parents by multi-logit regression and estimate the structure of a DAG via maximizing a regularized likelihood. The adaptive group Lasso penalty is employed to encourage sparsity by selecting grouped dummy variables encoding the level of a factor. We develop a blockwise coordinate descent algorithm to solve the penalized likelihood problem subject to the acyclicity constraint of a DAG. When intervention data are available, our method may construct a causal network, in which a directed edge represents a causal relation. We apply our method to various simulated networks and a real biological network. The results show that our method is very competitive, compared to other existing methods, in DAG estimation from both interventional and high-dimensional observational data. We also establish consistency in parameter and structure estimation for our method when the number of nodes is fixed.",2014,arXiv: Methodology
[A new method of sparse feature extraction for stellar spectra].,"The authors propose a novel method of feature extraction for stellar spectra parameterization. The basic procedures are: First, stellar spectra are decomposed by multi-scale Harr wavelet and the coefficients with high-frequency are rejected. Secondly, the optimal features are detected by the lasso algorithm. Finally, we input the optimal feature vector to non-parametric regression model to estimate the atmospheric parameters. Haar wavelet can remove the high-frequency noise from the stellar spectrum. Lasso algorithm can further compress data by analyzing their significance on parameterization and removing redundancy. Experiments show that the proposed Haar+lasso method improves the accuracy and efficiency of the estimation. The authors used this scheme to estimate the atmospheric parameters from a subsample of some 40,000 stellar spectra from SDSS. The accuracies of our predictions (mean absolute errors) for each parameter are 0.0071 dex for log Teff, 0.2252 dex for log g, and 0.1996 dex for [Fe/H]. Compared with the results of the existing literature, this scheme can derive more accurate atmospheric parameters.",2014,Guang pu xue yu guang pu fen xi = Guang pu
Emergency Department - Wait TimePrediction,"Oggigiorno sempre piu enti (pubblici / privati) come ospedali, uffici postali o call center offrono ai propri pazienti / clienti una previsione del tempo dâ€™attesa. 
Lâ€™Ente Ospedaliero Cantonale (EOC), piu precisamente il pronto soccorso (PS) dellâ€™Ospedale Civico di Lugano (OCL) richiede la realizzazione di un modello predittivo in grado di 
fornire ai propri pazienti il tempo dâ€™attesa. 
Attualmente questa informazione viene fornita dallâ€™infermiere che effettua il triage, il quale 
attraverso anni dâ€™esperienza, e in grado di fornire indicativamente una stima del tempo dâ€™attesa. Stima che a seconda delle urgenze, puo variare nel tempo. 
Complessivamente sono stati analizzati 3 modelli in grado di prevedere il tempo dâ€™attesa. Il 
primo modello analizzato e Moving Average, ampiamente utilizzato per lâ€™analisi e lo studio 
di serie storiche. Il secondo modello prevede lâ€™impiego di Lasso, una regressione lineare 
in grado di effettuare automaticamente la regolarizzazione e la selezione delle features rilevanti. Il terzo e ultimo modello, ovvero quello che ha prodotto i migliori risultati e una Neural 
Network Regression (NNR), una regressione con rete neurale in grado di apprendere modelli matematico-statistici attraverso il training. 
La rete neurale e stata utilizzata in due diverse applicazioni: la prima viene mostrata attraverso un monitor in pronto soccorso e permette di visualizzare in tempo reale lâ€™attesa 
prevista per ogni paziente, la seconda e unâ€™applicazione web, che permette ai singoli pazienti di visualizzare in maniera pratica e funzionale il tempo dâ€™attesa stimato. 
Il risultato e un prototipo che nei prossimi mesi subira una validazione da parte del personale 
del pronto soccorso, dopo la quale avverra lâ€™effettiva messa in produzione.",2018,
"Abstract MP38: Local Population Income, Geographic Space and Interactions Predict Increased Presence of Physical Activity Facilities in New York City Metropolitan Area Census Tracts, 1990-2010","Introduction: Physical activity is associated with improved health and is supported, in part, by the presence of facilities that provide space and equipment to pursue a variety of physical activities. We assessed the hypothesis that socio-geographic characteristics predict increased local availability of commercial physical activity facilities over time. Longitudinal examination of physical activity facility distribution can inform our response to current disparities in access to public and private physical activity venues. Methods: We used data from the National Establishment Time-Series (NETS), a longitudinal database of U.S. businesses, focusing on 4528 census tracts (23 counties) in the New York City metropolitan area and on decennial intervals for which population data were also available through the Census or American Community Survey (1990, 2000, 2010). Commercial physical activity facilities (e.g., gyms, tennis courts, martial arts studios) were defined based on Standard Industrial Classification (SIC) codes and name searches. Facility counts were aggregated to 2010 census tract boundaries and linked to local population characteristics. Comparisons across decennial intervals were used to define increasing count of physical activity facilities and shifting population demographics. Associations were evaluated using lasso logistic regression to estimate relationships with predictor variables and their interactions with model shrinkage and variable subset selection through 10-fold cross-validation for minimization of test set model deviance. Results: Census tracts with at least one physical activity facility increased over time (1990=1172, 2000=2295, 2010=2365). Greater tract-level median income, larger land area, and higher previous total physical activity facilities at start of decade were positively associated with greater odds for local increase in physical activity facilities (OR=1.27 per SD median income; OR=1.30 per SD land area; OR=1.14 per SD lagged facility count). Inclusion of two-way interaction terms increased R2 estimates from 0.30 to 0.33, suggesting explanation of an additional 3% of the variation in facility count increase. Subset selection through lasso to minimize cross-validation error resulted in retention of 11 of 21 possible two-way predictor interactions. The association between 10-year increase in median income with increased physical activity facility count was stronger in geographically larger census tracts (interaction OR=1.05); similarly, a stronger relationship was found for 10-year population count increase with physical facility count increase in larger census tracts (interaction OR=1.05). Conclusion: Local population, geographic, and business environment characteristics are associated with change in physical activity facilities. Inclusion of interaction terms improved prediction.",2015,Circulation
