title,abstract,year,journal
Detection of DNA Copy Number Variations Using Penalized Least Absolute Deviations Regression,"Deletions and amplifications of the human genomic DNA copy number are the cause of numerous diseases such as various forms of cancer. Therefore, the detection of DNA copy number variations (CNV) is important in understanding the genetic basis of disease. Various techniques and platforms have been developed for genome-wide analysis of DNA copy number, such as array-based comparative genomic hybridization (aCGH) and high-resolution mapping using high-density tiling oligonucleotide arrays. Since complicated biological and experimental processes are involved in these platforms, data can be contaminated by outliers. Inspired by the robustness property of the LAD regression, we propose a penalized LAD regression with the fused lasso penalty for detecting CNV. This method incorporates the spatial dependence and sparsity of CNV into the analysis and is computationally feasible for high-dimensional array-based data. We evaluate the proposed method using simulation studies, which indicate that it can correctly detect the numbers and locations of the true breakpoints while controlling the false positives appropriately. We demonstrate the proposed method on two real data examples.",2007,
"Synthesis, Characterization, and Applications of Polymer-grafted Lignin Surfactants","Surfactants are among the broadest class of compounds utilized in industrial applications. However, many industrially used surfactants have detrimental environmental effects and are primarily derived from petroleum-based feedstocks. Agreen surfactant based on lignin, a biopolymer found in plant cell walls, could potentially serve as a renewably sourced surfactant for dispersant and emulsion applications. As amajor waste by-product of paper production generating a use for lignin surfactants could lead to a commercially viable renewable feedstock and create another avenue for utilizingthis waste material. Lignin derivatives, primarily lignosulfonates, have seen extensive application as dispersants in cement formulations but have been slowly replaced in recent decades by more active petroleum-based synthetic polymers. In this work we sought to develop lignin surfactants by augmenting ligninâ€™s natural surfactant activity through the grafting hydrophilic of polymers onto a lignin core. Primarily focusing on utilizing kraft lignin and lignosulfonate cores, this thesis looks at the interplay between lignin core and polymer graft length to determine design principles for cementitious and agriculturalformulations. Chapter 2 in this thesis outlines a synthetic strategy for grafting lignin cores with hydrophilic polymers under basic, aqueous conditions and examining their ability to function as emulsifiers. This strategy was developed with poly (ethylene glycol) methyl ether (mPEG) grafts utilizing a classic SN2 grafting strategy. These polymer graftedlignins (PLGN) were then examined for their activity at air-water and oil-water interfaces and exhibited significantly greater activity than the unmodified lignin cores. It was found that higher levels of interfacial activity were exhibited by PEGylated lignins containing hydrophobic kraft lignin cores and shorter PEG graft lengths. Building upon the nowknown activity of these polymer grafted lignins we sought to utilize them as dispersants in cement and as emulsifiers in agricultural formulations. In Chapter 3 we showed that polymer grafting lignins via the strategies discussedin Chapter 2 improves the dispersing capability of the lignins in cement formulation systems. The grafting strategy was shown to work well with varying PEG graft lengths aswell as with hydrophilic acrylic polymers, poly (methacrylic acid) (PMAA) and poly (sulfopropyl methacrylate) (PSPMA). Slump, a measure of cement dispersing capability, was increased by lignins grafted with PEG, PMAA, or SPMA polymer grafts with PMAA grafts having the largest effects. The polymer grafted lignins with the charged, hydrophilic lignosulfonate cores also exhibited better dispersing abilities likely due to the electrostatic interactions the of the lignosulfonate core with the highly charged cementmixture. While the polymer grafted lignins do not perform as well as commercially standard poly (carboxylate ether)s (PCE), the significant increase in dispersing abilitygained by grafting charged polymers to the lignin core indicates potential for lignins to be viable in future cement applications. Chapter 4 covers the steps utilized for market research on the viability of the PEGylated lignin surfactants synthesized in Chapter 2. This study was utilized todetermine the value propositions of the PGLNs and future directions for the project. It was found that the value proposition of being a multi-functional. green dispersant was of interest in agricultural applications leading to these applications being explored further in Chapters 5 and 6. Chapter 5 of this thesis takes a closer look at tank-mix agricultural applications, in which polymer grafted lignins with high oil-water interfacial activity were desired. Thehighly active PEGylated lignins synthesized in Chapter 1 were then formulated with the herbicide formulation, clethodim 2EC, and sprayed on v2 stage corn plants to examine whether they increased the effectiveness of the herbicide. Results imply that an increase in clethodim 2EC effectiveness was found in formulations containing the PEGylated kraft lignin with the short PEG chain. The increase in effectiveness was comparable to commercially available industrial standards, Superb HC and Prime Oil, thus indicating a potential application for these polymer grafted lignins. Chapter 6 builds upon the agricultural applications in Chapter 5 to further explore agricultural formulations using least absolute shrinkage and selection operator (LASSO) regression analysis techniques to quantitatively identify the physiochemical propertiesimportant to formulation biological activity with the aim of designing an optimal formulation containing polymer-grafted lignins. Emulsion concentrate (EC) agriculturalformulations dependent on oil-water interactions were examined using the oily herbicide clethodim while water dispersible granule (WDG) formulations will be examined using the solid herbicide cloransulam-methyl. Examining both EC and WDG formulations aided in creating a stronger understanding of polymer grafted ligninsâ€™ emulsifying anddispersing capabilities.",2019,
Confidence region and intervals for sparse penalized regression using variational inequality techniques,"LIANG YIN: CONFIDENCE REGION AND INTERVALS FOR SPARSE PENALIZED REGRESSION USING VARIATIONAL INEQUALITY TECHNIQUES. (Under the direction of Shu Lu and Yufeng Liu.) With the abundance of large data, sparse penalized regression techniques are commonly used in data analysis due to the advantage of simultaneous variable selection and prediction. By introducing biases on the estimators, sparse penalized regression methods can often select a simpler model than unpenalized regression. A number of convex as well as non-convex penalties have been proposed in the literature to achieve sparsity. Despite intense work in this area, it remains unclear on how to perform valid inference for sparse penalized regression with a general penalty. In this work, by making use of state-of-the-art optimization tools in variational inequality theory, we propose a unified framework to construct confidence intervals for sparse penalized regression with a wide range of penalties, including the well-known least absolute shrinkage and selection operator (LASSO) penalty and the minimax concave penalty (MCP). We study the inference for two types of parameters: the parameters under the population version of the penalized regression and the parameters in the underlying linear model. Theoretical convergence properties of the proposed methods are obtained. Simulated and real data examples are presented to demonstrate the validity and effectiveness of the proposed inference procedure.",2015,
Prediction of Cancer Cell Sensitivity to Drugs,"We consider the problem of using high-dimensional genomic covariates to predict the drug response of cancer cell lines. The cell lines are grouped according to tissue type, which we show may be an important factor in determining the dependence of drug response on genomic covariates. We develop predictors using l1-penalized linear regression models, and develop a novel variant which we call the indicator lasso which exploits inherent group structure. The superior performance of this new method in simulations over other methods which neglect group structure is illustrated. We finish by presenting the drug response prediction results.",2013,
Predicting late radiation-induced xerostomia with parotid gland PET biomarkers and dose metrics.,"PURPOSE
To assess associations between parotid gland PET biomarkers and late radiation-induced xerostomia, and to validate improvement of xerostomia predictive models by adding pre-treatment PET features to models based on dose and pre-treatment xerostomia.


MATERIALS AND METHODS
Intensity PET features from 47 patients treated on institutional prospective clinical trials for HPV-associated oropharyngeal squamous cell carcinoma with uniform chemoRT were analyzed. Associations between 90th percentile of the parotid gland standardized uptake values (P90) from pre-treatment and post-treatment PET scans, mean parotid gland doses, and late xerostomia defined by the Xerostomia Questionnaire (XQ) and salivary flow rates were quantified. Multivariable analysis was applied for dose and PET features using penalized logistic regression for feature selection and generation of predictive models using the LASSO technique, and optimism bias was estimated by bootstrap resampling.


RESULTS
Significant associations between late xerostomia and both mean parotid gland dose and P90 were demonstrated, and were generally stronger for post-treatment PET scans. The addition of P90 from pre-treatment PET scans improved the prediction model for late moderate or severe xerostomia compared to the base model, from AUCÂ =Â 0.74 to 0.78 (p-value <0.001) for XQ summary score and from 0.77 to 0.84 (p-value <0.001) for the single eating-related XQ item with the largest inter-patient variability; however, only the latter remained significant on cross validation (AUCÂ =Â 0.69 to 0.70 and 0.73 to 0.80, respectively).


CONCLUSIONS
The addition of pre-treatment parotid gland PET biomarkers improved a predictive model for late patient-reported xerostomia over dose and pre-treatment xerostomia.",2020,Radiotherapy and oncology : journal of the European Society for Therapeutic Radiology and Oncology
Regularised Model Identification Improves Accuracy of Multisensor Systems for Noninvasive Continuous Glucose Monitoring in Diabetes Management,"Continuous glucose monitoring (CGM) by suitable portable sensors plays a central role in the treatment of diabetes, a disease currently affecting more than 350 million people worldwide. Noninvasive CGM (NI-CGM), in particular, is appealing for reasons related to patient comfort (no needles are used) but challenging. NI-CGM prototypes exploiting multisensor approaches have been recently proposed to deal with physiological and environmental disturbances. In these prototypes, signals measured noninvasively (e.g., skin impedance, temperature, optical skin properties, etc.) are combined through a static multivariate linear model for estimating glucose levels. In this work, by exploiting a dataset of 45 experimental sessions acquired in diabetic subjects, we show that regularisation-based techniques for the identification of the model, such as the least absolute shrinkage and selection operator (better known as LASSO), Ridge regression, and Elastic-Net regression, improve the accuracy of glucose estimates with respect to techniques, such as partial least squares regression, previously used in the literature. More specifically, the Elastic-Net model (i.e., the model identified using a combination of and norms) has the best results, according to the metrics widely accepted in the diabetes community. This model represents an important incremental step toward the development of NI-CGM devices effectively usable by patients.",2013,J. Applied Mathematics
Using prior expansions for prior-data conflict checking,"Any Bayesian analysis involves combining information represented through different model components, and when different sources of information are in conflict it is important to detect this. Here we consider checking for prior-data conflict in Bayesian models by expanding the prior used for the analysis into a larger family of priors, and considering a marginal likelihood score statistic for the expansion parameter. Consideration of different expansions can be informative about the nature of any conflict, and extensions to hierarchically specified priors and connections with other approaches to prior-data conflict checking are discussed. Implementation in complex situations is illustrated with two applications. The first concerns testing for the appropriateness of a LASSO penalty in shrinkage estimation of coefficients in linear regression. Our method is compared with a recent suggestion in the literature designed to be powerful against alternatives in the exponential power family, and we use this family as the prior expansion for constructing our check. A second application concerns a problem in quantum state estimation, where a multinomial model is considered with physical constraints on the model parameters. In this example, the usefulness of different prior expansions is demonstrated for obtaining checks which are sensitive to different aspects of the prior.",2019,arXiv: Methodology
Can miRNAs predict EFS in pediatric AML?,"Purpose Children with acute myeloid leukemia (AML) whose disease is refractory to standard induction chemotherapy therapy or who experience relapse after initial response have dismal outcomes. We sought to comprehensively pro le pediatric AML microRNA (miRNA) samples to identify dysregulated genes and assess the utility of miRNAs for improved outcome prediction. Patients and Methods To identify miRNA biomarkers that are associated with treatment failure, we performed a comprehensive sequence-based characterization of the pediatric AML miRNA landscape. miRNA sequencing was performed on 1,362 samplesâ€”1,303 primary, 22 refractory, and 37 relapse samples. One hundred sixty-four matched samplesâ€”127 primary and 37 relapse samplesâ€”were analyzed by using RNA sequencing. Results By using penalized lasso Cox proportional hazards regression, we identi ed 36 miRNAs the expression levels at diagnosis of which were highly associated with event-free survival. Combined expression of the 36 miRNAs was used to create a novel miRNA-based risk classi cation scheme (AMLmiR36). This new miRNA-based risk classi er identi es those patients who are at high risk (hazard ratio, 2.830; P â‰¤ .001) or low risk (hazard ratio, 0.323; P â‰¤ .001) of experiencing treatment failure, independent of conventional karyotype or mutation status. The performance of AMLmiR36 was independently assessed by using 878 patients from two different clinical trials (AAML0531 and AAML1031). Our analysis also revealed that miR-106a-363 was abundantly expressed in relapse and refractory samples, and several candidate targets of miR-106a-5p were involved in oxidative phosphorylation, a process that is suppressed in treatment-resistant leukemic cells. Conclusion To assess the utility of miRNAs for outcome prediction in patients with pediatric AML, we designed and validated a miRNA-based risk classi cation scheme. We also hypothesized that the abundant expression of miR-106a could increase treatment resistance via modulation of genes that are involved in oxidative phosphorylation.",2018,
Identifying Key Predictors of Recidivism among Offenders Attending a Batterer Intervention Program: A Survival Analysis,"espanolLas estrategias para reducir la violencia contra la mujer en las relaciones de pareja pueden dirigirse a diferentes objetivos. Los programas de intervencion para agresores de pareja son uno de los principales acercamientos para su tratamiento. El resultado mas utilizado para la evaluacion de la efectividad de estos programas es la reincidencia. Los esfuerzos para incrementar la efectividad de los programas de intervencion para agresores de pareja en reducir la reincidencia deberian centrarse en las variables predictoras clave de este resultado. El objetivo de este estudio fue identificar los predictores clave de la reincidencia oficial a partir de un amplio conjunto de variables obtenidas a partir de una muestra de hombres participando por mandato judicial en un programa de intervencion para agresores de pareja (N = 393), con un periodo de seguimiento de entre 0 y 69 meses. Con este objetivo, se realizo un analisis de supervivencia utilizando cuatro conjuntos de variables: variables individuales, variables relacionales y contextuales, variables relativas a la violencia y variables relativas al proceso de intervencion. Para incluir simultaneamente todas las variables en el analisis, se estimo un modelo de regresion de Cox utilizando ALASSO (adaptive least absolute shrinkage and selection operator). De un conjunto de ochenta y nueve variables, seis fueron seleccionadas como predictores clave: abandono del programa, riesgo de violencia futura contra otras personas, exposicion a violencia familiar, estatus de inmigrante, acumulacion de eventos vitales estresantes e ira rasgo. El area bajo la curva ROC (receiving operator characteristic) fue .808, indicando una buena prediccion del modelo. Los predictores clave de la reincidencia identificados en este estudio deberian ser considerados por los profesionales e investigadores en el ambito de la intervencion con agresores de pareja para mejorar sus estrategias de evaluacion e intervencion. Asimismo, se discuten las implicaciones practicas para futuras investigaciones. EnglishStrategies to reduce intimate partner violence against women (IPVAW) can be targeted at different levels. Batterer intervention programs (BIPs) are among the main treatment approaches for IPVAW offenders. The most common outcome used in the evaluation of BIP effectiveness is recidivism. Efforts to increase BIP effectiveness in reducing recidivism should focus on key predictive variables of this outcome. The aim of this study was to identify key predictors of official recidivism from a large set of variables drawn from a sample of IPVAW offenders court-mandated to a community-based BIP (N = 393), with a follow-up period of between 0 and 69 months. To this end, a survival analysis was conducted using four sets of variables: individual-level, relational- and contextual-level, violence-related, and intervention process-related variables. To include all variables in the analysis simultaneously, a Cox regression model was estimated with the adaptive least absolute shrinkage and selection operator (ALASSO). From a pool of eighty-nine variables, six were selected as key predictors of recidivism: dropout, risk of future violence against non-partners, family violence exposure, immigrant status, accumulation of stressful life events, and trait anger. The area under the receiving operator characteristic (ROC) curve was .808, indicating good prediction of the model. The key predictors of recidivism found in this study should be considered by professionals and researchers in the BIP field to improve their evaluation and intervention strategies. Practical implications for future research are also discussed.",2019,Psychosocial Intervention
Network-based group variable selection for detecting expression quantitative trait loci (eQTL),"BackgroundAnalysis of expression quantitative trait loci (eQTL) aims to identify the genetic loci associated with the expression level of genes. Penalized regression with a proper penalty is suitable for the high-dimensional biological data. Its performance should be enhanced when we incorporate biological knowledge of gene expression network and linkage disequilibrium (LD) structure between loci in high-noise background.ResultsWe propose a network-based group variable selection (NGVS) method for QTL detection. Our method simultaneously maps highly correlated expression traits sharing the same biological function to marker sets formed by LD. By grouping markers, complex joint activity of multiple SNPs can be considered and the dimensionality of eQTL problem is reduced dramatically. In order to demonstrate the power and flexibility of our method, we used it to analyze two simulations and a mouse obesity and diabetes dataset. We considered the gene co-expression network, grouped markers into marker sets and treated the additive and dominant effect of each locus as a group: as a consequence, we were able to replicate results previously obtained on the mouse linkage dataset. Furthermore, we observed several possible sex-dependent loci and interactions of multiple SNPs.ConclusionsThe proposed NGVS method is appropriate for problems with high-dimensional data and high-noise background. On eQTL problem it outperforms the classical Lasso method, which does not consider biological knowledge. Introduction of proper gene expression and loci correlation information makes detecting causal markers more accurate. With reasonable model settings, NGVS can lead to novel biological findings.",2010,BMC Bioinformatics
SU-F-R-24: Identifying Prognostic Imaging Biomarkers in Early Stage Lung Cancer Using Radiomics.,"PURPOSE
Patients diagnosed with early stage lung cancer have favorable outcomes when treated with surgery or stereotactic radiotherapy. However, a significant proportion (âˆ¼20%) of patients will develop metastatic disease and eventually die of the disease. The purpose of this work is to identify quantitative imaging biomarkers from CT for predicting overall survival in early stage lung cancer.


METHODS
In this institutional review board-approved HIPPA-compliant retrospective study, we retrospectively analyzed the diagnostic CT scans of 110 patients with early stage lung cancer. Data from 70 patients were used for training/discovery purposes, while those of remaining 40 patients were used for independent validation. We extracted 191 radiomic features, including statistical, histogram, morphological, and texture features. Cox proportional hazard regression model, coupled with the least absolute shrinkage and selection operator (LASSO), was used to predict overall survival based on the radiomic features.


RESULTS
The optimal prognostic model included three image features from the Law's feature and wavelet texture. In the discovery cohort, this model achieved a concordance index or CI=0.67, and it separated the low-risk from high-risk groups in predicting overall survival (hazard ratio=2.72, log-rank p=0.007). In the independent validation cohort, this radiomic signature achieved a CI=0.62, and significantly stratified the low-risk and high-risk groups in terms of overall survival (hazard ratio=2.20, log-rank p=0.042).


CONCLUSION
We identified CT imaging characteristics associated with overall survival in early stage lung cancer. If prospectively validated, this could potentially help identify high-risk patients who might benefit from adjuvant systemic therapy.",2016,Medical physics
"A Review of: â€œPharmaceutical Statistics Using SAS: A Practical Guide, by A. Dmitrienko, C. Chuang-Stein, and R. B. D'Agostino (eds.)â€","In their preface, the editors cite the significant developments in biostatistical methodology over the past decades that have lead to improvements in many areas of drug discovery and development. The objective of this book is to provide broad coverage of biostatistical methodology applied in the pharmaceutical industry to deal with practical problems. The focus is primarily on nonclinical and preclinical research and early development although there is some discussion of nonstandard topics in later development as well. The target audience of the book includes primarily biostatisticians engaged in pharmaceutical research and development, but it could also benefit basic scientists such as biologists and chemists as well as pharmacologists and pharmacokineticists. The book is organized into 14 chapters written by 42 authors (26 from the pharmaceutical industry, 13 academic, 2 nonpharmaceutical industry, and 1 government). Chapter 1 provides an overview of the role of the statistician in the pharmaceutical industry, describing activities in nonclinical, early, and late clinical development. The emerging opportunities for statisticians to demonstrate leadership in the Food and Drug Administrationâ€™s Critical Path Initiative are also described. Chapter 2 describes two modern classification methods used in drug discovery, namely boosting and partial least squares. The authors note that drug discovery is typically characterized by overspecified data, i.e., too many variables relative to the number of observations and/or highly correlated descriptors. Recursive partitioning followed by boosting (implemented using two author-supplied macros) and partial least squares linear discrimination analysis (using PROC PLS) are illustrated using a permeability data set. In Chapter 3, the authors observe that a series of optimization processes takes place after a basic skeletal structure having the desired target inhibition and activity is discovered. These processes investigate modification of the chemical structure to improve the pharmaceutical properties (e.g., solubility, permeability, etc,) of the drug. Statistical modeling of the pharmaceutical properties as a function of the chemical structure is a driving factor of the optimization process. Model building and variable selection using multiple linear regression, ridge regression, lasso regression, logistic regression, and discriminant analysis are discussed and illustrated, using a solubility data set. Chapter 4 discusses analytical method validation related to biological and/or chemical assays. The authors use the proposed classification of assay data by Lee",2008,Journal of Biopharmaceutical Statistics
Biometrics on visual preferences: A â€œpump and distillâ€ regression approach,"We present a statistical behavioural biometric approach for recognizing people by their aesthetic preferences, using colour images. In the enrollment phase, a model is learnt for each user, using a training set of preferred images. In the recognition/authentication phase, such model is tested with an unseen set of pictures preferred by a probe subject. The approach is dubbed â€œpump and distillâ€, since the training set of each user is pumped by bagging, producing a set of image ensembles. In the distill step, each ensemble is reduced into a set of surrogates, that is, aggregates of images sharing a similar visual content. Finally, LASSO regression is performed on these surrogates; the resulting regressor, employed as a classifier, takes test images belonging to a single user, predicting his identity. The approach improves the state-of-the-art on recognition and authentication tasks in average, on a dataset of 40000 Flickr images and 200 users. In practice, given a pool of 20 preferred images of a user, the approach recognizes his identity with an accuracy of 92%, and sets an authentication accuracy of 91% in terms of normalized Area Under the Curve of the CMC and ROC curve, respectively.",2014,2014 IEEE International Conference on Image Processing (ICIP)
Time-lagged Ordered Lasso for network inference,"BackgroundAccurate gene regulatory networks can be used to explain the emergence of different phenotypes, disease mechanisms, and other biological functions. Many methods have been proposed to infer networks from gene expression data but have been hampered by problems such as low sample size, inaccurate constraints, and incomplete characterizations of regulatory dynamics. Since expression regulation is dynamic, time-course data can be used to infer causality, but these datasets tend to be short or sparsely sampled. In addition, temporal methods typically assume that the expression of a gene at a time point depends on the expression of other genes at only the immediately preceding time point, while other methods include additional time points without any constraints to account for their temporal distance. These limitations can contribute to inaccurate networks with many missing and anomalous links.ResultsWe adapted the time-lagged Ordered Lasso, a regularized regression method with temporal monotonicity constraints, for de novo reconstruction. We also developed a semi-supervised method that embeds prior network information into the Ordered Lasso to discover novel regulatory dependencies in existing pathways. R code is available at https://github.com/pn51/laggedOrderedLassoNetwork.ConclusionsWe evaluated these approaches on simulated data for a repressilator, time-course data from past DREAM challenges, and a HeLa cell cycle dataset to show that they can produce accurate networks subject to the dynamics and assumptions of the time-lagged Ordered Lasso regression.",2018,BMC Bioinformatics
Classification of failure mode and prediction of shear strength for reinforced concrete beam-column joints using machine learning techniques,"Abstract Beam-column joints are one of critical components that control the oveerall performance of reinforced concrete building frames under seismic loadings. To identify the response mechanism, including the classification of failure mode and the prediction of associated shear strength, of beam-column joints, this paper introduces the application of machine learning techniques. The efficiency of various machine learning techniques is evaluated using extensive experimental data from 536 experimental tests, all of which exhibited either non-ductile joint shear failure prior to beam yielding or ductile joint shear failure after beam yielding. It has been seen from the comparison that lasso regression has a better efficiency and reasonable accuracy in the classification and prediction. The suggested formulations as a function of influential input variables can be easily used by structural engineers to provide an optimal rehabilitation strategy for existing buildings and to design new structures.",2018,Engineering Structures
"Lasso Logistic Regression, GSoft and the Cyclic Coordinate Descent Algorithm: Application to Gene Expression Data","Statistical methods generating sparse models are of great value in the gene expression field, where the number of covariates (genes) under study moves about the thousands while the sample sizes seldom reach a hundred of individuals. For phenotype classification, we propose different lasso logistic regression approaches with specific penalizations for each gene. These methods are based on a generalized soft-threshold (GSoft) estimator. We also show that a recent algorithm for convex optimization, namely, the cyclic coordinate descent (CCD) algorithm, provides with a way to solve the optimization problem significantly faster than with other competing methods. Viewing GSoft as an iterative thresholding procedure allows us to get the asymptotic properties of the resulting estimates in a straightforward manner. Results are obtained for simulated and real data. The leukemia and colon datasets are commonly used to evaluate new statistical approaches, so they come in useful to establish comparisons with similar methods. Furthermore, biological meaning is extracted from the leukemia results, and compared with previous studies. In summary, the approaches presented here give rise to sparse, interpretable models that are competitive with similar methods developed in the field.",2010,Statistical Applications in Genetics and Molecular Biology
Temporal transferability of stream fish distribution models: can uncalibrated SDMs predict distribution shifts over time?,"Aim 
We aim to assess the temporary transferability of species distribution models (SDMs) for stream fish species in terms of discrimination power and calibration. 
 
Location 
New River basin, eastern United States. 
 
Methods 
In this study, we used Lasso-regularized logistic regression (LLR), boosted regression trees (BRT), MaxEnt, and ensemble model (ENS) to evaluate the habitat suitability of 16 fish species with different rarity and temperature preference based on historical species occurrences obtained during 1950â€“1990. These SDMs were used to make probabilistic predictions of species presence in the independent datasets sampled during 2012â€“2014. We evaluated the temporal transferability of these SDMs in terms of discrimination power and calibration with the temporarily independent datasets. 
 
Results 
The area under the receiver-operating-characteristic curve (AUC) was over 0.6 for 13 (81%) of the species in the evaluations of ENS models with the independent datasets. Cool-water species and species with small local population size traits tended to have good temporal transferability. With observed species prevalence as the discrimination cut-off, LLR had the highest overall accuracy for 13 of the 16 species and highest specificity for 10 species, whereas MaxEnt had the highest sensitivity for 14 species. Biases, under- or over-fitting problems were common in the temporal model transfers, among the modelling approaches used here. 
 
Main conclusions 
SDMs developed with historical data generally had moderate to good discrimination power but they tended to systematically underestimate current probability of species presence. To predict species distribution shifts over time, SDMs should be well-calibrated with high discrimination power. We suggest reclassifying predicted probability of species occurrence to ordinal ranks to deal with (under- and over-estimation) bias, and fine-tuning variable selection with regularization or cross validation to remedy under- and over-fitting.",2016,Diversity and Distributions
Variational Inference of Penalized Regression with Submodular Functions,"Various regularizers inducing structuredsparsity are constructed as LovÃ¡sz extensions of submodular functions. In this paper, we consider a hierarchical probabilistic model of linear regression and its kernel extension with this type of regularization, and develop a variational inference scheme for the posterior estimate on this model. We derive an upper bound on the partition function with an approximation guarantee, and then show that minimizing this bound is equivalent to the minimization of a quadratic function over the polyhedron determined by the corresponding submodular function, which can be solved efficiently by the proximal gradient algorithm. Our scheme gives a natural extension of the Bayesian Lasso model for the maximum a posteriori (MAP) estimation to a variety of regularizers inducing structured sparsity, and thus this work provides a principled way to transfer the advantages of the Bayesian formulation into those models. Finally, we investigate the empirical performance of our scheme with several Bayesian variants of widely known models such as Lasso, generalized fused Lasso, and non-overlapping group Lasso.",2019,
åŸºäºŽLassoæ–¹æ³•ä¸ŽLogisticå›žå½’çš„ä¸Šå¸‚å…¬å¸è´¢åŠ¡é¢„è­¦åˆ†æž The Financial Early Warning Model of Listed Companies Based on Lasso Method and Logistic Regression,"ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œä¸Šå¸‚å…¬å¸è´¢åŠ¡æ•°æ®æŒ‡æ ‡è¶Šå¤šï¼Œé¢„è­¦æ•ˆæžœè¶Šå¥½ï¼Œä½†ç”±äºŽå¤šç§å› ç´ å½±å“ï¼Œè´¢åŠ¡æŒ‡æ ‡è¿‡å¤šä¼šå¯¼è‡´å˜é‡é—´å…·æœ‰å¤šé‡å…±çº¿æ€§ã€‚æœ¬æ–‡æå‡ºä¸€ç§åŸºäºŽLassoæ–¹æ³•çš„Logisticå›žå½’ä¸Šå¸‚å…¬å¸è´¢åŠ¡é¢„è­¦æ¨¡åž‹ã€‚é¦–å…ˆåº”ç”¨Lassoæ³•å¯¹é«˜ç»´æ•°æ®è¿›è¡Œå˜é‡é€‰æ‹©ï¼Œè¾¾åˆ°é™ä½Žæ•°æ®ç»´åº¦å’Œæ¶ˆé™¤å˜é‡é—´å…±çº¿æ€§çš„ç›®çš„ï¼Œå†ç”¨Logisticå›žå½’æ³•å®žçŽ°å¯¹ä¸Šå¸‚å…¬å¸è´¢åŠ¡çŠ¶å†µçš„é¢„è­¦ã€‚ä»¿çœŸå®žéªŒç»“æžœè¡¨æ˜Žï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•èƒ½å¤Ÿæœ‰æ•ˆåœ°æ¶ˆé™¤æ•°æ®çš„å†—ä½™æ€§ï¼Œæé«˜é¢„è­¦çš„ç²¾ç¡®æ€§ï¼Œä¸ºä¼ä¸šç»è¥è€…æä¾›æœ‰æ•ˆçš„å‚è€ƒæ„è§ã€‚ Generally, the more the financial data indicators of listed companies are, the better the early warning is; However, due to a variety of factors, excessive financial indicators lead to multiple collinearity among variables. This paper presents a financial early-warning model of Logistic regression listed companies based on Lasso method. Firstly, the Lasso method is used to select variables for high-dimensional data, which can reduce the data dimension and eliminate the collinearity between variables. Then, the Logistic regression method is used to predict the financial status of listed companies. Simulation experiment shows that the method proposed in this paper can effectively eliminate the redundancy of data, improve the accuracy of early warning, and provide effective reference for enterprise operators.",2017,Advances in Applied Mathematics
Machine Learning Methods for Estimating Heterogeneous Causal Effectsâˆ—,"In this paper we study the problems of estimating heterogeneity in causal effects in experimental or observational studies and conducting inference about the magnitude of the differences in treatment effects across subsets of the population. In applications, our method provides a data-driven approach to determine which subpopulations have large or small treatment effects and to test hypotheses about the differences in these effects. For experiments, our method allows researchers to identify heterogeneity in treatment effects that was not specified in a pre-analysis plan, without concern about invalidating inference due to multiple testing. In most of the literature on supervised machine learning (e.g. regression trees, random forests, LASSO, etc.), the goal is to build a model of the relationship between a unitâ€™s attributes and an observed outcome. A prominent role in these methods is played by cross-validation which compares predictions to actual outcomes in test samples, in order to select the level of complexity of the model that provides the best predictive power. Our method is closely related, but it differs in that it is tailored for predicting causal effects of a treatment rather than a unitâ€™s outcome. The challenge is that the â€œground truthâ€ for a causal effect is not observed for any individual unit: we observe the unit with the treatment, or without the treatment, but not both at the same time. Thus, it is not obvious how to use cross-validation to determine whether a causal effect has been accurately predicted. We propose several novel cross-validation criteria for this problem and demonstrate through simulations the conditions under which they perform better than standard methods for the problem of causal effects. We then apply the method to a large-scale field experiment re-ranking results on a search engine.",2015,
Prediction of Early Season Nitrogen Uptake in Maize Using High-Resolution Aerial Hyperspectral Imagery,"The ability to predict spatially explicit nitrogen uptake (NUP) in maize (Zea mays L) during the early development stages provides clear value for making in-season nitrogen fertilizer applications that can improve NUP efficiency and reduce the risk of nitrogen loss to the environment. Aerial hyperspectral imaging is an attractive agronomic research tool for its ability to capture spectral data over relatively large areas, enabling its use for predicting NUP at the field scale. The overarching goal of this work was to use supervised learning regression algorithmsâ€”Lasso, support vector regression (SVR), random forest, and partial least squares regression (PLSR)â€”to predict early season (i.e., V6â€“V14) maize NUP at three experimental sites in Minnesota using high-resolution hyperspectral imagery. In addition to the spectral features offered by hyperspectral imaging, the 10th percentile Modified Chlorophyll Absorption Ratio Index Improved (MCARI2) was made available to the learning models as an auxiliary feature to assess its ability to improve NUP prediction accuracy. The trained models demonstrated robustness by maintaining satisfactory prediction accuracy across locations, pixel sizes, development stages, and a broad range of NUP values (4.8 to 182 kg ha-1). Using the four most informative spectral features in addition to the auxiliary feature, the mean absolute error (MAE) of Lasso, SVR, and PLSR models (9.4, 9.7, and 9.5 kg ha-1, respectively) was lower than that of random forest (11.2 kg ha-1). The relative MAE for the Lasso, SVR, PLSR, and random forest models was 16.5%, 17.0%, 16.6%, and 19.6%, respectively. The inclusion of the auxiliary feature not only improved overall prediction accuracy by 1.6 kg ha-1 (14%) across all models, but it also reduced the number of input features required to reach optimal performance. The variance of predicted NUP increased as the measured NUP increased (MAE of the Lasso model increased from 4.0 to 12.1 kg ha-1 for measured NUP less than 25 kg ha-1 and greater than 100 kg ha-1, respectively). The most influential spectral features were oftentimes adjacent to each other (i.e., within approximately 6 nm), indicating the importance of both spectral precision and derivative spectra around key wavelengths for explaining NUP. Finally, several challenges and opportunities are discussed regarding the use of these results in the context of improving nitrogen fertilizer management.",2020,Remote Sensing
Fragility analysis of continuous pipelines subjected to transverse permanent ground deformation,"Abstract The structural integrity of buried continuous pipelines can be jeopardized by transverse permanent ground deformation (PGD) induced by landslides. A probabilistic analysis can facilitate understanding the likelihood of damage to pipelines in landslide regions, further minimizing the risk. However, empirical fragility curves for the landslide-pipeline interaction problem are not available due to the lack of field data. The problem can be addressed by numerical approaches. In this study, a simplified two-dimensional numerical model is developed. It characterizes the pipes as beam-type structures and the surrounding soil as Winkler springs. It is compared here against three-dimensional continuum-based analyses, which could save extensively on computational efforts. All input parameters were sampled randomly and paired with the displacement demands to form a limited set of statistically significant, yet nominally identical, pipeline samples, and the demand models for the maximum tensile strain were evaluated. A supervised machine learning technique, called Lasso regression, was then used to establish a predictive relation between the input and the output using the limited dataset, based on which a large dataset (one million) was calculated for the fragility analysis. The use of a Winkler-based analysis and the supervised machine learning technique makes it possible to generate fragility curves for pipelines subjected to transverse PGD for the first time.",2018,Soils and Foundations
Comparing Ridge and LASSO estimators for data analysis,Abstract This paper is devoted to the comparison of Ridge and LASSO estimators. Test data is used to analyze advantages of each of the two regression analysis methods. All the required calculations are performed using the R software for statistical computing.,2017,Procedia Engineering
Social Media Aided Stock Market Predictions by Sparsity Induced Regression,"Prediction of the stock market has been a research topic for decades. Recently, data from social media like Google and Twitter are included in prediction models. This data serves as an indicator of sentiments that are potentially useful for prediction. Interpretation of current prediction methods is cumbersome. Beforehand it is not known which data is relevant for the prediction and hence which data should be added to the model. To improve interpretability and thereby credibility of the results, this thesis uses sparse regression methods that automatically discard data that is not useful for the prediction. Current methods induce sparsity via L1-regularization such as the LASSO. In contrast to traditional applications, this thesis assumes that a sparse, time-varying regression vector is estimated from time series data that arrives sequentially over time. Data can thus not be treated in batch form where constant behavior over a window is assumed and hence performance of current sparse regression methods is limited. Therefore, a new Weighted Sparse Kalman Filter (Weighted-SKF) is proposed that induces sparsity in the KF equations. The KF is able to track time-varying behavior, while the sparsity ensures that interpretable results are obtained. Simulations demonstrate that the Weighted-SKF outperforms current regression methods in identifying the time-varying support and regression vector. Moreover, the time-varying usefulness of social media data is demonstrated: the Weighted-SKF includes social media data in its prediction model only during large declines in the stock market.",2014,
Risk Factors of Manufacturing Industries,"In the work a comparison of risk factors influencing the probability of a negative value of companies (stage of prebankruptcy), for various manufacturing industries: the range of financial variables and corporate governance factors. The idea of corporate financial architecture were confirmed in the Russian industry: factors affecting the corporate structure of the company (characteristics of the CEO and ownership concentration) can be effectively applied for the analysis of financial stability of companies along with traditional financial indicators, improving the predictive quality of the models. Good stability of the obtained results was confirmed by serial build and comparison of logistic regression with lasso regularization and logistic regression models for all subsets. Next, were confirmed the statistical significance of 88,1% of the explaining factors of all final specifications. During the study period 2011Ã¢Â€Â“2015 dynamics of the intensity of bankruptcies in the manufacturing sector were not always uniform. In the analyzed period were identified both specific and general risk factors for considered sectors. The forecast quality of the final specifications for the industries demonstrated its high level in the control samples, the value of AUC (area under ROC-curve) is in the range from 85,06% in metallurgical production to 96,12% in mechanical engineering. Empirical results show that the sectoral differentiation of risk factors in the Russian manufacturing industry can be taken into consideration by enterprises, credit institutions and regulatory authorities in the conduct of crisis management and branch transformation of the existing methodological recommendations for developing financial and investment policy of the company.",2018,
