title,abstract,year,journal
Identifying Genetic Risk Factors via Sparse Group Lasso with Group Graph Structure,"Genome-wide association studies (GWA studies or GWAS) investigate the relationships between genetic variants such as single-nucleotide polymorphisms (SNPs) and individual traits. Recently, incorporating biological priors together with machine learning methods in GWA studies has attracted increasing attention. However, in real-world, nucleotide-level bio-priors have not been well-studied to date. Alternatively, studies at gene-level, for example, protein--protein interactions and pathways, are more rigorous and legitimate, and it is potentially beneficial to utilize such gene-level priors in GWAS. In this paper, we proposed a novel two-level structured sparse model, called Sparse Group Lasso with Group-level Graph structure (SGLGG), for GWAS. It can be considered as a sparse group Lasso along with a group-level graph Lasso. Essentially, SGLGG penalizes the nucleotide-level sparsity as well as takes advantages of gene-level priors (both gene groups and networks), to identifying phenotype-associated risk SNPs. We employ the alternating direction method of multipliers algorithm to optimize the proposed model. Our experiments on the Alzheimer's Disease Neuroimaging Initiative whole genome sequence data and neuroimage data demonstrate the effectiveness of SGLGG. As a regression model, it is competitive to the state-of-the-arts sparse models; as a variable selection method, SGLGG is promising for identifying Alzheimer's disease-related risk SNPs.",2017,ArXiv
Efficient Algorithms for Selecting Features with Arbitrary Group Constraints via Group Lasso,"Feature structure information plays an important role for regression and classification tasks. We consider a more generic problem: group lasso problem, where structures over feature space can be represented as a combination of features in a group. These groups can be either overlapped or non-overlapped, which are specified in different structures, e.g., structures over a line, a tree, a graph or even a forest. We propose a new approach to solve this generic group lasso problem, where certain features are selected in a group, and an arbitrary family of subset is allowed. We employ accelerated proximal gradient method to solve this problem, where a key step is solve the associated proximal operator. We propose a fast method to compute the proximal operator, where its convergence is rigorously proved. Experimental results on different structures (e.g., group, tree, graph structures) demonstrate the efficiency and effectiveness of the proposed algorithm.",2013,2013 IEEE 13th International Conference on Data Mining
Prostate cancer detection from model-free T1-weighted time series and diffusion imaging,"The combination of Dynamic Contrast Enhanced (DCE) images with diffusion MRI has shown great potential in prostate cancer detection. The parameterization of DCE images to generate cancer markers is traditionally performed based on pharmacokinetic modeling. However, pharmacokinetic models make simplistic assumptions about the tissue perfusion process, require the knowledge of contrast agent concentration in a major artery, and the modeling process is sensitive to noise and fitting instabilities. We address this issue by extracting features directly from the DCE T1-weighted time course without modeling. In this work, we employed a set of data-driven features generated by mapping the DCE T1 time course to its principal component space, along with diffusion MRI features to detect prostate cancer. The optimal set of DCE features is extracted with sparse regularized regression through a Least Absolute Shrinkage and Selection Operator (LASSO) model. We show that when our proposed features are used within the multiparametric MRI protocol to replace the pharmacokinetic parameters, the area under ROC curve is 0.91 for peripheral zone classification and 0.87 for whole gland classification. We were able to correctly classify 32 out of 35 peripheral tumor areas identified in the data when the proposed features were used with support vector machine classification. The proposed feature set was used to generate cancer likelihood maps for the prostate gland.",2015,
Forecasting Simultaneously High-Dimensional Time Series: A Robust Model-Based Clustering Approach,"This paper considers the problem of forecasting high-dimensional time series. It employs a robust clustering approach to perform classification of the component series. Each series within a cluster is assumed to follow the same model and the data are then pooled for estimation. The classification is model-based and robust to outlier contamination. The robustness is achieved by using the intrinsic mode functions of the Hilbertâ€“Huang transform at lower frequencies. These functions are found to be robust to outlier contamination. The paper also compares out-of-sample forecast performance of the proposed method with several methods available in the literature. The other forecasting methods considered include vector autoregressive models with=without LASSO, group LASSO, principal component regression, and partial least squares. The proposed method is found to perform well in out-of-sample forecasting of the monthly unemployment rates of 50 US states. Copyright Â© 2013 John Wiley & Sons, Ltd.",2013,Journal of Forecasting
Nonparametric regression with adaptive truncation via a convex hierarchical penalty,"&NA; We consider the problem of nonparametric regression with a potentially large number of covariates. We propose a convex, penalized estimation framework that is particularly well suited to highâ€dimensional sparse additive models and combines the appealing features of finite basis representation and smoothing penalties. In the case of additive models, a finite basis representation provides a parsimonious representation for fitted functions but is not adaptive when component functions possess different levels of complexity. In contrast, a smoothing splineâ€type penalty on the component functions is adaptive but does not provide a parsimonious representation. Our proposal simultaneously achieves parsimony and adaptivity in a computationally efficient way. We demonstrate these properties through empirical studies and show that our estimator converges at the minimax rate for functions within a hierarchical class. We further establish minimax rates for a large class of sparse additive models. We also develop an efficient algorithm that scales similarly to the lasso with the number of covariates and sample size.",2019,Biometrika
Prognostic Gene Discovery in Glioblastoma Patients using Deep Learning,"This study aims to discover genes with prognostic potential for glioblastoma (GBM) patients' survival in a patient group that has gone through standard of care treatments including surgeries and chemotherapies, using tumor gene expression at initial diagnosis before treatment. The Cancer Genome Atlas (TCGA) GBM gene expression data are used as inputs to build a deep multilayer perceptron network to predict patient survival risk using partial likelihood as loss function. Genes that are important to the model are identified by the input permutation method. Univariate and multivariate Cox survival models are used to assess the predictive value of deep learned features in addition to clinical, mutation, and methylation factors. The prediction performance of the deep learning method was compared to other machine learning methods including the ridge, adaptive Lasso, and elastic net Cox regression models. Twenty-seven deep-learned features are extracted through deep learning to predict overall survival. The top 10 ranked genes with the highest impact on these features are related to glioblastoma stem cells, stem cell niche environment, and treatment resistance mechanisms, including POSTN, TNR, BCAN, GAD1, TMSB15B, SCG3, PLA2G2A, NNMT, CHI3L1 and ELAVL4.",2019,Cancers
Use of multivariate statistical methods for the analysis of metabolomic data,"In the last decades, advances in technology have enabled the gathering of an increasingly amount of data in the field of biology and biomedicine. The so called ""-omics"" technologies such as genomics, epigenomics, transcriptomics or metabolomics, among others, produce hundreds, thousands or even millions of variables per data set. The analysis of â€™omicâ€™ data presents different complexities that can be methodological and computational. This has driven a revolution in the development of new statistical methods specifically designed for dealing with these type of data. To this methodological complexities one must add the logistic and economic restrictions usually present in scientific research projects that lead to small sample sizes paired to these wide data sets. This makes the analyses even harder, since there is a problem in having many more variables than observations. Among the methods developed to deal with these type of data there are some based on the penalization of the coefficients, such as lasso or elastic net, others based on projection techniques, such as PCA or PLS, and others based in regression or classification trees and ensemble methods such as random forest. All these techniques work fine when dealing with different â€™omicâ€™ data in matrix format (I xJ ), but sometimes, these I xJ data sets can be expanded by taking, for example, repeated measurements at different time points for each individual, thus having I xJxK data sets that raise more methodological com-",2019,
Comparison of the modified unbounded penalty and the LASSO to select predictive genes of response to chemotherapy in breast cancer,"Covariate selection is a fundamental step when building sparse prediction models in order to avoid overfitting and to gain a better interpretation of the classifier without losing its predictive accuracy. In practice the LASSO regression of Tibshirani, which penalizes the likelihood of the model by the L1 norm of the regression coefficients, has become the gold-standard to reach these objectives. Recently Lee and Oh developed a novel random-effect covariate selection method called the modified unbounded penalty (MUB) regression, whose penalization function can equal minus infinity at 0 in order to produce very sparse models. We sought to compare the predictive accuracy and the number of covariates selected by these two methods in several high-dimensional datasets, consisting in genes expressions measured to predict response to chemotherapy in breast cancer patients. These comparisons were performed by building the Receiver Operating Characteristics (ROC) curves of the classifiers obtained with the selected genes and by comparing their area under the ROC curve (AUC) corrected for optimism using several variants of bootstrap internal validation and cross-validation. We found consistently in all datasets that the MUB penalization selected a remarkably smaller number of covariates than the LASSO while offering a similar-and encouraging-predictive accuracy. The models selected by the MUB were actually nested in the ones obtained with the LASSO. Similar findings were observed when comparing these results to those obtained in their first publication by other authors or when using the area under the Precision-Recall curve (AUCPR) as another measure of predictive performance. In conclusion, the MUB penalization seems therefore to be one of the best options when sparsity is required in high-dimension. Further investigation in other datasets is however required to validate these findings.",2018,PLoS ONE
L1-Norm Regularized Deconvolution of Functional MRI BOLD Signal,"Deconvolution methods are used to denoise the blood oxygen level-dependent (BOLD) response, the signal that forms 
the basis of functional MRI (fMRI). In this work we propose a novel approach based on a temporal regularized deconvolution of the BOLD fMRI signal with the least absolute shrinkage and selection operator (LASSO) model, solved using the angle regression algorithm (LARS). In this way we were able to recover the underlying neurons activations and their dynamics",2018,
The Iso-lambda Descent Algorithm for the LASSO,"Following the introduction by Tibshirani of the LASSO tech- nique for feature selection in regression, two algorithms were proposed by Osborne et al. for solving the associated problem. One is an homo- topy method that gained popularity as the LASSO modication of the LARS algorithm. The other is a nite-step descent method that follows a path on the constraint polytope, and seems to have been largely ignored. One of the reason may be that it solves the constrained formulation of the LASSO, as opposed to the more practical regularized formulation. We give here an adaptation of this algorithm that solves the regularized problem, has a simpler formulation, and outperforms state-of-the-art al- gorithms in terms of speed.",2010,
AIC for the group Lasso in generalized linear models,"When covariates are assumed to be clustered in groups in regression problems, the group Lasso is useful, because it tends to drive all the weights in one group to zero together. The group Lasso includes a tuning parameter which controls a penalty level, and an unbiased estimator of the true prediction error is derived as a $$C_p$$
 -type criterion to select an appropriate value of the tuning parameter. However, in general, the $$C_p$$
 -type criterion cannot be derived in generalized linear regressions such as a logistic regression. Hence, this paper obtains the AIC for the group Lasso based on its original definition under the framework of generalized linear models. In terms of computational cost, our criterion is clearly better than the cross validation, but it is shown through simulation studies that the performance of the our criterion is almost the same as or better than that of the cross validation.",2019,
Sparse regression methods with measurement-error for magnetoencephalography,"Magnetoencephalography (MEG) is a neuroimaging method for mapping brain activity based on magnetic field recordings. The inverse problem associated with MEG is severely ill-posed and is complicated by the presence of high collinearity in the forward (leadfield) matrix. This means that accurate source localisation can be challenging. The most commonly used methods for solving the MEG problem do not employ sparsity to help reduce the dimensions of the problem. In this thesis we review a number of the sparse regression methods that are widely used in statistics, as well as some more recent methods, and assess their performance in the context of MEG data. Due to the complexity of the forward model in MEG, the presence of measurement-error in the leadfield matrix can create issues in the spatial resolution of the data. Therefore we investigate the impact of measurement-error on sparse regression methods as well as how we can correct for it. We adapt the conditional score and simulation extrapolation (SIMEX) methods for use with sparse regression methods and build on an existing corrected lasso method to cover the elastic net penalty. These methods are demonstrated using a number of simulations for different types of measurement-error and are also tested with real MEG data. The measurement-error methods perform well in simulations, including high dimensional examples, where they are able to correct for attenuation bias in the true covariates. However the extent of their correction is much more restricted in the more complex MEG data where covariates are highly correlated and there is uncertainty over the distribution of the error.",2017,
"Most Known Regularizers : Ridge Regression , the Lasso Estimate , and Elastic Net Regularization Methods","The work in this paper shows intensive empirical experiments using 13 datasets to understand the regularization effectiveness of ridge regression, the lasso estimate, and elastic net regularization methods. The study offers a deep understanding of how the datasets affect the goodness of the prediction accuracy of each regularization method for a given problem given the diversity in the datasets used. The results have shown that datasets play crucial rules on the performance of the regularization method and that the predication accuracy depends heavily on the nature of the sampled datasets.",2017,
Incorporating Additional Constraints in Sparse Estimation,"Abstract It is well known that a linear regression can benefit from knowledge that the underlying regression vector is sparse. The combinatorial problem of selecting the nonzero components of this vector can be relaxed by regularizing the squared error with a convex penalty function like the l 1 norm. However, in many applications, additional conditions on the structure of the regression vector and its sparsity pattern are available. Incorporating this information into the learning method may lead to a significant decrease of the estimation error. In this paper, we review a recently proposed family of convex penalty functions, which encode prior knowledge on the structure of the vector formed by the absolute values of the regression coefficients. This family subsumes the l 1 norm and is flexible enough to include different models of sparsity patterns, which are of practical and theoretical importance. We discuss special cases of these penalty functions in which the regularized empirical error function can be efficiently minimized by a proximal-point method. We compare this method to a previous method based on block coordinate descent and present numerical experiments which highlight the benefit of our framework over a greedy approach and the Lasso method.",2012,IFAC Proceedings Volumes
A Generic Path Algorithm for Regularized Statistical Estimation.,"Regularization is widely used in statistics and machine learning to prevent overfitting and gear solution towards prior information. In general, a regularized estimation problem minimizes the sum of a loss function and a penalty term. The penalty term is usually weighted by a tuning parameter and encourages certain constraints on the parameters to be estimated. Particular choices of constraints lead to the popular lasso, fused-lasso, and other generalized â„“1 penalized regression methods. In this article we follow a recent idea by Wu (2011, 2012) and propose an exact path solver based on ordinary differential equations (EPSODE) that works for any convex loss function and can deal with generalized â„“1 penalties as well as more complicated regularization such as inequality constraints encountered in shape-restricted regressions and nonparametric density estimation. Non-asymptotic error bounds for the equality regularized estimates are derived. In practice, the EPSODE can be coupled with AIC, BIC, Cp or cross-validation to select an optimal tuning parameter, or provides a convenient model space for performing model averaging or aggregation. Our applications to generalized â„“1 regularized generalized linear models, shape-restricted regressions, Gaussian graphical models, and nonparametric density estimation showcase the potential of the EPSODE algorithm.",2014,Journal of the American Statistical Association
[Prognosis in meningococcal disease: methodology and practice].,"78 patients with systemic meningococcal disease admitted to the Intensive Care Unit of the 2nd Moscow Hospital for Infectious Diseases were studied and the composite prognostic score was developed to estimate the risk of lethal outcome. The stepwise variable selection procedure for the multiple logistic regression was applied to 30 potential clinical and laboratory risk factors and markers. Five factors were selected for the score, namely the platelet count (< 150 x 10(6)/ml), the presence of hemorrhages into the eye or mucosal tissue, the interval from the last urination before admission (> 4 h), the respiration rate (> 170% of age-adjusted normal value) and age (< 2 or > 65 years) with regression coefficients 0.3, 0.2, 0.2 and 0.1, respectively. Both for the source clinical group and for the additional test group (64 patients), the scale was able to classify correctly 95% of cases using the data collected at admission. Ten prognostic scores proposed previously by foreign investigators were tested in the same patients and the best four scores were selected (GMSPS, Gedde-Dahl, Niklasson, Kahn); the scores classified correctly 85-90% of cases. This study is an example of methodological approaches to prognostic score construction in medicine.",1999,Klinicheskaia meditsina
Feature Selection for Thermal Comfort Modeling based on Constrained LASSO Regression,"Abstract Thermal comfort is influenced by many factors and can vary significantly between different individuals. Therefore, modeling personal thermal comfort is a complex challenge and requires a detailed knowledge about environmental as well as physical or even mental conditions of an occupant. However, only limited data are available which is usually restricted to environmental measurements. Furthermore in the context of commercial buildings, the calculation effort must be kept as low as possible that scaling issues related to the large number of occupants are reduced. To cope with this problem, the presented paper analyzes thermal sensation voting data collected in an open-plan office in Singapore and uses LASSO regression techniques for the identification of the most important comfort features. Well known relations between thermal comfort and the corresponding vote are considered via suitable constraints. To define a common set of optimal features, the individual regression problem is extended to an arbitrary number of occupants. This leads to multiple LASSO optimization problems that are coupled by nonlinear if-statements. A reformulation method is presented which results in a mixed integer quadratic program by introducing binary activation variables. Eventually, the method is applied to comfort modeling and the resulting model structures are compared regarding their complexity, number of selected features and prediction accuracy.",2019,IFAC-PapersOnLine
A variable informative criterion based on weighted voting strategy combined with LASSO for variable selection in multivariate calibration,"Abstract High-throughput spectra data with large number of variables (wavelength) will make the prediction of multivariate calibration model unreliable, in which case, the sparse statistical methods such as least absolute shrinkage and selection operator (LASSO) are gradually being valued by researchers. In this study, a novel variable informative criterion based on weighted voting strategy combined with least absolute shrinkage and selection operator (WV-LASSO) has been proposed. Monte Carlo Sampling (MCS) is used for generating a large number of sub-models. In each Monte Carlo circulation, the regression coefficients and variable selection information of LASSO model will be recorded. In the present work, weighted voting strategy based on regression coefficients information combined with selected variable frequency of all sub-models is used for evaluating the importance of variable. Different from specific methods, variable informative (importance) criterion can be more extensive and flexible for algorithm design. Then an approach called exponentially decreasing function (EDF) is applied to create a variable selection method with WV-LASSO. The performance of this method was evaluated by three near-infrared (NIR) datasets. Compared with some efficient variable selection methods based on different informative criterions including variable importance projection (VIP), Monte Carlo uninformative variable elimination (MC-UVE), randomization test (RT), competitive adaptive reweighted sampling (CARS), stability competitive adaptive reweighted sampling (SCARS), variable iterative space shrinkage approach (VISSA), interval variable iterative space shrinkage approach (iVISSA), LASSO coupled with sampling error profile analysis (SEPA-LASSO) and variable combination population analysis (VCPA), and so forth, the variable selection method proposed in this paper shows better prediction and interpretation ability and has potential for constructing various variable selection methods by combining other selection strategies.",2019,Chemometrics and Intelligent Laboratory Systems
Targeted Metabolomic Profiling of Plasma and Survival in Heart Failure Patients.,"OBJECTIVES
This study sought to derive and validate plasma metabolite associations with survival in heart failure (HF) patients.


BACKGROUND
Profiling of plasma metabolites to predict the course of HF appears promising, butÂ validation and incremental value of these profiles are less established.


METHODS
Patients (nÂ = 1,032) who met Framingham HF criteria with a history of reduced ejection fraction were randomly divided into derivation and validation cohorts (nÂ = 516 each). Amino acids, organic acids, and acylcarnitines were quantified using mass spectrometry in fasting plasma samples. We derived a prognostic metabolite profile (PMP) in the derivation cohort using Lasso-penalized Cox regression. Validity was assessed by 10-fold cross validation in the derivation cohort and by standard testing in the validation cohort. The PMP was analyzed as both a continuous variable (PMPscore) and dichotomized at the median (PMPcat), in univariate and multivariate models adjusted for clinical risk score and N-terminal pro-B-type natriuretic peptide.


RESULTS
Overall, 48% of patients were African American, 35% were women, and the average age was 69 years. AfterÂ aÂ median follow-up of 34 months, there were 256 deaths (127 and 129 in derivation and validation cohorts, respectively). Optimized modeling defined the 13 metabolite PMPs, which was cross validated as both the PMPscore (hazard ratio [HR]: 3.27; pÂ < 2Â Ã— 10-16) and PMPcat (HR: 3.04; pÂ = 2.93Â Ã— 10-8). The validation cohort showed similar results (PMPscore HR: 3.9; pÂ < 2Â Ã— 10-16 and PMPcat HR: 3.99; pÂ = 3.47Â Ã— 10-9). In adjusted models, PMP remained associated with mortality in the cross-validated derivation cohort (PMPscore HR: 1.63; pÂ = 0.0029; PMPcat HR: 1.47; pÂ =Â 0.081) and the validation cohort (PMPscore HR: 1.54; pÂ = 0.037; PMPcat HR: 1.69; pÂ = 0.043).


CONCLUSIONS
Plasma metabolite profiles varied across HF subgroups and were associated with survival incrementalÂ to conventional predictors. Additional investigation is warranted to define mechanisms and clinical applications.",2017,JACC. Heart failure
Radiomics analysis on T2-MR image to predict lymphovascular space invasion in cervical cancer,"Lymphovascular space invasion (LVSI) is an important determinant for selecting treatment plan in cervical cancer (CC). For CC patients without LVSI, conization is recommended; otherwise, if LVSI is observed, hysterectomy and pelvic lymph node dissection are required. Despite the importance, current identification of LVSI can only be obtained by pathological examination through invasive biopsy or after surgery. In this study, we provided a non-invasive and preoperative method to identify LVSI by radiomics analysis on T2-magnetic resonance image (MRI), aiming at assisting personalized treatment planning. We enrolled 120 CC patients with T2 image and clinical information, and allocated them into a training set (n = 80) and a testing set (n= 40) according to the diagnostic time. Afterwards, 839 image features were extracted to reflect the intensity, shape, and high-dimensional texture information of CC. Among the 839 radiomic features, 3 features were identified to be discriminative by Least absolute shrinkage and selection operator (Lasso)-Logistic regression. Finally, we built a support vector machine (SVM) to predict LVSI status by the 3 radiomic features. In the independent testing set, the radiomics model achieved area under the receiver operating characteristic curve (AUC) of 0.7356, classification accuracy of 0.7287. The radiomics signature showed significant difference between non-LVSI and LVSI patients (p<0.05). Furthermore, we compared the radiomics model with clinical model that uses clinical information, and the radiomics model showed significant improvement than clinical factors (AUC=0.5967 in the validation cohort for clinical model).",2019,
Variable Selection with Copula Entropy,"Variable selection is of significant importance for classification and regression tasks in machine learning and statistical applications where both predictability and explainability are needed. In this paper, a Copula Entropy (CE) based method for variable selection which use CE based ranks to select variables is proposed. The method is both model-free and tuning-free. Comparison experiments between the proposed method and traditional variable selection methods, such as Stepwise Selection, regularized generalized linear models and Adaptive LASSO, were conducted on the UCI heart disease data. Experimental results show that CE based method can select the `right' variables out effectively and derive better interpretable results than traditional methods do without sacrificing accuracy performance. It is believed that CE based variable selection can help to build more explainable models.",2019,ArXiv
A genetic association study detects haplotypes associated with obstructive heart defects,"The development of congenital heart defects (CHDs) involves a complex interplay between genetic variants, epigenetic variants, and environmental exposures. Previous studies have suggested that susceptibility to CHDs is associated with maternal genotypes, fetal genotypes, and maternalâ€“fetal genotype (MFG) interactions. We conducted a haplotype-based genetic association study of obstructive heart defects (OHDs), aiming to detect the genetic effects of 877 SNPs involved in the homocysteine, folate, and transsulfuration pathways. Genotypes were available for 285 mother-offspring pairs with OHD-affected pregnancies and 868 mother-offspring pairs with unaffected pregnancies. A penalized logistic regression model was applied with an adaptive least absolute shrinkage and selection operator (lasso), which dissects the maternal effect, fetal effect, and MFG interaction effects associated with OHDs. By examining the association between 140 haplotype blocks, we identified 9 blocks that are potentially associated with OHD occurrence. Four haplotype blocks, located in genes MGMT, MTHFS, CBS, and DNMT3L, were statistically significant using a Bayesian false-discovery probability threshold of 0.8. Two blocks in MGMT and MTHFS appear to have significant fetal effects, while the CBS and DNMT3L genes may have significant MFG interaction effects.",2014,Human Genetics
"Combining eye tracking, pupil dilation and EEG analysis for predicting web users click intention","Merging data originated in Pupil dilation and EEG and web user behaviour.Exploring the behaviour of web users from a physiological perspective.Combining web log and physiological originated data for analysing the web user behavior. In this paper a novel approach for analyzing web user behavior and preferences on a web site is introduced, consisting of a physiological-based analysis for the assessment of a web users click intention, by merging pupil dilation and electroencephalogram (EEG) responses.First, we conducted an empirical study using five real web sites from which the gaze position, pupil dilation and EEG of 21 human subjects were recorded while performing diverse information foraging tasks. We found the existence of a statistical differentiation between choice and not-choice pupil dilation curves, specifically that fixations corresponding to clicks had greater pupil size than fixations without a click.Then 7 classification models were proposed using 15 out of 789 pupil dilation and EEG features obtained from a Random Lasso feature selection process. Although good results were obtained for Accuracy (71,09% using Logistic Regression), the results for Precision, Recall and F-Measure remained low, which indicates that the behaviour we were studying was not well classified.The above results show that it is possible to create a classifier for web user click intention behaviour based on merging features extracted from pupil dilation and EEG responses. However we conclude that it is necessary to use better quality instruments for capturing the data.",2017,Inf. Fusion
A utility approach to individualized optimal dose selection using biomarkers.,"In many settings, including oncology, increasing the dose of treatment results in both increased efficacy and toxicity. With the increasing availability of validated biomarkers and prediction models, there is the potential for individualized dosing based on patient specific factors. We consider the setting where there is an existing dataset of patients treated with heterogenous doses and including binary efficacy and toxicity outcomes and patient factors such as clinical features and biomarkers. The goal is to analyze the data to estimate an optimal dose for each (future) patient based on their clinical features and biomarkers. We propose an optimal individualized dose finding rule by maximizing utility functions for individual patients while limiting the rate of toxicity. The utility is defined as a weighted combination of efficacy and toxicity probabilities. This approach maximizes overall efficacy at a prespecified constraint on overall toxicity. We model the binary efficacy and toxicity outcomes using logistic regression with dose, biomarkers and dose-biomarker interactions. To incorporate the large number of potential parameters, we use the LASSO method. We additionally constrain the dose effect to be non-negative for both efficacy and toxicity for all patients. Simulation studies show that the utility approach combined with any of the modeling methods can improve efficacy without increasing toxicity relative to fixed dosing. The proposed methods are illustrated using a dataset of patients with lung cancer treated with radiationÂ therapy.",2019,Biometrical journal. Biometrische Zeitschrift
Regularized Logistic Regression Fusion for Speaker Verification,"Fusion of the base classifiers is seen as the way to achieve stateof-the art performance in the speaker verfication systems. Standard approach is to pose the fusion problem as the linear binary classification task. Most successful loss function in speaker verification fusion has been the weighted logistic regression popularized by the FoCal toolkit. However, it is known that optimizing logistic regression can overfit severely without appropriate regularization. In addition, subset classifier selection can be achieved by using an external 0/1 loss function on the best subset. In this work, we propose to use LASSO based regularization on the FoCal cost function to achive improved performance and classifier subset selection method integrated into one optimization task. Proposed method is able to achieve 51% relative improvement in Actual DCF over the FoCal baseline. Index Terms: logistic regression, regularization, compressed sensing, linear fusion, speaker verification",2011,
Promising key genes associated with tumor microenvironments and prognosis of hepatocellular carcinoma,"BACKGROUND
Despite significant advances in multimodality treatments, hepatocellular carcinoma (HCC) remains one of the most common malignant tumors. Identification of novel prognostic biomarkers and molecular targets is urgently needed.


AIM
To identify potential key genes associated with tumor microenvironments and the prognosis of HCC.


METHODS
The infiltration levels of immune cells and stromal cells were calculated and quantified based on the ESTIMATE algorithm. Differentially expressed genes (DEGs) between high and low groups according to immune or stromal scores were screened using the gene expression profile of HCC patients in The Cancer Genome Atlas and were further linked to the prognosis of HCC. These genes were validated in four independent HCC cohorts. Survival-related key genes were identified by a LASSO Cox regression model.


RESULTS
HCC patients with a high immune/stromal score had better survival benefits than patients with a low score. A total of 899 DEGs were identified and found to be involved in immune responses and extracellular matrices, 147 of which were associated with overall survival. Subsequently, 52 of 147 survival-related DEGs were validated in additional cohorts. Finally, ten key genes (STSL2, TMC5, DOK5, RASGRP2, NLRC3, KLRB1, CD5L, CFHR3, ADH1C, and UGT2B15) were selected and used to construct a prognostic gene signature, which presented a good performance in predicting overall survival.


CONCLUSION
This study extracted a list of genes associated with tumor microenvironments and the prognosis of HCC, thereby providing several valuable directions for the prognostic prediction and molecular targeted therapy of HCC in the future.",2020,World Journal of Gastroenterology
Causal Inference Using Boosting in IV Regression Models,"Author(s): Xu, Hao | Advisor(s): Lee, Tae-Hwy | Abstract: This dissertation focuses on using the machine learning technique, boosting, for causal inference in the instrumental variable (IV) regression models.In Chapter 1, when endogenous variables are approximated by sieve functions of observable instruments, the number of instruments increases rapidly and many may be invalid or irrelevant. We introduce Double Boosting (DB) which consistently selects only valid and relevant instruments even when there are more instruments than the sample size. We estimate the parameter of interest using generalized method of moments (GMM) with selected instruments. We refer this method as Double Boosting GMM (DB-GMM). We show that DB does not select weakly relevant or weakly valid instruments. In Monte Carlo, we compare DB-GMM with other methods such as GMM using Lasso penalty (penalized GMM). In the application of estimating the BLP-type automobile demand function, where price is endogenous and instruments are high dimensional functions of product characteristics, we find the DB-GMM estimator of the price elasticity of demand is more elastic than other estimators.Extending from Chapter 1, Chapter 2 combines the DB selection algorithm from Chapter 1 with the multiple-layer neural networks (NN) for the first-stage IV estimation, where high dimensional sieve instrument variables are the activation functions at the last hidden layer of the neural networks.Chapter 3 studies the panel data models with many instruments. When the regressors are endogenous in the panel data models, we employed the 2SLS approach for the FE estimator. We denote it as FE-2SLS. We find that the FE-2SLS estimator is sensitive to the number of instruments, where it is inconsistent when the number of instruments increases. We show that using the two regularization methods, SCAD and L2Boosting, for instrument selection make the FE-2SLS estimator more robust and restore its consistency when there are many instruments. Furthermore, we consider a Stein-like combined estimator of the FE and FE-2SLS estimators and provide its asymptotic properties. A empirical study is conducted for the economics of real house price using the US state level panel data.",2018,
