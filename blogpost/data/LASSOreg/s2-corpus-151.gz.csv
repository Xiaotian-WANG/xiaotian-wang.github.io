title,abstract,year,journal
Sparse Regression by Projection and Sparse Discriminant Analysis.,"Recent years have seen active developments of various penalized regression methods, such as LASSO and elastic net, to analyze high dimensional data. In these approaches, the direction and length of the regression coefficients are determined simultaneously. Due to the introduction of penalties, the length of the estimates can be far from being optimal for accurate predictions. We introduce a new framework, regression by projection, and its sparse version to analyze high dimensional data. The unique nature of this framework is that the directions of the regression coefficients are inferred first, and the lengths and the tuning parameters are determined by a cross validation procedure to achieve the largest prediction accuracy. We provide a theoretical result for simultaneous model selection consistency and parameter estimation consistency of our method in high dimension. This new framework is then generalized such that it can be applied to principal components analysis, partial least squares and canonical correlation analysis. We also adapt this framework for discriminant analysis. Compared to the existing methods, where there is relatively little control of the dependency among the sparse components, our method can control the relationships among the components. We present efficient algorithms and related theory for solving the sparse regression by projection problem. Based on extensive simulations and real data analysis, we demonstrate that our method achieves good predictive performance and variable selection in the regression setting, and the ability to control relationships between the sparse components leads to more accurate classification. In supplemental materials available online, the details of the algorithms and theoretical proofs, and R codes for all simulation studies are provided.",2015,"Journal of computational and graphical statistics : a joint publication of American Statistical Association, Institute of Mathematical Statistics, Interface Foundation of North America"
Developing Nonresponse Weighting Adjustments for Population-Based HIV Impact Assessments Surveys in Three African Countries,"In collaboration with national Ministries of Health, the Centers for Disease Control and Prevention, and other partners, ICAP at Columbia University is conducting Populationbased HIV Impact Assessment (PHIA) surveys in 12 sub-Saharan African countries. We use data from the first three PHIA surveysâ€•in Malawi, Zambia, and Zimbabweâ€•to study the effect of survey nonresponse weighting adjustments on HIV prevalence estimates. In developing the nonresponse adjustments, decisions made about the variables to use in the adjustments, and about the formation of nonresponse cells, may affect survey estimates and their standard errors. The nonresponse adjustments in PHIA surveys are made in three stages, first to adjust for nonresponse to the household interview, then for person interview nonresponse in responding households, and finally for failure to obtain analyzable blood samples among persons responding to the interview. A sizable number of variables is available for use in making the nonresponse adjustments for the personal interview and blood sample nonresponse. This paper describes the use of the LASSO regression and CHAID for variable selection in making these nonresponse adjustments. It then examines how the use of different sets of variables employed in the nonresponse adjustments affects the surveysâ€™ HIV prevalence estimates.",2017,
Abstract 4791: Prognostic model of lower grade gliomas,"Background Lower grade gliomas (LGGs WHO grade II/III glioma) account for one third of all gliomas. Most LGGs generally show a slow progression, but some show a more aggressive clinical course, where several clinical/genetic factors, such as tumor size, presence of neurologic deficit before surgery and 1p19q LOH, have been reported to correlate patientsâ€™ survival. However, no large-scale studies prevent establishment of a reliable prognostication system. Methods Status of somatic mutations and copy number variations (CNVs) were investigated for 335 Japanese patients with LGG using whole exome/targeted sequencing and single nucleotide polymorphism-array karyotyping, respectively. Corresponding data were also publically available for 425 patients from the Cancer Genome Atlas (URL: http://cancergenome.nih.gov/). Correlation of genetic lesions and other parameters with overall survival (OS) was analyzed for combined 538 patients, which were divided into two sets, 269 training and 269 validation sets. First, the 269 patients of the training set were classified into 3 types according to the characteristic mutations and CNVs: Type 1 (mutated IDH with 1p/19q LOH), Type 2 (mutated IDH without 1p19q LOH), Type 3 (IDH wild type) patients. Using the LASSO Cox regression model, we built a classifier based on gene mutations, CNVs (over 5% of cases), sex, age, pathology, WHO grade and operation type (gross total resection or not). We validated the accuracy of this classifier in terms of prediction of OS in the independent group of 269 patients. Results Using the LASSO model, the patients in Type 1 and 3 were grouped into low- and high-risk groups, whereas Type 2 was not because no significant risk factors were extracted for Type 2 patients. Combining high-risk Type 1 and Type 2, between which OS was not significantly different, the entire training set was divided into 4 groups showing significantly different OS, low- and intermediate-risk groups and high- and very high-risk groups, respectively. In the training set, 5-year OS for low-, intermediate-, high-, and very high-risk groups was 100%, 82%, 49%, and 0%, which were 94%, 70%, 43%, and 14%, in the validation set, respectively. The performance of the new model was evaluated by receiver operator characteristic (ROC) analysis, which showed significantly higher accuracy compared to other models based solely on clinical/histological parameters, including pathology, WHO grade and Karnofsky performance status (KPS). Conclusion We established a new classifier based both on genetic and clinical parameters, which provides a reliable tool for predicting OS in LGGs patients and should be useful to guide therapy. Citation Format: Kosuke Aoki, Hiromichi Suzuki, Hideo Nakamura, Masahiro Mizoguchi, Tetsuya Abe, Satoru Miyano, Ichiro Takeuchi, Toshihiko Wakabayashi, Seishi Ogawa, Atsushi Natsume. Prognostic model of lower grade gliomas. [abstract]. In: Proceedings of the 106th Annual Meeting of the American Association for Cancer Research; 2015 Apr 18-22; Philadelphia, PA. Philadelphia (PA): AACR; Cancer Res 2015;75(15 Suppl):Abstract nr 4791. doi:10.1158/1538-7445.AM2015-4791",2015,Cancer Research
"Education, Certification, and the Earnings of Industrial Accountants","Utilizing a model of the relationship between skill accumulation and earnings known as human capital theory, we analyzed the incremental earnings associated with various educational and professional credential for a sample of members oftheNationalAssociation of Accountants (NM). Data were collected by means of a questionnaire survey ofNM members and were analyzed utilizing a multiple regression technique. Earnings is regressed on various education and certification variables. Employment characteristics and personal characteristics of the respondents are included as control variables. Our analysis documents positive earnings increments for the bachelors and MBA degrees and the CPA certificate. For the MBA and CPA, these returns are concentrated in the middle and later stages of accountants' careers. Also, we examine the variation in these returns across different subgroups of our sample. Individuals in finance positions have more responsibility and are more upwardly mobile than those in traditional accounting jobs. Prior experience in public accounting is found to be a partial substitute for the above credentials. Finally, the credentials generated more consistent returns in smaller firms. Observations have been made in the accounting literature about the relative 1dvantages to accountants of acquiring various educational degrees and/or profes;ional certifications. Some of these observations are based on analyses of the 'Department of Accounting and Department of Economics respectively, The University or [)avton. 300 Colle!!e Park, Dayton Ohio 45469-2250. difference in average earnings between those who have a given level of education or a given certificate versus those who do not. Such analyses involve a simple comparison of mean earnings disaggregated by one or two variables such as age, race, or sex (for example, see Serocke [1985] and Arbital [1986]). These analyzes do not take into account other variables that may explain the earnings differentials. For example, an earnings differential between accountants with a specific educational degree versus those lacking that degree may be due to a difference in the average years of work experience between the two groups rather than the degree itself. The relationships between educational credentials, professional certifications, and earnings in the accounting profession are of importance to undergraduate accounting students. After earning baccalaureate degrees, students must decide whether to prepare for professional exams and/or to enter graduate school. Examination of the inter-relationships between degree,certifications, and earnings can assist students in making these decisions in an informed and rational manner.",1990,
"Prognostic Impact of Genetic Subgroups and Development of Gene Classifiers for Response, PFS and OS In Multiple Myeloma Patients Treated with Bortezomib or Conventional Agents In HOVON65/GMMG-HD4 Trial","Abstract 445 Background. In newly diagnosed myeloma patients, bortezomib treatment induces high rates of complete response (CR) and very good partial response (VGPR). Recently, we published the clustering of gene expression profiles in 320 MM patients, who were included in a large prospective, randomized, phase III transplantation trial with bortezomib (PAD) versus conventional vincristine (VAD) based induction treatment (HOVON65/GMMG-HD4). We identified 12 distinct subgroups CD-1, CD-2, MF, MS, PR, HY, LB, Myeloid, including three novel defined subgroups NFÎºB, CTA, and PRL3 and a subgroup with no clear gene expression profile (NP). Aim. To look at the prognostic impact of these 12 clusters in the trial and group clusters together into a high risk (HR) and low risk (LR) group in the different treatment arms. Furthermore, to define a high risk signature to identify the patients at increased risk of disease progression. Methods. Gene expression profiles of myeloma cells obtained at diagnosis of 320 HOVON65/GMMG-HD4 patients were available. Response, progression free survival (PFS) and overall survival (OS) data were available for the first 628 patients, resulting in analysis of gene expression in relation to prognosis in 229 patients. The prognostic impact of the genetic subgroups separately and grouped into high and low risk were evaluated using Kaplan Meier and Cox regression analysis using exhaustive search (R). For the high risk gene signature the HOVON65 gene expression data was used as training set with PFS as outcome measure. Two independent myeloma datasets with survival data were used as an external validation, UAMS (GSE2658) and APEX (GSE9782)). The signature was generated by a Cox proportional hazard model in combination with LASSO (Least Absolute Shrinkages and Selection Operator) for simultaneous parameter estimation and variable selection using the R package glmnet. ISS stage was implemented by adjusting the individual covariant penalization factors of the LASSO. Results. The highest CR+nCR rates were found in the PRL3 and NP clusters, i.e. 78% and 86%, respectively (VAD), and 100% (PAD). The lowest CR+nCR rate was 17% in the CD1 cluster (PAD) and 0% in the CD2, MF and PR clusters (VAD). Based on the impact of clusters on PFS and OS in the VAD arm, the MS, MF, PR and CTA clusters were included into a High Risk (HR) group. This HR group showed a median PFS of 13 months and OS of 21 months vs. the Low Risk (LR) group consisting of the remainder of clusters with a median PFS of 31 months and a median OS not reached (P Conclusion. Distinctive gene expression clusters affect prognosis, which differ depending on treatment. In the conventional treatment arm (VAD), MS, MF, PR and CTA clusters confer a worse prognosis while with bortezomib based treatment, only the PR cluster affects prognosis negatively. These HR groups remain independent poor prognostic indicators. Based on the HOVON65/GMMG-HD4 study population, a high-risk signature was generated with strong and highly significant predicting ability in two independent data sets. Disclosures: Mulligan:Millenium: Employment. Goldschmidt:Celgene: Membership on an entity9s Board of Directors or advisory committees; Ortho Biotech: Membership on entity9s Board of Directors or advisory committees; Ortho Biotech: Research Funding; Celgene: Research Funding; Chugai Pharma: Research Funding; Amgen: Research Funding. Sonneveld:Ortho Biotech: Research Funding; International Myeloma Foundation: Research Funding; Ortho Biotech: Consultancy.",2010,Blood
Graph-guided joint prediction of class label and clinical scores for the Alzheimerâ€™s disease,"AbstractAccurate diagnosis of Alzheimerâ€™s disease and its prodromal stage, i.e., mild cognitive impairment, is very important for early treatment. Over the last decade, various machine learning methods have been proposed to predict disease status and clinical scores from brain images. 
It is worth noting that many features extracted from brain images are correlated significantly. In this case, feature selection combined with the additional correlation information among features can effectively improve classification/regression performance. Typically, the correlation information among features can be modeled by the connectivity of an undirected graph, where each node represents one feature and each edge indicates that the two involved features are correlated significantly. In this paper, we propose a new graph-guided multi-task learning method incorporating this undirected graph information to predict multiple response variables (i.e., class label and clinical scores) jointly. Specifically, based on the sparse undirected feature graph, we utilize a new latent group Lasso penalty to encourage the correlated features to be selected together. Furthermore, this new penalty also encourages the intrinsic correlated tasks to share a common feature subset. To validate our method, we have performed many numerical studies using simulated datasets and the Alzheimerâ€™s Disease Neuroimaging Initiative dataset. Compared with the other methods, our proposed method has very promising performance.",2015,Brain Structure and Function
Hepatocellular carcinoma: radiomics nomogram on gadoxetic acid-enhanced MR imaging for early postoperative recurrence prediction,"BackgroundThis study was performed to prospectively develop and validate a radiomics nomogram for predicting postoperative early recurrence (â‰¤1â€‰year) of hepatocellular carcinoma (HCC) using whole-lesion radiomics features on preoperative gadoxetic acid-enhanced magnetic resonance (MR) images.MethodsIn total, 155 patients (training cohort: nâ€‰=â€‰108; validation cohort: nâ€‰=â€‰47) with surgically confirmed HCC were enrolled in this IRB-approved prospective study. Three-dimensional whole-lesion regions of interest were manually delineated along the tumour margins on multi-sequence MR images. Radiomics features were generated and selected to build a radiomics score using the least absolute shrinkage and selection operator (LASSO) method. Clinical characteristics and qualitative imaging features were identified by two independent radiologists and combined to establish a clinical-radiological nomogram. A radiomics nomogram comprising the radiomics score and clinical-radiological risk factors was constructed based on multivariable logistic regression analysis. Diagnostic performance and clinical usefulness were measured by receiver operation characteristic (ROC) and decision curves.ResultsIn total, 14 radiomics features were selected to construct the radiomics score. For the clinical-radiological nomogram, the alpha-fetoprotein (AFP) level, gross vascular invasion and non-smooth tumour margin were included. The radiomics nomogram integrating the radiomics score with clinical-radiological risk factors showed better discriminative performance (AUCâ€‰=â€‰0.844, 95%CI, 0.769 to 0.919) than the clinical-radiological nomogram (AUCâ€‰=â€‰0.796, 95%CI, 0.712 to 0.881; Pâ€‰=â€‰0.045), with increased clinical usefulness confirmed using a decision curve analysis.ConclusionsIncorporating multiple predictive factors, the radiomics nomogram demonstrated great potential in the preoperative prediction of early HCC recurrence after surgery.",2019,Cancer Imaging
Regression shrinkage and selection via the lasso: a retrospective,Summary. In the paper I give a brief review of the basic idea and some history and then discuss some developments since the original paper on regression shrinkage and selection via the lasso.,2011,Journal of The Royal Statistical Society Series B-statistical Methodology
A short note on model selection by LASSO methods in a change-point model,"In Ciuperca (2012) (Ciuperca. Model selection by LASSO methods in a change-point model, Stat. Papers, 2012; (in press)), the author considered a linear regression model with multiple change-points occurring at unknown times. In particular, the author studied the asymptotic properties of the LASSO-type and of the adaptive LASSO estimators. While the established results seem interesting, we point out some major errors in proof of the most important result of the quoted paper. Further, we present a corrected result and proof.",2014,arXiv: Methodology
Using machine learning to model problematic smartphone use severity: The significant role of fear of missing out.,"We examined a model of psychopathology variables, age and sex as correlates of problematic smartphone use (PSU) severity using supervised machine learning in a sample of Chinese undergraduate students. A sample of 1097 participants completed measures querying demographics, and psychological measures of PSU, depression and anxiety symptoms, fear of missing out (FOMO), and rumination. We used several different machine learning algorithms to train our statistical model of age, sex and the psychological variables in modeling PSU severity, trained using many simulated replications on a random subset of participants, and externally tested on the remaining subset of participants. Shrinkage algorithms (lasso, ridge, and elastic net regression) performing slightly but statistically better than other algorithms. Results from the training subset generalized to the test subset, without substantial worsening of fit using traditional fit indices. FOMO had the largest relative contribution in modeling PSU severity when adjusting for other covariates in the model. Results emphasize the significance of FOMO to the construct of PSU.",2019,Addictive behaviors
On the Practice of Rescaling Covariates,"Whether doing parametric or nonparametric regression with shrinkage, thresholding, penalized likelihood, Bayesian posterior estimators (e.g., ""ridge regression, lasso, principal component regression, waveshrink"" or ""Markov random field""), it is common practice to rescale covariates by dividing by their respective standard errors Ï. The stated goal of this operation is to provide unitless covariates to compare like with like, especially when penalized likelihood or prior distributions are used. We contend that this vision is too simplistic. Instead, we propose to take into account a more essential component of the structure of the regression matrix by rescaling the covariates based on the diagonal elements of the covariance matrix Î£ of the maximum-likelihood estimator. We illustrate the differences between the standard Ï- and proposed Î£-rescalings with various estimators and data sets. Copyright (c) 2008 The Author. Journal compilation (c) 2008 International Statistical Institute.",2008,International Statistical Review
Feature selection for spatial point processes,"Recent applications such as forestry datasets involve the observations of spatial point pattern data combined with the observation of many spatial covariates. We consider in this thesis the problem of estimating a parametric form of the intensity function in such a context. This thesis develops feature selection procedures and gives some guarantees on their validity. In particular, we propose two different feature selection approaches: the lasso-type methods and the Dantzig selector-type procedures. For the methods considering lasso-type techniques, we derive asymptotic properties of the estimates obtained from estimating functions derived from Poisson and logistic regression likelihoods penalized by a large class of penalties. We prove that the estimates obtained from such procedures satisfy consistency, sparsity, and asymptotic normality. For the Dantzig selector part, we develop a modified version of the Dantzig selector, which we call the adaptive linearized Dantzig selector (ALDS), to obtain the intensity estimates. More precisely, the ALDS estimates are defined as the solution to an optimization problem which minimizes the sum of coefficients of the estimates subject to linear approximation of the score vector as a constraint. We find that the estimates obtained from such methods have asymptotic properties similar to the ones proposed previously using an adaptive lasso regularization term. We investigate the computational aspects of the methods developped using either lasso-type procedures or the Dantzig selector-type approaches. We make links between spatial point processes intensity estimation and generalized linear models (GLMs), so we only have to deal with feature selection procedures for GLMs. Thus, easier computational procedures are implemented and computationally fast algorithm are proposed. Simulation experiments are conducted to highlight the finite sample performances of the estimates from each of two proposed approaches. Finally, our methods are applied to model the spatial locations a species of tree in the forest observed with a large number of environmental factors.",2017,
Pruning the Boosting Ensemble of Decision Trees,"We propose to use variable selection methods based on penalized regression for pruning decision tree ensembles. Pruning methods based on LASSO and SCAD are compared with the cluster pruning method. Comparative studies are performed on some artificial datasets and real datasets. According to the results of comparative studies, the proposed methods based on penalized regression reduce the size of boosting ensembles without decreasing accuracy significantly and have better performance than the cluster pruning method. In terms of classification noise, the proposed pruning methods can mitigate the weakness of AdaBoost to some degree.",2006,Communications for Statistical Applications and Methods
Simultaneous Co-clustering and Learning with Dimensionality Reduction,"In this project, we are interested in solving predictions problems involving dyadic data. The â€œNetflix problemâ€, where data may be organized into a matrix with the rows representing users, the columns representing movies and (some of) the matrix elements containing ratings, is an example of a setting where the data is naturally dyadic. The prediction problem here is essentially to fill in the missing entries of the matrix with predicted ratings. Recently, a technique called simultaneous co-clustering and learning (SCOAL) was proposed that was demonstrated to be effective in solving such dyadic prediction problems. SCOAL is essentially a divide-and-conquer approach that seeks to form clusters of feature vectors and build a separate local model for each one of these clusters. Such an approach inherently suffers from the risk of overfitting, especially when the number of clusters is large. Thus, to alleviate the effects of over-fitting, while at the same time, benefitting from the use of multiple models, we propose the use of dimensionality reduction in conjunction with SCOAL. We investigate two dimensionality reduction techniques: principle components regression and the least absolute shrinkage/selection operator (LASSO). We show that both these techniques provide significant reduction in test error (up to 10% in some settings) when linear predictive models are employed within each cluster.",2012,
Attribute Efficient Linear Regression with Data-Dependent Sampling,"In this paper we analyze a budgeted learning setting, in which the learner can only choose and observe a small subset of the attributes of each training example. We develop efficient algorithms for ridge and lasso linear regression, which utilize the geometry of the data by a novel data-dependent sampling scheme. When the learner has prior knowledge on the second moments of the attributes, the optimal sampling probabilities can be calculated precisely, and result in data-dependent improvements factors for the excess risk over the state-of-the-art that may be as large as $O(\sqrt{d})$, where $d$ is the problem's dimension. Moreover, under reasonable assumptions our algorithms can use less attributes than full-information algorithms, which is the main concern in budgeted learning settings. To the best of our knowledge, these are the first algorithms able to do so in our setting. Where no such prior knowledge is available, we develop a simple estimation technique that given a sufficient amount of training examples, achieves similar improvements. We complement our theoretical analysis with experiments on several data sets which support our claims.",2014,ArXiv
Structure identification and model selection in geographically weighted quantile regression models,"Abstract Geographical weighted quantile regression (GWQR) is an important tool for exploring the spatial non-stationarity of the regression relationship and providing the entire description of the response distribution, which is useful in practice because of its robustness against outliers and flexibility in dealing with non-normal distributions. This paper proposes the GWQlasso method for structure identification and variable selection in GWQR models. The proposed method combines the local-linear estimation of the GWQR model and the adaptive group lasso, which can simultaneously identify spatially varying coefficients, non-zero constant coefficients and zero coefficients. The selection consistency and the oracle property of the proposal are studied. Moreover, the derived algorithm for the GWQlasso method and the selection of the tuning parameter by the BIC criterion are established. Simulations and real examples are used to illustrate the method.",2018,spatial statistics
Nomogram for predicting severe morbidity after pheochromocytoma surgery,"PURPOSE
Although resection is the primary treatment strategy for pheochromocytoma, surgery is associated with a high risk of morbidity. At present, there is no nomogram for prediction of severe morbidity after pheochromocytoma surgery, thus the aim of the present study was to develop and validate a nomogram for prediction of severe morbidity after pheochromocytoma surgery.


METHODS
The development cohort consisted of 262 patients who underwent unilateral laparoscopic or open pheochromocytoma surgery at our center between January 1, 2007 and December 31, 2016. The patients' clinicopathological characters were recorded. The least absolute shrinkage and selection operator (LASSO) binary logistic regression model was used for data dimension reduction and feature selection, then multivariable logistic regression analysis was used to develop the predictive model. An independent validation cohort consisted of 128 consecutive patients from January 1, 2017 and December 31, 2018. The performance of the predictive model was assessed in regards to discrimination, calibration, and clinical usefulness.


RESULTS
Predictors of this model included sex, body mass index, coronary heart disease, arrhythmia, tumor size, intraoperative hemodynamic instability, and surgical duration. For the validation cohort, the model showed good discrimination with an AUROC of 0.818 (95% CI, 0.745, 0.891) and good calibration (Unreliability test, p=0.440). Decision curve analysis demonstrated that the model was also clinically useful.


CONCLUSIONS
A nomogram was developed to facilitate the individualized prediction of severe morbidity after pheochromocytoma surgery and may help to improve the perioperative strategy and treatment outcome.",2020,Endocrine Connections
Compound Identification Using Penalized Linear Regression on Metabolomics.,"Compound identification is often achieved by matching the experimental mass spectra to the mass spectra stored in a reference library based on mass spectral similarity. Because the number of compounds in the reference library is much larger than the range of mass-to-charge ratio (m/z) values so that the data become high dimensional data suffering from singularity. For this reason, penalized linear regressions such as ridge regression and the lasso are used instead of the ordinary least squares regression. Furthermore, two-step approaches using the dot product and Pearson's correlation along with the penalized linear regression are proposed in this study.",2016,Journal of modern applied statistical methods : JMASM
Evaluation of the Design Effect for Optimizing the Model Discrimination Strength to Detect Non-Zero Interactions in Factorial Experiments,"Author(s): Wales, Brandon Allen | Advisor(s): Ghosh, Subir | Abstract: It is important for a design to be able to discriminate between all pairwise model comparisons when searching for non-zero interactions. If a design does not have this capability, finding non-zero interactions may not be possible. We propose a procedure at analyzing the model discrimination strength when searching for non-zero interactions by understanding the pairwise differenced error sum of squares for a given design. This is done by calculating eigenvalues and eigenvectors of differenced projection matrices which are completely dependent on the design and not the observed values for the response variable. Using this procedure, we have compared two balanced designs and two Placket-Burman (1946) designs which both have n=12 runs, m=5 factors each with 2 levels, and searching over (5Â¦2)=10 two-factor interaction effects. Additionally, ridge regression and LASSO were used for a model selection approach which both use tuning parameters to calculate the model parameter estimates. All three model selection approaches were used on all four example designs to compare the performance when searching for non-zero interactions.",2017,
"Oracle Properties, Bias Correction, and Inference of the Adaptive Lasso for Time Series Extremum Estimators","We derive new theoretical results on the properties of the adaptive least absolute shrinkage and selection operator (adaptive lasso) for time series regression models. In particular we investigate the question of how to conduct finite sample inference on the parameters given an adaptive lasso model for some fixed value of the shrinkage parameter. Central in this study is the test of the hypothesis that a given adaptive lasso parameter equals zero, which therefore tests for a false positive. To this end we construct a simple (conservative) testing procedure and show, theoretically and empirically through extensive Monte Carlo simulations, that the adaptive lasso combines efficient parameter estimation, variable selection, and valid finite sample inference in one step. Moreover, we analytically derive a bias correction factor that is able to significantly improve the empirical coverage of the test on the active variables. Finally, we apply the introduced testing procedure to investigate the relation between the short rate dynamics and the economy, thereby providing a statistical foundation (from a model choice perspective) to the classic Taylor rule monetary policy model.",2015,
Computational Methods of Feature Selection,"PREFACE Introduction and Background Less Is More Huan Liu and Hiroshi Motoda Background and Basics Supervised, Unsupervised, and Semi-Supervised Feature Selection Key Contributions and Organization of the Book Looking Ahead Unsupervised Feature Selection Jennifer G. Dy Introduction Clustering Feature Selection Feature Selection for Unlabeled Data Local Approaches Summary Randomized Feature Selection David J. Stracuzzi Introduction Types of Randomizations Randomized Complexity Classes Applying Randomization to Feature Selection The Role of Heuristics Examples of Randomized Selection Algorithms Issues in Randomization Summary Causal Feature Selection Isabelle Guyon, Constantin Aliferis, and Andre Elisseeff Introduction Classical ""Non-Causal"" Feature Selection The Concept of Causality Feature Relevance in Bayesian Networks Causal Discovery Algorithms Examples of Applications Summary, Conclusions, and Open Problems Extending Feature Selection Active Learning of Feature Relevance Emanuele Olivetti, Sriharsha Veeramachaneni, and Paolo Avesani Introduction Active Sampling for Feature Relevance Estimation Derivation of the Sampling Benefit Function Implementation of the Active Sampling Algorithm Experiments Conclusions and Future Work A Study of Feature Extraction Techniques Based on Decision Border Estimate Claudia Diamantini and Domenico Potena Introduction Feature Extraction Based on Decision Boundary Generalities about Labeled Vector Quantizers Feature Extraction Based on Vector Quantizers Experiments Conclusions Ensemble-Based Variable Selection Using Independent Probes Eugene Tuv, Alexander Borisov, and Kari Torkkola Introduction Tree Ensemble Methods in Feature Ranking The Algorithm: Ensemble-Based Ranking against Independent Probes Experiments Discussion Efficient Incremental-Ranked Feature Selection in Massive Data Roberto Ruiz, Jesus S. Aguilar-Ruiz, and Jose C. Riquelme Introduction Related Work Preliminary Concepts Incremental Performance over Ranking Experimental Results Conclusions Weighting and Local Methods Non-Myopic Feature Quality Evaluation with (R)ReliefF Igor Kononenko and Marko Robnik Sikonja Introduction From Impurity to Relief ReliefF for Classification and RReliefF for Regression Extensions Interpretation Implementation Issues Applications Conclusion Weighting Method for Feature Selection in k-Means Joshua Zhexue Huang, Jun Xu, Michael Ng, and Yunming Ye Introduction Feature Weighting in k-Means W-k-Means Clustering Algorithm Feature Selection Subspace Clustering with k-Means Text Clustering Related Work Discussions Local Feature Selection for Classification Carlotta Domeniconi and Dimitrios Gunopulos Introduction The Curse of Dimensionality Adaptive Metric Techniques Large Margin nearest Neighbor Classifiers Experimental Comparisons Conclusions Feature Weighting through Local Learning Yijun Sun Introduction Mathematical Interpretation of Relief Iterative Relief Algorithm Extension to Multiclass Problems Online Learning Computational Complexity Experiments Conclusion Text Classification and Clustering Feature Selection for Text Classification George Forman Introduction Text Feature Generators Feature Filtering for Classification Practical and Scalable Computation A Case Study Conclusion and Future Work A Bayesian Feature Selection Score Based on Naive Bayes Models Susana Eyheramendy and David Madigan Introduction Feature Selection Scores Classification Algorithms Experimental Settings and Results Conclusion Pairwise Constraints-Guided Dimensionality Reduction Wei Tang and Shi Zhong Introduction Pairwise Constraints-Guided Feature Projection Pairwise Constraints-Guided Co-Clustering Experimental Studies Conclusion and Future Work Aggressive Feature Selection by Feature Ranking Masoud Makrehchi and Mohamed S. Kamel Introduction Feature Selection by Feature Ranking Proposed Approach to Reducing Term Redundancy Experimental Results Summary Feature Selection in Bioinformatics Feature Selection for Genomic Data Analysis Lei Yu Introduction Redundancy-Based Feature Selection Empirical Study Summary A Feature Generation Algorithm with Applications to Biological Sequence Classification Rezarta Islamaj Dogan, Lise Getoor, and W. John Wilbur Introduction Splice-Site Prediction Feature Generation Algorithm Experiments and Discussion Conclusions An Ensemble Method for Identifying Robust Features for Biomarker Discovery Diana Chan, Susan M. Bridges, and Shane C. Burgess Introduction Biomarker Discovery from Proteome Profiles Challenges of Biomarker Identification Ensemble Method for Feature Selection Feature Selection Ensemble Results and Discussion Conclusion Model Building and Feature Selection with Genomic Data Hui Zou and Trevor Hastie Introduction Ridge Regression, Lasso, and Bridge Drawbacks of the Lasso The Elastic Net The Elastic-Net Penalized SVM Sparse Eigen-Genes Summary INDEX",2007,
Extension to mixed models of the Supervised Component-based Generalised Linear Regression,"We address the component-based regularisation of a multivariate Generalized Linear Mixed Model (GLMM). A set of random responses Y is modelled by a GLMM, using a set X of explanatory variables, a set T of additional covariates, and random effects used to introduce the dependence between statistical units. Variables in X are assumed many and redundant, so that regression demands regularisation. By contrast, variables in T are assumed few and selected so as to require no regularisation. Regularisation is performed building an appropriate number of orthogonal components that both contribute to model Y and capture relevant structural information in X. To estimate the model, we propose to maximise a criterion specific to the Supervised Component-based Generalised Linear Regression (SCGLR) within an adaptation of Schall's algorithm. This extension of SCGLR is tested on both simulated and real data, and compared to Ridge- and Lasso-based regularisations.",2019,arXiv: Methodology
A Minimax Framework for Classification with Applications to Images and High Dimensional Data,"This paper introduces a minimax framework for multiclass classification, which is applicable to general data including, in particular, imagery and other types of high-dimensional data. The framework consists of estimating a representation model that minimizes the fitting errors under a class of distortions of interest to an application, and deriving subsequently categorical information based on the estimated model. A variety of commonly used regression models, including lasso, elastic net and ridge regression, can be regarded as special cases that correspond to specific classes of distortions. Optimal decision rules are derived for this classification framework. By using kernel techniques the framework can account for nonlinearity in the input space. To demonstrate the power of the framework we consider a class of signal-dependent distortions and build a new family of classifiers as new special cases. This family of new methods-minimax classification with generalized multiplicative distortions-often outperforms the state-of-the-art classification methods such as the support vector machine in accuracy. Extensive experimental results on images, gene expressions and other types of data verify the effectiveness of the proposed framework.",2014,IEEE Transactions on Pattern Analysis and Machine Intelligence
SNP Selection in Genome-Wide and Candidate Gene Studies via Penalized Logistic Regression,"Penalized regression methods offer an attractive alternative to single marker testing in genetic association analysis. Penalized regression methods shrink down to zero the coefficient of markers that have little apparent effect on the trait of interest, resulting in a parsimonious subset of what we hope are true pertinent predictors. Here we explore the performance of penalization in selecting SNPs as predictors in genetic association studies. The strength of the penalty can be chosen either to select a good predictive model (via methods such as computationally expensive cross validation), through maximum likelihood-based model selection criterion (such as the BIC), or to select a model that controls for type I error, as done here. We have investigated the performance of several penalized logistic regression approaches, simulating data under a variety of disease locus effect size and linkage disequilibrium patterns. We compared several penalties, including the elastic net, ridge, Lasso, MCP and the normal-exponential-Î³ shrinkage prior implemented in the hyperlasso software, to standard single locus analysis and simple forward stepwise regression. We examined how markers enter the model as penalties and P-value thresholds are varied, and report the sensitivity and specificity of each of the methods. Results show that penalized methods outperform single marker analysis, with the main difference being that penalized methods allow the simultaneous inclusion of a number of markers, and generally do not allow correlated variables to enter the model, producing a sparse model in which most of the identified explanatory markers are accounted for.",2010,Genetic Epidemiology
Regression and sparse regression methods for viscosity estimation of acid milk from it's SLS features,"Statistical solutions find wide spread use in food and medicine quality control. We investigate the effect of different regression and sparse regression methods for a viscosity estimation problem using the spectro-temporal features from new Sub-Surface Laser Scattering (SLS) vision system. From this investigation, we propose the optimal solution for regression estimation in case of noisy and inconsistent optical measurements, which is the case in many practical measurement systems. The principal component regression (PLS), partial least squares (PCR) and least angle regression (LAR) methods are compared with sparse LAR, lasso and Elastic Net (EN) sparse regression methods. Due to the inconsistent measurement condition, Locally Weighted Scatter plot Smoothing (Loess) has been employed to alleviate the undesired variation in the estimated viscosity. The experimental results of applying different methods show that, the sparse regression lasso outperforms other methods. In addition, the use of local smoothing has improved the results considerably for all regression methods. Due to the sparsity of lasso, this result would assist to design a simpler vision system with less spectral bands.",2012,"2012 19th International Conference on Systems, Signals and Image Processing (IWSSIP)"
Streaming constrained binary logistic regression with online standardized data. Application to scoring heart failure,"We study a stochastic gradient algorithm for performing online a constrained binary logistic regression in the case of streaming or massive data. Assuming that observed data are realizations of a random vector, these data are standardized online in particular to avoid a numerical explosion or when a shrinkage method such as LASSO is used. We prove the almost sure convergence of a variable step-size constrained stochastic gradient process with averaging when a varying number of new data is introduced at each step. 24 stochastic approximation processes are compared on real or simulated datasets, classical processes with raw data, processes with online standardized data, with or without averaging and with variable or piecewise constant step-sizes. The best results are obtained by processes with online standardized data, with averaging and piecewise constant step-sizes. This can be used to update online an event rate score in heart failure patients.",2019,
Biclustering via Mixtures of Regression Models,Biclustering of observations and the variables is of interest in many scientific disciplines; In a single set of data matrix it is handled through the singular value decomposition. Here we deal with two sets of variables: Response and predictor sets. We model the joint relationship via regression models and then apply SVD on the coefficient matrix. The sparseness condition is introduced via Group Lasso; the approach discussed here is quite general and is illustrated with an example from Finance.,2019,
Simultaneous variable selection and outlier detection using LASSO with applications to aircraft landing data analysis,"OF THE DISSERTATION Simultaneous Variable Selection and Outlier Detection Using LASSO with Applications to Aircraft Landing Data Analysis by Wei Li Dissertation Director: Regina Y. Liu, Minge Xie, and Cun-Hui Zhang We propose a LASSO-type penalized regression method for simultaneous variable selection and outlier detection in high dimensional linear regression. We apply a mean-shift model to incorporate the coefficients associated with the potential outliers by expressing them as different intercept terms. The sparsity assumption is imposed on both X-covariates and the outlier indicator variables. With suitable penalty factors between X-covaraites and the outlier indicators, we show that the proposed method selects a model of the correct order of dimensionality, under the sparse Riesz condition on the correlation of design variables and a joint sparse Reisz condition on the augmented design matrix. We also show that the estimation/prediction of the selected model can be controlled at a level determined by the sizes of the true model, the outliers and the thresholding level. Moreover, the estimation has a positive breakdown point when both the dimension p and the sample size n tend to infinity, and p >> n. We also provide a generalized version for the estimator by adjusting the penalty weight factor. Finally, we apply the proposed method to analyze an aircraft landing performance data set, for identifying the precursors for undesirable landing performance and reducing the risk of runway overruns.",2012,
Regularized brain reading with shrinkage and smoothing,"Functional neuroimaging measures how the brain responds to complex stimuli. However, sample sizes are modest, noise is substantial, and stimuli are high dimensional. Hence, direct estimates are inherently imprecise and call for regularization. We compare a suite of approaches which regularize via shrinkage: ridge regression, the elastic net (a generalization of ridge regression and the lasso), and a hierarchical Bayesian model based on small area estimation (SAE). We contrast regularization with spatial smoothing and combinations of smoothing and shrinkage. All methods are tested on functional magnetic resonance imaging (fMRI) data from multiple subjects participating in two different experiments related to reading, for both predicting neural response to stimuli and decoding stimuli from responses. Interestingly, when the regularization parameters are chosen by cross-validation independently for every voxel, low/high regularization is chosen in voxels where the classification accuracy is high/low, indicating that the regularization intensity is a good tool for identification of relevant voxels for the cognitive task. Surprisingly, all the regularization methods work about equally well, suggesting that beating basic smoothing and shrinkage will take not only clever methods, but also careful modeling.",2015,The Annals of Applied Statistics
On the application of machine learning techniques to derive seismic fragility curves,"Abstract Deriving the fragility curves is a key step in seismic risk assessment within the performance-based earthquake engineering framework. The objective of this study is to implement machine learning tools (i.e., classification-based tools in particular) for predicting the structural responses and the fragility curves. In this regard, ten different classification-based methods are explored: logistic regression, lasso regression, support vector machine, Naive Bayes, decision tree, random forest, linear and quadratic discriminant analyses, neural networks, and K-nearest neighbors with the structural responses resulted from the multiple strip analyses. In addition, this study examines the impact of class imbalance in training dataset, which is typical among data of structural responses, when developing classification-based models for predicting structural responses. The statistical results using the implemented dataset demonstrate that among applied methods, random forest and quadratic discriminant analysis are, respectively, preferable with the imbalanced and balanced datasets since they show the highest efficiency in predicting the structural responses. Moreover, a detailed procedure is presented on how to derive the fragility curves based on the classification-based tools. Finally, the sensitivity of the applied machine learning methods to the size of employed dataset is investigated. The results explain that logistic regression, lasso regression, and Naive Bayes are not sensitive to the size of dataset (i.e., the number of performed time history analyses); while the performance of discriminant analysis significantly depends on the size of applied dataset.",2019,Computers & Structures
Peut-on attraper les utilisateurs de VÃ©lo'v au Lasso ?,"A statistical analysis of the trips made using Veloâ€™v, the community shared bicycle system in Lyon city, is conducted in attempt to relate, through a statistical regression model, social, demographic and economical data of the various neighborhoods of the city with the actual trips made from and to the different parts of the city. For that, parcimonious linear regression methods with positivity constraints are studied. As the lasso solution is found to be not fully adequate for the present problem when interpretation is attempted, a further specificy of he data is introduced: real data on demand are truncated because of a constraint of capacity in bikes at the stands. The LARS algorithm is adapted to this situation of truncated observations. A study of the proposed new algorithm is conducted and it is found to perform well in controlled simulations, estimating well the linear model used to generate truncated observations. 1 Systeme Veloâ€™v : comment analyser lâ€™usage quâ€™en fait par la population ? Nous etudions le service Veloâ€™v de mise a disposition de velos en tant que transport public. Ce systeme automatise de location de velos est en service a Lyon depuis 2005. Sa dynamique de fonctionnement sâ€™apparente par de multiples aspects a celle dâ€™un systeme complexe, câ€™est-a-dire quâ€™au-dessus de comportements individuels simples, on obtient un comportement collectif complexe. Dans des premiers travaux [1, 2], nous avons formule des analyses globales de la dynamique temporelle du service et de la repartition spatiale des trajets. Le present travail est dâ€™etudier des methodes statistiques pour relier les donnees socioeconomiques de la ville (venant de lâ€™INSEE) avec lâ€™usage quâ€™il est fait des Veloâ€™V, quartier par quartier et dependant du moment de la journee. Pre-traitement et formalisation. Le probleme est restreint dans ce travail a la recherche dâ€™un modele suppose lineaire (a defaut dâ€™autres a priori) entre les flux de Veloâ€™v aux stations et les caracteristiques demographiques et socio-economiques du quartier entourant chaque station. Le premier point est de reconcilier les niveaux de description differents entre les donnees : les flux de Veloâ€™v sont connus a chaque station, tandis que les variables demographiques et socio-economique sont connues a lâ€™echelle des ilots (morceau de quartier) ou des IRIS (Ilots Regroupes pour lâ€™Inference Statistique, decoupes par lâ€™INSEE) a lâ€™echelle au-dessus. Les stations sont souvent sur ou pres des axes majeurs de circulations qui decoupent aussi la ville en IRIS : associer les stations a lâ€™IRIS le plus proche nâ€™est clairement pas une solution acceptable. Inspires alors par lâ€™utilisation des modes raster en geographie, il est logique de lisser les flux de Veloâ€™v et de les reaffecter aux IRIS par un noyau de lissage, tout en conservant le nombre total de trajets effectues. Pour faire cela, notons #v(k, t) un flux de Veloâ€™v a la station k âˆˆ S entre le temps t et t+âˆ† (avec typiquementâˆ† = 1h en accord avec [1]). On utilise un lissage spatial local de longueur typique R0 qui correspond a ce mode raster ou a proposer une hypothese sur un noyau de 0 50 100 150 200 0 20 40 60 80 Nombre de Veloâ€™v entre 8h et 9h",2011,
