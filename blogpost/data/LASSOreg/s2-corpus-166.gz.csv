title,abstract,year,journal
Liquid electrolyte informatics using an exhaustive search with linear regression.,"Exploring new liquid electrolyte materials is a fundamental target for developing new high-performance lithium-ion batteries. In contrast to solid materials, disordered liquid solution properties have been less studied by data-driven information techniques. Here, we examined the estimation accuracy and efficiency of three information techniques, multiple linear regression (MLR), least absolute shrinkage and selection operator (LASSO), and exhaustive search with linear regression (ES-LiR), by using coordination energy and melting point as test liquid properties. We then confirmed that ES-LiR gives the most accurate estimation among the techniques. We also found that ES-LiR can provide the relationship between the ""prediction accuracy"" and ""calculation cost"" of the properties via a weight diagram of descriptors. This technique makes it possible to choose the balance of the ""accuracy"" and ""cost"" when the search of a huge amount of new materials was carried out.",2018,Physical chemistry chemical physics : PCCP
High-dimensional regression and classification under a class of convex loss functions,"The weighted L1 penalty was used to revise the traditional Lasso in the linear regression model under quadratic loss. We make use of this penalty to investigate the highdimensional regression and classification under a wide class of convex loss functions. We show that for the dimension growing nearly exponentially with the sample size, the penalized estimator possesses the oracle property for suitable weights, and its induced classifier is shown to be consistent to the optimal Bayes rule. Moreover, we propose two methods, called componentwise regression (CR) and penalized componentwise regression (PCR), for estimating weights. Both theories and simulation studies provide supporting evidence for the advantage of PCR over CR in high-dimensional regression and classification. The effectiveness of the proposed method is illustrated using real data sets.",2013,Statistics and Its Interface
A note on adaptive Lp regularization,"In this paper, the adaptive Lp regularization is proposed for parameter estimation and variable selection. In particular, we focus on the (0 <; p <; 1) case when the adaptive Lp regularizer has a nonconvex penalty. Besides some traditional properties for penalized linear regression model, such as unbiasedness and sparsity, we have shown that the adaptive Lp regularization also enjoy the oracle property. A modified iterative algorithm is utilized to solve the adaptive Lp. By comparing with ordinary least square, adaptive lasso and Lp, the numerical results show that the adaptive Lp is more accurate and sparse.",2012,The 2012 International Joint Conference on Neural Networks (IJCNN)
Evaluating the HER-2 status of breast cancer using mammography radiomics features.,"PURPOSE
The aim of our study was to evaluate the HER-2 status in breast cancer patients using mammography (MG) radiomics features.


METHODS
A total of 306 Chinese female patients with invasive ductal carcinoma of no special type (IDC-NST) enrolled from January 2013 to July 2018 were divided into a training set (nâ€¯=â€¯244) and a testing set (nâ€¯=â€¯62). One hundred and eighty-six radiomics features were extracted from digital MG images based on the training set. The least absolute shrinkage and selection operator (LASSO) method was used to select the optimal predictive features for HER-2 status from the training set. Both support vector machine (SVM) and logistic regression models were employed based on the selected features. The area under the receiver operating characteristic (ROC) curves (AUCs) of the training set and testing set were used to evaluate the predictive performance of the models.


RESULTS
Compared with the SVM model, the performance of the logistic regression model using a combination of cranial caudal (CC) and mediolateral oblique (MLO) MG views was optimal. In the training set, the sensitivity, specificity, accuracy and area under the curve (AUC) values of the logistic regression model for evaluating HER-2 status based on quantitative radiomics features were 87.29%, 58.73%, 80.00% and 0.846 (95% confidence interval (CI), 0.800-0.887), respectively, and in the testing set, the values were 73.91%, 68.75%, 77.00% and 0.787 (95% CI, 0.673-0.885), respectively.


CONCLUSIONS
Radiomics features could be an efficient tool for the preoperative evaluation of HER-2 status in patients with breast cancer.",2019,European journal of radiology
"[Machine learning for predictive analyses in health: an example of an application to predict death in the elderly in SÃ£o Paulo, Brazil].","This study aims to present the stages related to the use of machine learning algorithms for predictive analyses in health. An application was performed in a database of elderly residents in the city of SÃ£o Paulo, Brazil, who participated in the Health, Well-Being, and Aging Study (SABE) (n = 2,808). The outcome variable was the occurrence of death within five years of the elder's entry into the study (n = 423), and the predictors were 37 variables related to the elder's demographic, socioeconomic, and health profile. The application was organized according to the following stages: division of data in training (70%) and testing (30%), pre-processing of the predictors, learning, and assessment of the models. The learning stage used 5 algorithms to adjust the models: logistic regression with and without penalization, neural networks, gradient boosted trees, and random forest. The algorithms' hyperparameters were optimized by 10-fold cross-validation to select those corresponding to the best models. For each algorithm, the best model was assessed in test data via area under the ROC curve (AUC) and related measures. All the models presented AUC ROC greater than 0.70. For the three models with the highest AUC ROC (neural networks and logistic regression with LASSO penalization and without penalization, respectively), quality measures of the predicted probability were also assessed. The expectation is that with the increased availability of data and trained human capital, it will be possible to develop predictive machine learning models with the potential to help health professionals make the best decisions.",2019,Cadernos de saude publica
EEG signal analysis using sparse approximations,"Electroencephalogram (EEG) is physiological signal generated in the brain. Electroencephalography is a method to record the electrical activity of the brain in order to detect the abnormalities of the brain. However, EEG also detects the signals which are not originated from the brain called artifacts. This paper deals with the analysis and extraction of EEG signals in sparse representation using sparse algorithms Orthogonal Matching Pursuit (OMP) and LASSO. OMP is an iterative greedy algorithm which replaces optimization problem in each step of Matching Pursuit(MP), an earlier algorithm for solving sparse approximation problems by least squares minimization. LASSO is an optimization technique which involves regression analysis concepts. Sparse approximations are used in practical applications like feature extraction, Denoising, Inpainting etc.",2017,"2017 Third International Conference on Biosignals, Images and Instrumentation (ICBSII)"
Chapter 4 Feature Selection in Feature Network Models : Finding Predictive Subsets of Features with the Positive Lasso 1,"A set of features is the basis for the network representation of proximity data achieved by Feature Network Models (FNM). Features are binary variables that characterize the objects in an experiment, with some measure of proximity as response variable. Sometimes features are provided by theory and play an important role in the construction of the experimental conditions. In some research settings, the features are not known a priori. This paper shows how to generate features in this situation and how to select an adequate subset of features that takes into account a good compromise between model fit and model complexity, using a new version of Least Angle Regression that restricts coefficients to be nonnegative, called the Positive Lasso. It will be shown that features can be generated efficiently with Gray codes that are naturally linked to the FNM. The model selection strategy makes use of the fact that FNM can be considered as a univariate multiple regression model. A simulation study shows that the proposed strategy leads to satisfactory results if the number of objects ! 22. If the number of objects is larger than 22, the number of features selected by our method exceeds the true number of features in some conditions.",2006,
Fast Sparse Group Lasso,"Sparse Group Lasso is a method of linear regression analysis that finds sparse parameters in terms of both feature groups and individual features. Block Coordinate Descent is a standard approach to obtain the parameters of Sparse Group Lasso, and iteratively updates the parameters for each parameter group. However, as an update of only one parameter group depends on all the parameter groups or data points, the computation cost is high when the number of the parameters or data points is large. This paper proposes a fast Block Coordinate Descent for Sparse Group Lasso. It efficiently skips the updates of the groups whose parameters must be zeros by using the parameters in one group. In addition, it preferentially updates parameters in a candidate group set, which contains groups whose parameters must not be zeros. Theoretically, our approach guarantees the same results as the original Block Coordinate Descent. Experiments show that our algorithm enhances the efficiency of the original algorithm without any loss of accuracy.",2019,
Identification of a six-gene signature predicting overall survival for hepatocellular carcinoma,"BackgroundHepatocellular carcinoma (HCC) remains a major challenge for public health worldwide. Considering the great heterogeneity of HCC, more accurate prognostic models are urgently needed. To identify a robust prognostic gene signature, we conduct this study.Materials and methodsLevel 3 mRNA expression profiles and clinicopathological data were obtained in The Cancer Genome Atlas Liver Hepatocellular Carcinoma (TCGA-LIHC). GSE14520 dataset from the gene expression omnibus (GEO) database was downloaded to further validate the results in TCGA. Differentially expressed mRNAs between HCC and normal tissue were investigated. Univariate Cox regression analysis and lasso Cox regression model were performed to identify and construct the prognostic gene signature. Time-dependent receiver operating characteristic (ROC), Kaplanâ€“Meier curve, multivariate Cox regression analysis, nomogram, and decision curve analysis (DCA) were used to assess the prognostic capacity of the six-gene signature. The prognostic value of the gene signature was further validated in independent GSE14520 cohort. Gene Set Enrichment Analyses (GSEA) was performed to further understand the underlying molecular mechanisms. The performance of the prognostic signature in differentiating between normal liver tissues and HCC were also investigated.ResultsA novel six-gene signature (including CSE1L, CSTB, MTHFR, DAGLA, MMP10, and GYS2) was established for HCC prognosis prediction. The ROC curve showed good performance in survival prediction in both the TCGA HCC cohort and the GSE14520 validation cohort. The six-gene signature could stratify patients into a high- and low-risk group which had significantly different survival. Cox regression analysis showed that the six-gene signature could independently predict OS. Nomogram including the six-gene signature was established and shown some clinical net benefit. Furthermore, GSEA revealed several significantly enriched oncological signatures and various metabolic process, which might help explain the underlying molecular mechanisms. Besides, the prognostic signature showed a strong ability for differentiating HCC from normal tissues.ConclusionsOur study established a novel six-gene signature and nomogram to predict overall survival of HCC, which may help in clinical decision making for individual treatment.",2019,Cancer Cell International
Hinging Hyperplanes for Time-Series Segmentation,"Division of a time series into segments is a common technique for time-series processing, and is known as segmentation. Segmentation is traditionally done by linear interpolation in order to guarantee the continuity of the reconstructed time series. The interpolation-based segmentation methods may perform poorly for data with a level of noise because interpolation is noise sensitive. To handle the problem, this paper establishes an explicit expression for segmentation from a compact representation for piecewise linear functions using hinging hyperplanes. This expression enables the use of regression to obtain a continuous reconstructed signal and, as a consequence, application of advanced techniques in segmentation. In this paper, a least squares support vector machine with lasso using a hinging feature map is given and analyzed, based on which a segmentation algorithm and its online version are established. Numerical experiments conducted on synthetic and real-world datasets demonstrate the advantages of our methods compared to existing segmentation algorithms.",2013,IEEE Transactions on Neural Networks and Learning Systems
The flare package for high dimensional linear regression and precision matrix estimation in R,"This paper describes an R package named flare, which implements a family of new high dimensional regression methods (LAD Lasso, SQRT Lasso, lq Lasso, and Dantzig selector) and their extensions to s...",2015,Journal of Machine Learning Research
RLS-Based Detection for Massive Spatial Modulation MIMO,"Most detection algorithms in spatial modulation (SM) are formulated as linear regression via the regularized least-squares (RLS) method. In this method, the transmit signal is estimated by minimizing the residual sum of squares penalized with some regularization. This paper studies the asymptotic performance of a generic RLS-based detection algorithm employed for recovery of SM signals. We derive analytically the asymptotic average mean squared error and the error rate for the class of bi-unitarily invariant channel matrices.The analytic results are employed to study the performance of SM detection via the box-LASSO. The analysis demonstrates that the performance characterization for i.i.d. Gaussian channel matrices is valid for matrices with non-Gaussian entries, as well. This justifies the partially approved conjecture given in [1]. The derivations further extend the former studies to scenarios with non-i.i.d. channel matrices. Numerical investigations validate the analysis, even for practical system dimensions.",2019,2019 IEEE International Symposium on Information Theory (ISIT)
4 A new algorithm for finding the lasso solution,"Adaptive ridge is a special form of ridge regression, balancing the quadratic penalization on each parameter of the model. This paper shows the equivalence between adaptive ridge and lasso (least absolute shrinkage and selection operator). This equivalence states that both procedures produce the same estimate. Least absolute shrinkage can thus be viewed as a particular quadratic penalization. From this observation, we derive an EM algorithm to compute the lasso solution. We finally present a series of applications of this type of algorithm in regression problems: kernel regression, additive modeling and neural net training.",1998,
A new approach to Pairs Trading : Using fundamental data to find optimal portfolios,"Since itsâ€™ invention at Morgan Stanley in 1987 pairs trading has grown to be one of the most common and most researched strategies for market neutral returns.The strategy identifies stocks, or other financial securities, that historically has co-moved and forms a trading pair. If the price relation is broken a short position is entered in the overperforming stock and a long in the underperforming. The positions are closed when the spread returns to the long-term relation. A pairs trading portfolio is formed by combining a number of pairs.To detect adequate pairs different types of data analysis has been used. The most common way has been to study historical price data with different statistical models such as the distance method. Gatev et al (2006) used this method and provided the most extensive research on the subject and this study will follow the standards set by that article and add new interesting factors. This is done through an investigation on how the analysis can be improved by using the stocks fundamental data, e.g. P/E, P/B, leverage, industry classification. This data is used to set up restrictions and Lasso models (type of regression) to optimize the trading portfolio and achieve higher returns. All models have been back-tested using S&P 500 stocks between 2001-04-01 and 2015-04-01 with portfolios changed every six months.The most important finding of the study is that restricting stocks to have close P/E-ratios combined with traditional price series analysis increases returns. The most conservative measure gives annual returns of 3.99% to 4.98% depending on the trading rules for this portfolio. The returns are significantly (5%-level) higher than those obtained by the traditional distance method.Considerable variations in return levels is shown to be created when capital commitments are changed and trading rules, transaction costs and restrictions on unique portfolio stocks are implemented.Further research regarding how analysis of P/E-ratios can improve pairs trading is suggested.The thesis has been written independently without an external client and studied an area that the author found interesting.",2015,
Low-Rank and Sparse Modeling of High-dimensional Vector Autoregressions,"Network modeling of high-dimensional time series in presence of unobserved latent variables is an important problem in macroeconomics and finance. In macroeconomic policy making and forecasting, it is often impossible to observe and incorporate all the relevant series in the analysis. Failure to include these variables often results in spurious connectivity among the observed time series in structural analyses, which may have serious policy implications. In order to accurately estimate a network of Granger causal interactions after accounting for latent effects, we introduce a novel approach of low-rank and sparse vector autoregression (VAR). We argue that in presence of a few latent pervasive factors, the transition matrix of a misspecified VAR model among the observed series can be approximated as the sum of a low-rank and a sparse component. Exploiting this connection, we consider estimating low-rank plus sparse VAR models using a combination of nuclear norm and lasso penalties. We establish non-asymptotic upper bounds on the estimation error rates of the low-rank and the sparse components and demonstrate the advantage of the proposed methodology over ordinary and sparse VAR estimates via numerical experiments.",2015,
Lasso-Kalman smoother for tracking sparse signals,"Fixed-interval smoothing of time-varying vector processes is an estimation approach with well-documented merits for tracking applications. The optimal performance in the linear Gauss-Markov model is achieved by the Kalman smoother (KS), which also admits an efficient recursive implementation. The present paper deals with vector processes for which it is known a priori that many of their entries equal to zero. In this context, the process to be tracked is sparse, and the performance of sparsity-agnostic KS schemes degrades considerably. On the other hand, it is shown here that a sparsity-aware KS exhibits complexity which grows exponentially in the vector dimension. To obtain a tractable alternative, the KS cost is regularized with the sparsity-promoting â„“1 norm of the vector process - a relaxation also used in linear regression problems to obtain the least-absolute shrinkage and selection operator (Lasso). The Lasso (L)KS derived in this work is not only capable of tracking sparse time-varying vector processes, but can also afford an efficient recursive implementation based on the alternating direction method of multipliers (ADMoM). Finally, a weighted (W)-LKS is also introduced to cope with the bias of the LKS, and simulations are provided to validate the performance of the novel algorithms.",2009,"2009 Conference Record of the Forty-Third Asilomar Conference on Signals, Systems and Computers"
Variable selection in linear models,"Variable selection in linear models is essential for improved inference and interpretation, an activity which has become even more critical for high dimensional data. In this article, we provide a selective review of some classical methods including Akaike information criterion, Bayesian information criterion, Mallow's Cp and risk inflation criterion, as well as regularization methods including Lasso, bridge regression, smoothly clipped absolute deviation, minimax concave penalty, adaptive Lasso, elastic-net, and group Lasso. We discuss how to select the penalty parameters. We also provide a review for some screening procedures for ultra high dimensions. WIREs Comput Stat 2014, 6:1â€“9. doi: 10.1002/wics.1284 
 
Conflict of interest: The authors have declared no conflicts of interest for this article. 
 
For further resources related to this article, please visit the WIREs website.",2014,Wiley Interdisciplinary Reviews: Computational Statistics
Les facteurs clÃ©s de succÃ¨s des projets d'aide au dÃ©veloppement,"Cette these examine les facteurs cles de succes, les interactions entre eux, et leur relation avec les dimensions du succes des projets d'aide au developpement. Elle s'articule autour de trois articles de recherche, l'un conceptuel sur l'experience des agences d'aide en matiere d'identification des facteurs cles de succes (FCS) de leurs projets et les deux autres, empiriques, qui epousent deux perspectives : celle des superviseurs de projet des agences d'aide multilaterale au developpement notamment la Banque mondiale et celle des coordonnateurs de projet sur le terrain, en l'occurrence en Afrique. Sur le plan methodologique, ces travaux adoptent une approche contingente de l'etude des projets et de leur succes; et les donnees ont ete collectees par questionnaire. D'abord, la these fait le point sur l'experience des agences d'aide dans l'identification des criteres et des FCS des projets de developpement. C'est l'objet du premier article qui s'intitule : Â« Identification des criteres et des facteurs cles de succes des projets dans la documentation des agences d'aide au developpement Â». (Une toute premiere version de ce papier a deja fait l'objet de publication dans la Revue Management & Avenir sous le titre : Â« Les agences d'aide au developpement font-elles assez en matiere de formulation des facteurs cles de succes des projets? Â»). L'objectif specifique de ce premier article est de mettre en evidence les criteres et les FCS que les agences d'aide au developpement utilisent, d'analyser l'evolution des criteres et des FCS a travers les decennies de developpement et de faire un rapprochement entre la gestion de projet et la gestion des projets de developpement international. Dans ce premier article, nous presentons les agences d'aide dans leur diversite et dans leur contribution au developpement et nous envisageons les projets comme des vecteurs du developpement. Ensuite, nous tentons de definir les concepts de resultats de developpement, de succes, de criteres de succes et de facteurs de succes des projets de developpement avant de presenter les criteres de succes et les FCS des agences d'aide. Puis nous analysons l'evolution des criteres et des FCS a travers les decennies de developpement. Enfin, nous faisons un rapprochement entre la gestion de projet et la gestion des projets de developpement international sur les criteres et les FCS ainsi que sur leur evolution dans le temps. Les resultats montrent que si les agences ont bien defini et harmonise les criteres de succes, il reste beaucoup a faire pour les criteres impact et durabilite, et les FCS. Ils suggerent qu'un rapprochement entre la gestion de projet et la gestion des projets de developpement international est possible et qu'en matiere de succes des projets, la gestion des projets de developpement international peut tirer des enseignements de la litterature de la gestion de projet pour mieux comprendre les criteres et les FCS des projets de developpement. Partant de ces enseignements, le deuxieme article, qui s'appuie sur le premier, analyse les interrelations entre quatre FCS bien connus tant en gestion de projet qu'en gestion des projets de developpement international mais captant Â« l'exercice global de supervision des projets de la Banque mondiale Â» (la conception, le suivi, la coordination, et la formation) et leurs influences respectives sur deux dimensions du succes des projets tout autant bien connues (le succes de la gestion et le succes du livrable). Cet article s'intitule : Â« World Bank projects' critical success factors and their interactions : an empirical investigation Â». Ce deuxieme article, utilisant plutot les modeles d'equations structurelles comme methode d'analyse statistique des donnees, a ete accepte pour publication au PMI Research Conference 2010. (Une premiere version de cet article, utilisant plutot la regression multiple comme methode d'analyse statistique des donnees, a fait l'objet de publication dans les actes du 9eme congres de l'IRNOP (International Research Network on Organising by Projects) en 2009 sous le titre : Â« The most critical success factors for World Bank projects : the Task Team Leaders' perspective Â». Ces deux versions different dans la mesure ou la seconde fait des analyses confirmatoires et teste ainsi la validite des instruments de mesure et du modele lui-meme). Dans ce deuxieme article, nous mettons d'abord en evidence la specificite des projets de developpement international, les problemes qu'ils posent, et les notions de succes, de critere et de facteur de succes tant dans la litterature de la gestion de projet que dans celle plus specifique de la gestion des projets de developpement international. Ensuite, nous posons des hypotheses sur les relations entre les FCS et les dimensions du succes des projets. Apres nous procedons a une analyse factorielle en composantes principales des criteres et des FCS avec le logiciel SPSS et nous analysons les interrelations entre FCS a l'aide des modeles d'equations structurel1es (estimees notamment avec le logiciel AMOS). Les resultats de ce deuxieme article confirment l'existence empirique d'un facteur de second-ordre que nous avons baptise Â« FCS relies a la gestion de projet Â» mais captant l'exercice global de supervision des projets. Les resultats empiriques montrent en outre que les FCS affectent de facon significativement differente les deux dimensions du succes des projets et que la premiere (le succes de la gestion) n'affecte pas significativement la deuxieme (le succes du livrable). Ils confirment aussi l'existence empirique de FCS non relies a la gestion de projet mais plutot relies au projet lui-meme comme le budget et l'experience du superviseur de projet et indiquent que ces derniers ne semblent pas avoir un effet sur les FCS relies a la gestion de projet (conception, suivi, coordination, et formation) mais captant l'exercice global de supervision des projets. Enfin ils suggerent que dans la perception des TTL, la hierarchie des FCS est la suivante : la conception, le suivi, la coordination et la formation dans cet ordre. Enfin, tres peu a ete ecrit sur la gestion de projet a l'intention des gestionnaires de projet du Tiers-Monde, notamment a l'intention des coordonnateurs africains des projets de developpement et la litterature est muette sur la facon d'adopter les outils et les techniques de gestion de projet importes. Quels sont les concepts, outils et techniques de gestion de projet que les gestionnaires africains des projets de developpement peuvent deployer? Quel est leur degre d'application actuelle? Sachant que la planification de projet reste un FCS, quelle est la force de la relation entre l'effort de planification et le succes des projets de developpement? Ce sont les questions qui sont abordees dans le dernier article : Â« Project management in the international development industry : the project coordinator's perspective Â». Il a ete publie dans la revue International Journal of Managing Projects in Business. Il met en evidence la perception des coordonnateurs de projets de developpement, en fait les gestionnaires de projet dans un secteur specifique et non-traditionnel de la gestion de projet, le developpement international, et analyse la relation empirique entre l'effort de gestion de projet dans la phase d'execution (le degre d'utilisation des outils de gestion de projet), le succes et les criteres de succes des projets. Dans ce dernier article, nous abordons l'importance des outils tant en gestion de projet en general qu'en gestion des projets de developpement en particulier. Nous proposons egalement un tableau synoptique des outils utilises dans la gestion des projets de developpement international que nous classons en trois grandes categories : les outils d'identification et de planification; les outils de realisation et de suivi; et les outils de mesure de performance, d'evaluation et de gouvernance. Nous donnons ensuite un bref apercu des travaux sur le succes et les criteres de succes avant de presenter et de discuter les resultats de l'analyse factorielle en composantes principales des outils de gestion de projet, et de l'analyse de correlation et de regression entre l'effort de gestion de projet et le succes. En somme, ce dernier article suggere que le succes des projets est insensible a l'effort de planification mais supporte de facon significative l'idee que le recours au suivi et a l'evaluation peut ameliorer les resultats des projets. 
______________________________________________________________________________ 
MOTS-CLES DE Lâ€™AUTEUR : aide au developpement, gestion de projet, criteres de succes, facteurs cles de succes, planification de projet, outils et techniques de gestion de projet, Banque mondiale, Afrique, Task Managers, coordonnateurs nationaux de projet, modele d'equations structurelles.",2011,
Un nuevo modelo predictivo basado en variables clÃ­nicas y en polimorfismos en genes de citocinas permite predecir la incidencia de EICR grave post-trasplante hematopoyÃ©tico alogÃ©nico,"El trasplante alogenico de progenitores hematopoyeticos (alo-TPH) es el tratamiento de eleccion para la curacion de enfermedades como las leucemias agudas y otras neoplasias hematologicas, inmunodeficiencias severas o errores congenitos del metabolismo y la enfermedad de injerto contra receptor (EICR), reaccion aloinmune de las celulas del donante contra celulas sanas de distintos tejidos del receptor, es una de las complicaciones mas relevantes post-TPH y la principal causa de morbilidad y mortalidad del mismo. Entre un 30 y un 50% de los pacientes que reciben un trasplante alogenico desarrollan la EICR pero el anticipar dicha complicacion sigue siendo un tema aun no resuelto. Hasta ahora se hace principalmente usando variables clinicas. Sin embargo, en los ultimos anos, se esta viendo tambien la importancia de las variables geneticas y, aunque la seleccion de donantes adecuados para el TPH se basa fundamentalmente en la compatibilidad del sistema HLA entre donante (D) y receptor (R), se esta estudiando la influencia de otros genes ya que aun en trasplantes HLA identicos se observan complicaciones como la EICR o el rechazo del injerto. La fisiopatologia de la EICR, se basa en una ?tormenta de citocinas? principalmente proinflamatorias originada en el R por efecto de los regimenes de acondicionamiento a la que se le anade la alorreactividad de los linfocitos T del D, que infiltran directamente diferentes organos produciendo el dano tisular.Las citocinas son moduladores proteicos de la respuesta inmune y, por tanto, influyen en la alorreactividad D/R tras el al-TPH y pueden determinar el exito del mismo.El objetivo del trabajo era analizar los polimorfismos geneticos que pueden tener impacto real en la incidencia de la EICR para establecer un modelo predictivo genetico y clinico para el desarrollo de la EICR post-TPH alogenico de D HLA identico familiar.La asociacion entre variables clinicas y geneticas en D y R con el desarrollo de la EICR aguda y la EICR cronica se estudio usando el analisis multivariante por regression penalizada de tipo LASSO y observamos que estos modelos eran muy utiles para anticipar la EICRa y la EICRc graves. Segun LASSO el mejor modelo clinico para anticipar la EICRa grados III-IV incluyo tres variables: el acondicionamiento, la irradiacion corporal total y el tipo de patologia y obtiene una tasa de clasificaciones correctas para los pacientes que van a desarrollar la EICR (TCC1) de un 50% y un VPN de 91,8%. Por otro lado el modelo clinico-genetico incluyo las mismas variables clinicas que el modelo clinico mas 11 citocinas (IL-1A, IL-1B, IL-2, IL-6, IL-7R, IL-10, IL-17A, IL-23R, INF?, TGFs y TNF?). Este modelo obtuvo una TCC1 del 100% y un VPN del 98,6%.El mejor modelo clinico para predecir la EICRc extensa incluyo la edad del R en el momento del trasplante, el sexo del R, la fuente de progenitores hematopoyeticos y el haber presentado antes del dia 100 la EICRa. Con este modelo se conseguia una TCC del 66.7% y un VPN del 82,9%. Cuando al modelo se le incluyeron las variables geneticas, las variables clinicas se mantuvieron y se le anadieron al modelo 8 citocinas (IL-1B, IL-2, IL-7R, IL-10, IL-17A, IL-23R, INF? y la TGFs) mejorando asi los resultados de la TCC1 con un 80% y un VPN del 85,1%.Basandonos en los resultados del coeficiente de regresion s de LASSO calculamos las ecuaciones de riesgo y con el valor de riesgo clasificamos a los pacientes en alto riesgo (mayor del punto de corte, Y=1) o bajo riesgo (menor al punto de corte). Los dos modelos clasifican bien pero clasifica mejor el modelo clinico-genetico con una significacion estadistica p < 0.001. En conclusion, los modelos predictivos con variables clinicas y geneticas estratifican mejor a los pacientes que los modelos exclusivamente clinicos y podrian permitir el manejo optimizado de las estrategias de inmunomodulacion post-TPH dirigidas a potenciar el efecto de injerto contra leucemia con el fin de minimizar el riesgo de recidiva.",2018,
L1 methods for shrinkage and correlation,"This dissertation explored the idea of L1 norm in solving two statistical problems including multiple linear regression and diagnostic checking in time series. In recent years L1 shrinkage methods have become popular in linear regression as they can achieve simultaneous variable selection and parameter estimation. Their objective functions containing a least squares term and an L1 penalty term which can produce sparse solutions (Fan and Li, 2001). Least absolute shrinkage and selection operator (Lasso) was the first L1 penalized method proposed and has been widely used in practice. But the Lasso estimator has noticeable bias and is inconsistent for variable selection. Zou (2006) proposed adaptive Lasso and proved its oracle properties under some regularity conditions. We investigate the performance of adaptive Lasso by applying it to the problem of multiple undocumented change-point detection in climate. Artificial factors such as relocation of weather stations, recalibration of measurement instruments and city growth can cause abrupt mean shifts in historical temperature data. These changes do not reflect the true atmospheric evolution and unfortunately are often undocumented due to various reasons. It is imperative to locate the occurrence of these abrupt mean shifts so that raw data can be adjusted to only display the true atmosphere evolution. We have built a special linear model which accounts for long-term temperature change (global warming) by linear trend and is featured by p = n (the number of variables equals the number of observations). We apply adaptive Lasso to estimate the underlying sparse model and allow the trend parameter to be unpenalized in the objective function. Bayesian Information Criterion (BIC) and the CM criterion (Caussinus and Mestre, 2004) are used to select the finalized model. Multivariate t simultaneous confidence intervals can post-select the change-points detected by adaptive Lasso to attenuate overestimation. Considering that the oracle properties of adaptive Lasso are obtained under the condition of linear independence between predictor variables, adaptive Lasso should be used with caution since",2013,
Graph Neural Lasso for Dynamic Network Regression,"In this paper, we will study the dynamic network regression problem, which focuses on inferring both individual entities' changing attribute values and the dynamic relationships among the entities in the network data simultaneously. To resolve the problem, a novel graph neural network, namely graph neural lasso (GNL), will be proposed in this paper. To model the real-time changes of nodes in the network, GNL extends gated diffusive unit (GDU) to the regression scenario and uses it as the basic neuron unit. GNL can effectively model the dynamic relationships among the nodes based on an attention mechanism.",2019,ArXiv
Avalon: towards QoS awareness and improved utilization through multi-resource management in datacenters,"Existing techniques for improving datacenter utilization while guaranteeing the QoS are based on the assumption that queries have similar behaviors. However, user queries in emerging compute demanding services demonstrate significantly diverse behavior and require adaptive parallelism. Our study shows that the end-to-end latency of the compute demanding query is determined together by the system-wide load, its workload, its parallelism, contention on shared cache, and memory bandwidth. When hosting such new services, the current cross-query resource allocation results in either severe QoS violation or significant resource under-utilization. To maximize hardware utilization while guaranteeing the QoS, we present Avalon, a runtime system that independently allocates shared resources for each query. Avalon first provides an automatic feature identification tool based on Lasso regression, to identify features that are relevant to a query's performance. Then, it establishes models that can precisely predict a query's duration under various resource configurations. Based on the accurate prediction model, Avalon proactively allocates ""just-enough"" cores and shared cache spaces to each query, so that the remaining resource can be assigned to execute best-effort applications. During runtime, Avalon monitors the progress of each query and mitigates any possible QoS violation due to memory bandwidth contention, occasional I/O contention, or unpredictable system interference. Our results show that Avalon improves utilization by 28.9% on average compared with state-of-the-art techniques while achieving 99%-ile latency target.",2019,Proceedings of the ACM International Conference on Supercomputing
Development and Validation of a Nomogram for Preoperative Prediction of Perineural Invasion in Colorectal Cancer,"BACKGROUND In colorectal cancer (CRC), perineural invasion (PNI) is usually identified histologically in biopsy or resection specimens and is considered a high-risk feature for recurrence of CRC and is an indicator for adjuvant therapy. Preoperative identification of PNI could help determine the need for adjuvant therapy and the approach to surgical resection. This study aimed to develop and validate a nomogram for the preoperative prediction of PNI in patients with CRC. MATERIAL AND METHODS A total of 664 patients with CRC from a single center were classified into a training dataset (n=468) and a validation dataset (n=196). The least absolute shrinkage and selection operator (LASSO) regression model was used to select potentially relevant features. Multivariate logistic regression analysis was used to develop the nomogram. The performance of the nomogram was assessed based on its calibration, discrimination, and clinical utility. RESULTS The nomogram consisted of five clinical features and provided good calibration and discrimination in the training dataset, with an area under the curve (AUC) of 0.704 (95% CI, 0.657-0.751). Application of the nomogram in the validation cohort showed acceptable discrimination, with the AUC of 0.692 (95% CI, 0.617-0.766) and good calibration. Decision curve analysis (DCA) showed that the nomogram was clinically useful. CONCLUSIONS The nomogram developed in this study might allow clinicians to predict the risk of PNI in patients with CRC preoperatively. The nomogram showed favorable discrimination and calibration values, which may help optimize preoperative treatment decision-making for patients with CRC.",2019,Medical Science Monitor : International Medical Journal of Experimental and Clinical Research
Structures de dÃ©pendance complexes pour modÃ¨les Ã  composantes supervisÃ©es,"Une forte redondance des variables explicatives cause de gros problemes d'identifiabilite et d'instabilite des coefficients dans les modeles de regression. Meme lorsque l'estimation est possible, l'interpretation des resultats est donc extremement delicate. Il est alors indispensable de combiner a leur vraisemblance un critere supplementaire qui regularise l'estimateur. Dans le sillage de la regression PLS, la strategie de regularisation que nous considerons dans cette these est fondee sur l'extraction de composantes supervisees. Contraintes a l'orthogonalite entre elles, ces composantes doivent non seulement capturer l'information structurelle des variables explicatives, mais aussi predire autant que possible les variables reponses, qui peuvent etre de types divers (continues ou discretes, quantitatives, ordinales ou nominales). La regression sur composantes supervisees a ete developpee pour les GLMs multivaries, mais n'a jusqu'alors concerne que des modeles a observations independantes.
Or dans de nombreuses situations, les observations sont groupees. Nous proposons une extension de la methode aux GLMMs multivaries, pour lesquels les correlations intra-groupes sont modelisees au moyen d'effets aleatoires. A chaque etape de l'algorithme de Schall permettant l'estimation du GLMM, nous procedons a la regularisation du modele par l'extraction de composantes maximisant un compromis entre qualite d'ajustement et pertinence structurelle. Compare a la regularisation par penalisation de type ridge ou LASSO, nous montrons sur donnees simulees que notre methode non seulement permet de reveler les dimensions explicatives les plus importantes pour l'ensemble des reponses, mais fournit souvent une meilleure prediction. La methode est aussi evaluee sur donnees reelles.
Nous developpons enfin des methodes de regularisation dans le contexte specifique des donnees de panel (impliquant des mesures repetees sur differents individus aux memes dates). Deux effets aleatoires sont introduits : le premier modelise la dependance des mesures relatives a un meme individu, tandis que le second modelise un effet propre au temps (possedant donc une certaine inertie) partage par tous les individus. Pour des reponses Gaussiennes, nous proposons d'abord un algorithme EM pour maximiser la vraisemblance du modele penalisee par la norme L2 des coefficients de regression. Puis nous proposons une alternative consistant a donner une prime aux directions les plus ""fortes"" de l'ensemble des predicteurs. Une extension de ces approches est egalement proposee pour des donnees non-Gaussiennes, et des tests comparatifs sont effectues sur donnees Poissonniennes.",2019,
Toxicogenomic prediction with group sparse regularization based on transcription factor network information,"Regression analysis such as linear regression and logistic regression has often been employed to construct toxicogenomic predictive models, which forecast toxicological effects of chemical compounds in human or animals based on gene expression data. While in general these techniques can generate an accurate and sparse model when a regularization term is added to a loss function, they ignore structural relationships behind genes which form vast regulatory networks and interact with each other. Recently, several reports proposed structured sparsity-inducing norms to incorporate prior structural information and make a model reflecting relationships between variables. In this study, assuming that genes regulated by the same transcription factor should be selected together, we applied the latent group Lasso technique on toxicogenomic data with transcription factor networks as prior knowledge. We compared generated classifiers for liver weight gain in rats between the latent group Lasso and Lasso. The latent group Lasso was comparable or superior to the Lasso in terms of predictive performances (balanced accuracy: 74% vs. 72%, sensitivity: 62% vs. 62%, specificity: 86% vs. 83%). Besides, groups selected by the latent group Lasso suggested involvement of Wnt/Î²-catenin signaling pathway. Such mechanism-related analysis could not have been possible with the Lasso and is one of the advantages of the latent group Lasso.",2015,Fundamental Toxicological Sciences
Study of Wine Evaluation Based on Lasso Regression,"Appraisal of quality is a crucial link in wine industry. At present, appraisal of wine quality mainly relies on sensory evaluation method, which may derive subjective biases. It shows particular importance for an establishment of objective and reasonable wine evaluation system to regulate the wine market. In this paper, after identifying the existence of multicollinearity of the physicochemical indicators of grape and liquor, Lasso regression is employed to build an evaluation model. Corresponding to the physicochemical indicators, this model is set up from four aspects: appearance, taste, aroma and overall balance. Lasso regression is used in each aspect. Weighted sum of the four aspects, overall assessment of the wine is obtained. As a conclusion, the new wine evaluation system is more objective and credible, thus it can substitute for sensory evaluation methods.",2013,
A Selection Operator for Summary Association Statistics Reveals Allelic Heterogeneity of Complex Traits,"In recent years, as a secondary analysis in genome-wide association studies (GWASs), conditional and joint multiple-SNP analysis (GCTA-COJO) has been successful in allowing the discovery of additional association signals within detected loci. This suggests that many loci mapped in GWASs harbor more than a single causal variant. In order to interpret the underlying mechanism regulating a complex trait of interest in each discovered locus, researchers must assess the magnitude of allelic heterogeneity within the locus. We developed a penalized selection operator for jointly analyzing multiple variants (SOJO) within each mapped locus on the basis of LASSO (least absolute shrinkage and selection operator) regression derived from summary association statistics. We found that, compared to stepwise conditional multiple-SNP analysis, SOJO provided better sensitivity and specificity in predicting the number of alleles associated with complex traits in each locus. SOJO suggested causal variants potentially missed by GCTA-COJO. Compared to using top variants from genome-wide significant loci in GWAS, using SOJO increased the proportion of variance prediction for height by 65% without additional discovery samples or additional loci in the genome. Our empirical results indicate that human height is not only a highly polygenic trait, but also has high allelic heterogeneity within its established hundreds of loci.",2017,American Journal of Human Genetics
Bayesian Penalized Regression,"We consider ordinary least squares, lasso, bridge, and ridge regression methods under a unified framework. The particular method is determined by the form of the penalty term, which is typically chosen by cross validation. We introduce a fully Bayesian approach which allows selection of the penalty through posterior inference if desired. We also show how to use a type of model averaging approach to eliminate the nuisance penalty parameters and perform inference through the marginal posterior distribution of the regression coefficients. We develop a component-wise Markov chain Monte Carlo algorithm for sampling and establish conditional and marginal posterior consistency for the Bayesian model. Numerical results show that the method tends to select the optimal penalty and performs well in both variable selection and prediction. Both simulated and real data examples are provided.",2017,
Sparse Algorithms Are Not Stable,"We consider two desired properties of learning algorithms: sparsity and algorithmic stability. Both properties are believed to lead to good generalization ability. We show that these two properties are fundamentally at odds with each other: A sparse algorithm cannot be stable and vice versa. Thus, one has to trade off sparsity and stability in designing a learning algorithm. In particular, our general result implies that '1-regularized regression (Lasso) cannot be stable, while '2-regularized regression is known to have strong stability properties and is therefore not sparse. Index Termsâ€”Stability, sparsity, Lasso, regularization.",2012,
Genomic Selection for Drought Tolerance Using Genome-Wide SNPs in Maize,"Traditional breeding strategies for selecting superior genotypes depending on phenotypic traits have proven to be of limited success, as this direct selection is hindered by low heritability, genetic interactions such as epistasis, environmental-genotype interactions, and polygenic effects. With the advent of new genomic tools, breeders have paved a way for selecting superior breeds. Genomic selection (GS) has emerged as one of the most important approaches for predicting genotype performance. Here, we tested the breeding values of 240 maize subtropical lines phenotyped for drought at different environments using 29,619 cured SNPs. Prediction accuracies of seven genomic selection models (ridge regression, LASSO, elastic net, random forest, reproducing kernel Hilbert space, Bayes A and Bayes B) were tested for their agronomic traits. Though prediction accuracies of Bayes B, Bayes A and RKHS were comparable, Bayes B outperformed the other models by predicting highest Pearson correlation coefficient in all three environments. From Bayes B, a set of the top 1053 significant SNPs with higher marker effects was selected across all datasets to validate the genes and QTLs. Out of these 1053 SNPs, 77 SNPs associated with 10 drought-responsive transcription factors. These transcription factors were associated with different physiological and molecular functions (stomatal closure, root development, hormonal signaling and photosynthesis). Of several models, Bayes B has been shown to have the highest level of prediction accuracy for our data sets. Our experiments also highlighted several SNPs based on their performance and relative importance to drought tolerance. The result of our experiments is important for the selection of superior genotypes and candidate genes for breeding drought-tolerant maize hybrids.",2017,Frontiers in Plant Science
Chapter 7 â€“ Regression,"In this chapter, we introduce the notion of using data to make predictions. We start with linear regression, using subthreshold excitatory potentials to predict spiking behavior of neurons in auditory cortex. We introduce the notions of fitting a line to points and determining slope and intercept as beta weights. We then extend the concept of regression to the increasingly important method of logistic regression, where we predict binary or categorical outcomes using data from color perception, which we also use as an opportunity to introduce string parsing. We conclude the chapter by discussing modern machine learning methods like lasso and ridge regression.",2017,
Abstract 884: Circulating microRNAs as non-invasive biomarkers for early detection of lung cancer,"Lung cancer is the leading cause of cancer deaths worldwide. In 2008, 1.61 million new cases, and 1.38 million deaths due to lung cancer were recorded. This high mortality rate is mainly due to the late stage at which lung cancer is diagnosed. While early diagnosis has been successfully implemented through tomography-based population screenings in high-risk individuals, there is a need for simpler, non-invasive and more accessible methodologies for effective early cancer detection programs. Circulating microRNA (miRNA) profiles have been suggested as promising diagnostic and prognostic biomarkers for cancer, including lung cancer. However, the results have so far been inconsistent between studies. The objective of this study was to explore the potential of circulating miRNAs in plasma for early detection of lung cancer using global profiling approach. Plasma samples were collected from 100 early stage (I to IIIA) non-small-cell lung cancer patients (35 lung adenocarcinoma and 65 squamous cell carcinoma patients) and 100 age- and gender-matched healthy controls. 754 circulating miRNAs were analyzed via quantitative RT-PCR using TaqMan Low Density Arrays. Data were quantile normalized and limma analysis with adjustment for multiple testing to control for false discovery rate (FDR, Benjamini-Hochberg method) was performed to identify differentially regulated miRNA between cases and controls. Penalized Lasso logistic regression model (with penalty parameter tuning conducted by 10-fold cross-validation) was used to compute the least redundant panel of miRNAs for discriminating between cases and controls. The area under the receiver operating characteristic curve (AUC) was calculated to assess the discriminatory power of the model. Internal validation was conducted by calculating the bootstrap optimism-corrected AUC for the selected model. Sixty one plasma miRNAs were found to be significantly differentially expressed between lung cancer cases and controls including 33 upregulated and 28 downregulated miRNAs (p-value Citation Format: Magdalena B. Wozniak, Ghislaine Scelo, David Muller, Anush Moukeria, David Zaridze, Paul Brennan. Circulating microRNAs as non-invasive biomarkers for early detection of lung cancer. [abstract]. In: Proceedings of the 105th Annual Meeting of the American Association for Cancer Research; 2014 Apr 5-9; San Diego, CA. Philadelphia (PA): AACR; Cancer Res 2014;74(19 Suppl):Abstract nr 884. doi:10.1158/1538-7445.AM2014-884",2014,Cancer Research
Variational Bayes logistic regression as regularized fusion for NIST SRE 2010,"Fusion of the base classifiers is seen as a way to achieve high performance in state-of-the-art speaker verification systems. Typically, we are looking for base classifiers that would be complementary. We might also be interested in reinforcing good base classifiers by including others that are similar to them. In any case, the final ensemble size is typically small and has to be formed based on some rules of thumb. We are interested to find out a subset of classifiers that has a good generalization performance. We approach the problem from sparse learning point of view. We assume that the true, but unknown, fusion weights are sparse. As a practical solution, we regularize weighted logistic regression loss function by elastic-net and LASSO constraints. However, all regularization methods have an additional parameter that controls the amount of regularization employed. This needs to be separately tuned. In this work, we use variational Bayes approach to automatically obtain sparse solutions without additional cross-validation. Variational Bayes method improves the baseline method in 3 out of 4 sub-conditions. Index Terms: logistic regression, regularization, compressed sensing, linear fusion, speaker verification",2012,
Impact of Ozone on the Growth and Yield of Trees : A Review,"Data f rom 25 exper iments on seed l ings o f 43 tree s p e c i e s a n d h y b r i d s show that ozone (0,) can reduce growth and photosynthesis at concentrations common in many areas of the USA. Seedlings have been primarily employed for such studies for logislic reasons, and will likely provide the greatest breadth of information for some time IO come. However, a number of impediments limit application of seedling response s tud ies IO a s s e s s m e n t o f i m p a c t s o n r e g i o n a l t i m b e r p r o d u c tion. Large trees differ from seedlings in a number of ways, including C allocation and canopy structure, and methods must be developed IO acrount for these differences if information from seedling studies is to prove useful IO forest impact assessmenl. Understanding how comp e t i t i o n m e d i a t e s i n d i v i d u a l tree r e s p o n s e s w i l l r e q u i r e i n v e s t i g a t i o n o f whether systematic differences of microclimate leaf morphology that exist across canopies affects foliage sensitivity IO 0,. and whether the maximum growth rates of genolypes are correlated with susceptibility IO 0,. Definitive information on these factors is necessary IO assess imparts of 0, on stand development and diameter distributions in both mulliand single species stands. Of critical economic importance is whether 0, preferentially damages taller, more valuable individuals within stands and more valuable, faster growing stand types. Of the several air pollutants common in various regions of the USA, ozone (0,) is the only one likely to impact Pest Impact Assessment Technology Research Work Unit, USDA Forest Service, Southeastern Forest Exp. Stn., Research Triangle Park, NC 27709. Contribution of the Pest Impact Assessment Technol Res. Work Unit, USDA Forest Service, Southeastern Forest Exp. Stn. Received 13 July 1987. *Corresponding author. Published in J. Environ. Qua). 17:347-360 (1988). large areas for which sufficient response information is available to assess exposure-response relationships. Controlled exposures of trees to SO, or NO, have been limited to concentrations of 0.05 pL/L and above, concentrations that are rare in most forested areas of the country (Altshuller, 1983; National Research Council, 1986). Additionally NO, and SO, are subject to relatively large spatial and temporal variability (Seinfeld, 1986; Roberts, 1984), complicating estimation of exposure for rural areas with Little monitoring. Assessment of acid deposition is complicated by negative and positive impacts (Bell, 1986), unresolved mechanisms of action (Society of American Foresters, 1984), and the probable importance of indirect effects (Ulrich, 1983). Several excellent 0, reviews are available (Guderian, 1985; Heath, 1980; Heck et al., 1986; Mudd, 1984; Runeckles, 1986), but they have not focused on responses of tree species. Several have addressed impacts on trees but they are either theoretical syntheses or qualitative discussions of response (Winner and Atkinson, 1986; Harkov and Brennan, 1979; Taylor and Norby, 1984; Kozlowski and Constantinidou, 1986a, b). Here 1 review data from controlled exposures and discuss how such data might be incorporated in large-scale economic assessments. This review consists of three parts: (i) a critique of available experimental approaches, (ii) a review of tree response data from controlled fumigations, and (iii) a discussion of difficulties extrapolating these results to regional economic damage assessments. This analysis is J. Environ. Qual., Vol. 17, no. 3, 1988 3 4 7 restricted to estimation of timber market impact, ignoring other benefits that forests provide. EXPERIMENTAL APPROACHES Fisher (1981) cites two methods for determining damage functions for polluation impact assessment: statistical field studies (e.g., Miller, 1983; Kercher and Exelrod, 1981), and controlled exposure-response experiments (as in Heck et al., 1986). Statistical field studies exploiting spatial or tempera1 contrasts in 0, have been made difficult by low spatial resolution and few years of comparable O1 concentration data (Pinkerton and Lefohn, 1986; USEPA, 1986). A third approach providing the most rapic input to policy makers is expert opinion, possibly structured around interdisciplinary workshops (e.g., Helling and Chambers, 1973; Bonnickson and Becker, 1983). The most prominent model of the experimental approach to regional air pollution impact assessment is The National Crop Loss Assessment Network (NCLAN) (Heck et al., 1986), in which yield reductions were estimated from a series of exposure-response fumigation studies. However, long rotations and the large sizes of trees prevent rotation-long fumigation as employed by NCLAN, complicating estimation of stand-level yield impacts. Greater heterogeneity of soils, topography and species, and poorly characterized intraspecific variability each present additional problems for stand and regional extrapolation. Fumigation Chamber Designs Indoor growth chambers, greenhouses, and continuously stirred tank reactors (CSTRâ€™s) are the most common fumigation environments for tree studies. In these indoor environments, plants differ morphologically and physiologically from those grown outdoors, and they react differently to 0, (Lewis and Brennan, 1977). Outdoor exposures usually employ open-top chambers (OTC) or, rarely, chamberless designs (Reich and Amundson, 1984). Chamberless designs produce the fewest microclimatic artifacts, particularly in winter (Olszyk et al., 1986), but control of fumigation levels under differing winds has proved difficult (Guderain et al., 1985). Open-top chambers after temperature, humidity, and air flow, extending leaf retention and increasing height growth over chamberless controls (Duchelle et al., 1982; Wang et al., 1986). Chamberless designs may benefit from improvements in airflow control, but OTCâ€™s currently provide the most realistic data on yield response. Ideally, exposure-response studies should include treatments representing four or more concentrations of 0, that span the range of control scenarios under policy consideration, allowing nonlinear regression analysis of impacts. While unusual in tree response studies published thus far, use of this design is becoming more common. Relevant Dosing Regimes The relevance of experimental dosing regimes depends on patterns of exposure common in forests. The most commonly cited exposure statistic is the daily 7-hr mean, averaged over the growing season. The regional patterns of 0, characterized by this measure obscure smaller temporal and spatial patterns. Daily 0, concentrations tend to peak around 1400 h near urban areas, but diurnal swings are dampened and often displaced later in the day in more remote areas (Lefohn and Jones, 1986; USEPA, 1986; Miller et al., 1982; Berry, 1964). Mean 0, concentrations in urban and rural areas are often fairly similar, about 0.040 to 0.055 pL/L in the southeast (Pinkerton and Lefohn, 1986). However, peak events are more extreme in cities (USEPA, 1986). In the southeast, hourly means greater than 0.120 FL/L, occurred at only 2 of 28 rural sites, but they occurred at least once in nearly every city (Pinkerton and Lefohn, 1986). While controlled exposure studies usually manipulate exposure means, other exposure parameters are known to affect plant response (Male; 1982). These include the variance of 0, concentrationâ€˜and the timing of episodes and respites (Jensen, 1979; Musselman et al., 1983; Hogsett et al., 1985b). Indoor fumigations usually employ square-wave dosing regimes, with constant daytime 0, concentrations. Outdoor fumigations use treatments based on fixed 0, additions or complete removals of 0, from ambient air, resulting in realistic diurnal and seasonal variations, but with constant variance across treatments. Most fumigations include exposure to ambient 0, at night across ail treatments. The impact of this nighttime exposure is unknown. Although stomates are typically closed at night, Reich and Lassoie (1985) found that longterm 0, fumigation alters normal diurnal patterns of stomata1 conductance, raising the possibility of significant nighttime uptake of 0,. Extending 0, fumigation from 8 to 24 h did result in greater damage in one study (Ashmore et al., 1987, poster presented at the 19th Annual Air Pollution Workshop, Helena, MT). Note that published multiyear fumigations have also not controlled 0, exposures during the winter months (Wang et al., 1986; Duchelle et al., 1982; Chevone et al., 1983). There are two schools of thought on appropriate control 0, concentrations: zero and natural. Most researchers use charcoal-filtered air for their no-O> treatment, which brings 0, concentrations to an unspecified level near zero. This practice clarifies the mechanisms of 0, impact and highlights impacts at low 0, concentrations. Other researchers (e.g., Reich and Lassoie, 1985) contend that such low concentrations of 0, do not constitute realistic controls because pristine levels of 0, are closer to 0.025 pL/L (USEAP, 1986). These researchers use control concentrations of 0.025 to 0.030 pL/L. Types of Response Measures Experiments have identified biochemical and physiological effects of 0,. At the biochemical level, 0, oxidizes sulfhydryl and fatty acid double bonds, increases membrane permeability, and disrupts membrane-bound photosynthetic systems (Guderian et al., 1985; Mudd, 1984). Foliar sugar and polysaccharide levels are lowered as well (Miller et al., 1969). At the physiological level, 348 J. Environ. Qual., Vol. 17, no. 3, 1988 net photosynthesis is reduced, dark respiration is increased (Barnes, 1972), and C transport to roots is lowered (McLaughlin and McConathy, 1983). Other physiological impacts include coincident and long-term reductions in stomata1 conductance (Hill and Littlefield, 1969; Reich and Amundson, 1985; Coyne and Bingham, 1982), accelerated leaf senescence (Reich, 1983; Jensen, 1982; Noble and Jensen,",2004,
Functional logistic regression with fused lasso penalty,"ABSTRACT This study considers the binary classification of functional data collected in the form of curves. In particular, we assume a situation in which the curves are highly mixed over the entire domain, so that the global discriminant analysis based on the entire domain is not effective. This study proposes an interval-based classification method for functional data: the informative intervals for classification are selected and used for separating the curves into two classes. The proposed method, called functional logistic regression with fused lasso penalty, combines the functional logistic regression as a classifier and the fused lasso for selecting discriminant segments. The proposed method automatically selects the most informative segments of functional data for classification by employing the fused lasso penalty and simultaneously classifies the data based on the selected segments using the functional logistic regression. The effectiveness of the proposed method is demonstrated with simulated and real data examples.",2018,Journal of Statistical Computation and Simulation
Key Considerations and Methods in the Study of Gene-Environment Interactions.,"With increased involvement of genetic data in most epidemiological investigations, gene-environment (G Ã— E) interactions now stand as a topic, which must be meticulously assessed and thoroughly understood. The level, mode, and outcomes of interactions between environmental factors and genetic traits have the capacity to modulate disease risk. These must, therefore, be carefully evaluated as they have the potential to offer novel insights on the ""missing heritability problem"", reaching beyond our current limitations. First, we review a definition of G Ã— E interactions. We then explore how concepts such as the early manifestation of the genetic components of a disease, the heterogeneity of complex traits, the clear definition of epidemiological strata, and the effect of varying physiological conditions can affect our capacity to detect (or miss) G Ã— E interactions. Lastly, we discuss the shortfalls of regression models to study G Ã— E interactions and how other methods such as the ReliefF algorithm, pattern recognition methods, or the LASSO (Least Absolute Shrinkage and Selection Operator) method can enable us to more adequately model G Ã— E interactions. Overall, we present the elements to consider and a path to follow when studying genetic determinants of disease in order to uncover potential G Ã— E interactions.",2016,American journal of hypertension
The Epidemiological Characteristics and Prognostic Factors of Low-Grade Brainstem Glioma: A Real-World Study of Pediatric and Adult Patients,"Purpose: Our current understanding of low-grade brainstem glioma (LGBSG) is still limited. This study aimed to conduct a large-scale population-based real-world study to understand the epidemiological characteristics of LGBSG and determine the predictive factors of cancer-specific survival (CSS) and overall survival (OS) of LGBSG patients. Patients and Methods: We used Surveillance Epidemiology and End Results database to conduct this study of patients with histologically confirmed LGBSG. Patient demographics, tumor characteristics, and treatment options were compared between pediatric and adult patients. Univariate and multivariate analyses were employed to determine prognostic factors of CSS and OS. Kaplanâ€“Meier curve and decision tree were used to confirm the prognostic factors. All variables were further identified by L1-penalized (Lasso) regression and then a nomogram was established to predict the 5- and 8-year CSS and OS rate. The precision of the nomogram was evaluated by calibration plots, Harrell's concordance index, and time-dependent receiver operating characteristic curve. The clinical use of nomogram was estimated by decision curve analysis. Results: A cohort of 305 patients with LGBSG, including 165 pediatric and 140 adult patients, was analyzed. Adult and pediatric patients showed different patterns concerning tumor size, tumor extension, adjuvant therapy, and survival rate. Univariate analysis revealed that pediatric group, gross total resection (GTR), World Health Organization grade II, radiotherapy, extension to ventricular system, and diffuse astrocytic and oligodendroglial tumor (DAOT) were significantly associated with CSS. Multivariate analysis showed that pediatric group, metastasis, ventricular system involvement, and DAOT were independently associated with CSS. The prognostic factors were further confirmed by Kaplanâ€“Meier curve and decision tree. Kaplanâ€“Meier curve also showed that adjuvant therapy added no benefits in patients with GTR and non-GTR. In addition, the nomogram was developed and the C-index of internal validation for CSS was 0.87 (95% CI, 0.78â€“0.96). Conclusion: This study shows that pediatric and adult patients have different tumor characteristics, treatment options, and survival rate. Pediatric group, DAOT, ventricular system involvement, and metastasis were identified as independent prognostic factors for CSS by multivariate analysis. Adjuvant therapy showed no benefits on CSS in patients with GTR and non-GTR. The nomogram was discriminative and clinically useful.",2020,Frontiers in Oncology
Using Textual Transcripts of Parliamentary Interventions for Profiling Portuguese Politicians,"This work presents an experimental study on the subject of profiling political actors through textual transcriptions of their parliamentary interventions. Supervised learning techniques were used to learn models, which attempt to classify Portuguese politicians according to their gender, their age group, or their political affiliation and orientation. Experiments were made using different types of classification models, using state-of-the-art feature weighting schemes, using stylometric features from state-of-theart approaches for author profiling, and using features derived from distributional word clustering or from concise semantic analysis. Experiments with the group Lasso regularization technique for logistic regression models were also performed. The experiments showed that language usage is indeed indicative of a personâ€™s characteristics and ideology.",2016,
Radiomics Signatures of Computed Tomography Imaging for Predicting Risk Categorization and Clinical Stage of Thymomas,"Purpose
The aim of this study is to develop and compare performance of radiomics signatures using texture features extracted from noncontrast enhanced CT (NECT) and contrast enhanced CT (CECT) images for preoperative predicting risk categorization and clinical stage of thymomas.


Materials and Methods
Between January 2010 and October 2018, 199 patients with surgical resection and histopathologically confirmed thymoma were enrolled in this retrospective study. We extracted 841 radiomics features separately from volume of interest (VOI) in NECT and CECT images. The features with poor reproducibility and highly redundancy were removed. Then a least absolute shrinkage and selection operator method (LASSO) logistic regression model with 10-fold cross validation was used for further feature selection and radiomics signatures build. The predictive performances of radiomics signatures were assessed by receiver operating characteristic (ROC) analysis. The areas under the receiver operating characteristic curve (AUC) between radiomics signatures were compared by using Delong test.


Result
In differentiating high risk thymomas from low risk thymomas, the AUC, sensitivity, and specificity were 0.801(95% CI 0.740-0.863), 0.752 and 0.767 for radiomics signature based on NECT images, and 0.827 (95% CI 0.771 -0.884), 0.798, and 0.722 for radiomics signature based on CECT images. But there was no significant difference (p=0.365) between them. In differentiating advanced stage thymomas from early stage thymomas, the AUC, sensitivity, and specificity were 0.829 (95%CI 0.757-0.900), 0.712, and 0.806 for radiomics signature based on NECT images and 0.860 (95%CI 0.803-0.917), 0.699, and 0.889 for radiomics signature based on CECT images. There was no significant difference (p=0.069) between them. The accuracy was 0.819 for radiomics signature based on NECT images, 0.869 for radiomics signature based on CECT images, and 0.779 for radiologists. Both radiomics signatures had a better performance than radiologists. But there was significant difference (p = 0.025) only between CECT radiomics signature and radiologists.


Conclusion
Radiomics signatures based on texture analysis from NECT and CECT images could be utilized as noninvasive biomarkers for differentiating high risk thymomas from low risk thymomas and advanced stage thymomas from early stage thymoma. As a quantitative method, radiomics signature can provide complementary diagnostic information and help to plan personalized treatment for patients with thymomas.",2019,BioMed Research International
M ar 2 01 0 POST-l 1-PENALIZED ESTIMATORS IN HIGH-DIMENSIONAL LINEAR REGRESSION MODELS,"In this paper we study post-penalized estimators which apply ordinary, unpenalized linear regression to the model selected by first-step penalized estimators, typically LASSO. It is well known that LASSO can estimate the regression function at nearly the oracle rate, and is thus hard to improve upon. We show that post-LASSO performs at least as well as LASSO in terms of the rate of convergence, and has the advantage of a smaller bias. Remarkably, this performance occurs even if the LASSO-based model selection â€œfailsâ€ in the sense of missing some components of the â€œtrueâ€ regression model. By the â€œtrueâ€ model we mean here the best s-dimensional approximation to the regression function chosen by the oracle. Furthermore, post-LASSO can perform strictly better than LASSO, in the sense of a strictly faster rate of convergence, if the LASSO-based model selection correctly includes all components of the â€œtrueâ€ model as a subset and also achieves a sufficient sparsity. In the extreme case, when LASSO perfectly selects the â€œtrueâ€ model, the post-LASSO estimator becomes the oracle estimator. An important ingredient in our analysis is a new sparsity bound on the dimension of the model selected by LASSO which guarantees that this dimension is at most of the same order as the dimension of the â€œtrueâ€ model. Our rate results are non-asymptotic and hold in both parametric and nonparametric models. Moreover, our analysis is not limited to the LASSO estimator in the first step, but also applies to other estimators, for example, the trimmed LASSO, Dantzig selector, or any other estimator with good rates and good sparsity. Our analysis covers both traditional trimming and a new practical, completely data-driven trimming scheme that induces maximal sparsity subject to maintaining a certain goodness-of-fit. The latter scheme has theoretical guarantees similar to those of LASSO or post-LASSO, but it dominates these procedures as well as traditional trimming in a wide variety of experiments. First arXiv version: December 2009.",2010,
Abstract B41: Methylation profiling of ovarian cancer to study etiologic and prognostic heterogeneity and to develop a molecular classifier.,"Background: Ovarian cancer is a heterogeneous disease that is divisible into multiple subtypes with variable pathogenesis, etiology and biological behavior. We analyzed DNA methylation profiling data to identify biologic subgroups of ovarian cancer and study their relationship with histologic subtypes and prognosis. Additionally, we developed a molecular classifier in relation to standard histologic subtype for classification of ovarian tumors in epidemiologic studies. Methods: A total of 180 paraffin embedded ovarian epithelial tumor tissues, including the four major epithelial ovarian tumor subtypes (serous, endometrioid, mucinous and clear cell) and tumors of low malignant potential (LMP) were selected from two different sources: The Polish Ovarian Cancer study, an incident population-based case-control study conducted from 2001-2004, and the Surveillance, Epidemiology, and End Results Residual Tissue Repository (SEER RTR), which included ovarian tumors blocks collected between 1994 and 2004 from the Iowa and Hawaii SEER registries. The distributions of tumor histologic subtypes and grades from the studies were similar. All analyses were restricted to Caucasian women. Methylation profiling was conducted using the Illumina 450K methylation array. Analyses were restricted to the 22 autosomal chromosomes and non-SNP probes. Fourteen samples did not pass quality control and were excluded from the analysis. Of the 166 evaluable samples, 29 cases (17.5%) had their histologic subtype recoded after expert review. In order to compute and validate our histological signatures, the samples were divided into a training set (N=110) and a validation set (N=56). In addition, 10 high grade serous cases with 450K methylation data from the ovarian TCGA effort were included for replication. Signatures were computed using a LASSO logistic regression. Results: Among 166 samples, 32 (19%) were from the Polish study and 134 (81%) were from the SEER RTR. Unsupervised hierarchical clustering of the 5,000 most variable CpG sites showed 4 major clusters: Cluster 1 with 79% invasive serous and 14% endometrioid carcinomas, including the majority of high grade carcinomas; cluster 2 with 77% either endometrioid or clear cell carcinomas; cluster 3 with 71% serous LMP; and cluster 4 with 73% mucinous carcinomas. We observed significant survival differences across these clusters (long-rank test P= 4.64Ã—10-6), similar to differences observed for histologic subtypes. We used the training set to determine a parsimonious classifier based on methylation markers for histologic subtypes. We applied these signatures to an independent validation set from the Polish Study, SEER RTR, and the ovarian TCGA and found that 77% of the samples were correctly classified. Among the cases for which the histology was recoded after expert review, the methylation signatures correctly classified the histology subtype in 76% of the cases. Conclusions: Unsupervised analysis of DNA methylation profiling identified 4 distinct clusters of ovarian carcinomas, consistent with data indicating that ovarian cancer is heterogeneous with respect to cells of origin, carcinogenic pathways and histology. High grade serous carcinomas were grouped with high grade endometrioid cancers, while the remaining endometrioid carcinomas clustered with clear cell carcinomas, consistent with a common origin for a subset of these tumors from orthotopic or ectopic endometrial tissue. Our results suggest that methylation signatures provide a classification of ovarian tumors that overlaps with histologic subtypes and probable mechanisms of origin. Ongoing analyses will compare performance of the methylation classifier with histologic subtype in relation to risk factors and prognosis. Citation Format: Clara Bodelon, Keith Killian, Joshua Sampson, Holly Stevenson, William Anderson, Rayna Matsuno, Louise Brinton, Jolanta Lissowska, Mark Sherman, Nicolas Wentzensen. Methylation profiling of ovarian cancer to study etiologic and prognostic heterogeneity and to develop a molecular classifier. [abstract]. In: Proceedings of the AACR Special Conference on Advances in Ovarian Cancer Research: Exploiting Vulnerabilities; Oct 17-20, 2015; Orlando, FL. Philadelphia (PA): AACR; Clin Cancer Res 2016;22(2 Suppl):Abstract nr B41.",2016,Clinical Cancer Research
