title,abstract,year,journal
Simultaneous Analysis of Lasso and Dantzig,"We exhibit an approximate equivalence between the Lasso es-timator and Dantzig selector. For both methods we derive parallel oracle inequalities for the prediction risk in the general nonparamet-ric regression model, as well as bounds on the p estimation loss for 1 â‰¤ p â‰¤ 2 in the linear model when the number of variables can be much larger than the sample size.",2007,
"A four-marker signature of TNF-RII, TGF-Î±, TIMP-1 and CRP is prognostic of worse survival in high-risk surgically resected melanoma","BackgroundE1694 tested GM2-KLH-QS21 vaccine versus high-dose interferon-Î±2b (HDI) as adjuvant therapy for operable stage IIB-III melanoma. We tested banked serum specimens from patients in the vaccine arm of E1694 for prognostic biomarkers.MethodsAushon Multiplex Platform was used to quantitate baseline serum levels of 115 analytes from 40 patients. Least absolute shrinkage and selection operator proportional hazard regression (Lasso PH) was used to select markers that are most informative for relapse-free survival (RFS) and overall survival (OS). Regular Cox PH models were then fit with the markers selected by the Lasso PH. Survival receiver operating characteristic (ROC) analysis was used to evaluate the ability of the models to predict 1-year RFS and 5-year OS.ResultsFour markers that include Tumor Necrosis Factor alpha Receptor II (TNF-RII), Transforming Growth Factor alpha (TGF-Î±), Tissue Inhibitor of Metalloproteinases 1 (TIMP-1), and C-reactive protein (CRP) were found to be most informative for the prediction of OS (high levels correlate with worse prognosis). The dichotomized risk score based on the four markers could significantly separate the OS curves (pâ€‰=â€‰0.0005). When using the four-marker PH model to predict 5-year OS, we achieved an area under the curve (AUC) of 89% (cross validated AUCâ€‰=â€‰72%). High baseline TNF-RII was also significantly associated with worse RFS. The RFS with high (above median) TNF-RII was significantly lower than low TNF-RII (pâ€‰=â€‰0.01).ConclusionsThe biomarker signature consisting of TNFR-II, TGF-Î±, TIMP-1 and CRP is significantly prognostic of survival in patients with high-risk melanoma and warrants further investigation.",2013,Journal of Translational Medicine
Metabolic reprogramming-associated genes predict overall survival for rectal cancer.,"Metabolic reprogramming has become a hot topic recently in the regulation of tumour biology. Although hundreds of altered metabolic genes have been reported to be associated with tumour development and progression, the important prognostic role of these metabolic genes remains unknown. We downloaded messenger RNA expression profiles and clinicopathological data from The Cancer Genome Atlas and the Gene Expression Omnibus database to uncover the prognostic role of these metabolic genes. Univariate Cox regression analysis and lasso Cox regression model were utilized in this study to screen prognostic associated metabolic genes. Patients with high-risk demonstrated significantly poorer survival outcomes than patients with low-risk in the TCGA database. Also, patients with high-risk still showed significantly poorer survival outcomes than patients with low-risk in the GEO database. What is more, gene set enrichment analyses were performed in this study to uncover significantly enriched GO terms and pathways in order to help identify potential underlying mechanisms. Our study identified some survival-related metabolic genes for rectal cancer prognosis prediction. These genes might play essential roles in the regulation of metabolic microenvironment and in providing significant potential biomarkers in metabolic treatment.",2020,Journal of cellular and molecular medicine
Pattern Recognition in Non-Stationary Environmental Time Series Using Sparse Regression,"Abstract The various real-world tasks of environmental management make it necessary to obtain the hindcasts and forecasts of natural events (wind, ocean waves and currents, sea ice, etc.) using data-driven techniques for metocean processes simulation. The models can be fitted to specific fragments of the non-stationary multivariate time series individually to reproduce metocean environment with desired characteristics. In the paper, the approach based on the LASSO regularised regression is proposed for the environmental time series clustering. It allows the identify the situations with specific interaction between variables, that can be interpreted by the values regression coefficients. The weather generator was used to produce both synthetic time series similar to the general dataset and the identified clusters. The obtained results can be used to increase the quality of the computationally lightweight environmental modelsâ€™ identification and interpretation.",2019,Procedia Computer Science
Macroeconomic variable selection for creditor recovery rates,"We study the relationship between U.S. corporate bond recovery rates and macroeconomic variables used in the credit risk literature. The least absolute shrinkage and selection operator (LASSO) is used in selecting macroeconomic variables. The LASSO-selected macroeconomic variables are considered to be explanatory variables in ordinary least squares regressions, bootstrap aggregating (bagging), regression trees, boosting, LASSO, ridge regression and support vector regression techniques. We compare the out-of-sample predictive power of two types of models (LASSO-selected models with models that add principal components derived from 179 macroeconomic variables as explanatory variables). We find the recovery models with LASSO-selected macroeconomic variables outperform suggested models in the literature.",2018,Journal of Banking and Finance
Effect of m6A RNA Methylation Regulators on Malignant Progression and Prognosis in Renal Clear Cell Carcinoma,"Objectives: This study aims to explore the roles of 13 m6A RNA methylation regulators in clear cell renal cell carcinoma (ccRCC), and identify a risk signature and prognostic values of m6A RNA methylation regulators in ccRCC. Materials and Methods: RNA sequence data of ccRCC was obtained from The Cancer Genome Atlas (TCGA) database. Differentially expressed of 13 m6A RNA methylation regulators in ccRCC stratified by different clinicopathological characteristics were unveiled using â€œlimmaâ€ package in R version 3.6.0. Cox regression and LASSO analyses were conducted to identify the powerful independent prognostic factors in ccRCC associated with overall survival (OS). Protein-protein interaction (PPI) network and correlation analyses of the 13 m6A RNA methylation regulators were performed using â€œSTRINGâ€ and R package, respectively. Principal component analysis (PCA) was also done using R. In addition, gene ontology (GO), GSEA and Kyoto Encyclopedia of Genes and Genomes pathways were used to functionally annotate the differentially expressed genes in different subgroups. Results: Most of the 13 m6A RNA methylation regulators are differentially expressed in ccRCC tissue samples stratified by different clinicopathological characteristics in 537 patients. Next, a risk signature for predicting prognosis of ccRCC patients, was established based on two powerful independent prognostic m6A RNA methylation regulators (METTL14 and METTL3). Then, two subgroups (cluster1 and 2) were identified by consensus clustering to the two powerful independent factors and the cluster1 had a poorer prognosis than cluster2. Furthermore, the genes in cluster1 were significantly enriched in cancer-related pathways, biological process, and hallmarks, including â€œcell adhesion molecules (CAMs),â€ â€œleukocyte migration,â€ â€œWnt/Î²-catenin signaling,â€ and so on. Conclusion: M6A RNA methylation regulators play important roles in the initiation and progression of ccRCC and provide a novel sight to understand m6A RNA modification in ccRCC.",2020,Frontiers in Oncology
Problem set up and motivation,"Today, we analyze an application of random projection to compute approximate solutions of constrained least-squares problems. This method is often referred to as sketched least-squares. Suppose that we are given an observation vector y âˆˆ R n and matrix A âˆˆ R nÃ—d , and that for some convex set C âŠ‚ R d , we would like to compute the constrained least-squares solution x LS : = argmin xâˆˆC 1 2 y âˆ’ Ax 2 2 : =f (x). In general, this solution may not be unique, but we assume throughout this lecture that uniqueness holds (so that n â‰¥ d necessarily). Different versions of the constrained least-squares problem arise in many applications: â€¢ In the simplest case of an unconstrained problem (C = R d), it corresponds to the usual least-squares estimator, which has been widely studied. Most past work on sketching least-squares has focused on this case. â€¢ When C is a scaled form of the 1-ballâ€”that is, C = {x âˆˆ R d | x 1 â‰¤ R}â€”then the constrained problem is known as the Lasso. It is widely used for estimating sparse regression vectors. â€¢ The support vector machine for classification, when solved in its dual form, leads to a least-squares problem over a polytope C. Problems of the form (3.1) can also arise as intermediate steps of using Newton's method to solve a constrained optimization problem. The original problem can be difficult to solve if the first matrix dimension n is too large. Thus, in order to reduce both storage and computation requirements, a natural idea is to randomly project the original data to a lower-dimensional space. In particular, given a random sketch matrix S âˆˆ R mÃ—n , consider the sketched least-squares problem x : = argmin xâˆˆC 1 2 S(y âˆ’ Ax) 2 2 : =g(x) .",2015,
A Distributed ADMM Approach for Collaborative Regression Learning in Edge Computing,"With the recent proliferation of Internet-of-Things (IoT), enormous amount of data are produced by wireless sensors and connected devices at the edge of network. Conventional cloud computing raises serious concerns on communication latency, bandwidth cost, and data privacy. To address these issues, edge computing has been introduced as a new paradigm that allows computation and analysis to be performed in close proximity with data sources. In this paper, we study how to conduct regression analysis when the training samples are kept private at source devices. Specifically, we consider the lasso regression model that has been widely adopted for prediction and forecasting based on information gathered from sensors. By adopting the Alternating Direction Method of Multipliers (ADMM), we decompose the original regression problem into a set of subproblems, each of which can be solved by an IoT device using its local data information. During the iterative solving process, the participating device only needs to provide some intermediate results to the edge server for lasso training. Extensive experiments based on two datasets are conducted to demonstrate the efficacy and efficiency of our proposed scheme.",2019,Cmc-computers Materials & Continua
"Intrinsic dissolution rate and solubility studies on josamycin, a macrolide antibiotic","Abstract The effect of pH on the intrinsic dissolution rate and solubility of josamycin, a macrolide antibiotic, has been investigated to determine the possible effects of the gastro-intestinal pH on absorption. The intrinsic dissolution rate ( G ) was determined in various dissolution media over a pH range of 1.2â€“7.5 using a disc rotated at 50, 100, 200 and 300 rpm at each pH. The intrinsic dissolution rate at infinite rotation speed ( G âˆž ) was determined using an extrapolation procedure described previously (Nicklasson and Brodin, Acta Pharm. Suec. , 19 (1982) 109â€“118). A plot of log G âˆž vs time was linear (linear regression equation y = 1.4288 âˆ’ 0.6007 x , correlation coeffiecient = 0.9904) with values of G âˆž ranging by a factor of > 8500 from a maximum of 5.16 mg cm âˆ’2 s âˆ’1 at pH 1.2 to 5.81 Ã— 10 âˆ’4 mg cm âˆ’2 s âˆ’1 at pH 7.5. Furthermore, comparison of G âˆž values with limits suggested by Kaplan ( Drug Metab. Rev. , 1 (1972) 15â€“34) indicates that the absorption of josamycin could be dissolution rate-limited from an environment of pH 5.4 to 7.0, and is highly likely to be dissolution rate-limited from intestinal fluid at pH values above 7.0. The solubility of josamycin was equally dependent on pH and ranged from 212 mg ml âˆ’1 (21%) at pH 5.45 to 0.18 mg ml âˆ’1 (0.018%) at pH 9.0. Josamycin has a solubility of approx. 1% at pH 6.0 and decreases with increasing pH to approx. 0.019% at pH 8.5. This suggests that the absorption of josamycin may also be solubility rate-limited particularly from an intestinal environment of pH 6.0 and above.",1992,International Journal of Pharmaceutics
"Tutz Gerhard and Schmid Matthias, 2016, Modeling Discrete Time-to-Event Data, Springer Series in Statistics, Springer, 247 p.","This work deals primarily with analysis of survival data when a discrete-time scale is used. Books on this type of data usually draw on a more general continuous data framework and tested and confirmed models (for example, Coxâ€™s proportional hazard model). However, in many cases, particularly in the social sciences, data are collected and observed discretely. The authors review the main types of processing classically used in survival analysis (exploratory data analysis, regression models), providing a broad panorama of conceptual schemata, including multiple outcomes models (analyses of different exits from unemployment, for example) and repeated events models (the birth of a new child in fertility histories, for example). They also mention methodologies seldom presented in connection with this type of data: algorithmic segmentation techniques (CART models) and variable selection machine learning techniques (boosting). Chapter 1 presents the main concepts and several data sets that use the discrete framework and lists the advantages of methodologies developed for it: the tied events problem that arises with several proportional hazards models, use of the general linear model, simplified interpretation of the hazard ratio (instantaneous risk). Chapter 2 discusses widely used nonparametric types of analysis, lengthof-stay tables and Kaplan-Meier analyses. The mathematic formulas are clearly explained, as are the indicators selected from demographic length-of-stay tables. Bibliographical references, functions, and packages to use with R programs are included after each section, together with exercises that apply the chapterâ€™s main concepts. Chapter 3 reviews the different types of regression models, each applied to an example, and discusses them critically. It includes a comparison with the continuous framework, together with discussions on using the Cox model. The case of time-varying covariates is also discussed in detail. But it is surprising to see no reference here to Paul D. Allisonâ€™s work. Chapter 4, a comprehensive continuation of Chapter 3, presents various tools for testing predictor significance or a modelâ€™s predictive performance. Chapter 5 further extends this presentation, applying more complex models whose effect on risk predictors may not be linear. Chapter 6 raises the question of possible interactions between predictor variables and introduces segmentation techniques (regression trees, CART method) for processing survival data collected in discrete time. Various result stabilization techniques (bagging, random forests) are explained and adapted to survival data. Chapter 7 discusses the question of variable selection (lasso method, boosting). book Reviews",2018,Population
Hour-ahead solar PV power forecasting using SVR based approach,"The use of solar photovoltaic (PV) in power generation has grown in the last decade. Unlike the traditional power generation methods (i.e. oil and gas), the solar output power is fluctuating and uncertain, mainly due to clouds movement and other weather factors. Therefore, in order to have a stable power grid, the electricity utilities need to forecast the solar output power, so they can prepare ahead adequately. In this work, hour-ahead solar PV power forecasting is performed using Support Vector Regression (SVR), Polynomial Regression and Lasso. The implemented regression models were tested under different feature selection schemes. These features include weather conditions (i.e. sky condition, temperature, etc.), power generated in the last few hours, day and time information. Based on the comparative results obtained, the SVR forecasting model outperforms the other two models in terms of accuracy.",2017,2017 IEEE Power & Energy Society Innovative Smart Grid Technologies Conference (ISGT)
Dynamic contrast-enhanced MRI-based biomarkers of therapeutic response in triple-negative breast cancer,"To cite: Golden DI, Lipson JA, Telli ML, et al. J Am Med Inform Assoc 2013;20:1059â€“1066. ABSTRACT Objective To predict the response of breast cancer patients to neoadjuvant chemotherapy (NAC) using features derived from dynamic contrast-enhanced (DCE) MRI. Materials and methods 60 patients with triplenegative early-stage breast cancer receiving NAC were evaluated. Features assessed included clinical data, patterns of tumor response to treatment determined by DCEâ€“MRI, MRI breast imaging-reporting and data system descriptors, and quantitative lesion kinetic texture derived from the gray-level co-occurrence matrix (GLCM). All features except for patterns of response were derived before chemotherapy; GLCM features were determined before and after chemotherapy. Treatment response was defined by the presence of residual invasive tumor and/ or positive lymph nodes after chemotherapy. Statistical modeling was performed using Lasso logistic regression. Results Pre-chemotherapy imaging features predicted all measures of response except for residual tumor. Feature sets varied in effectiveness at predicting different definitions of treatment response, but in general, pre-chemotherapy imaging features were able to predict pathological complete response with area under the curve (AUC)=0.68, residual lymph node metastases with AUC=0.84 and residual tumor with lymph node metastases with AUC=0.83. Imaging features assessed after chemotherapy yielded significantly improved model performance over those assessed before chemotherapy for predicting residual tumor, but no other outcomes. Conclusions DCEâ€“MRI features can be used to predict whether triple-negative breast cancer patients will respond to NAC. Models such as the ones presented could help to identify patients not likely to respond to treatment and to direct them towards alternative therapies.",2013,
Shrinkage and Sparse Estimation for High-Dimensional Linear Models,"In this paper, the high-dimensional sparse linear regression model is considered, where the overall number of variables is larger than the number of observations. Many penalized regularization approaches including LASSO, group LASSO, and Elastic-Net, typically focus on selecting variables with strong effects. This may result in biased prediction, especially when weak signals outnumber strong signals. To solve this problem, we incorporate weak signals in variable selection and estimation. We propose a two-stage procedure, consisting of variable selection and post-selection estimation. The variable selection is done using the LASSO and Elastic-Net penalties to detect weak signals, whereas the post-selection estimation involves by shrinking a post-selection weighted ridge estimator in the direction of a selected candidate subset from the first stage. Monte-Carlo simulation experiment is conducted to evaluate the performance of each estimator in terms of the relative mean squared error. As a particular example, we apply the proposed method to analyze the GDP growth data.",2019,
"Identification of 102 Correlations between Serum Metabolites and Habitual Diet in a Metabolomics Study of the Prostate, Lung, Colorectal, and Ovarian Cancer Trial.","BACKGROUND
Metabolomics has proven useful for detecting objective biomarkers of diet that may help to improve dietary measurement. Studies to date, however, have focused on a relatively narrow set of lipid classes.


OBJECTIVE
The aim of this study was to uncover candidate dietary biomarkers by identifying serum metabolites correlated with self-reported diet, particularly metabolites in underinvestigated lipid classes, e.g. triglycerides and plasmalogens.


METHODS
We assessed dietary questionnaire data and serum metabolite correlations from 491 male and female participants aged 55-75 y in an exploratory cross-sectional study within the Prostate, Lung, Colorectal, and Ovarian Cancer Screening Trial (PLCO). Self-reported intake was categorized into 50 foods, food groups, beverages, and supplements. We examined 522 identified metabolites using 2 metabolomics platforms (Broad Institute and Massachusetts General Hospital). Correlations were identified using partial Pearson's correlations adjusted for age, sex, BMI, smoking status, study site, and total energy intake [Bonferroni-corrected level of 0.05/(50Â Ã—Â 522) = 1.9Â Ã—Â 10-6]. We assessed prediction of dietary intake by multiple-metabolite linear models with the use of 10-fold crossvalidation least absolute shrinkage and selection operator (LASSO) regression.


RESULTS
Eighteen foods, beverages, and supplements were correlated with â‰¥1 serum metabolite at the Bonferroni-corrected significance threshold, for a total of 102 correlations. Of these, only 5 have been reported previously, to our knowledge. Our strongest correlations were between citrus and proline betaine (rÂ =Â 0.55), supplements and pantothenic acid (rÂ =Â 0.46), and fish and C40:9Â phosphatidylcholine (PC) (rÂ =Â 0.35). The multivariate analysis similarly found reasonably large correlations between metabolite profiles and citrus (rÂ =Â 0.59), supplements (rÂ =Â 0.57), and fish (rÂ =Â 0.44).


CONCLUSIONS
Our study of PLCO participants identified many novel food-metabolite associations and replicated 5 previous associations. These candidate biomarkers of diet may help to complement measures of self-reported diet in nutritional epidemiology studies, though further validation work is still needed.",2019,The Journal of nutrition
Regularization in Finite-Sample System Identification,"Finite-sample system identification (FSID) methods infer properties of stochastic dynamical systems under minimal distributional assumptions; typically they build confidence regions with rigorous non-asymptotic guarantees. Similarly to bootstrap and Monte Carlo approaches, they generate alternative samples based on some mild regularities of the random elements of the system. An arch-typical example of such regularities is the case, when the noise sequence has a jointly symmetric distribution about zero. Sign-Perturbed Sums and, its generalizations, Data Perturbation (DP) methods are recently developed FSID algorithms that can construct exact confidence regions for finite samples. They have a number of additional favorable properties, e.g., the confidence sets of SPS for linear regression problems are star convex with the least-squares estimate as a star center, as well as they are strongly consistent, meaning that the regions shrink around the true parameter, and asymptotically cannot contain false parameters (w.p.1). Regularization is an important tool in regression which helps, for example, to handle ill-posed and ill-conditioned problems, reduce over-fitting, enforce sparsity, and in general to control the shape and smoothness of the regression function. The talk will address ways to incorporate regularization techniques to FSID constructions, from standard approaches like Tikhonov regularization (ridge regression), LASSO (least absolute shrinkage and selection operator), and elastic nets to regularization with suitably chosen Hilbert space norms, which also have important applications in machine learning.",2018,
Distributionally Robust Groupwise Regularization Estimator,"Regularized estimators in the context of group variables have been applied successfully in model and feature selection in order to preserve interpretability. We formulate a Distributionally Robust Optimization (DRO) problem which recovers popular estimators, such as Group Square Root Lasso (GSRL). Our DRO formulation allows us to interpret GSRL as a game, in which we learn a regression parameter while an adversary chooses a perturbation of the data. We wish to pick the parameter to minimize the expected loss under any plausible model chosen by the adversary - who, on the other hand, wishes to increase the expected loss. The regularization parameter turns out to be precisely determined by the amount of perturbation on the training data allowed by the adversary. In this paper, we introduce a data-driven (statistical) criterion for the optimal choice of regularization, which we evaluate asymptotically, in closed form, as the size of the training set increases. Our easy-to-evaluate regularization formula is compared against cross-validation, showing good (sometimes superior) performance.",2017,
Survival data.,"During recent years, penalized likelihood approaches have attracted a lot of interest both in the area of semiparametric regression and for the regularization of high-dimensional regression models. In this paper, we introduce a Bayesian formulation that allows to combine both aspects into a joint regression model with a focus on hazard regression for survival times. While Bayesian penalized splines form the basis for estimating nonparametric and flexible time-varying effects, regularization of highdimensional covariate vectors is based on scale mixture of normals priors. This class of priors allows to keep a (conditional) Gaussian prior for regression coefficients on the predictor stage of the model but introduces suitable mixture distributions for the Gaussian variance to achieve regularization. This scale mixture property allows to device general and adaptive Markov chain Monte Carlo simulation algorithms for fitting a variety of hazard regression models. In particular, unifying algorithms based on iteratively weighted least squares proposals can be employed both for regularization and penalized semiparametric function estimation. Since sampling based estimates do no longer have the variable selection property well-known for the Lasso in frequentist analyses, we additionally consider spike and slab priors that introduce a further mixing stage that allows to separate between influential and redundant parameters. We demonstrate the different shrinkage properties with three simulation settings and apply the methods to the PBC Liver dataset.",1995,Archives of disease in childhood
Natural Language-based Machine Learning Models for the Annotation of Clinical Radiology Reports.,"Purpose To compare different methods for generating features from radiology reports and to develop a method to automatically identify findings in these reports. Materials and Methods In this study, 96 303 head computed tomography (CT) reports were obtained. The linguistic complexity of these reports was compared with that of alternative corpora. Head CT reports were preprocessed, and machine-analyzable features were constructed by using bag-of-words (BOW), word embedding, and Latent Dirichlet allocation-based approaches. Ultimately, 1004 head CT reports were manually labeled for findings of interest by physicians, and a subset of these were deemed critical findings. Lasso logistic regression was used to train models for physician-assigned labels on 602 of 1004 head CT reports (60%) using the constructed features, and the performance of these models was validated on a held-out 402 of 1004 reports (40%). Models were scored by area under the receiver operating characteristic curve (AUC), and aggregate AUC statistics were reported for (a) all labels, (b) critical labels, and (c) the presence of any critical finding in a report. Sensitivity, specificity, accuracy, and F1 score were reported for the best performing model's (a) predictions of all labels and (b) identification of reports containing critical findings. Results The best-performing model (BOW with unigrams, bigrams, and trigrams plus average word embeddings vector) had a held-out AUC of 0.966 for identifying the presence of any critical head CT finding and an average 0.957 AUC across all head CT findings. Sensitivity and specificity for identifying the presence of any critical finding were 92.59% (175 of 189) and 89.67% (191 of 213), respectively. Average sensitivity and specificity across all findings were 90.25% (1898 of 2103) and 91.72% (18 351 of 20 007), respectively. Simpler BOW methods achieved results competitive with those of more sophisticated approaches, with an average AUC for presence of any critical finding of 0.951 for unigram BOW versus 0.966 for the best-performing model. The Yule I of the head CT corpus was 34, markedly lower than that of the Reuters corpus (at 103) or I2B2 discharge summaries (at 271), indicating lower linguistic complexity. Conclusion Automated methods can be used to identify findings in radiology reports. The success of this approach benefits from the standardized language of these reports. With this method, a large labeled corpus can be generated for applications such as deep learning. Â© RSNA, 2018 Online supplemental material is available for this article.",2018,Radiology
Variational Bayesian Complex Network Reconstruction,"Complex network reconstruction is a hot topic in many fields. A popular data-driven reconstruction framework is based on lasso. However, it is found that, in the presence of noise, it may be inefficient for lasso to determine the network topology. This paper builds a new framework to cope with this problem. The key idea is to employ a series of linear regression problems to model the relationship between network nodes, and then to use an efficient variational Bayesian method to infer the unknown coefficients. Based on the obtained information, the network is finally reconstructed by determining whether two nodes connect with each other or not. The numerical experiments conducted with both synthetic and real data demonstrate that the new method outperforms lasso with regard to both reconstruction accuracy and running speed.",2018,ArXiv
Reducing Sampling Ratios Improves Bagging in Sparse Regression.,"Bagging, a powerful ensemble method from machine learning, improves the performance of unstable predictors. Although the power of Bagging has been shown mostly in classification problems, we demonstrate the success of employing Bagging in sparse regression over the baseline method (L1 minimization). The framework employs the generalized version of the original Bagging with various bootstrap ratios. The performance limits associated with different choices of bootstrap sampling ratio L/m and number of estimates K is analyzed theoretically. Simulation shows that the proposed method yields state-of-the-art recovery performance, outperforming L1 minimization and Bolasso in the challenging case of low levels of measurements. A lower L/m ratio (60% - 90%) leads to better performance, especially with a small number of measurements. With the reduced sampling rate, SNR improves over the original Bagging by up to 24%. With a properly chosen sampling ratio, a reasonably small number of estimates K = 30 gives satisfying result, even though increasing K is discovered to always improve or at least maintain the performance.",2018,arXiv: Machine Learning
Inherited thrombocytopenia and Occam's razor.,"Background: In the phase 3 ENDEAVOR trial, treatment with carfilzomib administered at 56 mg/m2 twice weekly in combination with dexamethasone (Kd56) significantly improved progression-free survival (PFS) compared to treatment with bortezomib and dexamethasone (Vd) in patients with relapsed or refractory multiple myeloma (RRMM) (Dimopoulos MA, et al. Lancet Oncol . 2016;17:27-38). In this substudy of ENDEAVOR, we used whole transcriptome RNA sequencing (RNA-seq) to identify genes whose baseline expression levels in CD138+ cells were predictive of PFS in patients treated with Kd56 or Vd. The objective of this study was to develop a genomic classifier that could be used to stratify patients for benefit with Kd56 or Vd therapy. Methods: Patients were randomized to receive Kd56 or Vd at a 1:1 ratio. Patients who consented for this biomarker study and provided samples (Kd56, n = 155; Vd, n = 148) were included. CD138+ cells were isolated from bone marrow aspirate collected at baseline. Sequencing libraries for isolated RNA samples were prepared using an Illumina TruSeq RNA library construction kit and sequenced on an Illumina HiSeq 2500 platform. Sequencing reads were aligned against the human reference genome GRCh38 using STAR RNA-seq aligner and annotated with GENCODE v24 at the gene level. Expression counts were estimated using RSEM software and converted to counts per million for subsequent analyses using the edgeR package. Cox proportional hazard regression analysis with LASSO was used to model the relationship between patients9 baseline gene expression and PFS. A classifier was established and its predictive performance was assessed using the cross-validation scheme outlined by Simon et al (Brief Bioinform . 2011;12:203-214). The statistical significance of the cross-validated Kaplan-Meier curves and corresponding log-rank statistic was estimated by generating an approximate null distribution of the cross-validated log-rank statistic through 500 random permutations. For each permutation, the patients9 baseline gene expression profiles and treatment assignments were randomly re-shuffled against patients9 survival times and event indicators, and the same cross-validation procedures used in the model performance assessment were repeated to compute the cross-validated log-rank statistic for the permuted data. Results: Among the 303 Kd56 or Vd patients included in this biomarker study, patients in the Kd56 arm had a 58% reduced risk of progression or death compared with patients in the Vd arm (hazard ratio [HR]: 0.42; 95% confidence interval [CI]: 0.30-0.59; P= 4.5 x 10-7). We developed a linearized classifier using patients9 baseline gene expression (n = 303) to stratify patients for PFS benefit from Kd56 or Vd therapy. The cross-validated Kaplan-Meier curves and log-rank statistic for the classifier were statistically significant at P Conclusions: We identified a classifier with a set of genes whose baseline expression could potentially be used to stratify RRMM patients for greater treatment benefit with Kd56. As only one patient cohort was used for this study, the classifier identified here should be validated in prospective studies and with independent sets of patient cohorts. Further study of this group of genes may provide additional insights into the biology of multiple myeloma and how mechanism of action differs between carfilzomib and bortezomib. Disclosures Pelham: Amgen: Employment, Equity Ownership. Hu: Amgen: Employment, Equity Ownership. Moreau: Novartis: Consultancy, Honoraria; Celgene: Consultancy, Honoraria; Millennium: Consultancy, Honoraria; Bristol-Myers Squibb: Honoraria; Amgen: Honoraria; Takeda: Honoraria; Janssen: Consultancy, Honoraria; Celgene, Janssen, Takeda, Novartis, Amgen, Roche: Membership on an entity9s Board of Directors or advisory committees; Onyx Pharmaceutical: Consultancy, Honoraria. Oriol: Amgen: Consultancy, Honoraria, Membership on an entity9s Board of Directors or advisory committees, Other: sponsored symposia, Speakers Bureau; Celgene: Speakers Bureau; Takeda: Consultancy, Honoraria, Membership on an entity9s Board of Directors or advisory committees, Other: sponsored symposia; Janssen: Consultancy, Honoraria, Membership on an entity9s Board of Directors or advisory committees, Other: sponsored symposia, Speakers Bureau. Quach: Novartis: Honoraria, Membership on an entity9s Board of Directors or advisory committees; Celgene: Honoraria, Membership on an entity9s Board of Directors or advisory committees, Research Funding; Janssen: Honoraria, Membership on an entity9s Board of Directors or advisory committees; Amgen: Honoraria, Membership on an entity9s Board of Directors or advisory committees, Research Funding; BMS: Honoraria; Takeda: Honoraria. Kovacsovics: Seattle Genetics: Research Funding; Celgene: Consultancy; Flexus: Research Funding. Keats: Amgen: Research Funding. Feng: Amgen: Employment, Equity Ownership. Kimball: Amgen: Employment, Equity Ownership. Dimopoulos: Novartis: Consultancy, Honoraria; Amgen Inc, Celgene Corporation, Janssen Biotech Inc, Onyx Pharmaceuticals, an Amgen subsidiary, Takeda Oncology: Consultancy, Honoraria, Other: Advisory Committee: Amgen Inc, Celgene Corporation, Janssen Biotech Inc, Onyx Pharmaceuticals, an Amgen subsidiary, Takeda Oncology; Genesis Pharma: Research Funding.",2017,Blood
Sparse Locality Preserving Embedding,"Principal component analysis (PCA) is an unsupervised linear method of variables technique used in data compression, classification, and visualization[1]. The essence of PCA is to extract principal components, linear combinations of input variables that together best account for the variance in a data set. Linear discriminant analysis (LDA) is a favored tool for supervised linear classification in many areas because of its simplicity and robustness[2]. The goal of LDA is to provide low dimensional projections of data onto the most discriminative directions. Locality preserving projection (LPP) is a recently proposed method[3], which can be regarded as the linearization of Laplacian EigenMap[4]. When applied to face recognition tasks, LPP is also called LaplacianFaces. The idea behind LPP is that it considers the manifold structure of the data set, and preserves the locality of data in the embedding space. LPP has shown the superiority in terms of image indexing and face recognition. One of the major disadvantages of these three methods is that the derived projections are linear combinations of all the original features. Hence the learned results are difficult to interpret. In this paper, we propose a novel algorithm, called sparse locality preserving embedding (SpLPE), which is based on lasso regression framework for learning sparse projections by incorporating c 1 penalty with conventional locality preserving projections. The affinity graph constructed in LPP encodes both discriminant and geometrical structure in the data[3]. Once the Laplacian matrix is computed, we recast the generalized eigenvalue problem of LPP in the lasso regression framework to obtain sparse basis functions. The proposed SpLPE is a combination of locality preserving with sparsity. Additionally, our algorithm can be performed in either supervised or unsupervised mode.",2009,2009 2nd International Congress on Image and Signal Processing
"A Flexible, Interpretable, and Accurate Approach for Imputing the Expression of Unmeasured Genes","While there are >2 million publicly-available human microarray gene-expression profiles, these profiles were measured using a variety of platforms that each cover a pre-defined, limited set of genes. Therefore, key to reanalyzing and integrating this massive data collection are methods that can computationally reconstitute the complete transcriptome in partially-measured microarray samples by imputing the expression of unmeasured genes. Current state-of-the-art imputation methods are tailored to samples from a specific platform and rely on gene-gene relationships regardless of the biological context of the target sample. We show that sparse regression models that capture sample-sample relationships (termed SampleLASSO), built on-the-fly for each new target sample to be imputed, outperform models based on fixed gene relationships. Extensive evaluation involving three machine learning algorithms (LASSO, k-nearest-neighbors, and deep-neural-networks), two gene subsets (GPL96-570 and LINCS), and three imputation tasks (within and across microarray/RNA-seq) establishes that SampleLASSO is the most accurate model. Additionally, we demonstrate the biological interpretability of this method by showing that, for imputing a target sample from a certain tissue, SampleLASSO automatically leverages training samples from the same tissue. Thus, SampleLASSO is a simple, yet powerful and flexible approach for harmonizing large-scale gene-expression data.",2020,bioRxiv
Using LASSO to estimate marker effects for Genomic Selection,"Here we suggest a least absolute shrinkage and selection operator (LASSO) approach to estimate the marker effects for genomic selection using the least angle regression (LARS) algorithm, modified to include a crossâ€“validation step to define the best subset of markers to involve in the model. The LASSO-LARS was tested on simulated data which consisted of 5,865 individuals and 6,000 SNPs. The last generations of this dataset were the selection candidates. Using only animals from generations prior to the candidates, three approaches to splitting the population into training and validation sets for cross-validation were evaluated. Furthermore, different sizes of the validation sample were tested. Moreover, BLUP and Bayesian methods were carried out for comparison. The most reliable cross-validation method was the random splitting of overall population with a validation sample size of 50% of the reference population. The accuracy of the GEBVs (correlation with true breeding values) in the candidate population obtained by LASSO-LARS was 0.89 with 156 explanatory SNPs. This value was higher then those obtained by using BLUP and Bayesian methods, which were 0.75 and 0.84 respectively. It was concluded that LASSO-LARS approach is a good alternative way to estimate markers effects for genomic selection.",2009,Italian Journal of Animal Science
A SIPSS-Lasso-BPNN scheme for online voltage stability assessment,"Load active power at the voltage collapse point (PLL) is a useful index for online voltage stability assessment. This paper proposes a SIPSS-Lasso-BPNN scheme to offline fitting and online forecasting the load active power at the voltage collapse point PLL. The scheme consists of a SIPSS (Similarity Index of Power System State) based screening method, a Lasso (Least absolute shrinkage and select operator) method and a back propagation neural network (BPNN). The SIPSS based screening method screens the training samples according to their similarity indexes of power system states. The Lasso method selects the principal input features which are most explanatory to PLL via the shrunken regression analysis. The training samples are reduced by the above two methods. The BPNN is used to offline fit and online forecast the PLL through the reduced training samples. The test results on the New England 39 bus system shows that the SIPSS-Lasso-BPNN scheme can significantly improve the efficiency of BPNN offline training and guarantee the forecasting accuracy.",2014,2014 IEEE PES General Meeting | Conference & Exposition
What is in the news on a subject : automatic and sparse summarization of large document corpora,"News media play a significant role in our political and daily lives. The traditional approach in media analysis to news summarization is labor intensive. As the amount of news data grows rapidly, the need is acute for automatic and scalable methods to aid media analysis researchers so that they could screen corpora of news articles very quickly before detailed reading. In this paper we propose a general framework for subject-specific summarization of document corpora with news articles as a special case. We use the state-of-the art scalable and sparse statistical predictive framework to generate a list of short words/phrases as a summary of a subject. In particular, for a particular subject of interest (e.g., China), we first create a list of words/phrases to represent this subject (e.g., China, Chinas, and Chinese) and then create automatic labels for each document depending on the appearance pattern of this list in the document. The predictor vector is then high dimensional and contains counts of the rest of the words/phrases in the documents excluding phrases overlapping the subject list. Moreover, we consider several preprocessing schemes, including document unit choice, labeling scheme, tf-idf representation and L2 normalization, to prepare the text data before applying the sparse predictive framework. We examined four different scalable feature selection methods for summary list generation: phrase Co-occurrence, phrase correlation, L1-regularized logistic regression (L1LR), and L1-regularized linear regression (Lasso). We carefully designed and conducted a human survey to compare the different summarizers with human understanding based on news * Miratrix and Jia are co-first authors.",2011,
Big data analytics: integrating penalty strategies,"We present efficient estimation and prediction strategies for the classical multiple regression model when the dimensions of the parameters are larger than the number of observations. These strategies are motivated by penalty estimation and Stein-type estimation procedures. More specifically, we consider the estimation of regression parameters in sparse linear models when some of the predictors may have a very weak influence on the response of interest. In a high-dimensional situation, a number of existing variable selection techniques exists. However, they yield different subset models and may have different numbers of predictors. Generally speaking, the least absolute shrinkage and selection operator (Lasso) approach produces an over-fitted model compared with its competitors, namely the smoothly clipped absolute deviation (SCAD) method and adaptive Lasso (aLasso). Thus, prediction based only on a submodel selected by such methods will be subject to selection bias. In order to minimize the inherited bia...",2016,international journal of management science and engineering management
Slope-adaptive Variable Selection via Convex Optimization.,"We introduce a new estimator for the vector of coefficients Î² in the linear model y = XÎ² + z, where X has dimensions n Ã— p with p possibly larger than n. SLOPE, short for Sorted L-One Penalized Estimation, is the solution to [Formula: see text]where Î»1 â‰¥ Î»2 â‰¥ â€¦ â‰¥ Î» p â‰¥ 0 and [Formula: see text] are the decreasing absolute values of the entries of b. This is a convex program and we demonstrate a solution algorithm whose computational complexity is roughly comparable to that of classical â„“1 procedures such as the Lasso. Here, the regularizer is a sorted â„“1 norm, which penalizes the regression coefficients according to their rank: the higher the rank-that is, stronger the signal-the larger the penalty. This is similar to the Benjamini and Hochberg [J. Roy. Statist. Soc. Ser. B57 (1995) 289-300] procedure (BH) which compares more significant p-values with more stringent thresholds. One notable choice of the sequence {Î» i } is given by the BH critical values [Formula: see text], where q âˆˆ (0, 1) and z(Î±) is the quantile of a standard normal distribution. SLOPE aims to provide finite sample guarantees on the selected model; of special interest is the false discovery rate (FDR), defined as the expected proportion of irrelevant regressors among all selected predictors. Under orthogonal designs, SLOPE with Î»BH provably controls FDR at level q. Moreover, it also appears to have appreciable inferential properties under more general designs X while having substantial power, as demonstrated in a series of experiments running on both simulated and real data.",2015,The annals of applied statistics
The Ridge Iterative Regression and the Data-Augmentation Lasso,"We propose the ridge iterative regression (RIR) and the data-augmentation lasso (DAL) to improve the ridge regression and the lasso respectively. We prove that by updating the coefficient itself, the solution of the RIR converges to that of the ordinary least squares when the design matrix \(\varvec{X}\) has full column rank. The simulations and real-world data demonstrate that the DAL often outperforms the lasso in sparsity and estimation accuracy for the coefficient and attains smaller prediction error.",2019,
Evidence-Based Assessment from Simple Clinical Judgments to Statistical Learning: Evaluating a Range of Options Using Pediatric Bipolar Disorder as a Diagnostic Challenge.,"Reliability of clinical diagnoses is often low. There are many algorithms that could improve diagnostic accuracy, and statistical learning is becoming popular. Using pediatric bipolar disorder as a clinically challenging example, we evaluated a series of increasingly complex models ranging from simple screening to a supervised LASSO regression in a large (N=550) academic clinic sample. We then externally validated models in a community clinic (N=511) with the same candidate predictors and semi-structured interview diagnoses, providing high methodological consistency; the clinics also had substantially different demography and referral patterns. Models performed well according to internal validation metrics. Complex models degraded rapidly when externally validated. NaÃ¯ve Bayesian and logistic models concentrating on predictors identified in prior meta-analyses tied or bettered LASSO models when externally validated. Implementing these methods would improve clinical diagnostic performance. Statistical learning research should continue to invest in high quality indicators and diagnoses to supervise model training.",2018,Clinical psychological science : a journal of the Association for Psychological Science
Comparing Penalized Regression Analysis of Logistic Regression Model with Multicollinearity,"The goal of this research is to estimate the parameter of the logistic regression model by penalized regression analysis which consisted of ridge regression, lasso, and elastic net method. The logistic regression is considered between a binary dependent variable and 3 and 5 independent variables. The independent variables are generated from normal distribution, contaminated normal distribution, and t distribution on correlation coefficient at 0.1, 0.5, and 0.99 or called multicollinearity problem. The maximum likelihood estimator has used as the classical method by differential the log likelihood function with respect to the coefficients. Ridge regression is to choose the unknown ridge parameter by cross-validation, so ridge estimator is evaluated by the adding ridge parameter on penalty term. Lasso (least absolute shrinkage and selection operator) is added the penalty term on scales sum of the absolute value of the coefficients. The elastic net can be mixed between ridge regression and lasso on the penalty term. The criterion of these methods is compared by percentage of predicted accuracy value. The results are found that lasso is satisfied when the independent variables are simulated from normal and t distribution in most cases, and the lasso outperforms on the contaminated normal distribution.",2019,
