title,abstract,year,journal
Bayesian adaptive Lasso for quantile regression models with nonignorably missing response data,"Abstract Handling data with the nonignorably missing mechanism is still a challenging problem in statistics. In this paper, we develop a fully Bayesian adaptive Lasso approach for quantile regression models with nonignorably missing response data, where the nonignorable missingness mechanism is specified by a logistic regression model. The proposed method extends the Bayesian Lasso by allowing different penalization parameters for different regression coefficients. Furthermore, a hybrid algorithm that combined the Gibbs sampler and Metropolis-Hastings algorithm is implemented to simulate the parameters from posterior distributions, mainly including regression coefficients, shrinkage coefficients, parameters in the non-ignorable missing models. Finally, some simulation studies and a real example are used to illustrate the proposed methodology.",2019,Communications in Statistics - Simulation and Computation
Unsupervised Pool-Based Active Learning for Linear Regression,"In many real-world machine learning applications, unlabeled data can be easily obtained, but it is very time-consuming and/or expensive to label them. So, it is desirable to be able to select the optimal samples to label, so that a good machine learning model can be trained from a minimum amount of labeled data. Active learning (AL) has been widely used for this purpose. However, most existing AL approaches are supervised: they train an initial model from a small amount of labeled samples, query new samples based on the model, and then update the model iteratively. Few of them have considered the completely unsupervised AL problem, i.e., starting from zero, how to optimally select the very first few samples to label, without knowing any label information at all. This problem is very challenging, as no label information can be utilized. This paper studies unsupervised pool-based AL for linear regression problems. We propose a novel AL approach that considers simultaneously the informativeness, representativeness, and diversity, three essential criteria in AL. Extensive experiments on 14 datasets from various application domains, using three different linear regression models (ridge regression, LASSO, and linear support vector regression), demonstrated the effectiveness of our proposed approach.",2020,ArXiv
The Importance of Ideology varies across Sociocultural Contexts,"Author(s): Chen, Eric Evan | Advisor(s): Ditto, Peter H | Abstract: Although ideology is widely studied, less is known about how it varies across sociocultural contexts. Ideology is an organizing structure for political attitudes in that positions on a core set of political attitudes have been found to be aligned along a liberal-conservative ideological dimension. Some personality-based approaches to political psychology suggest that, because ideology arises from low-level psychological features, the political attitudinal structure of ideology is likely to be consistent across sociocultural contexts. However, the cultural psychology perspective suggests that both low-level psychological features and their higher-level political attitudinal manifestations may differ across cultures. The five studies in this dissertation examined this tension using eight datasets from the General Social Survey, applying linear and logistic regression and lasso regression, and the machine learning techniques of random forest classification and regression and support vector machine classification. Across these studies, the importance of ideology as an organizing structure varied across sociocultural contexts, especially across race, education, and income lines. The associations between ideological self-placement and measures of political attitudes were weaker for those with lower incomes and with no college education, and the associations were almost entirely absent for Black Americans. In addition, this dissertation examined other ways that political concerns are prioritized, beyond ideology.",2017,
High-dimensional generalized linear models and the lasso,"We consider high-dimensional generalized linear models with Lipschitz loss functions, and prove a nonasymptotic oracle inequality for the empirical risk minimizer with Lasso penalty. The penalty is based on the coefficients in the linear predictor, after normalization with the empirical norm. The examples include logistic regression, density estimation and classification with hinge loss. Least squares regression is also discussed.",2008,Annals of Statistics
FRM: a Financial Risk Meter based on penalizing tail events occurrence,"In this paper we propose a new measure for systemic risk: the Financial Risk Meter (FRM). This measure is based on the penalization parameter () of a linear quantile lasso regression. The FRM is calculated by taking the average of the penalization parameters over the 100 largest US publicly traded financial institutions. We demonstrate the suitability of this risk measure by comparing the proposed FRM to other measures for systemic risk, such as VIX, SRISK and Google Trends. We find that mutual Granger causality exists between the FRM and these measures, which indicates the validity of the FRM as a systemic risk measure. The implementation of this project is carried out using parallel computing, the codes are published on www.quantlet.de with keyword FRM. The R package RiskAnalytics is another tool with the purpose of integrating and facilitating the research, calculation and analysis methods around the FRM project. The visualization and the up-to-date FRM can be found on http://frm.wiwi.hu-berlin.de.",2017,
Estimating cognitive workload while driving using functional near infrared spectroscopy (fNIRS),"Introduction and Aim: We envision that driver assistive systems which adapt their functionality to the driverâ€™s cognitive state could be a promising approach to reduce road accidents due to human errors [1]. Workload is an important cognitive state because a cognitive overload or underload results in a decrease in human performance which may result in fatal incidents while driving. Here, we investigate if itâ€™s possible to predict variations of cognitive workload levels (WL) from fNIRS brain activation measurements while driving to inform adaptive assistive systems in future cars. Methods: In our study, we implemented the n-back working memory task with several continuously changing load levels as a speed regulation task into a realistic driving simulation in order to control and manipulate cognitive workload. We introduced five different workload levels (i.e. 0-back to 4-back) and speed signs every 20 seconds while the participant was driving on a highway with concurrent traffic. Depending on the current n-back task, the participant was supposed to remember the previous â€˜nâ€™ speed sequences and adjust his speed accordingly. A detailed explanation for the n-back experimental paradigm can be found in [2]. FNIRS data were recorded from the frontal and parietal cortices using a 32-channel neuroNIRX-system from five participants during the course of the whole experiment which lasted for around 30 minutes. Results: We used the multivariate linear regression approach by combining fNIRS data from all channels to predict WL using the elastic net regularization model which combines the L1 and L2 penalties of the lasso and ridge regression techniques to get a continuous estimate of workload over time. Fig.1 depicts a plot of WL induced by the n-back task (red curve) and the WL predicted by the model (blue curve) for an example participant. The correlation between the two curves is almost 0.8 for a 10-fold cross-validation. For the remaining four participants, we achieved a correlation between 0.6 and 0.8 which were all statistically significant (p < 0.05). 
Fig.1: Ten-fold cross-validated prediction of workload from deoxyhemoglobin fNIRS measurements using multivariate regression analysis in an example participant 
Discussion & Conclusion: It can be seen from Fig.1 that the predicted WL more or less follows the induced WL and could be used to predict if the WL is either increasing or decreasing. There are certain regions in the blue curve where the model seems to over- or underestimate WL which may be due to the incomplete model which currently neglects WL imposed by the concurrent driving task in the changing traffic situations. As a next step, we plan to introduce some non-linearity into the workload model to get much better prediction rates. 
Acknowledgements: This work was funded by a grant of the Volkswagen Foundation and the Ministry of Science and Culture of Lower Saxony to the Research Centre on Critical Systems Engineering for Socio-Technical Systems. Anirudh Unni and Klas Ihme contributed equally. 
 
References 
[1] Parasuraman, R. (1987). Human-computer monitoring. Human Factors, 29, 695-706 
[2] Unni et al. (2015). Brain activity measured with fNIRS for the prediction of cognitive workload. 6th IEEE Conference on Cognitive Infocommunications, 349-354",2016,
Preoperative assessment of lymph node metastasis in clinically node-negative rectal cancer patients based on a nomogram consisting of five clinical factors.,"Background
Currently, reliable approaches for accurate assessment of lymph node metastases (LNM), which is an important indication of preoperative chemoradiotherapy (CRT), are not available for clinically node-negative rectal cancer patients. This study aims to identify clinical factors associated with LNM and to establish a nomogram for LNM prediction in clinically node-negative rectal cancer patients.


Methods
The least absolute shrinkage and selection operator (LASSO) aggression and multivariate logistic regression analyses were applied to identify clinical factors associated with LNM. A nomogram was established to predict the probability of LNM in clinically node-negative rectal cancer patients based on the multivariate logistic regression model.


Results
Six potential risk factors were selected on the basis of LASSO aggression analysis, and five of them were identified as independent risk factors for LNM based on multivariate analysis, including MRI-reported tumor location, clinical T classification, MRI-reported tumor diameter, white blood cell count (WBC), and preoperative elevated tumor markers. A nomogram consisting of the five clinical factors was established and showed good discrimination. Decision curve analysis demonstrated that the established nomogram was reliable and accurate for LNM prediction in clinically node-negative rectal cancer patients.


Conclusions
A nomogram based on five clinical factors, including MRI-reported tumor location, clinical T classification, MRI-reported tumor diameter, WBC, and preoperative elevated tumor markers, are useful for assessing LNM in clinically node-negative rectal cancer patients, which is important for preoperative CRT regimens.",2019,Annals of translational medicine
Group sparse regularization for deep neural networks,"In this paper, we address the challenging task of simultaneously optimizing (i) the weights of a neural network, (ii) the number of neurons for each hidden layer, and (iii) the subset of active input features (i.e., feature selection). While these problems are traditionally dealt with separately, we propose an efficient regularized formulation enabling their simultaneous parallel execution, using standard optimization routines. Specifically, we extend the group Lasso penalty, originally proposed in the linear regression literature, to impose group-level sparsity on the networks connections, where each group is defined as the set of outgoing weights from a unit. Depending on the specific case, the weights can be related to an input variable, to a hidden neuron, or to a bias unit, thus performing simultaneously all the aforementioned tasks in order to obtain a compact network. We carry out an extensive experimental evaluation, in comparison with classical weight decay and Lasso penalties, both on a toy dataset for handwritten digit recognition, and multiple realistic mid-scale classification benchmarks. Comparative results demonstrate the potential of our proposed sparse group Lasso penalty in producing extremely compact networks, with a significantly lower number of input features, with a classification accuracy which is equal or only slightly inferior to standard regularization terms.",2017,ArXiv
A Review of 'Big Data' Variable Selection Procedures For Use in Predictive Modeling,"A REVIEW OF â€˜BIG DATAâ€™ VARIABLE SELECTION PROCEDURES FOR USE IN PREDICTIVE MODELING By Sarah Papke August 2017 Thesis supervised by Dr. Frank Dâ€™Amico. Several problems arise when attempting to use traditional predictive modeling techniques on â€˜big data.â€™ For instance, multiple linear regression models cannot be used on datasets with hundreds of variables. However several techniques are becoming common tools for selective inference as the need for analyzing big data increases. Forward selection and penalized regression models (such as LASSO, Ridge Regression, and Elastic Net) are simple modifications of multiple linear regression that can provide some guidance on simplifying a model through variable selection. Dimension reducing techniques, such as Partial Least Squares and Principal Components Analysis, are more complex than regression but have the ability to handle highly correlated independent variables. Each of the aforeiv mentioned techniques are valuable in predictive modeling if used properly. This paper provides a mathematical introduction to these developments in selective inference. A sample dataset is used to demonstrate modeling and interpretation. Further, the applications to big data, as well as advantages and disadvantages of each procedure, are discussed.",2017,
Geographically Weighted Regression Modeling with Fixed Gaussian Kernel Weighted on Spatial Data (Case Study of Food Security in Tanah Laut District of South Kalimantan),"Geographically Weighted Regression (GWR) is a regression model that takes into account the spatial heterogeneity effect. In regression models, often there is a relationship between two or more predictor variables is called multicollinearity. Geographically Weighted Lasso (GWL) is a method used to overcome spatial and spatial heterogeneity of local multicollinearity. The purpose of this study establishes the model by using the method of GWL in the case of spatial heterogeneity and overcome local multicollinearity on the issue of food insecurity in Tanah Laut district. Generally, food insecurity in Tanah Laut district is affected by the percentage of the population without access to electricity, the average number of store/grocery shop, and percentage of children under five and maternal mortality. GWL models obtained in accordance with the number of observation locations. The results validate the secondary data showed that the model obtained in the study are in accordance with the actual conditions in the field. Models with fixed weighting Gaussian Kernel is able to predict the eight villages with food security conditions are the same as the secondary data.",2014,
Solution of Linear Ill-posed Problems Using Overcomplete Dictionaries,"In this dissertation, we consider an application of overcomplete dictionaries to the solution of general ill-posed linear inverse problems. In the context of regression problems, there has been an enormous amount of effort to recover an unknown function using such dictionaries. While some research on the subject has been already carried out, there are still many gaps to address. In particular, one of the most popular methods, lasso, and its versions, is based on minimizing the empirical likelihood and unfortunately, requires stringent assumptions on the dictionary, the so-called, compatibility conditions. Though compatibility conditions are hard to satisfy, it is well known that this can be accomplished by using random dictionaries. In the first part of the dissertation, we show how one can apply random dictionaries to the solution of ill-posed linear inverse problems with Gaussian noise. We put a theoretical foundation under the suggested methodology and study its performance via simulations and real-data example. In the second part of the dissertation, we investigate application of lasso to the linear ill-posed problems with non-Gaussian noise. We have developed theoretical background for application of lasso to such problems and studied its performance via simulations.",2019,
"FÃ¶das, Ã¥ldras, lÃ¥na","Household debts is interesting for a number of institutions in society. One reason for this is that it gives a picture of how severely a potential economic crisis might affect the country. If a model can be found which precisely explains the household debt levels, it can be used to predict dangerous situations at an earlier stage, so that resources to counteract the situation can be employed before it turns into a large-scale meltdown. This thesis tries to find such a model, by examining whether there is a relation between debt level (measured as the quota between total debt and disposable income) and the natural variation in the number of births each year over a longer period of time. The thesis also examines whether the number of births, when used as explanatory variable, can outperform a model where interest rate is used as explanatory variable instead, with regard to explanatory power. The models used are estimated via application of time series analysis (AR(p)-models) and LASSO regression. The author finds signs indicating that there might be a connection between the number of births and debt levels, but nothing indicates that number of births is a better explanatory variable than for instance interest rate levels. It is also shown that the time series analysis gives the largest contribution in explaining debt levels, and a more rigorous analysis of time series is proposed for potential future studies on the topic. (Less)",2018,
Variable selection in Poisson HGLMs using h-likelihoood,"Selecting relevant variables for a statistical model is very important in regression analysis. Recently, variable selection methods using a penalized likelihood have been widely studied in various regression models. The main advantage of these methods is that they select important variables and estimate the regression coefficients of the covariates, simultaneously. In this paper, we propose a simple procedure based on a penalized h-likelihood (HL) for variable selection in Poisson hierarchical generalized linear models (HGLMs) for correlated count data. For this we consider three penalty functions (LASSO, SCAD and HL), and derive the corresponding variable-selection procedures. The proposed method is illustrated using a practical example.",2015,
Classification of low-grade and high-grade glioma using multi-modal image radiomics features,"Gliomas are primary brain tumors arising from glial cells. Gliomas can be classified into different histopathologic grades according to World Health Oraganization (WHO) grading system which represents malignancy. In this paper, we present a method to predict the grades of Gliomas using Radiomics imaging features. MICCAI Brain Tumor Segmentation Challenge (BRATs 2015) training data, its segmentation ground truth and the ground truth labels were used for this work. 45 radiomics features based on histogram, shape and gray-level co-occurrence matrix (GLCM) were extracted from each FLAIR, T1, T1-Contrast, T2 image to quantify the property of Gliomas. Significant features among 180 features were selected through L1-norm regularization (LASSO). Based on LASSO coefficient and selected feature values, we computed a LASSO score and gliomas were classified into low-grade glimoa (LGG) or high-grade glimoa (HGG) through logistic regression. Classification result was validated by a 10-fold cross validation. Our method achieved accuracy of 0.8981, sensitivity of 0.8889, specificity of 0.9074, and area under the curve (AUC) = 0.8870.",2017,2017 39th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)
Root Cause Diagnosis of Oscillation-Type Plant Faults Using Nonlinear Causality Analysis,"Abstract In industrial plants, productivity and product quality are often impacted by different types of faults. Specifically, oscillations commonly exist in many close-loop controlled processes. An oscillation generated in a single unit may propagate along process flows and feedback loops, affecting the performance of the entire plant. Therefore, it is critical to diagnose such oscillation-type plant faults and find out the root cause, so as to achieve fast recovery from abnormalities. In recent research, Granger causality (GC) test, which uses a statistical hypothesis test to judge whether a time series is useful in forecasting another, has been adopted to discover the root cause of plant-wide oscillations. However, the conventional GC is based on linear autoregressive (AR) models and cannot accurately handle the nonlinear causal relationship between time series. To solve this problem, a two-step diagnosis approach is proposed in this paper. In the first step, the faulty variables are isolated by the least absolute shrinkage and selection operator (LASSO) based reconstruction analysis method. Then, the nonlinear GC test based on Gaussian process regression (GPR) is conducted on the isolated process variables to discover the path of fault propagation and find out the root cause of the fault.",2017,IFAC-PapersOnLine
A Large-Scale Linear Regression Sentiment Model,"This report details the findings in building a large-scale linear regression sentiment model for an Amazon book review corpus. We studied and applied a number of regression and NLP techniques, including Unigram/Bigrams, stop-word removal, ridge and lasso regression. 1. DATA CORPUS The corpus contains 975194 non-distinct book reviews collected from Amazon by Mark Dredze and others at Johns Hopkins. Along side the textual reviews we have numerically scored sentiment data on a scale of 1-5. The goal is to learn a linear regression based sentiment model from the textual reviews which can be applied to future reviews to obtain an estimate of the numerical sentiment score. 2. DATA PROCESSING The original data was represented in XML format. A simple tokenizer had been applied to obtain a flattened representation consisting of a reverse-indexed dictionary matrix, which allowed review-text and sentiment scores can be extracted. We obtained document boundaries by segmenting at token positions corresponding to the <review></review> pair. The textual body of a review is extracted by matching token positions for the <review text></review text> pair. Review titles often contain sentiment charged summaries of the review and hence are extracted and concatenated with the review body. Finally, numerical ratings are obtained by matching token positions bewtween the <rating> </rating> pair. 2.1 Duplicate removal It turns out that many of the reviews in the corpus were duplicates. to remove duplicates, we computed a textual-hash of the first 20 words (or the maximum number of words in the textual-review if less than 20 words) and discarded reviews for which the hash matched a prior review. At the end of this process we obtained 494761 distinct reviews, roughly half the size of the original corpus. 2.2 Featurization We experimented with both bag-of-words and bag-of-bigrams models. Each review is featurized into a multinominal count of words and bigrams. These counts are then normailized via tf-idf scores, computed as: St,d = tft,d Ã— log2 N",2012,
Component-wise AdaBoost Algorithms for High-dimensional Binary Classi fication and Class Probability Prediction,"Freund and Schapire (1997) introduced ""Discrete AdaBoost"" (DAB) which has been mysteriously effective for the high-dimensional binary classi cation or binary prediction. In an effort to understand the myth, Friedman, Hastie and Tibshirani (FHT, 2000) show that DAB can be understood as statistical learning which builds an additive logistic regression model via Newton-like updating minimization of the exponential loss. From this statistical point of view, FHT proposed three modi fications of DAB, namely, Real AdaBoost (RAB), LogitBoost (LB), and Gentle AdaBoost (GAB). All of DAB, RAB, LB, GAB solve for the logistic regression via different algorithmic designs and different objective functions. The RAB algorithm uses class probability estimates to construct real-valued contributions of the weak learner, LB is an adaptive Newton algorithm by stagewise optimization of the Bernoulli likelihood, and GAB is an adaptive Newton algorithm via stagewise optimization of the exponential loss. The same authors of FHT published an influential textbook, The Elements of Statistical Learn- ing (ESL, 2001 and 2008). A companion book An Introduction to Statistical Learning (ISL) by James et al. (2013) was published with applications in R. However, both ESL and ISL (e.g., sections 4.5 and 4.6) do not cover these four AdaBoost algorithms while FHT provided some simulation and empirical studies to compare these methods. Given numerous potential applications, we believe it would be useful to collect the R libraries of these AdaBoost algorithms, as well as more recently developed extensions to Ad- aBoost for probability prediction with examples and illustrations. Therefore, the goal of this chapter is to do just that, i.e., (i) to provide a user guide of these alternative AdaBoost algorithms with step-by-step tutorial of using R (in a way similar to ISL, e.g., Section 4.6), (ii) to compare AdaBoost with alternative machine learning classi fication tools such as the deep neural network (DNN), logistic regression with LASSO and SIM-RODEO, and (iii) to demonstrate the empirical applications in economics, such as prediction of business cycle turning points and directional prediction of stock price indexes. We revisit Ng (2014) who used DAB for prediction of the business cycle turning points by comparing the results from RAB, LB, GAB, DNN, logistic regression and SIM-RODEO.",2018,
Exploring High-Order Functional Interactions via Structurally-Weighted LASSO Models,"A major objective of brain science research is to model and quantify functional interaction patterns among neural networks, in the sense that meaningful interaction patterns reflect the working mechanisms of neural systems and represent their relationships with the external world. Most current research approaches in the neuroimaging field, however, focus on pair-wise functional/effective connectivity and are thus unable to handle high-order, network-scale functional interactions. In this paper, we propose a novel structurally-weighted LASSO (SW-LASSO) regression model to represent the functional interaction among multiple regions of interests (ROIs) based on resting state fMRI (rsfMRI) data. In particular, the structural connectivity constraints derived from diffusion tenor imaging (DTI) data are used to guide the selection of the weights, thus adaptively adjusting the penalty levels of different coefficients which correspond to different ROIs. The robustness and accuracy of our models are evaluated and demonstrated via a series of carefully designed experiments. In an application example, the generated regression graphs show different assortative mixing patterns between Mild Cognitive Impairment (MCI) patients and normal controls (NC). Our results indicate that the proposed model has promising potential to enable the construction of high-order functional networks and their applications in clinical datasets.",2013,Information processing in medical imaging : proceedings of the ... conference
Travel-associated faecal colonisation with extended spectrum Î²-lactamase ( ESBL ) producing Enterobacteriaceae : incidence and risk factors,"Objectives: To study the acquisition of ESBL-producing Enterobacteriaceae (ESBL-PE) among the faecal flora during travel, with a focus on risk factors, antibiotic susceptibility and ESBL-encoding genes. Methods: An observational prospective multicenter cohort study of individuals attending vaccination clinics in Southeast Sweden was performed, in which the submission of faecal samples and questionnaires before and after travelling outside Scandinavia was requested. Faecal samples were screened for ESBL-PE by culturing on ChromID ESBL and an in-house method. ESBL-PE was confirmed by phenotypic and genotypic methods. Susceptibility testing was performed with Etest. Individuals who acquired ESBL-PE during travel (travelassociated (TA) carriers) were compared to non-carriers regarding risk factors, and unadjusted and adjusted odds ratios (OR) after manual stepwise elimination were calculated using logistic regression. Results: Of 262 enrolled individuals, 2.4% were colonised before travel. Among 226 evaluable participants, ESBL-PE was detected in the post-travel samples from 68 (30%) travellers. The most important risk factor in the final model was the geographic area visited: Indian subcontinent (OR: 24.8, p<0.001), Asia (OR: 8.63, p<0.001) and Africa north of the equator (OR: 4.94, p=0.002). Age and gastrointestinal symptoms also affected the risk significantly. Multi-resistance was seen in 77 (66%) of the ESBL-PE isolates, predominantly a combination of reduced susceptibility to third-generation cephalosporins, trimethoprimsulfamethoxazole and aminoglycosides. The most common species and ESBL-encoding gene were E. coli (90%) and CTX-M (73%), respectively. Conclusion: Acquisition of multi-resistant ESBL-PE among the faecal flora during international travel is common. The geographic area visited has the highest impact on ESBL-",2013,
Adaptive Lasso for Cox's proportional hazards model,"We investigate the variable selection problem for Cox's proportional hazards model, and propose a unified model selection and estimation procedure with desired theoretical properties and computational convenience. The new method is based on a penalized log partial likelihood with the adaptively weighted L 1 penalty on regression coefficients, providing what we call the adaptive Lasso estimator. The method incorporates different penalties for different coefficients: unimportant variables receive larger penalties than important ones, so that important variables tend to be retained in the selection process, whereas unimportant variables are more likely to be dropped. Theoretical properties, such as consistency and rate of convergence of the estimator, are studied. We also show that, with proper choice of regularization parameters, the proposed estimator has the oracle properties. The convex optimization nature of the method leads to an efficient algorithm. Both simulated and real examples show that the method performs competitively. Copyright 2007, Oxford University Press.",2007,Biometrika
A new technique to predict fly-rock in bench blasting based on an ensemble of support vector regression and GLMNET,"Fly-rock caused by blasting is one of the dangerous side effects that need to be accurately predicted in open-pit mines. This study proposed a new technique to predict the distance of fly-rock based on an ensemble of support vector regression models (SVRs) and Lasso and elastic-net regularized generalized linear model (GLMNET), called SVRsâ€“GLMNET. It was developed based on a combination of six SVR models and a GLMNET model. Accordingly, the dataset including 210 experimental data was divided into three parts, i.e., training, validating, and testing. Of the whole dataset, 70% was used for the development of the six SVR models first as the sub-models. Subsequently, 20% of the entire dataset (the validating dataset) was used to predict fly-rock based on the six developed SVR models. The predicted results from the six developed SVR models were used as the input variables to establish the GLMNET model (i.e., SVRsâ€“GLMNET model). Finally, the remaining 10% of the dataset was used for testing the performance of the proposed SVRsâ€“GLMNET model. A comparison and evaluation of the six developed SVR models and the proposed SVRsâ€“GLMNET model were implemented based on five statistical criteria, such as mean absolute error (MAE), mean absolute percentage error (MAPE), root-mean-square error (RMSE), variance account for (VAF), and determination of correlation (R2). The results indicated that the proposed SVRsâ€“GLMNET model provided the most dominant performance in predicting the distance of fly-rock caused by bench blasting in this study with an RMSE of 3.737, R2 of 0.993, MAE of 3.214, MAPE of 0.018, and VAF of 99.207. Whereas, the other models yielded poorer accuracy with RMSE of 7.058â€“12.779, R2 of 0.920â€“0.972, MAE of 3.438â€“7.848, MAPE of 0.021â€“0.055, and VAF of 90.538â€“97.003.",2019,Engineering with Computers
Bioprocess data mining using regularized regression and random forests,"BackgroundIn bioprocess development, the needs of data analysis include (1) getting overview to existing data sets, (2) identifying primary control parameters, (3) determining a useful control direction, and (4) planning future experiments. In particular, the integration of multiple data sets causes that these needs cannot be properly addressed by regression models that assume linear input-output relationship or unimodality of the response function. Regularized regression and random forests, on the other hand, have several properties that may appear important in this context. They are capable, e.g., in handling small number of samples with respect to the number of variables, feature selection, and the visualization of response surfaces in order to present the prediction results in an illustrative way.ResultsIn this work, the applicability of regularized regression (Lasso) and random forests (RF) in bioprocess data mining was examined, and their performance was benchmarked against multiple linear regression. As an example, we used data from a culture media optimization study for microbial hydrogen production. All the three methods were capable in providing a significant model when the five variables of the culture media optimization were linearly included in modeling. However, multiple linear regression failed when also the multiplications and squares of the variables were included in modeling. In this case, the modeling was still successful with Lasso (correlation between the observed and predicted yield was 0.69) and RF (0.91).ConclusionWe found that both regularized regression and random forests were able to produce feasible models, and the latter was efficient in capturing the non-linearity in the data. In this kind of a data mining task of bioprocess data, both methods outperform multiple linear regression.",2013,BMC Systems Biology
Change in BMI Accurately Predicted by Social Exposure to Acquaintances,"Research has mostly focused on obesity and not on processes of BMI change more generally, although these may be key factors that lead to obesity. Studies have suggested that obesity is affected by social ties. However these studies used survey based data collection techniques that may be biased toward select only close friends and relatives. In this study, mobile phone sensing techniques were used to routinely capture social interaction data in an undergraduate dorm. By automating the capture of social interaction data, the limitations of self-reported social exposure data are avoided. This study attempts to understand and develop a model that best describes the change in BMI using social interaction data. We evaluated a cohort of 42 college students in a co-located university dorm, automatically captured via mobile phones and survey based health-related information. We determined the most predictive variables for change in BMI using the least absolute shrinkage and selection operator (LASSO) method. The selected variables, with gender, healthy diet category, and ability to manage stress, were used to build multiple linear regression models that estimate the effect of exposure and individual factors on change in BMI. We identified the best model using Akaike Information Criterion (AIC) and R(2). This study found a model that explains 68% (p<0.0001) of the variation in change in BMI. The model combined social interaction data, especially from acquaintances, and personal health-related information to explain change in BMI. This is the first study taking into account both interactions with different levels of social interaction and personal health-related information. Social interactions with acquaintances accounted for more than half the variation in change in BMI. This suggests the importance of not only individual health information but also the significance of social interactions with people we are exposed to, even people we may not consider as close friends.",2013,PLoS ONE
Structural Breaks in Time Series,"This chapter covers methodological issues related to estimation, testing and computation for models involving structural changes. Our aim is to review developments as they relate to econometric applications based on linear models. Substantial advances have been made to cover models at a level of generality that allow a host of interesting practical applications. These include models with general stationary regressors and errors that can exhibit temporal dependence and heteroskedasticity, models with trending variables and possible unit roots and cointegrated models, among others. Advances have been made pertaining to computational aspects of constructing estimates, their limit distributions, tests for structural changes, and methods to determine the number of changes present. A variety of topics are covered. The first part summarizes and updates developments described in an earlier review, Perron (2006), with the exposition following heavily that of Perron (2008). Additions are included for recent developments: testing for common breaks, models with endogenous regressors (emphasizing that simply using least-squares is preferable over instrumental variables methods), quantile regressions, methods based on Lasso, panel data models, testing for changes in forecast accuracy, factors models and methods of inference based on a continuous records asymptotic framework. Our focus is on the so-called off-line methods whereby one wants to retrospectively test for breaks in a given sample of data and form confidence intervals about the break dates. The aim is to provide the readers with an overview of methods that are of direct usefulness in practice as opposed to issues that are mostly of theoretical interest.",2018,arXiv: Econometrics
Grouped variable selection in high dimensional partially linear additive Cox model,"In the analysis of survival outcome supplemented with both clinical information and high-dimensional gene expression data, traditional Cox proportional hazard model fails to meet some emerging needs in biological research. First, the number of covariates is generally much larger the sample size. Secondly, predicting an outcome with individual gene expressions is inadequate because a geneâ€™s expression is regulated by multiple biological processes and functional units. There is a need to understand the impact of changes at a higher level such as molecular function, cellular component, biological process, or pathway. The change at a higher level is usually measured with a set of gene expressions related to the biological process. That is, we need to model the outcome with gene sets as variable groups and the gene sets could be partially overlapped also. In this thesis work, we investigate the impact of a penalized Cox regression procedure on regularization, parameter estimation, variable group selection, and nonparametric modeling of nonlinear effects with a time-to-event outcome. We formulate the problem as a partially linear additive Cox model with high-dimensional data. We group genes into gene sets and approximate the nonparametric components by truncated series expansions with B-spline bases. After grouping and approximation, the problem of variable selection becomes that of selecting groups of coefficients in a gene set or in an approximation. We apply the group Lasso to obtain an initial solution path and reduce the dimension of",2019,
Resampling-Based Variable Selection with Lasso for p >> n and Partially Linear Models,"The linear model of the regression function is a widely used and perhaps, in most cases, highly unrealistic simplifying assumption, when proposing consistent variable selection methods for large and highly-dimensional datasets. In this paper, we study what happens from theoretical point of view, when a variable selection method assumes a linear regression function and the underlying ground-truth model is composed of a linear and a non-linear term, that is at most partially linear. We demonstrate consistency of the Lasso method when the model is partially linear. However, we note that the algorithm tends to increase even more the number of selected false positives on partially linear models when given few training samples. That is usually because the values of small groups of samples happen to explain variation coming from the non-linear part of the response function and the noise, using a linear combination of wrong predictors. We demonstrate theoretically that false positives are likely to be selected by the Lasso method due to a small proportion of samples, which happen to explain some variation in the response variable. We show that this property implies that if we run the Lasso on several slightly smaller size data replications, sampled without replacement, and intersect the results, we are likely to reduce the number of false positives without losing already selected true positives. We propose a novel consistent variable selection algorithm based on this property and we show it can outperform other variable selection methods on synthetic datasets of linear and partially linear models and datasets from the UCI machine learning repository.",2015,2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA)
Radiomics of 18F-FDG PET/CT images predicts clinical benefit of advanced NSCLC patients to checkpoint blockade immunotherapy,"Immunotherapy has improved outcomes for patients with non-small cell lung cancer (NSCLC), yet durable clinical benefit (DCB) is experienced in only a fraction of patients. Here, we test the hypothesis that radiomics features from baseline pretreatment 18F-FDG PET/CT scans can predict clinical outcomes of NSCLC patients treated with checkpoint blockade immunotherapy. This study included 194 patients with histologically confirmed stage IIIB-IV NSCLC with pretreatment PET/CT images. Radiomics features were extracted from PET, CT, and PET+CT fusion images based on minimum Kullbackâ€“Leibler divergence (KLD) criteria. The radiomics features from 99 retrospective patients were used to train a multiparametric radiomics signature (mpRS) to predict DCB using an improved least absolute shrinkage and selection operator (LASSO) method, which was subsequently validated in both retrospective (Nâ€‰=â€‰47) and prospective test cohorts (Nâ€‰=â€‰48). Using these cohorts, the mpRS was also used to predict progression-free survival (PFS) and overall survival (OS) by training nomogram models using multivariable Cox regression analyses with additional clinical characteristics incorporated. The mpRS could predict patients who will receive DCB, with areas under receiver operating characteristic curves (AUCs) of 0.86 (95%CI 0.79â€“0.94), 0.83 (95%CI 0.71â€“0.94), and 0.81 (95%CI 0.68â€“0.92) in the training, retrospective test, and prospective test cohorts, respectively. In the same three cohorts, respectively, nomogram models achieved C-indices of 0.74 (95%CI 0.68â€“0.80), 0.74 (95%CI 0.66â€“0.82), and 0.77 (95%CI 0.69â€“0.84) to predict PFS and C-indices of 0.83 (95%CI 0.77â€“0.88), 0.83 (95%CI 0.71â€“0.94), and 0.80 (95%CI 0.69â€“0.91) to predict OS. PET/CT-based signature can be used prior to initiation of immunotherapy to identify NSCLC patients most likely to benefit from immunotherapy. As such, these data may be leveraged to improve more precise and individualized decision support in the treatment of patients with advanced NSCLC.",2019,European Journal of Nuclear Medicine and Molecular Imaging
Gaussian Graphical Models,"This chapter describes graphical models for multivariate continuous data based on the Gaussian (normal) distribution. We gently introduce the undirected models by examining the partial correlation structure of two sets of data, one relating to meat composition of pig carcasses and the other to body fat measurements. We then give a concise exposition of the model theory, covering topics such as maximum likelihood estimation using the IPS algorithm, hypothesis testing, and decomposability. We also explain the close relation between the models and linear regression models. We describe various approaches to model selection, including stepwise selection, the glasso algorithm and the SIN algorithm and apply these to the example datasets. We then turn to directed Gaussian graphical models that can be represented as DAGs. We explain a key concept, Markov equivalence, and describe how certain mixed graphs called pDAGS and essential graphs are used to represent equivalence classes of models. We describe various model selection algorithms for directed Gaussian models, including PC algorithm, the hill-climbing algorithm, and the max-min hill-climbing algorithm and apply them to the example datasets. Finally, we briefly describe Gaussian chain graph models and illustrate use of a model selection algorithm for these models.",2012,
