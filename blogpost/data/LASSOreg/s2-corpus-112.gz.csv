title,abstract,year,journal
"Race vis-Ã -vis Latitude: Their Influence on Intelligence, Infectious Diseases, and Income","Fuerst and Kirkegaard (this issue) have done a Herculean job in generating data on the relationships existing between European ancestry and complex cognitive ability (CCA) - the construct measured by IQ scores and/or student achievements in math, reading, and/or science (Rindermann, 2007) - simultaneously considering infectious diseases, socioeconomic outcomes, and other variables in several American countries. Their analytic approach is rigorous and includes lasso regression. But their findings contradict the results of a ministudy of 11 large American countries in which both European ancestry and absolute latitude determined national PISA scores; in that study, whereas the cognitive effects of absolute latitude remained significant when European ancestry was controlled, those of ancestry lost significance when absolute latitude was controlled (Leon, 2015b). Hence, rather than commenting their valuable paper, I decided to resolve the empirical issue by generating new evidence at a higher level of analysis. Neither the study by Leon (2015b) of 11 countries nor the Fuerst and Kirkegaard study of USA state data distinguished between exogenous and endogenous variables. I decided to use fully saturated path models entailing two exogenous and three endogenous variables and targeted US states because in this country latitude has been found to correlate with CCA (Kanazawa, 2006; Leon, 2015a; McDaniel, 2006a, 2006b; Pesta & Posnanski, 2014; Ryan, Bartels, & Townsend, 2010) and other relevant variables, such as infectious diseases (Eppig, Fincher, & Thornhill, 2011), income per capita (Ram, 1999), as well as the seroprevalence of infectious diseases (McQuillan et al., 2004) and georesidential pattern of Whites vis-a-vis African Americans and Hispanics (U.S. Census, 2010). Whereas the percentage of Whites is higher in the northern than southern United States, these minorities are more demographically important in the southern U.S.I calculated the percentage of Whites in each of the 48 contiguous U.S. states using data from the U.S. Census (2010), which uses self-identification. Other important categories are African American, Hispanic, Native American, East Asian, and Pacific Islander. The latitude of each capital city was obtained from the World Atlas (2015). Math and reading scores from male and female publicschool students in 4thand 8thgrade are from the most recent report of the National Assessment of Educational Progress, corresponding to 2013 (NAEP, 2015). Data on income per capita came from the U.S. Department of Commerce (2015). Finally, I incorporated measurements of prevalence of infectious diseases per state from a standardized Parasite Stress USA index encompassing cholera, measles, meningitis, pertussis, rubella, tetanus, and tuberculosis in 1993-2007 (Eppig, Fincher & Thornhill, 2011; Fincher & Thornhill, 2012). I did not include more variables to avoid problems of multicollinearity with a small number of observations.Percent White is treated as an exogenous variable in the path analyses because the causes of its distribution in the U.S. territory are historical and unrelated to the other variables in the path models; the earliest English immigrants settled in the U.S. northeast in the 16th century because it was close to England, and then moved south and west; they were followed by immigration from Africa into the rural South because slaves were needed in the cotton plantations, while Hispanics came later, mainly from Mexico and Cuba into the southern U.S. because they came from places south of the U.S.In all figures, I use+ pâ‰¤ .10, * p â‰¤ .05, **p â‰¤ .01, and ***p â‰¤ .001. The results shown in Figure 1 reveal that CCA is influenced only by latitude, whereas income is determined positively by CCA and negatively by Percent White. The latter finding can be understood considering that the White population is most prevalent in the U.S. northwest and north-midwest, that is, states with low population density and scarce natural resources (see Fig. â€¦",2016,Mankind Quarterly
Evaluation of digital soil mapping approaches with large sets of environmental covariates,"Abstract. The spatial assessment of soil functions requires maps of basic soil properties. Unfortunately, these are either missing for many regions or are not available at the desired spatial resolution or down to the required soil depth. The field-based generation of large soil datasets and conventional soil maps remains costly. Meanwhile, legacy soil data and comprehensive sets of spatial environmental data are available for many regions. Digital soil mapping (DSM) approaches relating soil data (responses) to environmental data (covariates) face the challenge of building statistical models from large sets of covariates originating, for example, from airborne imaging spectroscopy or multi-scale terrain analysis. We evaluated six approaches for DSM in three study regions in Switzerland (Berne, Greifensee, ZH forest) by mapping the effective soil depth available to plants (SD), pH, soil organic matter (SOM), effective cation exchange capacity (ECEC), clay, silt, gravel content and fine fraction bulk density for four soil depths (totalling 48 responses). Models were built from 300â€“500Â environmental covariates by selecting linear models through (1)Â grouped lasso and (2)Â an ad hoc stepwise procedure for robust external-drift kriging (georob). For (3)Â geoadditive models we selected penalized smoothing spline terms by component-wise gradient boosting (geoGAM). We further used two tree-based methods: (4)Â boosted regression trees (BRTs) and (5)Â random forest (RF). Lastly, we computed (6)Â weighted model averages (MAs) from the predictions obtained from methodsÂ 1â€“5. Lasso, georob and geoGAM successfully selected strongly reduced sets of covariates (subsets of 3â€“6â€‰% of all covariates). Differences in predictive performance, tested on independent validation data, were mostly small and did not reveal a single best method for 48Â responses. Nevertheless, RF was often the best among methods 1â€“5 (28 of 48Â responses), but was outcompeted by MA for 14 of these 28Â responses. RF tended to over-fit the data. The performance of BRT was slightly worse than RF. GeoGAM performed poorly on some responses and was the best only for 7 of 48Â responses. The prediction accuracy of lasso was intermediate. All models generally had small bias. Only the computationally very efficient lasso had slightly larger bias because it tended to under-fit the data. Summarizing, although differences were small, the frequencies of the best and worst performance clearly favoured RF if a single method is applied and MA if multiple prediction models can be developed.",2017,
Approximate Message Passing Algorithms for Generalized Bilinear Inference,"Recent developments in compressive sensing (CS) combined with increasing demands for effective high-dimensional inference techniques across a variety of disciplines have motivated extensive research into algorithms exploiting various notions of parsimony, including sparsity and low-rank constraints. In this dissertation, we extend the generalized approximate message passing (GAMP) approach, originally proposed for high-dimensional generalized-linear regression in the context of CS, to handle several classes of bilinear inference problems. First, we consider a general form of noisy CS where there is uncertainty in the measurement matrix as well as in the measurements. Matrix uncertainty is motivated by practical cases in which there are imperfections or unknown calibration parameters in the signal acquisition hardware. While previous work has focused on analyzing and extending classical CS algorithms like the LASSO and Dantzig selector for this problem setting, we propose a new algorithm called Matrix Uncertain GAMP (MU-GAMP) whose goal is minimization of mean-squared error of the signal estimates in the presence of these uncertainties, without attempting to estimate the uncertain measurement matrix itself. Next, we extend GAMP to the generalized-bilinear case, in which the measurement matrix is estimated jointly with the signals of interest, enabling its application to matrix completion, robust PCA, dictionary learning, and related matrix-factorization problems. We derive this Bilinear GAMP (BiG-AMP) algorithm as an approximation of the sum-product ii belief propagation algorithm in the high-dimensional limit, where central limit theorem arguments and Taylor-series approximations apply, and under the assumption of statistically independent matrix entries with known priors. In addition, we propose an adaptive damping mechanism that aids convergence under finite problem sizes, an expectation-maximization (EM)-based method to automatically tune the parameters of the assumed priors, and two rank-selection strategies. We then discuss the specializations of EM-BiG-AMP to the problems of matrix completion, robust PCA, and dictionary learning, and present the results of an extensive empirical study comparing EM-BiG-AMP to state-of-the-art algorithms on each problem. Our numerical results, using both synthetic and real-world datasets, demonstrate that EM-BiG-AMP yields excellent reconstruction accuracy (often best in class) while maintaining competitive runtimes and avoiding the need to tune algorithmic parameters. Finally, we propose a parametric extension known as P-BiG-AMP, which recovers BiG-AMP as a special case, that relaxes the assumption of statistically independent matrix entries by introducing parametric models for the two matrix factors. The resulting algorithm is rigorously justified for random affine parameterizations and constructed to allow its use with an even wider class of non-linear parameterizations, enabling numerous potential applications.",2014,
"Factors Affecting Exclusive Breastfeeding, Using Adaptive LASSO Regression","Background
Exclusive breastfeeding (EBF) in the first six months of the life can significantly improve maternal and children health, and it is especially important in low- and middle-income countries. We aimed to determine the factors affecting EBF duration in a sample of Iranian infants.


Methods
This prospective study was conducted between April 2012 and October 2014 in Fars, Iran. Women (N=2640), who had given birth to healthy term infants were categorized into EBF versus non-EBF groups. Demographic information from mothers and infants, medical and drug history, and pregnancy related factors were compared between the two groups. Multivariable analysis was performed using Adaptive Lasso regression. P<0.05 was considered significant.


Results
The mean duration of EBF was 4.63Â±1.99 months. There was an inverse association between the mother's educational level and duration of EBF (P<0.001). Also, we found that mothers who were housewives had a significantly longer duration of EBF (4.68Â±1.97) compared to mothers with either part-time (4.21Â±2.01) or full-time jobs (4.02Â±2.12) (P<0.001). By eliminating the redundant factors, the proposed multivariable model showed the infant's weight gain during EBF, singleton/multiple pregnancies, maternal perception of quantity of breast milk, post-partum infection, use of pacifier, neonate's irritability, birth place and mother's full-time job as the most important factors affecting the duration of EBF. Twin pregnancies, post-partum infection, cesarean section by maternal request, use of a pacifier and irritability in the neonatal period significantly reduced the duration of EBF.


Conclusion
Health policy-makers should promote EBF programs among the educated as well as working mothers in order to positively affect the community's health status.",2018,International Journal of Community Based Nursing and Midwifery
Bayesian sigmoid shrinkage with improper variance priors and an application to wavelet denoising,"The normal Bayesian linear model is extended by assigning a flat prior to the @dth power of the variance components of the regression coefficients 0<@d=<12 in order to improve prediction accuracy. In the case of orthonormal regressors, easy-to-compute analytic expressions are derived for the posterior distribution of the shrinkage and regression coefficients. The expected shrinkage is a sigmoid function of the squared value of the least-squares estimate divided by its standard error. This gives a small amount of shrinkage for large values and, provided @d is small, heavy shrinkage for small values. The limit behavior for both small and large values approaches that of the ideal coordinatewise shrinker in terms of the expected squared error of prediction, when @d is close to 0. In a simulation study of wavelet denoising, the proposed Bayesian shrinkage model yielded a lower mean squared error than soft thresholding (lasso), and was competitive with two recent wavelet shrinkage methods based on mixture prior distributions.",2006,Comput. Stat. Data Anal.
The Hunting in the Province of Elassona,"The aim of this research is to be examined and analysed the socio- economic characteristics of hunters of province Elassona, in the Prefecture of Larissa. The methodology that will follow will be models of linear regression with the method of least square and with models Logit for the more complete analysis of characteristics of hunters. The authentic project analyse also with descriptive statistics as X2 -Karl Pearson- of independence, X2 of good fitting and graphs, the socio- economic characteristics of hunters of province Elassona. I Have done This project as I was student in the department of Economic Science in university of Thessaly in city Volos. But I preferred to send you only the econometric part, as I consider it more important, in contrary the statistical analysis, which is more simple and trite. Front however we begin the analysis, it deserves we are reported in the A Pan-Hellenic hunting congress that was organised in Piraeus in 1932 with subjects as the determination the hunting species and not, the advisable way and time of these hunting, the effective protection of endemic prey and the prohibition of transaction during the year, as well as the organisation of local councils of hunting and the local funds of hunting. Many of the conclusions of congress propose most serious curtailments in the means of hunting, allowing only the firearm arm and prohibiting the manufacture, marketing, possession and use of each other means, as for example loops, fish-hook, plates, poisons as well as other auxiliary manufacture as the, barkers, partridges hunting as well as the hunting of partridges in water or hare with follow-up of traces in the snow. In 1939 are published the law L.1926 ""About hunting"", which in the substance incarnated the requirements and the wishes of hunting circles. Then the first efforts of protection of wild life concerned mainly the migrating species, after the migrating birds are faced with... exceptions. The time hunting as it is fixed by the article 261(as was modified by article L. 996/1971 and the article 8 L. 177/1975): 1. The hunting year begins from 1 August and expires 31 July of next year. 2. The hunting period for the preys which the hunting of them allowed, it begins: a) Of hare, from 15 September and it expires 10 January. b) The mountainous partridge from 10 September and it expires 10 March. c) Of flat partridge from 1 October and it expires the 30 November. d) For the remaining preys from 15 September and it expires 10 March, apart from the turtledoves, tree-living (eagle-fighter fig-eating) and remaining pigeon-species, as the quails, that the hunting of them begins on 20 August. Is allowed the hunting only at the duration of day and from 11 March and up to the beginning of hunting period is allowed the fighting with poisons of harmful preys, with concern and responsibility of hunting associations, as is allowed also the arrest, without arm, in the nests of this nurslings, up to their destruction . From 1950 and afterwards the environmental problems they exceed also the most ominous forecasts and acquire characteristically universality with main parameter the inequality growth between developed also developing world. The fruition of the wild animals have been checked to a large extent but have not been checked the increasing destructions of ecotypes The ecosystems that first fell victims of growth of societies were the wetlands that were connected with the event and the distribution of illnesses, the venturous ness and the animosity for the person. France possesses first prizes in Europe draining by 140.000 hectares the year. Ireland from the initial extent of 1.175.579 hectares in 1982 had only 578.350 hectares, that is to say the 49%.The USA has finally lost the 54% from her wetlands. Switzerland from 1800 up to today has destroyed finally the 85-90% wetlands while in Central and Southern America of the 1/5 of wetlands that was recognized as international importance they are threatened immediately by desiccation for agricultural and veterinary uses. Greece only the last 70-80 years has lost the 61% of wetlands certain from which they were irreplaceable for the wild birds.",2003,Econometrics
HMLasso: Lasso for High Dimensional and Highly Missing Data,"Sparse regression such as Lasso has achieved great success in dealing with high dimensional data for several decades. However, there are few methods applicable to missing data, which often occurs in high dimensional data. Recently, CoCoLasso was proposed to deal with high dimensional missing data, but it still suffers from highly missing data. In this paper, we propose a novel Lasso-type regression technique for Highly Missing data, called `HMLasso'. We use the mean imputed covariance matrix, which is notorious in general due to its estimation bias for missing data. However, we effectively incorporate it into Lasso, by using a useful connection with the pairwise covariance matrix. The resulting optimization problem can be seen as a weighted modification of CoCoLasso with the missing ratios, and is quite effective for highly missing data. To the best of our knowledge, this is the first method that can efficiently deal with both high dimensional and highly missing data. We show that the proposed method is beneficial with regards to non-asymptotic properties of the covariance matrix. Numerical experiments show that the proposed method is highly advantageous in terms of estimation error and generalization error.",2018,ArXiv
Statistical methods in neuroimaging genetics : pathways sparse regression and cluster size inference,"In the field of neuroimaging genetics, brain images are used as phenotypes in the search for genetic variants associated with brain structure or function. This search presents a formidable statistical challenge, not least because of the very high dimensionality of genotype and phenotype data produced by modern SNP (single nucleotide polymorphism) arrays and high resolution MRI. This thesis focuses on the use of multivariate sparse regression models such as the group lasso and sparse group lasso for the identification of gene pathways associated with both univariate and multivariate quantitative traits. The methods described here take particular account of various factors specific to pathways genome-wide association studies including widespread correlation (linkage disequilibrium) between genetic predictors, and the fact that many variants overlap multiple pathways. A resampling strategy that exploits finite sample variability is employed to provide robust rankings for pathways, SNPs and genes. Comprehensive simulation studies are presented comparing one proposed method, pathways group lasso with adaptive weights, to a popular alternative. This method is extended to the case of a multivariate phenotype, and the resulting pathways sparse reduced-rank regression model and algorithm is applied to a study identifying gene pathways associated with structural change in the brain characteristic of Alzheimerâ€™s disease. The original model is also adapted for the task of â€™pathwaysdrivenâ€™ SNP and gene selection, and this latter model, pathways sparse group lasso with adaptive weights, is applied in a search for SNPs and genes associated with elevated lipid levels in two separate cohorts of Asian adults. Finally, in a separate section an existing method for the identification of spatially extended clusters of image voxels with heightened activation is evaluated in an imaging genetic context. This method, known as cluster size inference, rests on a number of assumptions. Using real imaging and SNP data, false positive rates are found to be poorly controlled outside of a narrow range of parameters related to image smoothness and activation thresholds for cluster formation.",2013,
Predictive modelling of surface roughness in fused deposition modelling using data fusion,"To realise high quality, additively manufactured parts, real-time process monitoring and advanced predictive modelling tools are crucial for accelerating quality assurance in additive manufacturing. While previous research has demonstrated the effectiveness of physics- and model-based diagnosis and prognosis for additive manufacturing, very little research has been reported on real-time monitoring and predictive modelling of the surface roughness of additively manufactured parts. This paper presents a data fusion approach to predicting surface roughness in fused deposition modelling (FDM) processes. The predictive models are trained using random forests (RFs), support vector regression (SVR), ridge regression (RR), and least absolute shrinkage and selection operator (LASSO). A real-time monitoring system is developed to monitor the health condition of a FDM machine in real-time using multiple sensors. RFs, SVR, RR, and LASSO are demonstrated on the condition monitoring data collected from these sensors. To integrate the data sources, a feature-level data fusion method is introduced. Experimental results have shown that the predictive models trained by the machine learning algorithms are capable of predicting the surface roughness of additively manufacturing parts with very high accuracy. The prediction accuracy can be further improved using the data fusion method.",2019,International Journal of Production Research
On the validity of the pairs bootstrap for lasso estimators,"We study the validity of the pairs bootstrap for lasso estimators in linear regression models with random covariates and heteroscedastic error terms. We show that the naive pairs bootstrap does not provide a valid method for approximating the distribution of the lasso estimator. To overcome this deficiency, we introduce a modified pairs bootstrap procedure and prove its consistency. Finally, we consider the adaptive lasso and show that the modified pairs bootstrap consistently estimates the distribution of the adaptive lasso estimator.",2015,Biometrika
Tossing Coins Under Monotonicity,"This paper considers the following problem: we are given n coin tosses of coins with monotone increasing probability of getting heads (success). We study the performance of the monotone constrained likelihood estimate, which is equivalent to the estimate produced by isotonic regression. We derive adaptive and non-adaptive bounds on the performance of the isotonic estimate, i.e., we demonstrate that for some probability vectors the isotonic estimate converges much faster than in general. As an application of this framework we propose a two step procedure for the binary monotone single index model, which consists of running LASSO and consequently running an isotonic regression. We provide thorough numerical studies in support of our claims.",2019,
Automatic Model Selection in Archetype Analysis,"Archetype analysis involves the identification of representative objects from amongst a set of multivariate data such that the data can be expressed as a convex combination of these representative objects. Existing methods for archetype analysis assume a fixed number of archetypes a priori. Multiple runs of these methods for different choices of archetypes are required for model selection. Not only is this computationally infeasible for larger datasets, in heavy-noise settings model selection becomes cumbersome. In this paper, we present a novel extension to these existing methods with the specific focus of relaxing the need to provide a fixed number of archetypes beforehand. Our fast iterative optimization algorithm is devised to automatically select the right model using BIC scores and can easily be scaled to noisy, large datasets. These benefits are achieved by introducing a Group-Lasso component popular for sparse linear regression. The usefulness of the approach is demonstrated through simulations and on a real world application of document analysis for identifying topics.",2012,
Shrinkage Estimation of Regression Models with Multiple Structural Changes,In this paper we consider the problem of determining the number of structural changes in multiple linear regression models via group fused Lasso (least absolute shrinkage and selection operator ). We show that with probability tending to one our method can correctly determine the unknown number of breaks and the estimated break dates are sufficiently close to the true break dates. We obtain estimates of the regression coefficients via post Lasso and establish the asymptotic distributions of the estimates of both break ratios and regression coefficients. We also propose and validate a datadriven method to determine the tuning parameter. Monte Carlo simulations demonstrate that the proposed method works well in finite samples. We illustrate the use of our method with a predictive regression of the equity premium on fundamental information.,2016,Econometric Theory
Nonlinear regression modeling via the lasso-type regularization,"We consider the problem of constructing nonlinear regression models with Gaussian basis functions, using lasso regularization. Regularization with a lasso penalty is an advantageous in that it estimates some coefficients in linear regression models to be exactly zero. We propose imposing a weighted lasso penalty on a nonlinear regression model and thereby selecting the number of basis functions effectively. In order to select tuning parameters in the regularization method, we use a deviance information criterion proposed by Spiegelhalter et al. (2002), calculating the effective number of parameters by Gibbs sampling. Simulation results demonstrate that our methodology performs well in various situations.",2010,MI Preprint Series
Estimating a Person â€™ s Age based on MRI Brain Scans,2 Methods 3 2.1 Subjects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 2.2 Data validation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 2.3 Morphometric analysis of brain structure . . . . . . . . . . . . . 4 2.4 Building the Model . . . . . . . . . . . . . . . . . . . . . . . . . . 6 2.4.1 Hierarchies of predictors . . . . . . . . . . . . . . . . . . . 6 2.4.2 Using LASSO Regression . . . . . . . . . . . . . . . . . . 7,2015,
Regularized Quantile Regression Averaging for probabilistic electricity price forecasting,"Quantile Regression Averaging (QRA) has sparked interest in the electricity price forecasting community after its unprecedented success in the Global Energy Forecasting Competition 2014, where the top two winning teams in the price track used variants of QRA. However, recent studies have reported the method's vulnerability to low quality predictors when the set of regressors is larger than just a few. To address this issue, we consider a regularized variant of QRA, which utilizes the Least Absolute Shrinkage and Selection Operator (LASSO) to automatically select the relevant regressors. We evaluate the introduced technique â€“ dubbed LASSO QRA or LQRA for short â€“ using datasets from the Polish and Nordic power markets, a set of 25 point forecasts obtained for calibration windows of different lengths and 20 different values of the regularization parameter. By comparing against nearly 30 benchmarks, we provide evidence for its superior predictive performance in terms of the Kupiec test, the pinball score and the test for conditional predictive accuracy.",2019,HSC Research Reports
"Amplitude Supported Prospects, Analysis and Predictive Models for Reducing Risk of Geological Success","Summary Direct Hydrocarbon Indicators (DHI) are commonly used for exploration prospects. Amplitudes as an independent source of information could be used as conditional probability within Bayes Theorem to assess risk of geological success. Following research is aiming to construct predictive model for estimating probability of hydrocarbons observing DHI, P(dhi|hc). In order to build such model, we used Rose & Associates DHI Interpretation and Risk Analysis Consortium database, which contains extensive descriptions of 336 drilled prospects, with known results, across various categories: Geology, Data Quality, Amplitude Characteristics and Pitfalls. Multiple Logistic Regression was used for predicting probability P(dhi|hc). Three methods were considered within the study: two data-driven models - stepwise regression and lasso shrinkage method plus the third one, a combination of data-and expertise- driven approach - stepwise regression plus manual addition of predictors to the model. All three models with key predictors are described and give similar accuracy of prediction âˆ’ 77%. Performed data analysis and calculated models reveal several insights into R&A DHI Consortium database and amplitude prospects characterisation. The best method to create such models is probably a combination of data and expertise driven approaches, while selection of most appropriate model is a question of company's strategy.",2019,
Identification de biomarqueurs prÃ©dictifs de la survie et de l'effet du traitement dans un contexte de donnÃ©es de grande dimension,"Avec la revolution recente de la genomique et la medecine stratifiee, le developpement de signatures moleculaires devient de plus en plus important pour predire le pronostic (biomarqueurs pronostiques) ou lâ€™effet dâ€™un traitement (biomarqueurs predictifs) de chaque patient. Cependant, la grande quantite dâ€™information disponible rend la decouverte de faux positifs de plus en plus frequente dans la recherche biomedicale. La presence de donnees de grande dimension (nombre de biomarqueurs â‰« taille dâ€™echantillon) souleve de nombreux defis statistiques tels que la non-identifiabilite des modeles, lâ€™instabilite des biomarqueurs selectionnes ou encore la multiplicite des tests.Lâ€™objectif de cette these a ete de proposer et dâ€™evaluer des methodes statistiques pour lâ€™identification de ces biomarqueurs et lâ€™elaboration dâ€™une prediction individuelle des probabilites de survie pour des nouveaux patients a partir dâ€™un modele de regression de Cox. Pour lâ€™identification de biomarqueurs en presence de donnees de grande dimension, la regression penalisee lasso est tres largement utilisee. Dans le cas de biomarqueurs pronostiques, une extension empirique de cette penalisation a ete proposee permettant dâ€™etre plus restrictif sur le choix du parametre Î» dans le but de selectionner moins de faux positifs. Pour les biomarqueurs predictifs, lâ€™interet sâ€™est porte sur les interactions entre le traitement et les biomarqueurs dans le contexte dâ€™un essai clinique randomise. Douze approches permettant de les identifier ont ete evaluees telles que le lasso (standard, adaptatif, groupe ou encore ridge+lasso), le boosting, la reduction de dimension des effets propres et un modele implementant les effets pronostiques par bras. Enfin, a partir dâ€™un modele de prediction penalise, differentes strategies ont ete evaluees pour obtenir une prediction individuelle pour un nouveau patient accompagnee dâ€™un intervalle de confiance, tout en evitant un eventuel surapprentissage du modele. La performance des approches ont ete evaluees au travers dâ€™etudes de simulation proposant des scenarios nuls et alternatifs. Ces methodes ont egalement ete illustrees sur differents jeux de donnees, contenant des donnees dâ€™expression de genes dans le cancer du sein.",2016,
Application of statistical and computational methodology to predict brainstem dosimetry for trigeminal neuralgia stereotactic radiosurgery.,"OBJECTIVES
To apply advanced statistical and computational methodology in evaluating the impact of anatomical and technical variables on normal tissue dosimetry of trigeminal neuralgia (TN) stereotactic radiosurgery (SRS).


METHODS
Forty patients treated with LINAC-based TN SRS with 90 Gy maximum dose were randomly selected for the study. Parameters extracted from the treatment plans for the study included three dosimetric output variables: the maximum dose to the brainstem (BSmax), the volume of brainstem that received at least 10 Gy (V10BS), and the volume of normal brain that received at least 12 Gy (V12). We analyzed five anatomical variables: the incidence angle of the nerve with the brainstem surface (A), the nerve length (L), the nerve width as measured both axially (WA) and sagittally (WS), the distance measured along the nerve between the isocenter and the brainstem surface (D), and one technical variable: the utilized cone size (CS). Univariate correlation was calculated for each pair among all parameters. Multivariate regression models were fitted for the output parameters using the optimal input parameters selected by the Gaussian graphic model LASSO. Repeated twofold cross-validations were used to evaluate the models.


RESULTS
Median BSmax, V10BS, and V12 for the 40 patients were 35.7 Gy, 0.14 cc, and 1.28 cc, respectively. Median A, L, WA, WS, D, and CS were 43.7Â°, 8.8 mm, 2.8 mm, 2.7 mm, 4.8 mm, and 6 mm, respectively. Of the three output variables, BSmax most strongly correlated with the input variables. Specifically, it had strong, negative correlations with the input anatomical variables and a positive correlation with CS. The correlation between D and BSmax at -0.51 was the strongest correlation between single input and output parameters, followed by that between CS and V10BS at 0.45, and that between A and BSmax at -0.44. V12 was most correlated with cone size alone, rather than anatomy. LASSO identified an optimal 3-feature combination of A, D, and CS for BSmax and V10BS prediction. Using cross-validation, the multivariate regression models with the three selected features yielded stronger correlations than the correlation between the BSmax and V10BS themselves.


CONCLUSIONS
For the first time, an advanced statistical and computational methodology was applied to study the impact of anatomical and technical variables on TN SRS. The variables were found to impact brainstem doses, and reasonably strong correlation models were established using an optimized 3-feature combination including the nerve incidence angle, cone size, and isocenter-brainstem distance.",2018,Medical physics
Blood Lead Levels and Associated Factors among Children in Guiyu of China: A Population-Based Study,"OBJECTIVES
Children's health problems caused by the electronic waste (e-waste) lead exposure in China remains. To assess children's blood lead levels (BLLs) in Guiyu of China and investigate risk factors of children's elevated BLLs in Guiyu.


MATERIAL AND METHODS
842 children under 11 years of age from Guiyu and Haojiang were enrolled in this population-based study during 2011-2013. Participants completed a lifestyle and residential environment questionnaire and their physical growth indices were measured, and blood samples taken. Blood samples were tested to assess BLLs. Children's BLLs between the two groups were compared and factors associated with elevated BLLs among Guiyu children were analyzed by group Lasso logistic regression model.


RESULTS
Children living in Guiyu had significant higher BLLs (7.06 Âµg/dL) than the quantity (5.89 Âµg/dL) of Haojiang children (P<0.05). Subgroup analyses of BLLs exceeding 10 Âµg/dL showed the proportion (24.80%) of high-level BLLs for Guiyu children was greater than that (12.84%) in Haojiang (P<0.05). Boys had greater BLLs than girls, irrespectively of areas (P<0.05). The number of e-waste piles or recycling workshops around the house (odds ratio, 2.28; 95% confidence interval [CI], 1.37 to 3.87) significantly contributed to the elevated BLLs of children in Guiyu, and girls had less risk (odds ratio, 0.51; 95% CI, 0.31 to 0.83) of e-waste lead exposure than boys.


CONCLUSIONS
This analysis reinforces the importance of shifting e-waste recycling piles or workshops to non-populated areas as part of a comprehensive response to e-waste lead exposure control in Guiyu. To correct the problem of lead poisoning in children in Guiyu should be a long-term mission.",2014,PLoS ONE
Intracranial Pressure Forecasting in Children Using Dynamic Averaging of Time Series Data,"Increased Intracranial Pressure (ICP) is a serious and often life-threatening condition. If the increased pressure pushes on critical brain structures and blood vessels, it can lead to serious permanent problems or even death. In this study, we propose a novel regression model to forecast ICP episodes in children, 30 min in advance, by using the dynamic characteristics of continuous intracranial pressure, vitals and medications during the last two hours. The correlation between physiological parameters, including blood pressure, respiratory rate, heart rate and the ICP, is analyzed. Linear regression, Lasso regression, support vector machine and random forest algorithms are used to forecast the next 30 min of the recorded ICP. Finally, dynamic features are created based on vitals, medications and the ICP. The weak correlation between blood pressure and the ICP (0.2) is reported. The Root-Mean-Square Error (RMSE) of the random forest model decreased from 1.6 to 0.89% by using the given medication variables in the last two hours. The random forest regression gave an accurate model for the ICP forecast with 0.99 correlation between the forecast and experimental values.",2018,
OP0018â€…Tool and threshold predicting a successful biological dmards tapering in patients with ra remission determination,"Background Tapering trials confirmed the feasibility of TNF inhibitors (TNFi) tapering for a relevant proportion of patients in remission and/or low disease activity. However, there are no consensual predictors of a good response to therapeutic spacing among patients with rheumatoid arthritis (RA) in remission. Objectives To determine the most predictive tool and threshold of a successful TNFi tapering. Methods Population: The Spacing of TNF-blocker injections in Rheumatoid Arthritis Study (STRASS) trial included 137 RA patients fulfilled the ACR 1987 criteria with sustained (at least 6 months) DAS28 <2.6. Patients were randomly assigned to one of the two following strategies: in the Maintain arm, patients continued to receive TNFi at the standard full regimen and in the Spacing arm, the strategy applied progressive spacing of ADA or ETN subcutaneous injections up to discontinuation at the forth step in the spacing arm. We used the data of the Spacing arm. Analysis: The performances of several variables (DAS28, SDAI, CDAI, CRP, ACPA status, HAQ, patient/physician global assessment, and booleen remission criteria) were assessed for the prediction of successful TNFi tapering, defined as reaching at least 25% tapering of the full regimen during at least 6 months, using sensitivity and specificity for dichotomous variables, or the area under the ROC curve (AUC) and its 95% confidence interval for continuous variables. A predictive score of successful tapering was constructed using LASSO regression modeling technique to avoid overfitting (R software version 3.2.1). Results The main characteristic of the 64 patients of the Spacing arm were the following (mean Â± SD): age 54.3Â±10.7 years, disease duration 8.3Â±5.4 years, and DAS 28 1.9Â±0.6. The baseline variables were similar between patients who failed or succeeded at TNFi spacing, except for the HAQ score (0.30 in the group success and 0.89 in the failure group, p=0.01) and the CRP (2.35 mg/l versus 3.48 mg/l, respectively, p=0.02). Baseline variables performance in predicting successful TNFi spacing: None of the tested variables was able to predict successful TNFi spacing, except the HAQ score and the CRP. A HAQ threshold â‰¥1.125 had a specificity (Spe) of 93% and an AUC: 0.713 (CI95%: 0.540â€“0.886). A CRP threshold â‰¥6.8 mg/l had a Spe of 0.97 and an AUC: 0.689 (CI95%: 0.547â€“0.831). Composite criteria: A composite criteria able to predict successful TNFi spacing has been elaborated, including ACPA status, Boolean criteria, SDAI, CRP and HAQ. A composite score lower than 0.502 was able to predict a successful TNFi spacing: Spe: 100%; Se: 54%; AUC: 0.829 (CI95%: 0.671 - 0.986). Conclusions The remission maintenance in rheumatoid arthritis after TNFi spacing is possible. Our results showed that in a population of RA patients in remission with TNFi, baseline HAQ and CRP are independent predictor factors of successful tapering. We have developed a composite index able to predict successful TNFi spacing, with an AUC of 0.829 and a specificity of 100%. A validation study will be needed to confirm its ability to select patients for treatment decrease. Disclosure of Interest None declared",2017,Annals of the Rheumatic Diseases
Literature-Based Genetic Risk Scores for Coronary Heart DiseaseClinical Perspective,"Backgroundâ€” Genome-wide association studies (GWAS) have identified many single-nucleotide polymorphisms (SNPs) associated with coronary heart disease (CHD) or CHD risk factors (RF). Using a case-cohort study within the prospective Cardiovascular Registry Maastricht (CAREMA) cohort, we tested if genetic risk scores (GRS) based on GWAS-identified SNPs are associated with and predictive for future CHD.

Methods and Resultsâ€” Incident cases (n=742), that is, participants who developed CHD during a median follow-up of 12.1 years (range, 0.0â€“16.9 years), were compared with a randomly selected subcohort of 2221 participants selected from the total cohort (n=21 148). We genotyped 179 SNPs previously associated with CHD or CHD RF in GWAS as published up to May 2, 2011. The allele-count GRS, composed of all SNPs, the 153 RF SNPs, or the 29 CHD SNPs were not associated with CHD independent of CHD RF. The weighted 29 CHD SNP GRS, with weights obtained from GWAS for every SNP, were associated with CHD independent of CHD RF (hazard ratio, 1.12 per weighted risk allele; 95% confidence interval, 1.04â€“1.21) and improved risk reclassification with 2.8% ( P =0.031). As an exploratory approach to achieve weighting, we performed least absolute shrinkage and selection operator (LASSO) regression analysis on all SNPs and the CHD SNPs. The CHD LASSO GRS performed equal to the weighted CHD GRS, whereas the Overall LASSO GRS performed slightly better than the weighted CHD GRS.

Conclusionsâ€” A GRS composed of CHD SNPs improves risk prediction when adjusted for the effect sizes of the SNPs. Alternatively LASSO regression analysis may be used to achieve weighting; however, validation in independent populations is required.",2012,Circulation-cardiovascular Genetics
A Modified Principal Component Technique Based on the LASSO,"In many multivariate statistical techniques, a set of linear functions of the original p variables is produced. One of the more difficult aspects of these techniques is the interpretation of the linear functions, as these functions usually have nonzero coefficients on all p variables. A common approach is to effectively ignore (treat as zero) any coefficients less than some threshold value, so that the function becomes simple and the interpretation becomes easier for the users. Such a procedure can be misleading. There are alternatives to principal component analysis which restrict the coefficients to a smaller number of possible values in the derivation of the linear functions, or replace the principal components by â€œprincipal variables.â€ This article introduces a new technique, borrowing an idea proposed by Tibshirani in the context of multiple regression where similar problems arise in interpreting regression equations. This approach is the so-called LASSO, the â€œleast absolute shrinkage and selection o...",2003,Journal of Computational and Graphical Statistics
A Constructive Approach to High-dimensional Regression,"We develop a constructive approach to estimating sparse, high-dimensional linear regression models. The approach is a computational algorithm motivated from the KKT conditions for the $\ell_0$-penalized least squares solutions. It generates a sequence of solutions iteratively, based on support detection using primal and dual information and root finding. We refer to the algorithm as SDAR for brevity. Under a sparse Rieze condition on the design matrix and certain other conditions, we show that with high probability, the $\ell_2$ estimation error of the solution sequence decays exponentially to the minimax error bound in $O(\sqrt{J}\log(R))$ steps; and under a mutual coherence condition and certain other conditions, the $\ell_{\infty}$ estimation error decays to the optimal error bound in $O(\log(R))$ steps, where $J$ is the number of important predictors, $R$ is the relative magnitude of the nonzero target coefficients. Computational complexity analysis shows that the cost of SDAR is $O(np)$ per iteration. Moreover the oracle least squares estimator can be exactly recovered with high probability at the same cost if we know the sparsity level. We also consider an adaptive version of SDAR to make it more practical in applications. Numerical comparisons with Lasso, MCP and greedy methods demonstrate that SDAR is competitive with or outperforms them in accuracy and efficiency.",2017,arXiv: Computation
SAGA and Restricted Strong Convexity,"SAGA is a fast incremental gradient method on the finite sum problem and its effectiveness has been tested on a vast of applications. In this paper, we analyze SAGA on a class of non-strongly convex and non-convex statistical problem such as Lasso, group Lasso, Logistic regression with $\ell_1$ regularization, linear regression with SCAD regularization and Correct Lasso. We prove that SAGA enjoys the linear convergence rate up to the statistical estimation accuracy, under the assumption of restricted strong convexity (RSC). It significantly extends the applicability of SAGA in convex and non-convex optimization.",2017,arXiv: Machine Learning
