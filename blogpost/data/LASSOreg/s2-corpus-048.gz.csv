title,abstract,year,journal
Support recovery without incoherence: A case for nonconvex regularization,"We demonstrate that the primal-dual witness proof method may be used to establish variable selection consistency and $\ell_\infty$-bounds for sparse regression problems, even when the loss function and/or regularizer are nonconvex. Using this method, we derive two theorems concerning support recovery and $\ell_\infty$-guarantees for the regression estimator in a general setting. Our results provide rigorous theoretical justification for the use of nonconvex regularization: For certain nonconvex regularizers with vanishing derivative away from the origin, support recovery consistency may be guaranteed without requiring the typical incoherence conditions present in $\ell_1$-based methods. We then derive several corollaries that illustrate the wide applicability of our method to analyzing composite objective functions involving losses such as least squares, nonconvex modified least squares for errors-in variables linear regression, the negative log likelihood for generalized linear models, and the graphical Lasso. We conclude with empirical studies to corroborate our theoretical predictions.",2014,ArXiv
Effects of sea level rise in the United States and climate change perception in the United Kingdom,"This thesis has three separate parts. In the first part I report the first ex post study of the economic impact of sea level rise. I apply two econometric approaches to estimate the past effects of sea level rise on the economy of the USA, viz. Barro type growth regressions adjusted for spatial patterns and a matching estimator. The unit of analysis is 3063 counties of the USA. I fit growth regressions for 13 time periods and I estimate numerous varieties for both growth regressions and matching estimator. Although there is some evidence that sea level rise has a positive effect on economic growth, in most specifications the estimated effects are insignificant. Therefore, I cannot confirm the implicit assumption of previous ex-ante studies, in particular that sea level rise has in general negative effect on economies. 
 
In the second part I fit Ricardian regressions of agricultural land values for 2830 counties of the USA on past sea level rise, taking account of spatial autocorrelation and heteroscedasticity. I find a significant, hill-shaped relationship. Hence, the outcomes are mixed. Mild sea level rise increases, while more pronounced sea level rise causes land values to fall. The results are robust to a set of variations. 
 
In the third part I explore an unprecedented dataset of almost 6,000 observations to identify main predictors of climate knowledge, climate risk perception and willingness to pay (WTP) for climate change mitigation. Among nearly 70 potential explanatory variables I detect the most important ones using a multisplit lasso estimator. Importantly, I test significance of individuals' preferences about time, risk and equity. The study is innovative as these behavioural characteristics were recorded by including experimental methods into a live sample survey. This unique way of data collection combines advantages of surveys and experiments. The most important predictors of environmental attitudes are numeracy, cognitive ability, inequity aversion and political and ideological world-view.",2018,
Regularization and Noise Injection for Improving Genetic Network Models,"The most fundamental problem in genetic network modeling is generally known as the dimensionality problem. Typical time-course gene expression data sets contain measurements of thousands of genes taken over fewer than twenty time-steps. A large dynamic network cannot be learned from data with such a limited number of time-steps without the use of additional constraints, preferably derived from biological knowledge. In this chapter, we present an approach that can nd rough estimates of the underlying genetic network based on limited time-course gene expression data by employing the fact that gene expression measurements are relatively noisy and genetic networks are thought to be robust. The method expands the data set by adding noisy duplicates, thereby simultaneously tackling the dimensionality problem and making the solutions more robust against (the already large) noise in the data. For linear models, this concept is strongly related to shrinkage methods, such as ridge regression and lasso regression, and in the limiting case equivalent to the Moore-Penrose pseudoinverse. The strength of the proposed concept of noise injection lies, however, in the fact that it can be employed to any modelling approach, including non-linear models.",2006,
Development and validation of a predictive model of acute glucose response to exercise in individuals with type 2 diabetes,"BackgroundOur purpose was to develop and test a predictive model of the acute glucose response to exercise in individuals with type 2 diabetes.Design and methodsData from three previous exercise studies (56 subjects, 488 exercise sessions) were combined and used as a development dataset. A mixed-effects Least Absolute Shrinkage Selection Operator (LASSO) was used to select predictors among 12 potential predictors. Tests of the relative importance of each predictor were conducted using the Lindemann Merenda and Gold (LMG) algorithm. Model structure was tested using likelihood ratio tests. Model accuracy in the development dataset was assessed by leave-one-out cross-validation.Prospectively captured data (47 individuals, 436 sessions) was used as a test dataset. Model accuracy was calculated as the percentage of predictions within measurement error. Overall model utility was assessed as the number of subjects with â‰¤1 model error after the third exercise session. Model accuracy across individuals was assessed graphically. In a post-hoc analysis, a mixed-effects logistic regression tested the association of individualsâ€™ attributes with model error.ResultsMinutes since eating, a non-linear transformation of minutes since eating, post-prandial state, hemoglobin A1c, sulfonylurea status, age, and exercise session number were identified as novel predictors. Minutes since eating, its transformations, and hemoglobin A1c combined to account for 19.6% of the variance in glucose response. Sulfonylurea status, age, and exercise session each accounted for <1.0% of the variance. In the development dataset, a model with random slopes for pre-exercise glucose improved fit over a model with random intercepts only (likelihood ratio 34.5, pâ€‰<â€‰0.001). Cross-validated model accuracy was 83.3%.In the test dataset, overall accuracy was 80.2%. The model was more accurate in pre-prandial than postprandial exercise (83.6% vs. 74.5% accuracy respectively). 31/47 subjects had â‰¤1 model error after the third exercise session. Model error varied across individuals and was weakly associated with within-subject variability in pre-exercise glucose (Odds ratio 1.49, 95% Confidence interval 1.23-1.75).ConclusionsThe preliminary development and test of a predictive model of acute glucose response to exercise is presented. Further work to improve this model is discussed.",2013,Diabetology & Metabolic Syndrome
Grouped Orthogonal Matching Pursuit for Variable Selection and Prediction,"We consider the problem of variable group selection for least squares regression, namely, that of selecting groups of variables for best regression performance, leveraging and adhering to a natural grouping structure within the explanatory variables. We show that this problem can be efficiently addressed by using a certain greedy style algorithm. More precisely, we propose the Group Orthogonal Matching Pursuit algorithm (Group-OMP), which extends the standard OMP procedure (also referred to as ""forward greedy feature selection algorithm"" for least squares regression) to perform stage-wise group variable selection. We prove that under certain conditions Group-OMP can identify the correct (groups of) variables. We also provide an upperbound on the lâˆž norm of the difference between the estimated regression coefficients and the true coefficients. Experimental results on simulated and real world datasets indicate that Group-OMP compares favorably to Group Lasso, OMP and Lasso, both in terms of variable selection and prediction accuracy.",2009,
11 â€“ Linear Models,"The standard Gauss-Markov model is introduced, and estimation and inferential issues for this model are addressed. Model selection procedures such as Akaike Information Criterion, Bayesian Information Criterion, cross-validation, and generalized cross-validation are discussed. A brief introduction is given to alternative approaches for regression such as ridge, lasso, and partial least squares. Random- and mixed-effect models are introduced along with strategies for estimating the variance components and the nonrandom parts of such models. Many examples are given from regression, ANOVA, ANCOVA, and mixed linear models in order to clarify the key concepts discussed in this chapter.",2016,
Incorporating predictor network in penalized regression with application to microarray data.,"We consider penalized linear regression, especially for ""large p, small n"" problems, for which the relationships among predictors are described a priori by a network. A class of motivating examples includes modeling a phenotype through gene expression profiles while accounting for coordinated functioning of genes in the form of biological pathways or networks. To incorporate the prior knowledge of the similar effect sizes of neighboring predictors in a network, we propose a grouped penalty based on the L(gamma)-norm that smoothes the regression coefficients of the predictors over the network. The main feature of the proposed method is its ability to automatically realize grouped variable selection and exploit grouping effects. We also discuss effects of the choices of the gamma and some weights inside the L(gamma)-norm. Simulation studies demonstrate the superior finite-sample performance of the proposed method as compared to Lasso, elastic net, and a recently proposed network-based method. The new method performs best in variable selection across all simulation set-ups considered. For illustration, the method is applied to a microarray dataset to predict survival times for some glioblastoma patients using a gene expression dataset and a gene network compiled from some Kyoto Encyclopedia of Genes and Genomes (KEGG) pathways.",2010,Biometrics
A Lasso-type Robust Variable Selection for Time-Course Microarray Data,"Lasso has been widely used for variable selection because of its sparsity, and a number of its extensions have been developed. In this article, we propose a robust variant of Lasso for the time-course multivariate response, and develop an algorithm which transforms the optimization into a sequence of ridge regressions. The proposed method enables us to effectively handle multivariate responses and employs a basis representation of the regression parameters to reduce the dimensionality. We assess the proposed method through simulation and apply it to the microarray data.",2015,Communications in Statistics - Theory and Methods
Three-Month Real-Time Dengue Forecast Models: An Early Warning System for Outbreak Alerts and Policy Decision Support in Singapore,"BACKGROUND
With its tropical rainforest climate, rapid urbanization, and changing demography and ecology, Singapore experiences endemic dengue; the last large outbreak in 2013 culminated in 22,170 cases. In the absence of a vaccine on the market, vector control is the key approach for prevention.


OBJECTIVES
We sought to forecast the evolution of dengue epidemics in Singapore to provide early warning of outbreaks and to facilitate the public health response to moderate an impending outbreak.


METHODS
We developed a set of statistical models using least absolute shrinkage and selection operator (LASSO) methods to forecast the weekly incidence of dengue notifications over a 3-month time horizon. This forecasting tool used a variety of data streams and was updated weekly, including recent case data, meteorological data, vector surveillance data, and population-based national statistics. The forecasting methodology was compared with alternative approaches that have been proposed to model dengue case data (seasonal autoregressive integrated moving average and step-down linear regression) by fielding them on the 2013 dengue epidemic, the largest on record in Singapore.


RESULTS
Operationally useful forecasts were obtained at a 3-month lag using the LASSO-derived models. Based on the mean average percentage error, the LASSO approach provided more accurate forecasts than the other methods we assessed. We demonstrate its utility in Singapore's dengue control program by providing a forecast of the 2013 outbreak for advance preparation of outbreak response.


CONCLUSIONS
Statistical models built using machine learning methods such as LASSO have the potential to markedly improve forecasting techniques for recurrent infectious disease outbreaks such as dengue.


CITATION
Shi Y, Liu X, Kok SY, Rajarethinam J, Liang S, Yap G, Chong CS, Lee KS, Tan SS, Chin CK, Lo A, Kong W, Ng LC, Cook AR. 2016. Three-month real-time dengue forecast models: an early warning system for outbreak alerts and policy decision support in Singapore. Environ Health Perspect 124:1369-1375;â€‚http://dx.doi.org/10.1289/ehp.1509981.",2016,Environmental Health Perspectives
Heuristics as a special case of Bayesian Inference,"Heuristics as a special case of Bayesian Inference Paula Parpart University College London Matt Jones University of Colorado Boulder Brad Love University College London Keywords: ; ; ; ; ; ; ; Abstract: Probabilistic inference models (e.g. Bayesian models) are often cast as being rational and at odds with simple heuristic approaches. We show that prominent decision heuristics, take-the-best and tallying, are special cases of Bayesian inference. We developed two Bayesian learning models by extending two popular regularized regression approaches, lasso and ridge regression. The priors of these Bayesian models match the environmental structures necessary for tallying and take-the- best to succeed. Provably, the Bayesian models become equivalent to the heuristics as their priors become more extreme; hence they subsume heuristics and standard linear regression. In a re-analysis of datasets favouring heuristic approaches, we show that our Bayesian extension of ridge regression outperforms tallying and linear regression. A similar result holds for our Bayesian extension of lasso regression and the take-the-best heuristic. This indicates that true environmental structure and potentially psychological processing often lie somewhere between the assumptions of heuristic and standard regression approaches.",2014,Cognitive Science
Time Varying Quantile Lasso,"In the present chapter we study the dynamics of penalization parameter \(\lambda \) of the least absolute shrinkage and selection operator (Lasso) method proposed by Tibshirani (J Roy Stat Soc Series B 58:267â€“288, 1996) and extended into quantile regression context by Li and Zhu (J Comput Graph Stat 17:1â€“23, 2008). The dynamic behaviour of the parameter \(\lambda \) can be observed when the model is assumed to vary over time and therefore the fitting is performed with the use of moving windows. The proposal of investigating time series of \(\lambda \) and its dependency on model characteristics was brought into focus by Hardle et al. (J Econom 192:499â€“513, 2016), which was a foundation of FinancialRiskMeter. Following the ideas behind the two aforementioned projects, we use the derivation of the formula for the penalization parameter \(\lambda \) as a result of the optimization problem. This reveals three possible effects driving \(\lambda \); variance of the error term, correlation structure of the covariates and number of nonzero coefficients of the model. Our aim is to disentangle these three effects and investigate their relationship with the tuning parameter \(\lambda \), which is conducted by a simulation study. After dealing with the theoretical impact of the three model characteristics on \(\lambda \), empirical application is performed and the idea of implementing the parameter \(\lambda \) into a systemic risk measure is presented.",2016,
Nonparametric Shrinkage estimation for Aalen's additive hazards model,"Aalen's nonparametric additive model in which the regression coefficients are assumed to be unspecified functions of time is a flexible alternative to Cox's proportional hazards model when the proportionality assumption is in doubt. In this paper, we incorporate a general linear hypothesis into the estimation of the time-varying regression coefficients. We combine unrestricted least squares estimators and estimators that are restricted by the linear hypothesis and produce James-Stein-type shrinkage estimators of the regression coefficients. We develop the asymptotic joint distribution of such restricted and unrestricted estimators and use this to study the relative performance of the proposed estimators via their integrated asymptotic distributional risks. We conduct Monte Carlo simulations to examine the relative performance of the estimators in terms of their integrated mean square errors. We also compare the performance of the proposed estimators with a recently devised LASSO estimator as well as with ridge-type estimators both via simulations and data on the survival of primary billiary cirhosis patients.",2014,Australian & New Zealand Journal of Statistics
A Study on Domestic Drama Rating Prediction,"Abstract Audience rating competition in the domestic drama market has increased recently due to the introductionof commercial broadcasting and diversiï¬cation of channels. There is now a need for thorough studies andanalysis on audience rating. Especially, a drama rating is an important measure to estimate advertisementcosts for producers and advertisers. In this paper, we study the drama rating prediction models using variousdata mining techniques such as linear regression, LASSO regression, random forest, and gradient boosting.The analysis results show that initial drama ratings are aï¬€ected by structural elements such as broadcastingstation and broadcasting time. Average drama ratings are also inï¬‚uenced by earlier public opinion such asthe number of internet searches about the drama.Keywords: drama rating, linear regression, LASSO regression, random forest, gradient boosting, importantvariables 1. ì„œë¡  ìµœê·¼ ë“œë¼ë§ˆ ì‹œìž¥ì—ëŠ” ë‹¤ì–‘í•œ ì£¼ì œë¥¼ ë‹¤ë£¨ê³  ì—¬ëŸ¬ ìœ ëª… ë°°ìš°ë“¤ì„ìºìŠ¤íŒ…í•œ ë“œë¼ë§ˆë“¤ì´ë“±ìž¥í•˜ê³  ìžˆë‹¤.2011ë…„ ë§ ë°©ì†¡ë²•ì´ê°œì •ë¨ì— ë”°ë¼ ì¢…í•©íŽ¸ì„±ì±„ë„ì´ê°œêµ­í•œ ì´í›„ ì§€ìƒíŒŒ ë°©ì†¡ì‚¬ ì¤‘ì‹¬ì´ì—ˆë˜ ë“œë¼ë§ˆ ì‹œìž¥ì´ì¼€ì´ë¸” ë° ì¢…í•©íŽ¸ì„±ì±„ë„ê¹Œì§€ í™•ëŒ€ë˜ì—ˆìœ¼ë©° ìŠ¤ë§ˆíŠ¸í°, íƒœë¸”ë¦¿ ë“±ë“œë¼ë§ˆë¥¼ ì‹œì²­í•  ìˆ˜ìžˆëŠ” ë°©ë²•ì´ë‹¤ì–‘í™”ë˜ì—ˆë‹¤. ì´ë ‡ê²Œ ë°©ì†¡ì‹œìž¥ì´ë¹ ë¥´ê²Œ ë³€í™”í•˜ê³  ìžˆìŒì—ë„ ë¶ˆêµ¬í•˜ê³  ì‹œì²­ë¥ ì€TVë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ì „í†µì ì¸ë°©ë²•ìœ¼ë¡œ ì¸¡ì •ë˜ì–´ í”„ë¡œê·¸ëž¨ ì œìž‘ìžì™€ ê´‘ê³ ì£¼ë“¤ì—ê²Œ ê´‘ê³ ë¹„ ê·œëª¨ë¥¼ ì‚°ì •í•˜ëŠ” ë° ë§¤ìš° ì¤‘ìš”í•œ ì²™ë„ë¡œ í™œìš©ë˜ê³  ìžˆë‹¤. íŠ¹ížˆ ë“œë¼ë§ˆëŠ” ëŒ€ì¤‘ì  ì¸ê¸°ë‚˜ ê·¸ì— ë”°ë¥¸ ì‚¬íšŒì  ì˜í–¥ë ¥ì´ë¼ëŠ” ì°¨ì›ì—ì„œë‹¤ë¥¸ ì–´ë–¤ìž¥ë¥´ì˜í”„ë¡œê·¸ëž¨ë³´ë‹¤ë„ ì¤‘ìš”í•œ ì˜ë¯¸ë¥¼ ì§€ë‹ˆê¸° ë•Œë¬¸ì— (Bae, 2005) ë“œë¼ë§ˆ ì‹œì²­ë¥ ì„ì˜ˆì¸¡í•˜ëŠ” ê²ƒì€ì œìž‘ìžì™€ ê´‘ê³ ì£¼ ìž…ìž¥ì—ì„œë§¤ìš° ì¤‘ìš”í•˜ë‹¤. ë“œë¼ë§ˆ ì‹œì²­ë¥ ì— ê´€í•œ ì—°êµ¬ëŠ” ì´ë¯¸ ì˜¤ëž˜ì „ë¶€í„° ì§„í–‰ë˜ì–´ì™”ì§€ë§ŒëŒ€ë¶€ë¶„ì˜ì—°êµ¬ë“¤ì´ì¼ë°˜íšŒê·€ëª¨í˜•ì— ê¸°ë°˜í•˜ê³  ìžˆìœ¼ë©° ë¶„ì„ëŒ€ìƒì´íŠ¹ì • ë°©ì†¡ì‚¬ ë˜ëŠ” ì§€ìƒíŒŒ ë°©ì†¡ì‚¬ì˜ë“œë¼ë§ˆì— í•œì •ë˜ì–´ ìžˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” ì´ëŸ° í•œê³„ì ì„ê·¹ë³µí•˜ê³ ìžìµœê·¼ ë°©ì†¡ì‹œìž¥ì˜ë³€í™”ë¥¼ ê³ ë ¤í•œ ë‹¤ì–‘í•œ í†µê³„ì  ì˜ˆì¸¡ ëª¨í˜•ì„ì œì‹œí•˜ê³ ìží•œë‹¤.ë³¸ ì—°êµ¬ì˜ëª©ì ì€ì§€ìƒíŒŒ ë°©ì†¡ì‚¬ì™€ ì¼€ì´ë¸”, ì¢…í•©íŽ¸ì„±ì±„ë„ì„ëª¨ë‘í¬í•¨í•œ ë“œë¼ë§ˆë¥¼ ëŒ€ìƒìœ¼ë¡œ í•˜ì—¬ ë‹¤ì–‘í•œ ë°ì´í„°ë§ˆì´ë‹ ê¸°ë²•ì„í™œìš©í•œ ì‹œì²­ë¥  ì˜ˆì¸¡ ëª¨í˜•ì„ì œì‹œí•˜ê³  ì‹œì²­ë¥  ì˜ˆì¸¡ì— ì¤‘ìš”í•œ ì˜í–¥ì„ë¯¸ì¹˜ëŠ” ìš”",2015,
Forward stagewise regression and the monotone lasso,"We consider the least angle regression and forward stagewise algorithms for solving penalized least squares regression problems. In Efron et al. (2004) it is proven that the least angle regression algorithm, with a small modication, solves the lasso (L1 constrained) regression problem. Here we give an analogous result for incremental forward stagewise regression, showing that it ts a monotone version of the lasso. We also study a condition under which the coecien t paths of the lasso are monotone, and hence the dieren t algorithms all coincide. Finally, we compare the lasso and forward stagewise procedures in a simulation study involving a large number of correlated predictors.",2007,Electronic Journal of Statistics
Radiomics signature on 3T dynamic contrast-enhanced magnetic resonance imaging for estrogen receptor-positive invasive breast cancers,"To evaluate the ability of a radiomics signature based on 3T dynamic contrast-enhanced (DCE) magnetic resonance imaging (MRI) to distinguish between low and non-low Oncotype DX (OD) risk groups in estrogen receptor (ER)-positive invasive breast cancers.Between May 2011 and March 2016, 67 women with ER-positive invasive breast cancer who performed preoperative 3T MRI and OD assay were included. We divided the patients into low (OD recurrence score [RS]â€Š<18) and non-low risk (RSâ€Šâ‰¥18) groups. Extracted radiomics features included 8 morphological, 76 histogram-based, and 72 higher-order texture features. A radiomics signature (Rad-score) was generated using the least absolute shrinkage and selection operator (LASSO). Univariate and multivariate logistic regression analyses were performed to investigate the association between clinicopathologic factors, MRI findings, and the Rad-score with OD risk groups, and the areas under the receiver operating characteristic curves (AUC) were used to assess classification performance of the Rad-score.The Rad-score was constructed for each tumor by extracting 10 (6.3%) from 158 radiomics features. A higher Rad-score (odds ratio [OR], 65.209; Pâ€Š<.001), Ki-67 expression (OR, 17.462; Pâ€Š=â€Š.007), and high p53 (ORâ€Š=â€Š8.449; Pâ€Š=â€Š.077) were associated with non-low OD risk. The Rad-score classified low and non-low OD risk with an AUC of 0.759.The Rad-score showed the potential for discrimination between low and non-low OD risk groups in patients with ER-positive invasive breast cancers.",2019,Medicine
DetecciÃ³n de fallos dinÃ¡mica y descentralizada basada en mÃ©todos de regresiÃ³n,"This article proposes a dynamic and decentralized fault detection method. The plant is divided in blocks using the existing relationships between variables found by regression methods. Each group of variables has a dynamic fault detection method, DPCA, which sends its results to a central processor that fuses the local results using the Bayesian Inference Criterion (BIC), resulting in a global diagnosis. This proposal has been tested on an industrial plant model and compared with Tabla 4: Retado en la detecciÃ³n, en instantes. Q Fallo LASSO Random Forest DPCA 1 0 0 5 2 10 7 13 3 566 566 4 0 0 2 5 0 0 2 6 0 0 1 7 0 0 1 8 16 16 21 9 728 10 45 31 50 11 3 3 7 12 0 0 8 13 36 36 40 14 0 0 1 15 742 727 16 13 11 196 17 18 16 24 18 15 78 84 19 43 11 82 20 78 78 84 21 283 264 286 Tabla 5: False alarms rates and faults detected (in %) LASSO Random Forest DPCA FAR T 2 0 0 0,6 FAR Q 0 0 28,1 Detected faults T 2 21 20 17 Detected faults Q 21 20 18 the DPCA method to verify its effectiveness.",2020,
Variable and threshold selection to control predictive accuracy in logistic regression,"type=""main"" xml:id=""rssc12058-abs-0001""> Using data collected from the â€˜Sequenced treatment alternatives to relieve depressionâ€™ study, we use logistic regression to predict whether a patient will respond to treatment on the basis of early symptom change and patient characteristics. Model selection criteria such as the Akaike information criterion AIC and mean-squared-error of prediction MSEP may not be appropriate if the aim is to predict with a high degree of certainty who will respond or not respond to treatment. Towards this aim, we generalize the definition of the positive and negative predictive value curves to the case of multiple predictors. We point out that it is the ordering rather than the precise values of the response probabilities which is important, and we arrive at a unified approach to model selection via two-sample rank tests. To avoid overfitting, we define a cross-validated version of the positive and negative predictive value curves and compare these curves after smoothing for various models. When applied to the study data, we obtain a ranking of models that differs from those based on AIC and MSEP, as well as a tree-based method and regularized logistic regression using a lasso penalty. Our selected model performs consistently well for both 4-week-ahead and 7-week-ahead predictions.",2014,Journal of The Royal Statistical Society Series C-applied Statistics
The Spike-and-Slab Lasso regression modeling with compositional covariates: An application on Brazilian children malnutrition data.,"There are considerable challenges in analyzing large-scale compositional data. In this paper, we introduce the Spike-and-Slab Lasso linear regression in the presence of compositional covariates for parameter estimation and variable selection. We consider the well-known isometric log-ratio (ilr) coordinates to avoid misleading statistical inference. The separable and non-separable (adaptative) Spike-and-Slab Lasso penalties are compared to verify the advantages of each approach. The proposed method is illustrated on simulated and on real Brazilian child malnutrition data.",2019,Statistical methods in medical research
Supplement to â€œ A Generalized Least Squares Matrix Decomposition â€,"In addition to sparseness, there is much interest in penalties that encourage smoothness, especially in the context of functional data analysis. We show how the GPMF can be used with smooth penalties and propose a generalized gradient descent method to solve for these smooth GPMF factors. Many have proposed to obtain smoothness in the factors by using a quadratic penalty. Rice and Silverman (1991) suggested P (v) = v Î©v, where Î© is the matrix of squared second or fourth differences. As this penalty is not homogeneous of order one, we use the Î©norm penalty: P (v) = (v Î©v)âˆ’1/2 = ||v ||Î©. Since this penalty is a norm or a semi-norm, the GPMF solution given in Theorem 2 can be employed. We seek to minimize the following Î©-norm penalized regression problem: 1 2 ||X Quâˆ’v ||R + Î»v ||v ||Î©. Notice that this problem is similar in structure to the group lasso problem of Yuan and Lin (2006) with one group. To solve the Î©-norm penalized regression problem, we use a generalized gradient descent method. (We note that there are more elaborate first order solvers, introduced in recent works such as Becker et al. (2010, 2009), and we describe a simple version of such solvers). Suppose",2013,
Identification of Urinary Polyphenol Metabolite Patterns Associated with Polyphenol-Rich Food Intake in Adults from Four European Countries,"We identified urinary polyphenol metabolite patterns by a novel algorithm that combines dimension reduction and variable selection methods to explain polyphenol-rich food intake, and compared their respective performance with that of single biomarkers in the European Prospective Investigation into Cancer and Nutrition (EPIC) study. The study included 475 adults from four European countries (Germany, France, Italy, and Greece). Dietary intakes were assessed with 24-h dietary recalls (24-HDR) and dietary questionnaires (DQ). Thirty-four polyphenols were measured by ultra-performance liquid chromatography-electrospray ionization-tandem mass spectrometry (UPLC-ESI-MS-MS) in 24-h urine. Reduced rank regression-based variable importance in projection (RRR-VIP) and least absolute shrinkage and selection operator (LASSO) methods were used to select polyphenol metabolites. Reduced rank regression (RRR) was then used to identify patterns in these metabolites, maximizing the explained variability in intake of pre-selected polyphenol-rich foods. The performance of RRR models was evaluated using internal cross-validation to control for over-optimistic findings from over-fitting. High performance was observed for explaining recent intake (24-HDR) of red wine (r = 0.65; AUC = 89.1%), coffee (r = 0.51; AUC = 89.1%), and olives (r = 0.35; AUC = 82.2%). These metabolite patterns performed better or equally well compared to single polyphenol biomarkers. Neither metabolite patterns nor single biomarkers performed well in explaining habitual intake (as reported in the DQ) of polyphenol-rich foods. This proposed strategy of biomarker pattern identification has the potential of expanding the currently still limited list of available dietary intake biomarkers.",2017,Nutrients
Analyse canonique rÃ©gularisÃ©e pour des donnÃ©es fortement multidimensionnelles,"Motive par la mise en evidence des relations entre l'expression de genes et d'autres variables biologiques, notre travail consiste a presenter et developper une methodologie repondant a ce probleme. Parmi les methodes statistiques abordant ce sujet, l'Analyse Canonique (AC) semblait bien appropriee, mais la haute dimensionalite est actuellement l'un des obstacles majeurs pour les techniques statistiques d'analyse de donnees issues de biopuces. Naturellement l'axe de ce travail a ete la recherche de solutions tenant compte de cet aspect crucial dans la mise en oeuvre de l'AC. Parmi les approches envisagees pour contourner ce probleme, nous nous sommes interesses a des methodes de regularisation. Ainsi, la methode developpee ici, appelee Analyse Canonique Regularisee (ACR), est basee sur le principe de regularisation ridge introduit initialement en regression lineaire multiple. L'ACR necessitant le choix de deux parametres de reglage pour sa mise en oeuvre, nous avons propose la methode de validation croisee par sous-groupes pour traiter ce probleme. Nous avons presente en detail des applications de l'ACR a des donnees fortement multidimensionnelles provenant d'etudes genomiques ainsi qu'a des donnees provenant d'autres domaines. Sur ce point on s'est interesse a une visualisation des donnees aidant a l'interpretation des resultats obtenus. A cet effet, nous avons propose un certaine nombre de methodes graphiques : representations des variables (graphiques des correlations), representations des individus ainsi que des representations alternatives comme les graphiques de reseaux et les cartes de double classification (heatmaps). Pour la mise en oeuvre de l'AC, nous avons developpe le package CCA (disponible en ligne sur le site cran.r-project.org). Ce package permet le traitement de donnees avec plus de variables que d'unites experimentales par l'ACR, la manipulation des valeurs manquantes et la realisation des graphiques aidant a l'interpretation des resultats. Enfin, dans le cadre des methodes de retrecissement (shrinkage) nous avons introduit la methode CCALAS ayant comme objectif d'obtenir une sorte de selection des variables et ainsi de simplifier l'interpretation des representations graphiques en AC. Cette approche basee sur la methode LASSO ouvre les voies pour differentes perspectives de travail tant au niveau methodologique qu'au niveau de la mise en oeuvre.",2007,
Assessment of Sparse Regression Machine Learning Methods for Genome-Wide Association Studies,"The data from genome-wide association studies (GWAS) in humans are still predominantly analyzed using single marker association methods. As an alternative to Single Marker Analysis (SMA), all or subsets of markers can be tested simultaneously. This approach requires a form of Penalized Regression (PR) as the number of SNPs is much larger than the sample size. Here we review PR methods in the context of GWAS, extend them to perform penalty parameter and SNP selection by False Discovery Rate (FDR) control, and assess their performance (including penalties incorporating linkage disequilibrium) in comparison with SMA. PR methods were compared with SMA on realistically simulated GWAS data consisting of genotype data from single and multiple chromosomes and a continuous phenotype and on real data. Based on our comparisons our analytic FDR criterion may currently be the best approach to SNP selection using PR for GWAS. We found that PR with FDR control provides substantially more power than SMA with genome-wide type-I error control but somewhat less power than SMA with BenjaminiHochberg FDR control. PR controlled the FDR conservatively while SMA-BH may not achieve FDR control in all situations. Differences among PR methods seem quite small when the focus is on variable selection with FDR control. Incorporating LD into PR by adapting penalties developed for covariates measured on graphs can improve power but also generate morel false positives or wider regions for follow-up. We recommend using the Elastic Net with a mixing weight for the Lasso penalty near 0.5 as the best method.",2014,
Piecewise Linear Regularized Solution Paths,"We consider the generic regularized optimization problem Î²(Î») = argminÎ² L(y, XÎ²) + Î»J(Î²). Efron, Hastie, Johnstone and Tibshirani [Ann. Statist. 32 (2004) 407-499] have shown that for the LASSO-that is, if L is squared error loss and J(Î²) = âˆ¥Î²âˆ¥ 1 is the l 1 norm of Î²-the optimal coefficient path is piecewise linear, that is, âˆ‚Î²(Î»)/âˆ‚Î». is piecewise constant. We derive a general characterization of the properties of (loss L, penalty J) pairs which give piecewise linear coefficient paths. Such pairs allow for efficient generation of the full regularized coefficient paths. We investigate the nature of efficient path following algorithms which arise. We use our results to suggest robust versions of the LASSO for regression and classification, and to develop new, efficient algorithms for existing problems in the literature, including Mammen and van de Geer's locally adaptive regression splines.",2007,Annals of Statistics
[18F] FDG Positron Emission Tomography (PET) Tumor and Penumbra Imaging Features Predict Recurrence in Nonâ€“Small Cell Lung Cancer,"We identified computational imaging features on 18F-fluorodeoxyglucose positron emission tomography (PET) that predict recurrence/progression in non-small cell lung cancer (NSCLC). We retrospectively identified 291 patients with NSCLC from 2 prospectively acquired cohorts (training, n = 145; validation, n = 146). We contoured the metabolic tumor volume (MTV) on all pretreatment PET images and added a 3-dimensional penumbra region that extended outward 1 cm from the tumor surface. We generated 512 radiomics features, selected 435 features based on robustness to contour variations, and then applied randomized sparse regression (LASSO) to identify features that predicted time to recurrence in the training cohort. We built Cox proportional hazards models in the training cohort and independently evaluated the models in the validation cohort. Two features including stage and a MTV plus penumbra texture feature were selected by LASSO. Both features were significant univariate predictors, with stage being the best predictor (hazard ratio [HR] = 2.15 [95% confidence interval (CI): 1.56-2.95], P < .001). However, adding the MTV plus penumbra texture feature to stage significantly improved prediction (P = .006). This multivariate model was a significant predictor of time to recurrence in the training cohort (concordance = 0.74 [95% CI: 0.66-0.81], P < .001) that was validated in a separate validation cohort (concordance = 0.74 [95% CI: 0.67-0.81], P < .001). A combined radiomics and clinical model improved NSCLC recurrence prediction. FDG PET radiomic features may be useful biomarkers for lung cancer prognosis and add clinical utility for risk stratification.",2019,Tomography
Homogeneity Pursuit,"This article explores the homogeneity of coefficients in high-dimensional regression, which extends the sparsity concept and is more general and suitable for many applications. Homogeneity arises when regression coefficients corresponding to neighboring geographical regions or a similar cluster of covariates are expected to be approximately the same. Sparsity corresponds to a special case of homogeneity with a large cluster of known atom zero. In this article, we propose a new method called clustering algorithm in regression via data-driven segmentation (CARDS) to explore homogeneity. New mathematics are provided on the gain that can be achieved by exploring homogeneity. Statistical properties of two versions of CARDS are analyzed. In particular, the asymptotic normality of our proposed CARDS estimator is established, which reveals better estimation accuracy for homogeneous parameters than that without homogeneity exploration. When our methods are combined with sparsity exploration, further efficiency can be achieved beyond the exploration of sparsity alone. This provides additional insights into the power of exploring low-dimensional structures in high-dimensional regression: homogeneity and sparsity. Our results also shed lights on the properties of the fused Lasso. The newly developed method is further illustrated by simulation studies and applications to real data. Supplementary materials for this article are available online.",2015,Journal of the American Statistical Association
A Novel Pruning Algorithm for Smoothing Feedforward Neural Networks Based on Group Lasso Method,"In this paper, we propose four new variants of the backpropagation algorithm to improve the generalization ability for feedforward neural networks. The basic idea of these methods stems from the Group Lasso concept which deals with the variable selection problem at the group level. There are two main drawbacks when the Group Lasso penalty has been directly employed during network training. They are numerical oscillations and theoretical challenges in computing the gradients at the origin. To overcome these obstacles, smoothing functions have then been introduced by approximating the Group Lasso penalty. Numerical experiments for classification and regression problems demonstrate that the proposed algorithms perform better than the other three classical penalization methods, Weight Decay, Weight Elimination, and Approximate Smoother, on both generalization and pruning efficiency. In addition, detailed simulations based on a specific data set have been performed to compare with some other common pruning strategies, which verify the advantages of the proposed algorithm. The pruning abilities of the proposed strategy have been investigated in detail for a relatively large data set, MNIST, in terms of various smoothing approximation cases.",2018,IEEE Transactions on Neural Networks and Learning Systems
"Structure of and relationships within and between the littoral, rock-substrate fish communities off four islands in the Canarian Archipelago","A total of 577 visual surveys (each of 5 min in duration and 100 m2 in area) were conducted throughout 1990 and 1991 at 32 locations off four Canary islands (i.e., Alegranza, Fuerteventura, Gran Canaria and Tenerife) with the objects of describing the coastal fish community, comparing the differences in the fish fauna within and between these islands, and determining the biotic and abiotic factors related to the structure of the fish communities. A total of 76 species were recorded; the most common were Abudefduf luridus, Canthigaster rostrata, Chromis limbatus, Sparisoma cretense and Thalassoma pavo (94.28, 86.48, 52.34, 73.31, and 94.10% frequency of occurrence, respectively). The abundance and average size of the commercially important species was greater in those locations where there was less fishing pressure. The stepwise linear regression models were capable of explaining only a low amount of variation in the dependent variables (i.e., number of species, number of individuals, average size and species diversity) of the fish community. The independent variables recorded were date, time of day, depth, slope of the substrate, substrate type, percentage of sand, percentage of algae, algal height, number of sea urchins (Diadema antillarum) and the individual islands. An ANOVA, using the islands only as independent variables, indicated that each island contributed significantly to the variation in the four dependent variables and there were significant differences among the islands. Detrended correspondence analysis (DCA) and a two-way indicator species analysis (TWINSPAN) determined associations between species and environmental attributes of the survey locations. The patterns in the TWINSPAN analysis indicated that localities had faunal resemblances based on the island off which where they were located.",1996,Marine Biology
The composite absolute penalties family for grouped and hierarchical variable selection,"Extracting useful information from high-dimensional data is an important focus of today's statistical research and practice. Penalized loss function minimization has been shown to be effective for this task both theoretically and empirically. With the virtues of both regularization and sparsity, the L 1 -penalized squared error minimization method Lasso has been popular in regression models and beyond. In this paper, we combine different norms including L 1 to form an intelligent penalty in order to add side information to the fitting of a regression or classification model to obtain reasonable estimates. Specifically, we introduce the Composite Absolute Penalties (CAP) family, which allows given grouping and hierarchical relationships between the predictors to be expressed. CAP penalties are built by defining groups and combining the properties of norm penalties at the across-group and within-group levels. Grouped selection occurs for nonoverlapping groups. Hierarchical variable selection is reached by defining groups with particular overlapping patterns. We propose using the BLASSO and cross-validation to compute CAP estimates in general. For a subfamily of CAP estimates involving only the L 1 and L âˆž norms, we introduce the iCAP algorithm to trace the entire regularization path for the grouped selection problem. Within this subfamily, unbiased estimates of the degrees of freedom (df) are derived so that the regularization parameter is selected without cross-validation. CAP is shown to improve on the predictive performance of the LASSO in a series of simulated experiments, including cases with p Â» n and possibly mis-specified groupings. When the complexity of a model is properly calculated, iCAP is seen to be parsimonious in the experiments.",2009,Annals of Statistics
An Empirical Analysis of ZeuS C&C Lifetime,"Botnets continue to pose a significant threat to network-based applications and communications over the Internet. A key mitigation strategy has been to take down command and control infrastructure of the botnets. The efficiency of those mitigation methods has not been extensively studied. In this paper we investigate several observable characteristics of botnet command and controls (C&C) and estimate the variability in the survival rate of these C&Cs and the factors that are related to such variability. Furthermore, we show that different type of mitigation efforts have different impact. Kaplan-Meier analysis is performed to evaluate C&C survival ratios in the particular case of the ZeuS botnet. Using a lasso penalized Cox regression model, we identify the factors that influence the lifetime of a C&C. Location, malware family type, registrar, hosting type and popularity are the fundamental factors that explain this variability. Our results show that location and type of hosting are the two factors that affect more significantly the C&C lifetime. Thus, ZeuS C&Cs in certain regions of Asia are prone to stay online longer that those located in Europe.",2015,
