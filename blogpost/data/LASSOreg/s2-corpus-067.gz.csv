title,abstract,year,journal
Dynamic and Modularized MicroRNA Regulation and Its Implication in Human Cancers,"MicroRNA is responsible for the fine-tuning of fundamental cellular activities and human disease development. The altered availability of microRNAs, target mRNAs, and other types of endogenous RNAs competing for microRNA interactions reflects the dynamic and conditional property of microRNA-mediated gene regulation that remains under-investigated. Here we propose a new integrative method to study this dynamic process by considering both competing and cooperative mechanisms and identifying functional modules where different microRNAs co-regulate the same functional process. Specifically, a new pipeline was built based on a meta-Lasso regression model and the proof-of-concept study was performed using a large-scale genomic dataset from ~4,200 patients with 9 cancer types. In the analysis, 10,726 microRNA-mRNA interactions were identified to be associated with a specific stage and/or type of cancer, which demonstrated the dynamic and conditional miRNA regulation during cancer progression. On the other hands, we detected 4,134 regulatory modules that exhibit high fidelity of microRNA function through selective microRNA-mRNA binding and modulation. For example, miR-18a-3p, âˆ’320a, âˆ’193b-3p, and âˆ’92b-3p co-regulate the glycolysis/gluconeogenesis and focal adhesion in cancers of kidney, liver, lung, and uterus. Furthermore, several new insights into dynamic microRNA regulation in cancers have been discovered in this study.",2017,Scientific Reports
Comparison of Machine Learning Methods With National Cardiovascular Data Registry Models for Prediction of Risk of Bleeding After Percutaneous Coronary Intervention,"Importance
Better prediction of major bleeding after percutaneous coronary intervention (PCI) may improve clinical decisions aimed to reduce bleeding risk. Machine learning techniques, bolstered by better selection of variables, hold promise for enhancing prediction.


Objective
To determine whether machine learning techniques better predict post-PCI major bleeding compared with the existing National Cardiovascular Data Registry (NCDR) models.


Design, Setting, and Participants
This comparative effectiveness study used the NCDR CathPCI Registry data version 4.4 (July 1, 2009, to April 1, 2015), machine learning techniques were used (logistic regression with lasso regularization and gradient descent boosting [XGBoost, version 0.71.2]), and output was then compared with the existing simplified risk score and full NCDR models. The existing models were recreated, and then performance was evaluated through additional techniques and variables in a 5-fold cross-validation in analysis conducted from October 1, 2015, to October 27, 2017. The setting was retrospective modeling of a nationwide clinical registry of PCI. Participants were all patients undergoing PCI. Percutaneous coronary intervention procedures were excluded if they were not the index PCI of admission, if the hospital site had missing outcomes measures, or if the patient underwent subsequent coronary artery bypass grafting.


Exposures
Clinical variables available at admission and diagnostic coronary angiography data were used to determine the severity and complexity of presentation.


Main Outcomes and Measures
The main outcome was in-hospital major bleeding within 72 hours after PCI. Results were evaluated by comparing C statistics, calibration, and decision threshold-based metrics, including the F score (harmonic mean of positive predictive value and sensitivity) and the false discovery rate.


Results
The post-PCI major bleeding rate among 3â€¯316â€¯465 procedures (patients' median age, 65 years; interquartile range, 56-73 years; 68.1% male) was 4.5%. The existing full model achieved a mean C statistic of 0.78 (95% CI, 0.78-0.78). The use of XGBoost and full range of selected variables achieved a C statistic of 0.82 (95% CI, 0.82-0.82), with an F score of 0.31 (95% CI, 0.30-0.31). XGBoost correctly identified an additional 3.7% of cases identified as high risk who experienced a bleeding event and an overall improvement of 1.0% of cases identified as low risk who did not experience a bleeding event. The data-driven decision threshold helped improve the false discovery rate of the existing techniques. The existing simplified risk score model improved the false discovery rate from more than 90% to 78.7%. Modifying the model and the data decision threshold improved this rate from 78.7% to 73.4%.


Conclusions and Relevance
Machine learning techniques improved the prediction of major bleeding after PCI. These techniques may help to better identify patients who would benefit most from strategies to reduce bleeding risk.",2019,JAMA Network Open
Mean and variance estimation in high-dimensional heteroscedastic models with non-convex penalties,"Despite its prevalence in statistical datasets, heteroscedasticity (non-constant sample variances) has been largely ignored in the high-dimensional statistics literature. Recently, studies have shown that the Lasso can accommodate heteroscedastic errors, with minor algorithmic modifications (Belloni et al., 2012; Gautier and Tsybakov, 2013). In this work, we study heteroscedastic regression with linear mean model and log-linear variances model with sparse high-dimensional parameters. In this work, we propose estimating variances in a post-Lasso fashion, which is followed by weighted-least squares mean estimation. These steps employ non-convex penalties as in Fan and Li (2001), which allows us to prove oracle properties for both post-Lasso variance and mean parameter estimates. We reinforce our theoretical findings with experiments.",2014,arXiv: Statistics Theory
Selective Inference and Learning Mixed Graphical Models,"This thesis studies two problems in modern statistics. First, we study selective inference, or inference for hypothesis that are chosen after looking at the data. The motiving application is inference for regression coefficients selected by the lasso. We present the Condition-on-Selection method that allows for valid selective inference, and study its application to the lasso, and several other selection algorithms. 
In the second part, we consider the problem of learning the structure of a pairwise graphical model over continuous and discrete variables. We present a new pairwise model for graphical models with both continuous and discrete variables that is amenable to structure learning. In previous work, authors have considered structure learning of Gaussian graphical models and structure learning of discrete models. Our approach is a natural generalization of these two lines of work to the mixed case. The penalization scheme involves a novel symmetric use of the group-lasso norm and follows naturally from a particular parametrization of the model. We provide conditions under which our estimator is model selection consistent in the high-dimensional regime.",2015,ArXiv
Development of a Predictive Model of Tuberculosis Transmission among Household Contacts,"Background
Household contacts of patients with tuberculosis (TB) are at great risk of TB infection. The aim of this study was to develop a predictive model of TB transmission among household contacts.


Method
This was a secondary analysis of data from a prospective cohort study, in which a total of 700 TB patients and 3417 household contacts were enrolled between 2010 and 2013 at two study sites in Peru. The incidence of secondary TB cases among household contacts of index cases was recorded. The LASSO regression method was used to reduce the data dimension and to filter variables. Multivariate logistic regression analysis was applied to develop the predictive model, and internal validation was performed. A nomogram was constructed to display the model, and the AUC was calculated. The calibration curve and decision curve analysis (DCA) were also evaluated.


Results
The incidence of TB disease among the contacts of index cases was 4.4% (149/3417). Ten variables (gender, age, TB history, diabetes, HIV, index patient's drug resistance, socioeconomic status, spoligotypes, and the index-contact share sleeping room status) filtered through the LASSO regression technique were finally included in the predictive model. The model showed good discriminatory ability, with an AUC value of 0.761 (95% CI, 0.723-0.800) for the derivation and 0.759 (95% CI, 0.717-0.796) for the internal validation. The predictive model showed good calibration, and the DCA demonstrated that the model was clinically useful.


Conclusion
A predictive model was developed that incorporates characteristics of both the index patients and the contacts, which may be of great value for the individualized prediction of TB transmission among household contacts.",2019,The Canadian Journal of Infectious Diseases & Medical Microbiology = Journal Canadien des Maladies Infectieuses et de la Microbiologie MÃ©dicale
A Prognostic Signature for Lower Grade Gliomas Based on Expression of Long Non-Coding RNAs,"Diffuse low-grade and intermediate-grade gliomas (together known as lower grade gliomas, WHO grade II and III) develop in the supporting glial cells of brain and are the most common types of primary brain tumor. Despite a better prognosis for lower grade gliomas, 70% of patients undergo high-grade transformation within 10Â years, stressing the importance of better prognosis. Long non-coding RNAs (lncRNAs) are gaining attention as potential biomarkers for cancer diagnosis and prognosis. We have developed a computational model, UVA8, for prognosis of lower grade gliomas by combining lncRNA expression, Cox regression, and L1-LASSO penalization. The model was trained on a subset of patients in TCGA. Patients in TCGA, as well as a completely independent validation set (CGGA) could be dichotomized based on their risk score, a linear combination of the level of each prognostic lncRNA weighted by its multivariable Cox regression coefficient. UVA8 is an independent predictor of survival and outperforms standard epidemiological approaches and previous published lncRNA-based predictors as a survival model. Guilt-by-association studies of the lncRNAs in UVA8, all of which predict good outcome, suggest they have a role in suppressing interferon-stimulated response and epithelial to mesenchymal transition. The expression levels of eight lncRNAs can be combined to produce a prognostic tool applicable to diverse populations of glioma patients. The 8 lncRNA (UVA8) based score can identify grade II and grade III glioma patients with poor outcome, and thus identify patients who should receive more aggressive therapy at the outset.",2018,Molecular Neurobiology
A prognostic tool to predict outcomes in children undergoing the Norwood operation,"Objectives: To create and validate a prediction model to assess outcomes associated with the Norwood operation. Methods: The publicâ€use dataset from a multicenter, prospective, randomized singleâ€ventricle reconstruction trial was used to create this novel prediction tool. A Bayesian lasso logistic regression model was used for variable selection. We used a hierarchical framework by representing discrete probability models with continuous latent variables that depended on the risk factors for a particular patient. Bayesian conditional probit regression and Markov chain Monte Carlo simulations were then used to estimate the effects of the predictors on the means of these latent variables to create a score function for each of the study outcomes. We also devised a method to calculate the risk of outcomes associated with the Norwood operation before the actual heart operation. The 2 study outcomes evaluated were inâ€hospital mortality and composite poor outcome. Results: The training dataset used 520 patients to generate the prediction model. The model included patient demographics, baseline characteristics, cardiac diagnosis, operation details, site volume, and surgeon experience. An online calculator for the tool can be accessed at https://soipredictiontool.shinyapps.io/NorwoodScoreApp/. Model validation was performed on 520 observations using an internal 10â€fold crossâ€validation approach. The prediction model had an area under the curve of 0.77 for mortality and 0.72 for composite poor outcome on the validation dataset. Conclusions: Our new prognostic tool is a promising first step in creating realâ€time risk stratification in children undergoing a Norwood operation; this tool will be beneficial for the purposes of benchmarking, family counseling, and research.",2017,The Journal of Thoracic and Cardiovascular Surgery
A mathematical framework for virtual IMRT QA using machine learning.,"PURPOSE
It is common practice to perform patient-specific pretreatment verifications to the clinical delivery of IMRT. This process can be time-consuming and not altogether instructive due to the myriad sources that may produce a failing result. The purpose of this study was to develop an algorithm capable of predicting IMRT QA passing rates a priori.


METHODS
From all treatment, 498 IMRT plans sites were planned in eclipse version 11 and delivered using a dynamic sliding window technique on Clinac iX or TrueBeam Linacs. 3%/3 mm local dose/distance-to-agreement (DTA) was recorded using a commercial 2D diode array. Each plan was characterized by 78 metrics that describe different aspects of their complexity that could lead to disagreements between the calculated and measured dose. A Poisson regression with Lasso regularization was trained to learn the relation between the plan characteristics and each passing rate.


RESULTS
Passing rates 3%/3 mm local dose/DTA can be predicted with an error smaller than 3% for all plans analyzed. The most important metrics to describe the passing rates were determined to be the MU factor (MU per Gy), small aperture score, irregularity factor, and fraction of the plan delivered at the corners of a 40 Ã— 40 cm field. The higher the value of these metrics, the worse the passing rates.


CONCLUSIONS
The Virtual QA process predicts IMRT passing rates with a high likelihood, allows the detection of failures due to setup errors, and it is sensitive enough to detect small differences between matched Linacs.",2016,Medical physics
Spatial Filtering for EEG-Based Regression Problems in Brainâ€“Computer Interface (BCI),"Electroencephalogram (EEG) signals are frequently used in brainâ€“computer interfaces (BCIs), but they are easily contaminated by artifacts and noise, so preprocessing must be done before they are fed into a machine learning algorithm for classification or regression. Spatial filters have been widely used to increase the signal-to-noise ratio of EEG for BCI classification problems, but their applications in BCI regression problems have been very limited. This paper proposes two common spatial pattern (CSP) filters for EEG-based regression problems in BCI, which are extended from the CSP filter for classification, by using fuzzy sets. Experimental results on EEG-based response speed estimation from a large-scale study, which collected 143 sessions of sustained-attention psychomotor vigilance task data from 17 subjects during a 5-month period, demonstrate that the two proposed spatial filters can significantly increase the EEG signal quality. When used in LASSO and <inline-formula><tex-math notation=""LaTeX"">$k$</tex-math></inline-formula>-nearest neighbors regression for user response speed estimation, the spatial filters can reduce the root-mean-square estimation error by <inline-formula><tex-math notation=""LaTeX"">$10.02-19.77\%$</tex-math></inline-formula>, and at the same time increase the correlation to the true response speed by <inline-formula><tex-math notation=""LaTeX"">$19.39-86.47\%$</tex-math></inline-formula>.",2018,IEEE Transactions on Fuzzy Systems
From Lasso regression to Feature vector machine,"Lasso regression tends to assign zero weights to most irrelevant or redundant features, and hence is a promising technique for feature selection. Its limitation, however, is that it only offers solutions to linear models. Kernel machines with feature scaling techniques have been studied for feature selection with non-linear models. However, such approaches require to solve hard non-convex optimization problems. This paper proposes a new approach named the Feature Vector Machine (FVM). It reformulates the standard Lasso regression into a form isomorphic to SVM, and this form can be easily extended for feature selection with non-linear models by introducing kernels defined on feature vectors. FVM generates sparse solutions in the nonlinear feature space and it is much more tractable compared to feature scaling kernel machines. Our experiments with FVM on simulated data show encouraging results in identifying the small number of dominating features that are non-linearly correlated to the response, a task the standard Lasso fails to complete.",2005,
Fast and Accurate Least-Mean-Squares Solvers,"Least-mean squares (LMS) solvers such as Linear / Ridge / Lasso-Regression, SVD and Elastic-Net not only solve fundamental machine learning problems, but are also the building blocks in a variety of other methods, such as decision trees and matrix factorizations. 
We suggest an algorithm that gets a finite set of $n$ $d$-dimensional real vectors and returns a weighted subset of $d+1$ vectors whose sum is \emph{exactly} the same. The proof in Caratheodory's Theorem (1907) computes such a subset in $O(n^2d^2)$ time and thus not used in practice. Our algorithm computes this subset in $O(nd)$ time, using $O(\log n)$ calls to Caratheodory's construction on small but ""smart"" subsets. This is based on a novel paradigm of fusion between different data summarization techniques, known as sketches and coresets. 
As an example application, we show how it can be used to boost the performance of existing LMS solvers, such as those in scikit-learn library, up to x100. Generalization for streaming and distributed (big) data is trivial. Extensive experimental results and complete open source code are also provided.",2019,ArXiv
The effect of unemployment on the smoking behavior of couples.,"Although unemployment likely entails various externalities, research examining its spillover effects on spouses is scarce. This is the first paper to estimate effects of unemployment on the smoking behavior of both spouses. Using German Socio-Economic Panel data, we combine matching and difference-in-differences estimation, employing the post-double-selection method for control variable selection via Lasso regressions. One spouse's unemployment increases both spouses' smoking probability and intensity. Smoking relapses and decreased smoking cessation drive the effects. Effects are stronger if the partner already smokes and if the male partner becomes unemployed. Of several mechanisms discussed, we identify smoking to cope with stress as relevant.",2019,Health economics
Kernel Learning: Automatic Selection of Optimal Kernels,"Kernel methods are widely used to address a variety of learning tasks including classification, regression, ranking, clustering, and dimensionality reduction. The appropriate choice of a kernel is often left to the user. But, poor selections may lead to sub-optimal performance. Furthermore, searching for an appropriate kernel manually may be a time-consuming and imperfect art. Instead, the kernel selection process can be included as part of the overall learning problem. In this way, better performance guarantees can be given and the kernel selection process can be made automatic. In this workshop, we will be concerned with using sampled data to select or learn a kernel function or kernel matrix appropriate for the specific task at hand. We will discuss several scenarios, including classification, regression, and ranking, where the use of kernels is ubiquitous, and different settings including inductive, transductive, or semi-supervised learning. We also invite discussions on the closely related fields of features selection and extraction, and are interested in exploring further the connection with these topics. The goal is to cover all questions related to the problem of learning kernels: different problem formulations, the computational efficiency and accuracy of the algorithms that address these problems and their different strengths and weaknesses, and the theoretical guarantees provided. What is the computational complexity? Does it work in practice? The formulation of some other learning problems, e.g. multi-task learning problems, is often very similar. These problems and their solutions will also be discussed in this workshop. 7:30-8:00 Invited Speaker: Shai Ben-David The Sample Complexity of Learning the Kernel 8:00-8:20 Olivier Chapelle and Alain Rakotomamonjy Second Order Optimization of Kernel Parameters 8:20-8:50 Invited Speaker: William Stafford Noble Multi-Kernel Learning for Biology 8:50-9:20 Poster Session and Discussion 9:20-9:40 Corinna Cortes, Mehryar Mohri and Afshin Rostamizadeh Learning Sequence Kernels 9:40-10:00 Maria-Florina Balcan, Avrim Blum and Nathan Srebro Learning with Multiple Similarity Functions 10:00-10:30 Invited Speaker: Andreas Argyriou Multi-Task Learning via Matrix Regularization 10:30-15:30 Break until afternoon session. 15:30-16:00 Invited Speaker: Isabelle Guyon Feature Selection: From Correlation to Causality 16:00-16:20 Nathan Srebro and Shai Ben-David Learning Bounds for Support Vector Machines with Learned Kernels 16:20-16:50 Invited Speaker: Alex Smola Mixed Norm Kernels, Hyperkenels and Other Variants 16:50-17:20 Poster Session and Discussion 17:20-17:40 Marius Kloft, Ulf Brefeld, Pavel Laskov and SÃ¶ren Sonnenburg Non-sparse Multiple Kernel Learning 17:40-18:00 Peter Gehler Infinite Kernel Learning 18:00-18:30 Invited Speaker: John Shawe-Taylor Kernel Learning for Novelty Detection 18:30 Closing Remarks The Sample Complexity of Learning the Kernel Shai Ben-David, University of Waterloo The success of kernel based learning algorithms depends upon the suitability of the kernel to the learning task. Ideally, the choice of a kernel should based on prior information of the learner about the task at hand. However, in practice, kernel parameters are being tuned based on available training data. I will discuss the sample complexity overhead associated with such â€learning the kernelâ€ scenarios. I will address the setting in which the training data for the kernel selection is target labeled examples, as well as settings in which this training is based on different types of data, such as unlabeled examples and examples labeled by a different (but related) tasks. Part of this work is joint with Nati Srebro. Second Order Optimization of Kernel Parameters Olivier Chapelle et al., Yahoo! Research & University Rouen We investigate the use of second order optimization approaches for solving the multiple kernel learning (MKL) problem. We show that the hessian of the MKL can be computed efficiently and this information can be used to compute a better descent direction than the gradient (used in the state-of-the-art SimpleMKL algorithm). We then empirically show that our new approaches outperforms SimpleMKL in terms of computational efficiency. Multi-Kernel Learning for Biology William Stafford Noble, University of Washington One of the primary tasks facing biologists today is to integrate the different views of molecular biology that are provided by various types of experimental data. In yeast, for example, for a given gene we typically know the protein it encodes, that proteinâ€™s similarity to other proteins, the mRNA expression levels associated with the given gene under hundreds of experimental conditions, the occurrences of known or inferred transcription factor binding sites in the upstream region of that gene, and the identities of many of the proteins that interact with the given geneâ€™s protein product. Each of these distinct data types provides one view of the molecular machinery of the cell. Kernel methods allow us to represent these heterogeneous data types in a normal form, and to use kernel algebra to reason about more than one type of data simultaneously. Consequently, multi-kernel learning methods have been applied to a variety of biology applications. In this talk, I will describe several of these applications, outline the lessons we have learned from applying multi-kernel learning methods to real data, and suggest several avenues for future research in this area. Learning Sequence Kernels Corinna Cortes et al., Google Research & Courant Institute Kernel methods are used to tackle a variety of learning tasks including classification, regression, ranking, clustering, and dimensionality reduction. The appropriate choice of a kernel is often left to the user. But, poor selections may lead to a sub-optimal performance. Instead, sample points can be used to learn a kernel function appropriate for the task by selecting one out of a family of kernels determined by the user. This paper considers the problem of learning sequence kernel functions, an important problem for applications in computational biology, natural language processing, document classification and other text processing areas. For most kernel-based learning techniques, the kernels selected must be positive definite symmetric, which, for sequence data, are found to be rational kernels. We give a general formulation of the problem of learning rational kernels and prove that a large family of rational kernels can be learned efficiently using a simple quadratic program both in the context of support vector machines and kernel ridge regression. This improves upon previous work that generally results in a more costly semi-definite or quadratically constrained quadratic program. Furthermore, in the specific case of kernel ridge regression, we give an alternative solution for the optimal kernel matrix, which in fact coincides with the objective prescribed by kernel alignment techniques. Learning with Multiple Similarity Functions Maria-Florina Balcan et al., Microsoft Research & Carnegie Mellon University & Toyota Technological Institute Kernel functions have become an extremely popular tool in machine learning, with many applications and an attractive theory. There has also been substantial work on learning kernel functions from data [LCBGJ04,SB06,AHMP08]. A sufficient condition for a kernel to allow for good generalization on a given learning problem is that it induce a large margin of separation between positive and negative classes in its implicit space. In recent work [BBS08,BBS07,BB06] we have developed a theory that more broadly holds for general similarity functions that are not necessarily legal kernel functions. In particular, we give sufficient conditions for a similarity function to be useful for learning that (a) are fairly natural and intuitive (do not require an implicit space and allow for functions that are not positive semi-definite) and (b) strictly generalize the notion of a large-margin kernel function in that any such kernel also satisfies these conditions, though not necessarily vice-versa. We also have partial progress on extending the theory of learning with multiple kernel functions to these more general conditions. In this talk we describe the main definitions and results of [BBS08], give our results on learning with multiple similarity functions, and present several open questions about learning good general similarity functions from data. Multi-Task Learning via Matrix Regularization Andreas Argyriou, University College London We present a method for learning representations shared across multiple tasks. The method consists in learning a low-dimensional subspace on which task regression vectors lie. Our formulation is a convex optimization problem, which we solve with an alternating minimization algorithm. This algorithm can be shown to always converge to an optimal solution. Our method can also be viewed as learning a linear kernel shared across the tasks and hence as an instance of kernel learning in which there are infinite kernels available. Moreover, the method can easily be extended in order to learn multiple tasks using nonlinear kernels. To justify this, we present general results characterizing representer theorems for matrix learning problems like the one above, as well as standard representer theorems. Finally, we briefly describe how our method connects to approaches exploiting sparsity such as group Lasso. Feature Selection: From Correlation to Causality Isabelle Guyon, Clopinet, Berkeley Variable and feature selection have become the focus of much research in areas of application for which datasets with tens or hundreds of thousands of variables are available. These areas include text processing of internet documents, gene expression array analysis, and combinatorial chemistry. The objective of variable selection is three-fold: improving the prediction performance of the pr",2008,
Regularization and model selection for quantile varying coefficient model with categorical effect modifiers,"A varying coefficient model with categorical effect modifiers is an effective modeling strategy when the data set includes categorical variables. With categorial predictors the number of parameters can become very large. This paper focuses on the model selection problem for varying coefficient model with categorical effect modifiers under the framework of quantile regression. After distinguishing between nominal and ordinal effect modifiers, a unified (adaptive-) Lasso-type regularization technique is proposed that allows for selection of covariates and fusion of categories of categorical effect modifiers, which can identify whether the coefficient functions are really varying with the level of a potentially effect modifying factor and provide a sparse model at different quantile levels. Moreover, the large sample properties are derived under appropriate conditions including a fixed bound on the number of parameters. The proposed methods are illustrated and investigated by extensive simulation studies and two real data evaluations.",2014,Comput. Stat. Data Anal.
Model selection via penalization in the additive Cox model,"The Cox proportional hazards model is the most popular model for the analysis of survival data. It allows estimating the relationship between covariates and a possibly censored failure time. The corresponding partial likelihood estimators are used for the estimation and prediction of relative risk of failure. However, if the explanatory variables are highly correlated or if the number of failures is not much greater than the number of covariates of interest, then partial likelihood estimators are unstable and have large variance. Penalization is extensively used to address these difficulties. It decreases the predictor variability to improve the accuracy of prediction. Ridge regression (l2-penalization) is one of the main penalization procedures. It has been generalized to the nonparametric setting to reduce the possibility of overfitting with high dimensional models. Thus, smoothing splines are used to estimate flexibly covariate effects in the additive Cox model. Tibshirani's lasso (l1-penalization) has also been applied to the Cox model, providing an alternative to quadratic penalization. An attractive feature of the l1-penalization is that it shrinks coefficients and sets some of them to zero, performing parameter estimation and variable selection simultaneously. We propose a new algorithm for variable selection and function estimation in the additive Cox model. The method is based on a generalization of the lasso to the nonparametric setting. Our proposal maximizes a penalized partial likelihood that includes a double penalty: on the l1-norm of linear components and on the (generalized) l1-norm of nonlinear components of spline coefficients. Because of their nature, these penalties shrink linear and nonlinear compounds, some of them reducing exactly to zero. Hence they give parsimonious models, select significant variables, and reveal nonlinearities in the effects of predictors. Our approach is compared to standard methods by simulations and an example. Different techniques to choose the penalty parameters are also tested.",2005,
An Exploratory Analysis of Biased Learners in Soft-Sensing Frames,"Data driven soft sensor design has recently gained immense popularity, due to advances in sensory devices, and a growing interest in data mining. While partial least squares (PLS) is traditionally used in the process literature for designing soft sensors, the statistical literature has focused on sparse learners, such as Lasso and relevance vector machine (RVM), to solve the high dimensional data problem. In the current study, predictive performances of three regression techniques, PLS, Lasso and RVM were assessed and compared under various offline and online soft sensing scenarios applied on datasets from five real industrial plants, and a simulated process. In offline learning, predictions of RVM and Lasso were found to be superior to those of PLS when a large number of time-lagged predictors were used. Online prediction results gave a slightly more complicated picture. It was found that the minimum prediction error achieved by PLS under moving window (MW), or just-in-time learning scheme was decreased up to ~5-10% using Lasso, or RVM. However, when a small MW size was used, or the optimum number of PLS components was as low as ~1, prediction performance of PLS surpassed RVM, which was found to yield occasional unstable predictions. PLS and Lasso models constructed via online parameter tuning generally did not yield better predictions compared to those constructed via offline tuning. We present evidence to suggest that retaining a large portion of the available process measurement data in the predictor matrix, instead of preselecting variables, would be more advantageous for sparse learners in increasing prediction accuracy. As a result, Lasso is recommended as a better substitute for PLS in soft sensors; while performance of RVM should be validated before online application.",2019,ArXiv
Penalized-regression-based multimarker genotype analysis of Genetic Analysis Workshop 17 data,"Testing for association between multiple markers and a phenotype can not only capture untyped causal variants in weak linkage disequilibrium with nearby typed markers but also identify the effect of a combination of markers. We propose a sliding window approach that uses multimarker genotypes as variables in a penalized regression. We investigate a penalty with three separate components: (1) a group least absolute shrinkage and selection operator (LASSO) that selects multimarker genotypes in a gene to be included in or excluded from the model, (2) an allele-sharing penalty that encourages multimarker genotypes with similar alleles to have similar coefficients, and (3) a penalty that shrinks the size of coefficients while performing model selection. The penalized likelihood is minimized with a cyclic coordinate descent algorithm, allowing quick coefficient estimation for a large number of markers. We compare our method to single-marker analysis and a gene-based sparse group LASSO on the Genetic Analysis Workshop 17 data for quantitative trait Q2. We found that all of the methods were underpowered to detect the simulated rare causal variants at the low false-positive rates desired in association studies. However, the sparse group LASSO on multi-marker genotypes seems to provide some advantage over the sparse group LASSO applied to single SNPs within genes, giving further evidence that there may be an advantage to modeling combinations of rare variant alleles over modeling them individually.",2011,BMC Proceedings
Radiomics nomogram for preoperative differentiation of lung tuberculoma from adenocarcinoma in solitary pulmonary solid nodule.,"PURPOSE
To investigate the preoperative differential diagnostic performance of a radiomics nomogram in tuberculous granuloma (TBG) and lung adenocarcinoma (LAC) appearing as solitary pulmonary solid nodules (SPSNs).


METHOD
We retrospectively recruited 426 patients with SPSNs from two centers and assigned them to training (nâ€¯=â€¯123), internal validation (nâ€¯=â€¯121), and external validation cohorts (nâ€¯=â€¯182). A model of deep learning (DL) was built for tumor segmentation from routine computed tomography (CT) images and extraction of 3D radiomics features. We used the least absolute shrinkage and selection operator (LASSO) logistic regression to build a radiomics signature. A clinical model was developed with clinical factors, including age, gender, and CT-based subjective findings (eg, lesion size, lesion location, lesion margin, lobulated sharp, and spiculation sign). We constructed individualized radiomics nomograms incorporating the radiomics signature and clinical factors to validate the diagnostic ability.


RESULTS
Three factors - radiomics signature, age, and spiculation sign - were found to be independent predictors and were used to build the radiomics nomogram, which showed better diagnostic accuracy than any single model (all net reclassification improvement pâ€¯<â€¯0.05). The area under curve yielded was 0.9660 (95% confidence interval [CI], 0.9390-0.9931), 0.9342 (95% CI, 0.8944-0.9739), and 0.9064 (95% CI, 0.8639-0.9490) for the training, internal validation, and external validation cohorts, respectively. Decision curve analysis (DCA) and stratification analysis showed the nomogram has potential for generalizability.


CONCLUSION
The radiomics nomogram we developed can preoperatively distinguish between LAC and TBG in patient with a SPSN.",2020,European journal of radiology
Statistical Debugging Using a Hierarchical Model of Correlated Predicates,"The aim of statistical debugging is to identify faulty predicates that have strong effect on program failure. In this paper predicates are fitted into a linear regression model to consider the vertical effect of predicates on each other and on program termination status. Prior approaches have merely considered predicates in isolation. The proposed approach in this paper is a twostep procedure which includes hierarchical clustering and the Lasso regression method. Hierarchical clustering builds a tree structure of correlated predicates. The Lasso method is applied on the clusters in some specified levels of the tree. This makes the method scalable in terms of the size of a program. Unlike other statistical methods which do not provide any context of the failure, the predicates contained in the group that is provided by this method can be used as the bug signature. The method has been evaluated on two well-known test suites, Space and Siemens. The experimental results reveal the accuracy and precision of the approach comparing with similar techniques.",2011,
Learning Low-rank and Sparse Discriminative Correlation Filters for Coarse-to-Fine Visual Object Tracking,"Discriminative correlation filter (DCF) has achieved advanced performance in visual object tracking with remarkable efficiency guaranteed by its implementation in the frequency domain. However, the effect of the structural relationship of DCF and object features has not been adequately explored in the context of the filter design. To remedy this deficiency, this paper proposes a Low-rank and Sparse DCF (LSDCF) that improves the relevance of features used by discriminative filters. To be more specific, we extend the classical DCF paradigm from ridge regression to lasso regression, and constrain the estimate to be of low-rank across frames, thus identifying and retaining the informative filters distributed on a low-dimensional manifold. To this end, specific temporal-spatial-channel configurations are adaptively learned to achieve enhanced discrimination and interpretability. In addition, we analyse the complementary characteristics between hand-crafted features and deep features, and propose a coarse-to-fine heuristic tracking strategy to further improve the performance of our LSDCF. Last, the augmented Lagrange multiplier optimisation method is used to achieve efficient optimisation. The experimental results obtained on a number of well-known benchmarking datasets, including OTB2013, OTB50, OTB100, TC128, UAV123, VOT2016 and VOT2018, demonstrate the effectiveness and robustness of the proposed method, delivering outstanding performance compared to the state-of-the-art trackers.",2019,IEEE Transactions on Circuits and Systems for Video Technology
Projection algorithms for large scale optimization and genomic data analysis,"The advent of the Big Data era has spawned intense interest in scalable mathematical optimization methods. Traditional approaches such as Newtonâ€™s method fall apart whenever the features outnumber the examples in a data set. Consequently, researchers have intensely developed first-order methods that rely only on gradients and subgradients of a cost function.In this dissertation we focus on projected gradient methods for large-scale con-strained optimization. We develop a particular case of a proximal gradient methodcalled the proximal distance algorithm. Proximal distance algorithms combine theclassical penalty method of constrained minimization with distance majorization. Tooptimize the loss function $f(x)$ over a constraint set $C$, the proximal distance principle mandates minimizing the penalized loss $f(x) + \rho \mathrm{dist} \; (x,C)^2$ and following the solution $x_{\rho}$ to its limit as $\rho \to \infty$. At each iteration the squared Euclidean distance $\mathrm{dist} \; (x, C)^2$ is majorized by $\| x âˆ’ \Pi_{C}(x_k) \|_2^2$, where $\Pi_{C}(x_k)$ denotes the projection of the current iterate $x_k$ onto $C$. The minimum of the surrogate function $f(x) + \rho \| x âˆ’ \Pi_{C} (x_k) \|_2^2$ is given by the proximal map $\mathrm{prox}_{Ï^{âˆ’1}} \; f [ \Pi_{C} (x_k )]$. The next iterate $x_{k+1}$ automatically decreases the original penalized loss for fixed $\rho$. Since many explicit projections and proximal maps are known in analytic or computable form, the proximal distance algorithm provides a scalable computational framework for a variety of constraints.For the particular case of sparse linear regression, we implement a projected gradient algorithm known as iterative hard thresholding for a particular large-scale genomics analysis known as a genome-wide association study. A genome-wide association study (GWAS) correlates marker variation with trait variation in a sample of individuals. Each study subject is genotyped at a multitude of SNPs (single nucleotide polymorphisms) spanning the genome. Here we assume that subjects are unrelated and collected at random and that trait values are normally distributed or transformed to normality. Over the past decade, researchers have been remarkably successful in applying GWAS analysis to hundreds of traits. The massive amount of data produced in these studies present unique computational challenges. Penalized regression with LASSO or MCP penalties is capable of selecting a handful of associated SNPs from millions of potential SNPs. Unfortunately, model selection can be corrupted by false positives and false negatives, obscuring the genetic underpinning of a trait. Our parallel implementation of IHT accommodates SNP genotype compression and exploits multiple CPU cores and graphics processing units (GPUs). This allows statistical geneticists to leverage desktop workstations in GWAS analysis and to eschew expensive supercomputing resources. We evaluate IHT performance on both simulated and real GWAS data and conclude that it reduces false positive and false negative rates while remaining competitive in computational time with penalized regression.",2016,
An Efficient Approach to Sparse Linear Discriminant Analysis,"We present a novel approach to the formulation and the resolution of sparse Linear Discriminant Analysis (LDA). Our proposal, is based on penalized Optimal Scoring. It has an exact equivalence with penalized LDA, contrary to the multi-class approaches based on the regression of class indicator that have been proposed so far. Sparsity is obtained thanks to a group-Lasso penalty that selects the same features in all discriminant directions. Our experiments demonstrate that this approach generates extremely parsimonious models without compromising prediction performances. Besides prediction, the resulting sparse discriminant directions are also amenable to low-dimensional representations of data. Our algorithm is highly efficient for medium to large number of variables, and is thus particularly well suited to the analysis of gene expression data.",2012,
The 1â€year Renal Biopsy Index: a scoring system to drive biopsy indication at 1â€year postâ€kidney transplantation,"Surveillance biopsies after renal transplantation remain debatable. To drive the decision of such intervention, we propose a predictive score of abnormal histology at 1-year post-transplantation, named 1-year Renal Biopsy Index (1-RBI). We studied 466 kidney recipients from the DIVAT cohort alive with a functioning graft and a surveillance biopsy at 1-year post-transplantation. Patients displaying abnormal histology (49%) (borderline, acute rejection, interstitial fibrosis and tubular atrophy [IFTA] grade 2 or 3, glomerulonephritis) were compared to the normal or subnormal (IFTA grade 1) histology group. Obtained from a lasso penalized logistic regression, the 1-RBI was composed of recipient gender, serum creatinine at 3, 6, and 12 month post-transplantation and anticlass II immunization at transplantation (internal validation: AUC = 0.71, 95% CI [0.53-0.83]; external validation: AUC = 0.62, 95% CI [0.58-0.66]). While we could not determinate a threshold able to identify patients at high chance of normal or subnormal histology, we estimated and validated a discriminating threshold capable of identifying a subgroup of 15% of the patients with a risk of abnormal histology higher than 80%. The 1-RBI is computable online at www.divat.fr. The 1-RBI could be a useful tool to standardize 1-year biopsy proposal and may for instance help to indicate one in case of high risk of abnormal histology.",2018,Transplant International
Comparing Performance of Text Pre-processing Methods for Predicting A Binary Position by LASSO,"This work aims at comparing different methods of preparing textual inputs for LASSO logistic regression to predict a binary position with the textual data extracted from a public consultation of the European Commission. Texts are pre-processed and then input into LASSO to explain the stakeholderâ€™s position, and the mean squared errors are computed to compare different methods. In short, we do not find a clearcut winner. On average, tf-idf performs better than counts of distinct terms, and deleting terms that appear only once reduces the prediction errors.",2018,
High SNR Consistent Compressive Sensing Without Signal and Noise Statistics,"Recovering the support of sparse vectors in underdetermined linear regression models, \textit{aka}, compressive sensing is important in many signal processing applications. High SNR consistency (HSC), i.e., the ability of a support recovery technique to correctly identify the support with increasing signal to noise ratio (SNR) is an increasingly popular criterion to qualify the high SNR optimality of support recovery techniques. The HSC results available in literature for support recovery techniques applicable to underdetermined linear regression models like least absolute shrinkage and selection operator (LASSO), orthogonal matching pursuit (OMP) etc. assume \textit{a priori} knowledge of noise variance or signal sparsity. However, both these parameters are unavailable in most practical applications. Further, it is extremely difficult to estimate noise variance or signal sparsity in underdetermined regression models. This limits the utility of existing HSC results. In this article, we propose two techniques, \textit{viz.}, residual ratio minimization (RRM) and residual ratio thresholding with adaptation (RRTA) to operate OMP algorithm without the \textit{a priroi} knowledge of noise variance and signal sparsity and establish their HSC analytically and numerically. To the best of our knowledge, these are the first and only noise statistics oblivious algorithms to report HSC in underdetermined regression models.",2020,ArXiv
BIOTECHNO 2016 Proceedings,"We propose a Matroska feature selection method (Method 2) for microarray datasets (the datasets). We had already established a new theory of the discriminant analysis (Theory) and developed an optimal Linear Discriminant Function (OLDF) named Revised IP-OLDF. This LDF can naturally select features for the datasets. The dataset consists of several small genes subspaces that we call small Matroskas (SMs) and are linearly separable. We confirmed this feature selection of Revised IP-OLDF by Swiss banknote data and Japanese automobile data, also. Therefore, we need not struggle with high-dimension genes space. In this paper, we develop a LINGO program to find all SMs and confirm that the dataset consists of disjoint union of SMs and high-dimension subspace that is not linearly separable. Because it is very easy for us to analyze these SMs that are small samples, we may be able to find new facts of gene analysis. Lasso researchers will have better results compared with our results. KeywordsMinimum Number of Misclassifications (MNM); Revised IP-OLDF; SVM; Fisherâ€™s LDF; Gene Analysis; Small Matroska (SM); Basic Gene Subspase (BGS); Lasso. I. INTRODCTION Fisher [6] [7] developed a Linear Discriminant Function (Fisherâ€™s LDF) under Fisherâ€™s assumption and established the theory of discriminant analysis. Because Fisherâ€™s assumption was too strict for the real data, a Quadratic Discriminant Function (QDF) was developed. In addition to two discriminant functions, logistic regression [4] and a Regulalized Discriminant Analysis (RDA) [9] were proposed as the statistical discriminant functions. These statistical discriminant functions apply for many applications, and statistical software packages became essential tools for the science and industries. On the other hand, it is well known that Mathematical Programming (MP) can define the discriminant models [16]. Linear Programming (LP) sets out Least Absolute Deviation (LAD) discriminant function. Quadratic Programming (QP) defines an L2-norm discriminant function (Least square method). Nonlinear Programming (NLP) defines Lp-norm discriminant functions. Before 1997, there were many papers of MP-based discriminant functions summarized by Stam [57]. We think the first generation research ended in 1997 because these researches lacked examination of real data and comparison with statistical discriminant functions. Vapnik [61] proposed three Support Vector Machines (SVMs) such as a Hardmargin SVM (H-SVM), Soft-margin SVM (S-SVM) and kernel SVM in 1995. H-SVM clearly defined a Linearly Separable Data (LSD) and generalization ability. However, because most real data are not LSD, and H-SVM can be used only for LSD, we use S-SVM for actual data. QP defines these SVMs. Although kernel SVM is one of nonlinear discriminant function and provides an attractive idea, we do not discuss it in this research because our concern is a comparison of LDFs. Many researchers use SVMs because there are many examinations of real data compared with the first generation research of MP-based discriminant theory. From 1971 to 1974, we became a member of the project to develop a computer system for an Electrocardiogram (ECG) data. Project leader, Doc. Nomura gave us a theme to develop a diagnostic logic using Fisherâ€™s LDF. Our research was inferior to Nomuraâ€™s experimental decision tree algorithm. At first, we thought this failure was caused by our poor experience and knowledge of statistics. However, we considered the discriminant functions based on the variancecovariance matrices were not suitable for the medical diagnosis discussed in Section II. Moreover, we found all LDFs cannot correctly discriminate the cases on the discriminant hyperplane (Problem 1). In Section II, although Fisher established discriminant analysis based on variance-covariance matrices, we explain a new theory of MP-based discriminant analysis (Theory) [53]. At first, we developed an Optimal LDF based on a Minimum Number of Misclassifications (minimum NM, MNM) criterion (IP-OLDF) in (1) [19] [21]. It reveals two important facts of discriminant analysis. Those are 1) the relation of NM and LDF in the discriminant coefficient space, 2) monotonic decrease of MNM that is very crucial for gene analysis. It shows the good result by comparison with Fisherâ€™s LDF and QDF using Fisherâ€™s iris data [2] and Cephalo Pelvic Disproportion (CPD) data [14]. It finds Swiss banknote data is LSD [8]. All LDFs except for H-SVM and Revised IP-OLDF in (2) cannot discriminate LSD theoretically (Problem 2). Experimentally, Revised LPOLDF in (2), one of L1-norm LDF using LP, can discriminate LSD. Nevertheless, it tends to gather cases on the discriminant hyperplane (Problem 1). Student data [24] reveals the defect of IP-OLDF caused by Problem 1. Therefore, Revised IP-OLDF is developed. It is only LDF to solve Problem 1. The pass/fail determination using exam scores [28] shows the defect of QDF and RDA caused by the defect of generalized inverse of variance-covariance matrices (Problem 3). If we add random noise to constant values of some particular variable, we can solve Problem 3. Japanese automobile data [35] explain Problem 3, also. Because Fisher never formulate the equation of Standard Error (SE) of error rate and discriminant coefficient, discriminant analysis is not traditional inferential statistics based on normal distribution 1 Copyright (c) IARIA, 2016. ISBN: 978-1-61208-488-6 BIOTECHNO 2016 : The Eighth International Conference on Bioinformatics, Biocomputational Systems and Biotechnologies",2016,
Pemodelan Geographically Weighted Regression dengan Pembobot Fixed Gaussian Kernel pada Data Spasial (Studi Kasus Ketahanan Pangan di Kabupaten Tanah Laut Kalimantan Selatan),"Geographically Weighted Regression (GWR) merupakan suatu model regresi yang memperhatikan adanya efek heterogenitas spasial. Dalam model regresi, sering terdapat hubungan antara dua atau lebih variabel prediktor yang disebut multikolinieritas. Geographically Weighted Lasso (GWL) merupakan suatu metode spasial yang digunakan untuk mengatasi heterogenitas spasial dan multikolinieritas lokal. Tujuan penelitian ini membentuk model dengan menggunakan metode GWL dalam mengatasi kasus heterogenitas spasial dan multikolinieritas lokal pada masalah kerawanan pangan di Kabupaten Tanah Laut. Secara umum, kerawanan pangan di Kabupaten Tanah Laut dipengaruhi oleh persentase penduduk tanpa akses listrik, rata-rata jumlah toko/warung kelontong serta persentase kematian balita dan ibu melahirkan. Model GWL yang didapatkan sesuai dengan banyaknya lokasi pengamatan. Hasil validasi dengan data sekunder menunjukkan bahwa model yang diperoleh dalam penelitian telah sesuai dengan kondisi yang sebenarnya di lapangan. Model dengan pembobot Fixed Gaussian Kernel mampu memprediksi delapan desa dengan kondisi ketahanan pangan yang sama dengan data sekunder. K ata kunci : Multikolinieritas lokal, GWR, GWL, kerawanan pangan.",2014,
Circulating biomarkers may be unable to detect infection at the early phase of sepsis in ICU patients: the CAPTAIN prospective multicenter cohort study,"PurposeSepsis and non-septic systemic inflammatory response syndrome (SIRS) are the same syndromes, differing by their cause, sepsis being secondary to microbial infection. Microbiological tests are not enough to detect infection early. While more than 50 biomarkers have been proposed to detect infection, none have been repeatedly validated.AimTo assess the accuracy of circulating biomarkers to discriminate between sepsis and non-septic SIRS.MethodsThe CAPTAIN study was a prospective observational multicenter cohort of 279 ICU patients with hypo- or hyperthermia and criteria of SIRS, included at the time the attending physician considered antimicrobial therapy. Investigators collected blood at inclusion to measure 29 plasma compounds and ten whole blood RNAs, andâ€”for those patients included within working hoursâ€”14 leukocyte surface markers. Patients were classified as having sepsis or non-septic SIRS blindly to the biomarkers results. We used the LASSO method as the technique of multivariate analysis, because of the large number of biomarkers.ResultsDuring the study period, 363 patients with SIRS were screened, 84 having exclusion criteria. Ninety-one patients were classified as having non-septic SIRS and 188 as having sepsis. Eight biomarkers had an area under the receiver operating curve (ROC-AUC) over 0.6 with a 95% confidence interval over 0.5. LASSO regression identified CRP and HLA-DRA mRNA as being repeatedly associated with sepsis, and no model performed better than CRP alone (ROC-AUC 0.76 [0.68â€“0.84]).ConclusionsThe circulating biomarkers tested were found to discriminate poorly between sepsis and non-septic SIRS, and no combination performed better than CRP alone.",2018,Intensive Care Medicine
High-Dimensional Predictive Regression in the Presence of Cointegration,"While a great number of predictive variables for stock returns have been suggested, their prediction power is unstable. We propose a Least Absolute Shrinkage and Selection Operator (LASSO) estimator of a predictive regression in which stock returns are conditioned on a large set of lagged covariates, some of which are highly persistent and potentially cointegrated. We establish the asymptotic properties of the proposed LASSO estimator and validate our theoretical findings using simulation studies. The application of this proposed LASSO approach to forecasting stock returns suggests that cointegrating relationships among the persistent predictors leads to a significant improvement in the prediction of stock returns over various competing models in the mean squared error sense.",2020,Journal of Econometrics
