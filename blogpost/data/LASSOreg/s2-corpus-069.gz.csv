title,abstract,year,journal
Machine Learning-Based Temperature Prediction for Runtime Thermal Management Across System Components,"Elevated temperatures limit the peak performance of systems because of frequent interventions by thermal throttling. Non-uniform thermal states across system nodes also cause performance variation within seemingly equivalent nodes leading to significant degradation of overall performance. In this paper we present a framework for creating a lightweight thermal prediction system suitable for run-time management decisions. We pursue two avenues to explore optimized lightweight thermal predictors. First, we use feature selection algorithms to improve the performance of previously designed machine learning methods. Second, we develop alternative methods using neural network and linear regression-based methods to perform a comprehensive comparative study of prediction methods. We show that our optimized models achieve improved performance with better prediction accuracy and lower overhead as compared with the Gaussian process model proposed previously. Specifically we present a reduced version of the Gaussian process model, a neural networkâ€“based model, and a linear regressionâ€“based model. Using the optimization methods, we are able to reduce the average prediction errors in the Gaussian process from <inline-formula><tex-math notation=""LaTeX""> $4.2^\circ$</tex-math><alternatives><inline-graphic xlink:href=""ogrencimemik-ieq1-2732951.gif""/></alternatives> </inline-formula>C to <inline-formula><tex-math notation=""LaTeX"">$2.9^\circ$</tex-math><alternatives> <inline-graphic xlink:href=""ogrencimemik-ieq2-2732951.gif""/></alternatives></inline-formula>C. We also show that the newly developed models using neural network and Lasso linear regression have average prediction errors of <inline-formula><tex-math notation=""LaTeX"">$2.9^\circ$</tex-math><alternatives> <inline-graphic xlink:href=""ogrencimemik-ieq3-2732951.gif""/></alternatives></inline-formula>C and <inline-formula> <tex-math notation=""LaTeX"">$3.8^\circ$</tex-math><alternatives> <inline-graphic xlink:href=""ogrencimemik-ieq4-2732951.gif""/></alternatives></inline-formula>C respectively. The prediction overheads are 0.22, 0.097, and 0.026 ms per prediction for reduced Gaussian process, neural network, and Lasso linear regression models, respectively, compared with 0.57 ms per prediction for the previous Gaussian process model. We have implemented our proposed thermal prediction models on a two-node system configuration to help identify the optimal task placement. The task placement identified by the models reduces the average system temperature by up to <inline-formula><tex-math notation=""LaTeX"">$11.9^\circ$</tex-math><alternatives> <inline-graphic xlink:href=""ogrencimemik-ieq5-2732951.gif""/></alternatives></inline-formula>C without any performance degradation. Furthermore, these models respectively achieve 75, 82.5, and 74.17 percent success rates in correctly pointing to those task placements with better thermal response, compared with 72.5 percent success for the original model in achieving the same objective. Finally, we extended our analysis to a 16-node system and we were able to train models and execute them in real time to guide task migration and achieve on average 17 percent reduction in the overall system cooling power.",2018,IEEE Transactions on Parallel and Distributed Systems
A Differentiable Alternative to the Lasso Penalty,"Regularized regression has become very popular nowadays, particularly on high-dimensional problems where the addition of a penalty term to the log-likelihood allows inference where traditional methods fail. A number of penalties have been proposed in the literature, such as lasso, SCAD, ridge and elastic net to name a few. Despite their advantages and remarkable performance in rather extreme settings, where $p \gg n$, all these penalties, with the exception of ridge, are non-differentiable at zero. This can be a limitation in certain cases, such as computational efficiency of parameter estimation in non-linear models or derivation of estimators of the degrees of freedom for model selection criteria. With this paper, we provide the scientific community with a differentiable penalty, which can be used in any situation, but particularly where differentiability plays a key role. We show some desirable features of this function and prove theoretical properties of the resulting estimators within a regularized regression context. A simulation study and the analysis of a real dataset show overall a good performance under different scenarios. The method is implemented in the R package DLASSO freely available from CRAN.",2016,arXiv: Methodology
Penalized Cox models and Frailty,A very general mechanism for penalized regression has been added to the coxph function in S Plus A user written S Plus function can be supplied that gives addi tional term s to the partial likelihood along with the rst and second derivatives of those terms The variance and degrees of freedom for the extended model are then computed as outlined in Gray Several other arguments control optional aspects of the iteration This setup allows for general shrinkage methods including ridge regression the lasso smoothing splines and other techniques There is an interesting connection between penalized regression and random e ects or frailty models It happens that the gamma frailty model can be repre sented exactly as a penalized regression and a Gaussian frailty can be represented approximately Thus we can t these models as well using the generalized program,1998,
The Incidence of Venous Thromboembolism (VTE) in 892 Allogeneic Hematopoietic Cell Transplant (allo-HCT) Recipients ( A single institution study comparison of VTE incidence with sirolimus versus non-sirolimus-based GVHD prophylaxis),"s / Biol Blood Marrow Transplant 21 (2015) S266eS321 S297 multivariate analysis using the LASSO approach to logistic regression analysis. Results: Of 134 allo-HSCTs, 29 (21.6%) patients experienced CMV viremia. Among these patients, median age was 51 years (range 27-67), with 48 episodes of viremia. Nine (31%) viremic patients developed CMV disease. CMV disease occurred at a median of 124 days post HSCT (range 61-322). Patients with CMV disease had a median of 2 viremic episodes before disease (range 1-4). Disease occurred at a median of 33 days from the start of the last viremic episode, and 75 days from the start of the first episode of viremia. On univariate analysis, factors associated with progression to CMV disease were: steroid-refractory acute GVHD (60 vs. 20%, p1â„40.028); number of episodes of viremia >1x103 copies/mL (mean 2.4 vs 1.1, p1â„40.016); longer duration of viremia (mean 38 vs. 22 days, p1â„40.01); higher peak viral load (mean 4.49x105 vs. 8.31x103 copies/mL, p 1x103 copies/mL, a longer duration of viremia, and to have failed first-line pre-emptive ganciclovir therapy. This study, while limited, suggests that these risk factors may be predictive of CMV disease. Larger, prospective studies are needed to confirm these risk factors, some of which may be amenable to more aggressive anti-viral therapy.",2015,Biology of Blood and Marrow Transplantation
Learning to Share: simultaneous parameter tying and Sparsification in Deep Learning,"Deep neural networks (DNNs) usually contain millions, maybe billions, of parameters/weights, making both storage and computation very expensive. This has motivated a large body of work to reduce the complexity of the neural network by using sparsity-inducing regularizers. Another well-known approach for controlling the complexity of DNNs is parameter sharing/tying, where certain sets of weights are forced to share a common value. Some forms of weight sharing are hard-wired to express certain in- variances, with a notable example being the shift-invariance of convolutional layers. However, there may be other groups of weights that may be tied together during the learning process, thus further re- ducing the complexity of the network. In this paper, we adopt a recently proposed sparsity-inducing regularizer, named GrOWL (group ordered weighted l1), which encourages sparsity and, simulta- neously, learns which groups of parameters should share a common value. GrOWL has been proven effective in linear regression, being able to identify and cope with strongly correlated covariates. Unlike standard sparsity-inducing regularizers (e.g., l1 a.k.a. Lasso), GrOWL not only eliminates unimportant neurons by setting all the corresponding weights to zero, but also explicitly identifies strongly correlated neurons by tying the corresponding weights to a common value. This ability of GrOWL motivates the following two-stage procedure: (i) use GrOWL regularization in the training process to simultaneously identify significant neurons and groups of parameter that should be tied together; (ii) retrain the network, enforcing the structure that was unveiled in the previous phase, i.e., keeping only the significant neurons and enforcing the learned tying structure. We evaluate the proposed approach on several benchmark datasets, showing that it can dramatically compress the network with slight or even no loss on generalization performance.",2018,
Temporal causal modeling with graphical granger methods,"The need for mining causality, beyond mere statistical correlations, for real world problems has been recognized widely. Many of these applications naturally involve temporal data, which raises the challenge of how best to leverage the temporal information for causal modeling. Recently graphical modeling with the concept of ""Granger causality"", based on the intuition that a cause helps predict its effects in the future, has gained attention in many domains involving time series data analysis. With the surge of interest in model selection methodologies for regression, such as the Lasso, as practical alternatives to solving structural learning of graphical models, the question arises whether and how to combine these two notions into a practically viable approach for temporal causal modeling. In this paper, we examine a host of related algorithms that, loosely speaking, fall under the category of graphical Granger methods, and characterize their relative performance from multiple viewpoints. Our experiments show, for instance, that the Lasso algorithm exhibits consistent gain over the canonical pairwise graphical Granger method. We also characterize conditions under which these variants of graphical Granger methods perform well in comparison to other benchmark methods. Finally, we apply these methods to a real world data set involving key performance indicators of corporations, and present some concrete results.",2007,
Linear and Conic Programming Estimators in High-Dimensional Errors-in-variables Models,"We consider the linear regression model with observation error in the design. In this setting, we allow the number of covariates to be much larger than the sample size. Several new estimation methods have been recently introduced for this model. Indeed, the standard Lasso estimator or Dantzig selector turn out to become unreliable when only noisy regressors are available, which is quite common in practice. We show in this work that under suitable sparsity assumptions, the procedure introduced in Rosenbaum and Tsybakov (2013) is almost optimal in a minimax sense and, despite non-convexities, can be efficiently computed by a single linear programming problem. Furthermore, we provide an estimator attaining the minimax efficiency bound. This estimator is written as a second order cone programming minimisation problem which can be solved numerically in polynomial time.",2014,arXiv: Statistics Theory
GeneRank-based partly adaptive group-penalised multinomial regression for microarray classification,"This paper proposes a partly adaptive group-penalised multinomial regression for gene selection. Weights with biological significance are constructed by combing the gene expression information with gene ontology network via GeneRank. By introducing the weights into group lasso penalty, the partly adaptive group-penalised multinomial regression is proposed. Two algorithms for fitting the proposed model are presented on the base of blockwise descent. Experimental results on gene expression data of yeast diauxic shift demonstrate that the proposed method can select the stable genes and achieve the better classification performance.",2016,
Flexible discrete space models of animal movement,"Movement drives the spread of infectious disease, gene flow, and other critical ecological processes. To study these processes we need models for movement that capture complex behavior that changes over time and space in response to biotic and abiotic factors. Penalized likelihood approaches, such as penalized semiparametric spline expansions and LASSO regression, allow inference on complex models without overfitting. Continuous-time Markov chains (CTMCs) have been recently introduced as a flexible discrete-space model for animal movement. Modeling with CTMCs involves discretizing an animal's path to the resolution of a raster grid. The resulting stochastic process model can easily incorporate environmental and other covariates, represented as raster layers, that affect directional bias and overall movement rate. We introduce a weighted likelihood approach that allows for modeling movement using CTMCs, with path uncertainty due to missing data modeled by imputing continuous-time paths between telemetry locations. The framework we introduce allows for inference on CTMC movement models using existing software for fitting Poisson regression models, including penalized versions of Poisson regression. The result is a flexible, powerful, and accessible framework for modeling a wide range of animal movement behavior.",2016,arXiv: Applications
Penalized logistic regression with low prevalence exposures beyond high dimensional settings,"Estimating and selecting risk factors with extremely low prevalences of exposure for a binary outcome is a challenge because classical standard techniques, markedly logistic regression, often fail to provide meaningful results in such settings. While penalized regression methods are widely used in high-dimensional settings, we were able to show their usefulness in low-dimensional settings as well. Specifically, we demonstrate that Firth correction, ridge, the lasso and boosting all improve the estimation for low-prevalence risk factors. While the methods themselves are well-established, comparison studies are needed to assess their potential benefits in this context. This is done here using the dataset of a large unmatched case-control study from France (2005-2008) about the relationship between prescription medicines and road traffic accidents and an accompanying simulation study. Results show that the estimation of risk factors with prevalences below 0.1% can be drastically improved by using Firth correction and boosting in particular, especially for ultra-low prevalences. When a moderate number of low prevalence exposures is available, we recommend the use of penalized techniques.",2019,PLoS ONE
Large-scale nonconvex stochastic optimization by Doubly Stochastic Successive Convex approximation,"We consider supervised learning problems over training sets in which both the number of training examples and the dimension of the feature vectors are large. We focus on the case where the loss function defining the quality of the parameter we wish to estimate may be non-convex, but also has a convex regularization. We propose a Doubly Stochastic Successive Convex approximation scheme (DSSC) able to handle non-convex regularized expected risk minimization. The method operates by decomposing the decision variable into blocks and operating on random subsets of blocks at each step. The algorithm belongs to the family of successive convex approximation methods since we replace the original non-convex stochastic objective by a strongly convex sample surrogate function, and solve the resulting convex program, for each randomly selected block in parallel. The method operates on subsets of features (block coordinate methods) and training examples (stochastic approximation) at each step. In contrast to many stochastic convex methods whose almost sure behavior is not guaranteed in non-convex settings, DSSC attains almost sure convergence to a stationary solution of the problem. Numerical experiments on a non-convex variant of a lasso regression problem show that DSSC performs favorably in this setting.",2017,"2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
Placement Predict: A Review of Engineering Graduate Placement Statistics in India,"Prediction of graduate jobs is a key problem in the analysis of employability which can be addressed with data-driven strategies. Aspiring Minds dataset describes aptly the attributes of a freshly graduate engineering student, making it one of the most suitable datasets for prediction of graduate placements as well as for building machine learning models for analysis of employability. By implementing various machine learning repressors, we show that Ridge and Lasso regression performed marginally better than Random forest, followed closely by Support Vector Machines. The objective of this study was to predict the target salary and to analyse the overall employability of an engineering graduate. This methodology of data-driven approach can also serve as a foundation for future studies towards prediction of salary and placements, and identification of key features linked to employability of candidates.",2018,
A multivariate regression approach to association analysis of a quantitative trait network,"MOTIVATION
Many complex disease syndromes such as asthma consist of a large number of highly related, rather than independent, clinical phenotypes, raising a new technical challenge in identifying genetic variations associated simultaneously with correlated traits. Although a causal genetic variation may influence a group of highly correlated traits jointly, most of the previous association analyses considered each phenotype separately, or combined results from a set of single-phenotype analyses.


RESULTS
We propose a new statistical framework called graph-guided fused lasso to address this issue in a principled way. Our approach represents the dependency structure among the quantitative traits explicitly as a network, and leverages this trait network to encode structured regularizations in a multivariate regression model over the genotypes and traits, so that the genetic markers that jointly influence subgroups of highly correlated traits can be detected with high sensitivity and specificity. While most of the traditional methods examined each phenotype independently, our approach analyzes all of the traits jointly in a single statistical method to discover the genetic markers that perturb a subset of correlated traits jointly rather than a single trait. Using simulated datasets based on the HapMap consortium data and an asthma dataset, we compare the performance of our method with the single-marker analysis, and other sparse regression methods that do not use any structural information in the traits. Our results show that there is a significant advantage in detecting the true causal single nucleotide polymorphisms when we incorporate the correlation pattern in traits using our proposed methods.


AVAILABILITY
Software for GFlasso is available at http://www.sailing.cs.cmu.edu/gflasso.html.",2009,Bioinformatics
Simultaneous Signal Subspace Rank and Model Selection with an Application to Single-snapshot Source Localization,"This paper proposes a novel method for model selection in linear regression by utilizing the solution path of $\ell_{1}$ regularized least-squares (LS) approach (i.e., Lasso). This method applies the complex-valued least angle regression and shrinkage (c-LARS) algorithm coupled with a generalized information criterion (GIC) and referred to as the c-LARS-GIC method. c-LARS-GIC is a two-stage procedure, where firstly precise values of the regularization parameter, called knots, at which a new predictor variable enters (or leaves) the active set are computed in the Lasso solution path. Active sets provide a nested sequence of regression models and G I C then selects the best model. The sparsity order of the chosen model serves as an estimate of the model order and the LS fit based only on the active set of the model provides an estimate of the regression parameter vector. We then consider a source localization problem, where the aim is to detect the number of impinging source waveforms at a sensor array as well to estimate their direction-of-arrivals (DoA-S $)$ using only a single-snapshot measurement. We illustrate via simulations that, after formulating the problem as a grid-based sparse signal reconstruction problem, the proposed c-LARS-GIC method detects the number of sources with high probability while at the same time it provides accurate estimates of source locations.",2018,2018 26th European Signal Processing Conference (EUSIPCO)
Transcription Factor Profiling to Predict Recurrence-Free Survival in Breast Cancer: Development and Validation of a Nomogram to Optimize Clinical Management,"Breast cancer (BC) is the most frequently diagnosed cancer and the leading cause of cancer-related death in young women. Several prognostic and predictive transcription factor (TF) markers have been reported for BC; however, they are inconsistent due to small datasets, the heterogeneity of BC, and variation in data pre-processing approaches. This study aimed to identify an effective predictive TF signature for the prognosis of patients with BC. We analyzed the TF data of 868 patients with BC in The Cancer Genome Atlas (TCGA) database to investigate TF biomarkers relevant to recurrence-free survival (RFS). These patients were separated into training and internal validation datasets, with GSE2034 and GSE42568 used as external validation sets. A nine-TF signature was identified as crucially related to the RFS of patients with BC by univariate Cox proportional hazard analysis, least absolute shrinkage and selection operator (LASSO) Cox regression analysis, and multivariate Cox proportional hazard analysis in the training dataset. Kaplanâ€“Meier analysis revealed that the nine-TF signature could significantly distinguish high- and low-risk patients in both the internal validation dataset and the two external validation sets. Receiver operating characteristic (ROC) analysis further verified that the nine-TF signature showed a good performance for predicting the RFS of patients with BC. In addition, we developed a nomogram based on risk score and lymph node status, with C-index, ROC, and calibration plot analysis, suggesting that it displays good performance and clinical value. In summary, we used integrated bioinformatics approaches to identify an effective predictive nine-TF signature which may be a potential biomarker for BC prognosis.",2020,
Distributionally Robust Semi-supervised Learning,"We propose a novel method for semi-supervised learning based on data-driven distributionally robust optimization (DRO) using optimal transport metrics. Our proposed method enhances generalization error by using the non-labeled data to restrict the support of the worst case distribution in our DRO formulation. We enable the implementation of the DRO formulation by proposing a stochastic gradient descent algorithm which allows to easily implement the training procedure. We demonstrate the improvement in generalization error in semi-supervised extensions of regularized logistic regression and square-root LASSO. Finally, we include a discussion on the large sample behavior of the optimal uncertainty region in the DRO formulation. Our discussion exposes important aspects such as the role of dimension reduction in semi-supervised learning.",2017,arXiv: Machine Learning
Boosted Network Classifiers for Local Feature Selection,"Like all models, network feature selection models require that assumptions be made on the size and structure of the desired features. The most common assumption is sparsity, where only a small section of the entire network is thought to produce a specific phenomenon. The sparsity assumption is enforced through regularized models, such as the lasso. However, assuming sparsity may be inappropriate for many real-world networks, which possess highly correlated modules. In this paper, we illustrate two novel optimization strategies, namely, boosted expectation propagation (BEP) and boosted message passing (BMP), which directly use the network structure to estimate the parameters of a network classifier. BEP and BMP are ensemble methods that seek to optimize classification performance by combining individual models built upon local network features. Neither BEP nor BMP assumes a sparse solution, but instead they seek a weighted average of all network features where the weights are used to emphasize all features that are useful for classification. In this paper, we compare BEP and BMP with network-regularized logistic regression models on simulated and real biological networks. The results show that, where highly correlated network structure exists, assuming sparsity adversely effects the accuracy and feature selection power of the network classifier.",2012,IEEE Transactions on Neural Networks and Learning Systems
Computer-Intensive Statistics: A Promising Interplay between Statistics and Computer Science,"Editorial Statistics and computer science have grown as separate disciplines with little interaction for the past several decades. This however, has changed radically in recent years with the availability of massive and complex datasets in medicine, social media, and physical sciences. The statistical techniques developed for regular datasets simply cannot be scaled to meet the challenges of big data, notably the computational and statistical curses of dimensionality. The dire need to meet the challenges of big data has led to the development of statistical learning, machine learning and deep learning techniques. Rapid improvements in the speed and lower costs of statistical computation in recent years have freed statistical theory from its two serious limitations: the widespread assumption that the data follow the bell-shaped curve and exclusive focus on measures, such as mean, standard deviation, and correlation whose properties could be analyzed mathematically [1]. Computer-intensive statistical techniques have freed practical applications from the constraints of mathematical tractability and today can deal with most problems without the restrictive assumption of Gaussian distribution. These methods can be classified into frequentist and Bayesian methods. The former methods utilize the sample information only while the latter methods utilize both the sample and prior information. Frequentist statistical methods have benefitted enormously from the interaction of statistics with computer science. A very popular computer-intensive method is the bootstrap for estimating the statistical accuracy of a measure, such as correlation in a single sample. The procedure involves generating a very large number of samples with replacement from the original sample. Bootstrap as a measure of statistical accuracy has been shown to be extremely reliable in theoretical research [2,3]. Another widely used computer-intensive method for measuring the accuracy of statistical methods is cross validation. It works non-parametrically without the need for probabilistic modelling and measures the mean-squared-error for the test sample using the training sample to evaluate the performance of various machine learning methods for selecting the best method. Other frequentist statistical methods that rely on a powerful computing environment include jackknife for estimating bias and variance of an estimator, classification and regression trees for prediction, generalized linear models for parametric modelling with continuous, discrete or count response [4], generalized additive models for flexible semi-parametric regression modeling [5], the LASSO method for Cox proportional hazard regression in high dimensional settings [6], and EM algorithm [7] for finding iteratively the maximum likelihood or maximum a posteriori (MAP) estimates of parameters in complex statistical models with latent variables, alternating between performing an expectation (E) step, which evaluates the expectation of the log-likelihood evaluated using the current estimate for the parameters, and a maximization (M) step, which computes parameters maximizing the expected log-likelihood found on the E step. Bagging, random forests, and boosting [8,9] are some relatively recent developments in machine learning which use large amounts of data to fit a very rich class of functions to the data almost automatically. These methods represent a fitted model as a sum of regression trees. A regression tree by itself is a fairly weak prediction model, so these methods greatly improve prediction performance by constructing ensembles of either deep trees under random forests or shallow trees under boosting. Support vector machine (SVM) [10], an approach for linear and nonlinear classification developed in computer science, has been found to perform very well and is widely used by statisticians and data scientists. Neural networks are a class of learning methods developed separately in statistics and artificial intelligence, which use a computer-based model of human brain to perform complex tasks. These methods have found applications across several disciplines, including medicine, geosciences, hydrology, engineering, business, and economics. Some common statistical models, such as multiple linear regression, logistic regression, and linear discriminant analysis for classifying binary response are akin to neural networks. The main idea underlying these methods is to extract linear combinations of inputs as derived features and model the output as a nonlinear function of these features called the activation function. Bayesian statistical methods have also benefited greatly from computer-intensive methods, notably the Markov Chain Monte Carlo (MCMC) approach [11], which is a class Article Information",2018,
Weighted Robust Lasso and Adaptive Elastic Net Method for Regularization and Variable Selection in Robust Regression with Optimal Scaling Transformations,"In this paper, the weight least absolute deviation adaptive lasso optimal scaling method (WLAD-CATREG adaptive lasso) and weight least absolute deviation adaptive elastic net regression with optimal scaling method (WLAD-CATREG adoptive elastic net) will introduced, which is combined of weight least absolute deviation regression (WLAD-CATREG) and adaptive lasso (A-Lasso) or adaptive elastic net regression (A-Elastic net) with optimal scaling. Thus (WLAD-CATREG adoptive elastic net) method aim to automatically select variable, aspire to gropes effect and erase the bad effect of leverage points and outliers simultaneously, these aims cannot be achieved by (WLAD-CATREG), adaptive lasso regression (A-Lasso), weight robust adaptive lasso regression (WLAD-CATREG adoptive lasso), Weight least absolute deviation elastic net regression (WLAD-CATREG elastic net). Simulation study will be running to validated superiority of the (WLAD-CATREG adoptive Lasso) and (WLAD-CATREG adoptive elastic net).",2017,American Journal of Mathematics and Statistics
Bayesian Non--Negative Regularised Regresssion,"This paper proposes a novel Bayesian approach to the problem of variable selection and shrinkage in high dimensional sparse nonâ€“negative linear regression models. The regularisation method is an extension of the LASSO which has been re- cently cast in a Bayesian framework by Park and Casella (2008). Moreover, to deal with the additional problem of variable selection we propose a Stochastic Search Variable Selection (SSVS) method that relies on a dirac spikâ€“andâ€“slab prior where the slab component induces the sparse nonâ€“negative regularisation. The methodol- ogy is then applied to the problem of passive index tracking of large dimensional index in stock markets without short sales.",2017,
Establishment of a Genomic-Clinicopathologic Nomogram for Predicting Early Recurrence of Hepatocellular Carcinoma After R0 Resection,"A high rate of postoperative recurrence, especially early recurrence (ER) occurring within 1 year, seriously impedes patients with hepatocellular carcinoma (HCC) from achieving long-term survival. This study aimed to establish a genomic-clinicopathologic nomogram for precisely predicting ER in HCC patients after R0 resection. Two reliable datasets from The Cancer Genome Atlas (TCGA) and the Gene Expression Omnibus (GEO) databases were selected as the training and validation cohorts, respectively. The prognostic genes related to ER were screened out by univariate Cox regression analysis and differential expression analysis. The gene-based prognostic index was constructed using LASSO and Cox regression analyses, and its independent prognostic value was assessed by Kaplan-Meier and multivariate Cox analyses. Gene set enrichment analysis (GSEA) was performed to explore the biological pathways related to the prognostic index. Finally, the nomogram integrating all the independent prognostic factors was established and comprehensively evaluated by calibration plots, the C-index, receiver operating characteristic curves, and decision curve analysis. Nine dysregulated and prognostic genes related to ER (ZNF131, TATDN2, TXN, DDX55, KPNA2, ZNF30, TIMELESS, SFRP1, and COLEC11) were identified (all P <â€‰0.05). The prognostic index model based on the 9 genes was successfully constructed using the TCGA cohort and showed a certain capability to discriminate the ER group from the non-ER group (P <â€‰0.05) and good independent prognostic value in terms of predicting poor early recurrence-free survival (P <â€‰0.05). Eight biological pathways significantly related to ER were identified by GSEA, such as â€œcell cycleâ€, â€œhomologous recombinationâ€ and â€œp53 signaling pathway.â€ The genomic-clinicopathologic nomogram integrating the 9-gene-based prognostic index and TNM stage displayed significantly higher predictive accuracy and clinical application value than that of TNM stage model both in the training and validation cohorts (all P <â€‰0.05). The novel genomic-clinicopathologic nomogram may be a convenient and powerful tool for accurately predicting ER in HCC patients after R0 resection.",2020,Journal of Gastrointestinal Surgery
Stability of model selection for high-dimensional data,"The analysis of data generated by high throughput technologies such as DNA microarrays has markedly renewed the statistical methodology for multiple testing and feature selection in regression or classification issues. Such data are characterized by both their high-dimension, as the number of measured features is close to several thousands whereas the sample size is about some tens, and their heterogeneity, as the true signal and several confusing factors (uncontrolled and unobserved) are often observed at the same time. In such a framework, the usual statistical approaches are questioned and can lead to misleading decisions for example. Some recent papers (Efron 2007, Leek and Storey 2007 and 2008; Friguet et al, 2009 ) have focused on the negative impact of data heterogeneity on the consistency of the ranking which results from multiple testing procedures. This presentation aims at showing that data heterogeneity also a effects the stability of supervised classification model selection which is often used to identify relevant subsets of features. Key characteristics of selection methods are both classification or prediction performance and reproducibility of the selected variables to perturbation in the data. It is first shown that selected subsets using well-known procedures such as LASSO (Tibshirani, 1996) are subject to a high variability. The stability of this selection method is compared through a simulation study, considering several scenario of dependence between variables: independence, block dependence, factor structure and Toeplitz design (as also considered in Meinshausen and Buhlmann, 2010). Simulation studies show that most usual methods do not select theoretical best predictors and that interesting performances of classification are performed only when a high number of variables are selected. As suggested in Friguet et al. (2009), a supervised factor model is proposed to identify a low-dimensional linear kernel which captures data dependence and new strategies for model selection are deduced. This new strategy is finally shown to improve stability of the usual methods. Indeed, interesting performances of classification are reached for a smaller number of selected variables and best theoretical predictors are more often selected for structures with a high degree of dependence.",2013,
Driven by News Tone? Understanding Information Processing When Covariates are Unknown: The Case of Natural Gas Price Movements,"Digitization promotes the instant dissemination of news in financial markets. These news represent unprecedented amounts of unstructured data. This paper applies Big Data analytics to financial news related to the natural gas market. To date, we find evidence on 16 different variables as drivers of the natural gas price. However, these fundamental drivers cannot explain a significant share of natural gas price volatility. Thus, we first apply a LASSO shrinkage method to identify the most significant control variables. Our feature-selection LASSO method suggests 4 out of the 16 drivers as relevant. Second, we investigate the effect of news sentiment on the Henry Hub gas price as a potential driver of volatility. We include the 4 most relevant control variables as of the LASSO method into our regression model. Our findings suggest a significant positive effect of news sentiment on the natural gas price.",2015,
Immune Landscape of Invasive Ductal Carcinoma Tumor Microenvironment Identifies a Prognostic and Immunotherapeutically Relevant Gene Signature,"Background: Invasive ductal carcinoma (IDC) is a clinically and molecularly distinct disease. Tumor microenvironment (TME) immune phenotypes play crucial roles in predicting clinical outcomes and therapeutic efficacy. Method: In this study, we depict the immune landscape of IDC by using transcriptome profiling and clinical characteristics retrieved from The Cancer Genome Atlas (TCGA) data portal. Immune cell infiltration was evaluated via single-sample gene set enrichment (ssGSEA) analysis and systematically correlated with genomic characteristics and clinicopathological features of IDC patients. Furthermore, an immune signature was constructed using the least absolute shrinkage and selection operator (LASSO) Cox regression algorithm. A random forest algorithm was applied to identify the most important somatic gene mutations associated with the constructed immune signature. A nomogram that integrated clinicopathological features with the immune signature to predict survival probability was constructed by multivariate Cox regression. Results: The IDC were clustered into low immune infiltration, intermediate immune infiltration, and high immune infiltration by the immune landscape. The high infiltration group had a favorable survival probability compared with that of the low infiltration group. The low-risk score subtype identified by the immune signature was characterized by T cell-mediated immune activation. Additionally, activation of the interferon-Î± response, interferon-Î³ response, and TNF-Î± signaling via the NFÎºB pathway was observed in the low-risk score subtype, which indicated T cell activation and may be responsible for significantly favorable outcomes in IDC patients. A random forest algorithm identified the most important somatic gene mutations associated with the constructed immune signature. Furthermore, a nomogram that integrated clinicopathological features with the immune signature to predict survival probability was constructed, revealing that the immune signature was an independent prognostic biomarker. Finally, the relationship of VEGFA, PD1, PDL-1, and CTLA-4 expression with the immune infiltration landscape and the immune signature was analyzed to interpret the responses of IDC patients to immunotherapy. Conclusion: Taken together, we performed a comprehensive evaluation of the immune landscape of IDC and constructed an immune signature related to the immune landscape. This analysis of TME immune infiltration landscape has shed light on how IDC respond to immunotherapy and may guide the development of novel drug combination strategies.",2019,
Combining Sparse Group Lasso and Linear Mixed Model Improves Power to Detect Genetic Variants Underlying Quantitative Traits,"Genome-Wide association studies (GWAS), based on testing one single nucleotide polymorphism (SNP) at a time, have revolutionized our understanding of the genetics of complex traits. In GWAS, there is a need to consider confounding effects such as due to population structure, and take groups of SNPs into account simultaneously due to the ""polygenic"" attribute of complex quantitative traits. In this paper, we propose a new approach SGL-LMM that puts together sparse group lasso (SGL) and linear mixed model (LMM) for multivariate associations of quantitative traits. LMM, as has been often used in GWAS, controls for confounders, while SGL maintains sparsity of the underlying multivariate regression model. SGL-LMM first sets a fixed zero effect to learn the parameters of random effects using LMM, and then estimates fixed effects using SGL regularization. We present efficient algorithms for hyperparameter tuning and feature selection using stability selection. While controlling for confounders and constraining for sparse solutions, SGL-LMM also provides a natural framework for incorporating prior biological information into the group structure underlying the model. Results based on both simulated and real data show SGL-LMM outperforms previous approaches in terms of power to detect associations and accuracy of quantitative trait prediction.",2019,Frontiers in Genetics
Regularized logistic regression without a penalty term: An application to cancer classification with microarray data,"Research highlights? EDAs can be used to find regularized logistic classifiers. It avoids the determination of the regularization term. ? EDA is not influenced by large number of covariates. ? Yields to significant better performance on AUC measure, compared to ridge and Lasso logistic regressions. Regularized logistic regression is a useful classification method for problems with few samples and a huge number of variables. This regression needs to determine the regularization term, which amounts to searching for the optimal penalty parameter and the norm of the regression coefficient vector. This paper presents a new regularized logistic regression method based on the evolution of the regression coefficients using estimation of distribution algorithms. The main novelty is that it avoids the determination of the regularization term. The chosen simulation method of new coefficients at each step of the evolutionary process guarantees their shrinkage as an intrinsic regularization. Experimental results comparing the behavior of the proposed method with Lasso and ridge logistic regression in three cancer classification problems with microarray data are shown.",2011,Expert Syst. Appl.
Variable-Group Selection on Estimated Metabolites of Curcuma aeruginosa Related to Antioxidant Activity by Using Group Lasso Regression,"A metabolite may be expressed on a group of variables in mass-spectrometry experiments. Evaluation on metabolite effects should consider this group. Group lasso regression can be used to evaluate these groups. It shrinks some regression coefficients to zero by intermediate penalty on OLS loss function. The data used were antioxidant activity and mass/charge ion from LC-MS output of Curcuma aeruginosa compositions of 3 areas in Java. The significance metabolite groups were 148,060, 202,179, 204,159, 228,123, 238,150, 246,133, 312,274, and 398,335. Keywordsâ€” Variable-group selection, group lasso regression, antioxidant, Curcuma aeruginosa",2018,
A deep auto-encoder model for gene expression prediction,"BackgroundGene expression is a key intermediate level that genotypes lead to a particular trait. Gene expression is affected by various factors including genotypes of genetic variants. With an aim of delineating the genetic impact on gene expression, we build a deep auto-encoder model to assess how good genetic variants will contribute to gene expression changes. This new deep learning model is a regression-based predictive model based on the MultiLayer Perceptron and Stacked Denoising Auto-encoder (MLP-SAE). The model is trained using a stacked denoising auto-encoder for feature selection and a multilayer perceptron framework for backpropagation. We further improve the model by introducing dropout to prevent overfitting and improve performance.ResultsTo demonstrate the usage of this model, we apply MLP-SAE to a real genomic datasets with genotypes and gene expression profiles measured in yeast. Our results show that the MLP-SAE model with dropout outperforms other models including Lasso, Random Forests and the MLP-SAE model without dropout. Using the MLP-SAE model with dropout, we show that gene expression quantifications predicted by the model solely based on genotypes, align well with true gene expression patterns.ConclusionWe provide a deep auto-encoder model for predicting gene expression from SNP genotypes. This study demonstrates that deep learning is appropriate for tackling another genomic problem, i.e., building predictive models to understand genotypesâ€™ contribution to gene expression. With the emerging availability of richer genomic data, we anticipate that deep learning models play a bigger role in modeling and interpreting genomics.",2017,BMC Genomics
The influence function of penalized regression estimators,"To perform regression analysis in high dimensions, lasso or ridge estimation are a common choice. However, it has been shown that these methods are not robust to outliers. Therefore, alternatives as penalized M-estimation or the sparse least trimmed squares (LTS) estimator have been proposed. The robustness of these regression methods can be measured with the influence function. It quantifies the effect of infinitesimal perturbations in the data. Furthermore, it can be used to compute the asymptotic variance and the mean-squared error (MSE). In this paper we compute the influence function, the asymptotic variance and the MSE for penalized M-estimators and the sparse LTS estimator. The asymptotic biasedness of the estimators make the calculations non-standard. We show that only M-estimators with a loss function with a bounded derivative are robust against regression outliers. In particular, the lasso has an unbounded influence function.",2015,Statistics
Iterative hard thresholding for model selection in genome-wide association studies.,"A genome-wide association study (GWAS) correlates marker and trait variation in a study sample. Each subject is genotyped at a multitude of SNPs (single nucleotide polymorphisms) spanning the genome. Here, we assume that subjects are randomly collected unrelateds and that trait values are normally distributed or can be transformed to normality. Over the past decade, geneticists have been remarkably successful in applying GWAS analysis to hundreds of traits. The massive amount of data produced in these studies present unique computational challenges. Penalized regression with the â„“1 penalty (LASSO) or minimax concave penalty (MCP) penalties is capable of selecting a handful of associated SNPs from millions of potential SNPs. Unfortunately, model selection can be corrupted by false positives and false negatives, obscuring the genetic underpinning of a trait. Here, we compare LASSO and MCP penalized regression to iterative hard thresholding (IHT). On GWAS regression data, IHT is better at model selection and comparable in speed to both methods of penalized regression. This conclusion holds for both simulated and real GWAS data. IHT fosters parallelization and scales well in problems with large numbers of causal markers. Our parallel implementation of IHT accommodates SNP genotype compression and exploits multiple CPU cores and graphics processing units (GPUs). This allows statistical geneticists to leverage commodity desktop computers in GWAS analysis and to avoid supercomputing.


AVAILABILITY
Source code is freely available at https://github.com/klkeys/IHT.jl.",2017,Genetic epidemiology
"RÃ©duction de dimension en rÃ©gression logistique, application aux donnÃ©es actu-palu","Cette these est consacree a la selection de variables ou de modeles en regression logistique. Elle peut-etre divisee en deux parties, une partie appliquee et une partie methodologique. La partie appliquee porte sur l'analyse des donnees d'une grande enquete socio - epidemiologique denommee actu-palu. Ces grandes enquetes socio - epidemiologiques impliquent generalement un nombre considerable de variables explicatives. Le contexte est par nature dit de grande dimension. En raison du fleau de la dimension, le modele de regression logistique n'est pas directement applicable. Nous procedons en deux etapes, une premiere etape de reduction du nombre de variables par les methodes Lasso, Group Lasso et les forets aleatoires. La deuxieme etape consiste a appliquer le modele logistique au sous-ensemble de variables selectionne a la premiere etape. Ces methodes ont permis de selectionner les variables pertinentes pour l'identification des foyers a risque d'avoir un episode febrile chez un enfant de 2 a 10 ans a Dakar. La partie methodologique, composee de deux sous-parties, porte sur l'etablissement de proprietes techniques d'estimateurs dans le modele de regression logistique non parametrique. Ces estimateurs sont obtenus par maximum de vraisemblance penalise, dans un cas avec une penalite de type Lasso ou Group Lasso et dans l'autre cas avec une penalite de type 1 exposant 0. Dans un premier temps, nous proposons des versions ponderees des estimateurs Lasso et Group Lasso pour le modele logistique non parametrique. Nous etablissons des inegalites oracles non asymptotiques pour ces estimateurs. Un deuxieme ensemble de resultats vise a etendre le principe de selection de modele introduit par Birge et Massart (2001) a la regression logistique. Cette selection se fait via des criteres du maximum de vraisemblance penalise. Nous proposons dans ce contexte des criteres de selection de modele, et nous etablissons des inegalites oracles non asymptotiques pour les estimateurs selectionnes. La penalite utilisee, dependant uniquement des donnees, est calibree suivant l'idee de l'heuristique de pente. Tous les resultats de la partie methodologique sont illustres par des etudes de simulations numeriques.",2014,
