title,abstract,year,journal
Group Lasso for generalized linear models in high dimension,Nowadays an increasing amount of data is available and we have to deal with models in high dimension (number of covariates much larger than the sample size). Under sparsity assumption it is reasonable to hope that we can make a good estimation of the regression parameter. This sparsity assumption as well as a block structuration of the covariates into groups with similar modes of behavior is for example quite natural in genomics. A huge amount of scientific literature exists for Gaussian linear models including the Lasso estimator and also the Group Lasso estimator which promotes group sparsity under an a priori knowledge of the groups. We extend this Group Lasso procedure to generalized linear models and we study the properties of this estimator for sparse high-dimensional generalized linear models to find convergence rates. We provide oracle inequalities for the prediction and estimation error under assumptions on the covariables and under a condition on the design matrix. We show the ability of this estimator to recover good sparse approximation of the true model. At last we extend these results to the case of an Elastic net penalty and we apply them to the so-called Poisson regression case which has not been studied in this context contrary to the logistic regression.,2014,arXiv: Statistics Theory
Discrimination of mediastinal metastatic lymph nodes in NSCLC based on radiomic features in different phases of CT imaging,"We aimed to develop radiomic models based on different phases of computed tomography (CT) imaging and to investigate the efficacy of models for diagnosing mediastinal metastatic lymph nodes (LNs) in non-small cell lung cancer (NSCLC). Eighty-six NSCLC patients were enrolled in this study, and we selected 231 mediastinal LNs confirmed by pathology results as the subjects which were divided into training (nâ€‰=â€‰163) and validation cohorts (nâ€‰=â€‰68). The regions of interest (ROIs) were delineated on CT scans in the plain phase, arterial phase and venous phase, respectively. Radiomic features were extracted from the CT images in each phase. A least absolute shrinkage and selection operator (LASSO) algorithm was used to select features, and multivariate logistic regression analysis was used to build models. We constructed six models (orders 1â€“6) based on the radiomic features of the single- and dual-phase CT images. The performance of the radiomic model was evaluated by the area under the receiver operating characteristic curve (AUC), sensitivity, specificity, accuracy, positive predictive value (PPV) and negative predictive value (NPV). A total of 846 features were extracted from each ROI, and 10, 9, 5, 2, 2, and 9 features were chosen to develop models 1â€“6, respectively. All of the models showed excellent discrimination, with AUCs greater than 0.8. The plain CT radiomic model, model 1, yielded the highest AUC, specificity, accuracy and PPV, which were 0.926 and 0.925; 0.860 and 0.769; 0.871 and 0.882; and 0.906 and 0.870 in the training and validation sets, respectively. When the plain and venous phase CT radiomic features were combined with the arterial phase CT images, the sensitivity increased from 0.879 and 0.919 to 0.949 and 0979 and the NPV increased from 0.821 and 0.789 to 0.878 and 0.900 in the training group, respectively. All of the CT radiomic models based on different phases all showed high accuracy and precision for the diagnosis of LN metastasis (LNM) in NSCLC patients. When combined with arterial phase CT, the sensitivity and NPV of the model was be further improved.",2020,BMC Medical Imaging
A Functional Generalized Linear Model with Curve Selection in Cervical Pre-cancer Diagnosis Using Fluorescence Spectroscopy,"A functional generalized linear model is applied to spectroscopic data to discriminate disease from non-disease in the diagnosis of cervical pre- cancer. For each observation, multiple functional covariates are available, and it is of interest to select a few of them for efficient classification. In addition to multiple functional covariates, some non-functional covariates are also used to account for systematic differences caused by these covariates. Functional principal components are used to reduce the model to multivariate logistic regression and a grouped Lasso penalty is applied to the reduced model to select useful functional covariates among multiple curves.",2009,
Survival prediction and variable selection with simultaneous shrinkage and grouping priors,"The presented work is motivated by the need of reliably estimating and predicting the survival rates for individuals diagnosed with cancer, when gene expression profiles are available for identifying molecular risks factors for cancer. The regression analysis of such data is challenged by three characteristics of data: i time-to-event outcome, ii high-dimensional covariate space, and iii a group structure of genes. One strategy to simultaneously deal with all three of the aforementioned challenges is to build a penalized regression model using special penalty functions such as elastic net, fused lasso, and group lasso. To our knowledge, existing methods are sparse or non-existent, however, when a Bayesian estimation/inference is the goal for the penalized regression models. In this article, we propose a Bayesian semi-parametric framework for the regression analysis of gene expression data with survival outcomes. Our proposed Bayesian methods permit researchers to take advantage of numerous benefits including the ability to incorporate substantive prior information, the straightforward and automated quantification of uncertainty in prediction, and the prescriptive nature of computation. The performance of our proposed models for variable selection and prediction is thoroughly investigated through simulation studies, where we consider four scenarios based on different underlying group structures of covariates and covariate effects. The results generally show the satisfactory variable selection capability and predictability of our methods. Finally, we apply our proposed framework to three different gene expression data sets. We developed an efficient Markov chain Monte Carlo algorithm for the implementation of our proposed framework and provided an easy-to-use R package.",2015,Statistical Analysis and Data Mining
Intraoperative Hepatic Blood Inflow Can Predict Early Acute Kidney Injury following DCD Liver Transplantation: A Retrospective Observational Study,"Purpose
Acute kidney injury (AKI) is a major and severe complication following donation-after-circulatory-death (DCD) liver transplantation (LT) and is associated with increased postoperative morbidity and mortality. However, the risk factors and the prognosis factors of AKI still need to be further explored, and the relativity of intraoperative hepatic blood inflow (HBI) and AKI following LT has not been discussed yet. The purpose of this study was to investigate the correlation between HBI and AKI and to construct a prediction model of early acute kidney injury (EAKI) following DCD LT with the combination of HBI and other clinical parameters.


Methods
Clinical data of 132 patients who underwent DCD liver transplantation at the first hospital of China Medical University from April 2005 to March 2017 were analyzed. Data of 105 patients (the first ten years of patients) were used to develop the prediction model. Then we assessed the clinical usefulness of the prediction models in the validation cohort (27 patients). EAKI according to Kidney Disease Improving Global Outcomes (KDIGO) criteria based on serum creatinine increase during 7-day of postoperative follow-up.


Results
After Least Absolute Shrinkage and Selection Operator (LASSO) regression and simplification, a simplified prediction model consisting of the Child-Turcotte-Pugh (CTP) score (p=0.033), anhepatic phase (p=0.014), packed red blood cell (pRBC) transfusion (p=0.027), and the HBI indexed by height (HBI/h) (p=0.002) was established. The C-indexes of the model in the development and validation cohort were 0.823 [95% CI, 0.738-0.908] and 0.921 [95% CI, 0.816-1.000], respectively.


Conclusions
In this study, we demonstrated the utility of HBI/h as a predictor for EAKI following DCD LT, as well as the clinical usefulness of the prediction model through the combination of the CTP score, anhepatic phase, pRBC transfusion and HBI/h.",2019,BioMed Research International
Identifying incident dementia by applying machine learning to a very large administrative claims dataset,"Alzheimer's disease and related dementias (ADRD) are highly prevalent conditions, and prior efforts to develop predictive models have relied on demographic and clinical risk factors using traditional logistical regression methods. We hypothesized that machine-learning algorithms using administrative claims data may represent a novel approach to predicting ADRD. Using a national de-identified dataset of more than 125 million patients including over 10,000 clinical, pharmaceutical, and demographic variables, we developed a cohort to train a machine learning model to predict ADRD 4-5 years in advance. The Lasso algorithm selected a 50-variable model with an area under the curve (AUC) of 0.693. Top diagnosis codes in the model were memory loss (780.93), Parkinson's disease (332.0), mild cognitive impairment (331.83) and bipolar disorder (296.80), and top pharmacy codes were psychoactive drugs. Machine learning algorithms can rapidly develop predictive models for ADRD with massive datasets, without requiring hypothesis-driven feature engineering.",2019,PLoS ONE
Restarting accelerated gradient methods with a rough strong convexity estimate,"We propose new restarting strategies for accelerated gradient and accelerated coordinate descent methods. Our main contribution is to show that the restarted method has a geometric rate of convergence for any restarting frequency, and so it allows us to take profit of restarting even when we do not know the strong convexity coefficient. The scheme can be combined with adaptive restarting, leading to the first provable convergence for adaptive restarting schemes with accelerated gradient methods. Finally, we illustrate the properties of the algorithm on a regularized logistic regression problem and on a Lasso problem.",2016,arXiv: Optimization and Control
Forecasting Road Surface Temperature in Beijing Based on Machine Learning Algorithms,"With the influence of extreme weather, road surface temperature (RST), which threatens the safety of people's travel, has attracted more and more attention to the government and citizens. However, traditional methods are hard to meet real-time requirements in forecasting RST. In order to improve the predictive accuracy of RST and meet the real-time requirement, this paper compares three different machine learning algorithms including, Least Absolute Shrinkage and Selection Operator (LASSO), Random Forest (RF) and Gradient Boosting Regression Tree (GBRT). Using the RST data and BJ-RUC (Beijing-rapidly update cycle) data during November 2012 and June 2015, the performance of three models is evaluated. The experimental results show that GBRT performs the best and its MSE is 6.7853.",2018,
Parameter estimation for high dimensional change point regression models without grid search,"We propose an L1 regularized estimator for the parameters of a high dimensional change point regression model and provide the corresponding rates of convergence for the regression as well change point estimates. Importantly, the computational cost of our estimator is 2Lasso(n,p), where Lasso(n,p) represents the computational burden of one Lasso optimization. In comparison, existing grid search based approaches to this problem require a computational cost of at least nLasso(n,p) optimizations. We work under a subgaussian random design where the underlying assumptions in our study are milder than those currently assumed in the high dimensional change point regression literature. We allow the true change point parameter $\tau_{0n}$ to possibly move to the boundaries of its parametric space, and the jump size $\|\beta_0-\gamma_0\|_2$ to possibly diverge as $n$ increases. We also characterize the corresponding effects of these quantities on the rates of convergence of the regression and change point estimates. Simulations are performed to empirically evaluate performance of the proposed estimators. The methodology is applied to community level socio-economic data of the U.S., collected from the 1990 U.S. census and other sources.",2018,arXiv: Methodology
Connectivity-based parcellation of putamen region using resting state fMRI,"Functional magnetic resonance imaging (fMRI) has shown great potential in studying the underlying neural systems. Functional connectivity measured by fMRI provides an efficient approach to study the interactions and relationships between different brain regions. However, functional connectivity studies require accurate definition of brain regions, which is often difficult and may not be achieved through anatomical landmarks. In this thesis, we present a novel framework for parcellation of a brain region into functional subunits based on their connectivity patterns with other reference brain regions. The proposed method takes the prior neurological information into consideration and aims at finding spatially continuous and functionally consistent subregions in a given brain region. The proposed framework relies on a sparse spatially regularized fused lasso regression model for feature extraction. The usual lasso model is a linear regression model commonly applied in high dimensional data such as fMRI signals. Compared with lasso, the proposed model further considers the spatial order of each voxel and thus encourages spatially and functionally adjacent voxels to share similar regression coefficients despite of the possible spatial noise. In order to achieve the accurate parcellation results, we propose a process by iteratively merging voxels (groups) and tuning the parameters adaptively. In addition, a Graph-Cut optimization algorithm is adopted for assigning the overlapped voxels into separate sub-regions. With spatial information incorporated, spatially continuous and functionally consistent subunits can be obtained which are desired for subsequent brain connectivity analysis. The simulation results demonstrate that the proposed method could reliably",2015,
Medical Tumor Image Classification Algorithm and Its Application in Breast Cancer,"Medical image classification of tumors plays an important role in the diagnosis and treatment of medical diseases. With the development of computer science and technology, medical imaging has made great progress in imaging, image acquisition speed is getting faster and faster, and image resolution is getting higher and higher. However, the interpretation of images mainly comes from imaging doctors. On the one hand, the quantity and quality of images greatly increase their burden. On the other hand, their interpretation of images mainly depends on the image characteristics that can be observed by the naked eye, which inevitably will be affected by subjective factors such as personal experience. Based on the clinical needs, this paper first uses image enhancement algorithm to extract image features, and then introduces medical tumor image classification. Finally, the paper elaborates the designed medical image classification network S-Dense Net. In this paper, the images are evaluated from subjective and objective aspects. The experimental results show that S-Dense Net has higher accuracy and AUC (Area under Curve) values than traditional algorithms including Logistic regression, LASSO logistic regression, SVM and random forest and based on Google Net, ResNet-51, and Squeeze Net network algorithms.",2019,Investigacion Clinica
Double-estimation-friendly inference for high-dimensional misspecified models,"All models may be wrong---but that is not necessarily a problem for inference. Consider the standard $t$-test for the significance of a variable $X$ for predicting response $Y$ whilst controlling for $p$ other covariates $Z$ in a random design linear model. This yields correct asymptotic type~I error control for the null hypothesis that $X$ is conditionally independent of $Y$ given $Z$ under an \emph{arbitrary} regression model of $Y$ on $(X, Z)$, provided that a linear regression model for $X$ on $Z$ holds. An analogous robustness to misspecification, which we term the ""double-estimation-friendly"" (DEF) property, also holds for Wald tests in generalised linear models, with some small modifications. 
In this expository paper we explore this phenomenon, and propose methodology for high-dimensional regression settings that respects the DEF property. We advocate specifying (sparse) generalised linear regression models for both $Y$ and the covariate of interest $X$; our framework gives valid inference for the conditional independence null if either of these hold. In the special case where both specifications are linear, our proposal amounts to a small modification of the popular debiased Lasso test. We also investigate constructing confidence intervals for the regression coefficient of $X$ via inverting our tests; these have coverage guarantees even in partially linear models where the contribution of $Z$ to $Y$ can be arbitrary. Numerical experiments demonstrate the effectiveness of the methodology.",2019,arXiv: Statistics Theory
The Bayesian group-Lasso for analyzing contingency tables,"Group-Lasso estimators, useful in many applications, suffer from lack of meaningful variance estimates for regression coefficients. To overcome such problems, we propose a full Bayesian treatment of the Group-Lasso, extending the standard Bayesian Lasso, using hierarchical expansion. The method is then applied to Poisson models for contingency tables using a highly efficient MCMC algorithm. The simulated experiments validate the performance of this method on artificial datasets with known ground-truth. When applied to a breast cancer dataset, the method demonstrates the capability of identifying the differences in interactions patterns of marker proteins between different patient groups.",2009,
Genome-wide association studies using binned genotypes,"Linear mixed models (LMM) that tests trait association one marker at a time have been the most popular methods for genome-wide association studies. However, this approach has potential pitfalls: over conservativeness after Bonferroni correction, ignorance of linkage disequilibrium (LD) between neighboring markers, and power reduction due to overfitting SNP effects. So, multiple locus models that can simultaneously estimate and test all markers in the genome are more appropriate. Based on the multiple locus models, we proposed a bin model that combines markers into bins based on their LD relationships. A bin is treated as a new synthetic marker and we detect the associations between bins and traits. Since the number of bins can be substantially smaller than the number of markers, a penalized multiple regression method can be adopted by fitting all bins to a single model. We developed an innovative method to bin the neighboring markers and used the least absolute shrinkage and selection operator (LASSO) method. We compared BIN-Lasso with SNP-Lasso and Qâ€‰+â€‰K-LMM in a simulation experiment, and showed that the new method is more powerful with less Type I error than the other two methods. We also applied the bin model to a Chinese Simmental beef cattle population for bone weight association study. The new method identified more significant associations than the classical LMM. The bin model is a new dimension reduction technique that takes advantage of biological information (i.e., LD). The new method will be a significant breakthrough in associative genomics in the big data era.",2019,Heredity
Learning structural conjunction of image content by sparse graphical model,"In this paper we present a novel method on learning structural conjunction of image content by sparse graphical model. We first use matrix-variate distributions to formulate two statistical structure models and establish the connection between them. The connection leads us to sparse Gaussian graphical models in which sparse regression technique such as lasso is used for concentration matrix estimation as well as structure learning. Our proposed theoretical framework and structure selection methods provide an approach for exploiting structural conjunction of data. We apply this approach to construction of underlying structural correlation between image content, and demonstrate the effectiveness by solving image jigsaw problem.",2011,2011 18th IEEE International Conference on Image Processing
Abstract B1-52: Analysis of the iCOGS breast cancer GWAS reveals 4 unique signals in the PTHLH region in patients of European origin,"Abstracts: AACR Special Conference: Computational and Systems Biology of Cancer; February 8-11, 2015; San Francisco, CA Parathyroid hormone-related protein (PTHrP), the product of the PTHLH gene, has long been implicated in breast cancer. Its expression is thought to favour and, potentially, facilitate metastasis to bone. Paradoxically, a prospective clinical study clinical suggests that its production in primary breast cancers is actually protective, affording a better prognosis than its absence. Multiple recent Genome-Wide Association Studies (GWAS) have confirmed a single susceptibility locus immediately upstream of the PTHLH gene to be associated with breast cancer. This was again reproduced in the recent iCOGS GWAS, which involved over 90,000 European subjects. In spite of a single SNP, rs10771399, being uniquely reported and reproduced by prior studies, we demonstrate 4 discrete overlapping signals by utilising forward selection logistic regression and LASSO techniques. Two of these signals are centred and superimposed around rs10771399, ~40kbp upstream of PTHLH, a third lies a further 100kbp upstream, and the fourth lies a further 250kbp upstream over the next gene, CCDC91. While the causation of this signal remains elusive, multiple putatively contributory variations are captured by these signals. Citation Format: Adam N. Freeman, T John Martin, Michael A. Henderson, Enes Makalic, Miroslaw K. Kapuscinski, Daniel F. Schmidt, John Hopper. Analysis of the iCOGS breast cancer GWAS reveals 4 unique signals in the PTHLH region in patients of European origin. [abstract]. In: Proceedings of the AACR Special Conference on Computational and Systems Biology of Cancer; Feb 8-11 2015; San Francisco, CA. Philadelphia (PA): AACR; Cancer Res 2015;75(22 Suppl 2):Abstract nr B1-52.",2015,Cancer Research
Using the LASSO for gene selection in bladder cancer data,"Given a gene expression data array of a list of bladder cancer patients with their tumor states, it may be difficult to determine which genes can operate as disease markers when the array is large and possibly contains outliers and missing data. An additional difficulty is that observations (tumor states) in the regression problem are discrete ones. In this article, we solve these problems on concrete data using first a clustering approach, followed by Least Absolute Shrinkage and Selection Operator (LASSO) estimators in a nonlinear regression problem involving discrete variables, as described in the brand-new research work of Plan and Vershynin. Gene markers of the most severe tumor state are finally provided using the proposed approach.",2015,arXiv: Applications
Adaptively weighted group Lasso for semiparametric quantile regression models,"We propose an adaptively weighted group Lasso procedure for simultaneous variable selection and structure identification for varying coefficient quantile regression models and additive quantile regression models with ultra-high dimensional covariates. Under a strong sparsity condition, we establish selection consistency of the proposed Lasso procedure when the weights therein satisfy a set of general conditions. This consistency result, however, is reliant on a suitable choice of the tuning parameter for the Lasso penalty, which can be hard to make in practice. To alleviate this difficulty, we suggest a BIC-type criterion, which we call high-dimensional information criterion (HDIC), and show that the proposed Lasso procedure with the tuning parameter determined by HDIC still achieves selection consistency. Our simulation studies support strongly our theoretical findings.",2019,Bernoulli
Cost-effective survival prediction for patients with advanced prostate cancer using clinical trial and real-world hospital registry datasets,"INTRODUCTION
Predictive survival modeling offers systematic tools for clinical decision-making and individualized tailoring of treatment strategies to improve patient outcomes while reducing overall healthcare costs. In 2015, a number of machine learning and statistical models were benchmarked in the DREAM 9.5 Prostate Cancer Challenge, based on open clinical trial data for metastatic castration resistant prostate cancer (mCRPC). However, applying these models into clinical practice poses a practical challenge due to the inclusion of a large number of model variables, some of which are not routinely monitored or are expensive to measure.


OBJECTIVES
To develop cost-specified variable selection algorithms for constructing cost-effective prognostic models of overall survival that still preserve sufficient model performance for clinical decision making.


METHODS
Penalized Cox regression models were used for the survival prediction. For the variable selection, we implemented two algorithms: (i) LASSO regularization approach; and (ii) a greedy cost-specified variable selection algorithm. The models were compared in three cohorts of mCRPC patients from randomized clinical trials (RCT), as well as in a real-world cohort (RWC) of advanced prostate cancer patients treated at the Turku University Hospital. Hospital laboratory expenses were utilized as a reference for computing the costs of introducing new variables into the models.


RESULTS
Compared to measuring the full set of clinical variables, economic costs could be reduced by half without a significant loss of model performance. The greedy algorithm outperformed the LASSO-based variable selection with the lowest tested budgets. The overall top performance was higher with the LASSO algorithm.


CONCLUSION
The cost-specified variable selection offers significant budget optimization capability for the real-world survival prediction without compromising the predictive power of the model.",2020,International journal of medical informatics
Predicting KOSPI Stock Index using Machine Learning Algorithms with Technical Indicators,"Recently, there have been many attempts to employ the machine learning methodologies such as Robo-Advisor in the financial sector with growing interest in machine learning in various sectors. Especially, these mechanical and quantitative decision making have some big advantages not only reduces the costs such as fees but also enable us to make effective decision. In this research, we developed a machine learning model to forecast the KOSPI index and analyze the accuracy of the prediction. We use three machine learning model : SVM(support vector machines), Lasso regression, and ANN(Artificial Neural Network). We divided our data into two parts : 'in sample' data and 'out of sample' data. The 'in sample' data is from January 1st, 2000 to December 31st, 2010. And the 'out of sample' data is from January 1st, 2011 to September 15th, 2015. The result of the experiment, the 'in sample' data, the SVM showed higher accuracy compare to the ANN. On the other hand, in the 'out of sample' data, ANN was superior than SVM. For the Lasso *êµì‹ ì €ìž ë³¸ ì—°êµ¬ëŠ” 2016ë…„ë„ ìƒëª…ëŒ€í•™êµ êµë‚´ì—°êµ¬ë¹„ë¥¼ ì§€ì›ë°›ì•„ ìˆ˜í–‰í•˜ì˜€ìŒ.",2016,
Smooth Blockwise Iterative Thresholding: A Smooth Fixed Point Estimator Based on the Likelihoodâ€™s Block Gradient,"The proposed smooth blockwise iterative thresholding estimator (SBITE) is a model selection technique defined as a fixed point reached by iterating a likelihood gradient-based thresholding function. The smooth Jamesâ€“Stein thresholding function has two regularization parameters Î» and Î½, and a smoothness parameter s. It enjoys smoothness like ridge regression and selects variables like lasso. Focusing on Gaussian regression, we show that SBITE is uniquely defined, and that its Stein unbiased risk estimate is a smooth function of Î» and Î½, for better selection of the two regularization parameters. We perform a Monte Carlo simulation to investigate the predictive and oracle properties of this smooth version of adaptive lasso. The motivation is a gravitational wave burst detection problem from several concomitant time series. A nonparametric wavelet-based estimator is developed to combine information from all captors by block-thresholding multiresolution coefficients. We study how the smoothness parameter s tem...",2011,Journal of the American Statistical Association
Predicting Age of Onset in TTR-FAP Patients with Genealogical Features,"This work describes a problem oriented approach to analyze and predict the Age of Onset of Patients diagnosed with Transthyretin Familial Amyloid Polyneuropathy (TTR-FAP). We constructed, from a set of clinical and familial records, three sets of features which represent different characteristics of a patient, before becoming symptomatic. Using those features, we tested a set of machine learning regression methods, namely Decision Tree (Regression Tree), Elastic Net, Lasso, Linear Regression, Random Forest Regressor, Ridge Regression and Support Vector Machine Regressor (SVM). Later, we defined a baseline model that represents the current medical practice to serve as a guideline for us to measure the accuracy of our approach. Our results show a significant improvement of machine learning methods when compared with the current baseline.",2018,2018 IEEE 31st International Symposium on Computer-Based Medical Systems (CBMS)
Longitudinal predictors of self-injurious thoughts and behaviors in sexual and gender minority adolescents.,"Sexual and gender minority (SGM) populations are at increased risk for several negative psychological outcomes, including self-injury. Although correlates of self-injurious thoughts and behaviors (SITBs) have been identified, it is unclear which factors are prospective predictors of SITB engagement in SGM youth. The current study investigated an online sample of 252 SGM adolescents over a 6-month period. Participants reported attitudes based on SGM identity, depression, self-criticism, body image, family support and family strain, friend NSSI engagement, and experiences of everyday discrimination. Lasso and elastic net regularized logistic regressions were used to examine which baseline variables were associated with SITB engagement at follow-up. Models resulted in excellent predictive accuracy of nonsuicidal self-injury and suicidal ideation (mean Area Under the Receiving Operating Characteristics Curve [AUC] of 0.90 and 0.91), good predictive accuracy for suicide plans (mean AUC = 0.85), and fair predictive accuracy for suicidal behaviors (mean AUC = 0.78). Several variables emerged as prospectively related to SITB risk, with varied associations across different SITBs. Results suggest that minority-specific factors may predict SITBs in SGM adolescents. (PsycINFO Database Record (c) 2019 APA, all rights reserved).",2019,Journal of abnormal psychology
Identification of a prognostic signature for old-age mortality by integrating genome-wide transcriptomic data with the conventional predictors: the Vitality 90+ Study,"BackgroundPrediction models for old-age mortality have generally relied upon conventional markers such as plasma-based factors and biophysiological characteristics. However, it is unknown whether the existing markers are able to provide the most relevant information in terms of old-age survival or whether predictions could be improved through the integration of whole-genome expression profiles.MethodsWe assessed the predictive abilities of survival models containing only conventional markers, only gene expression data or both types of data together in a Vitality 90+ study cohort consisting of nâ€‰=â€‰151 nonagenarians. The all-cause death rate was 32.5% (49 of 151 individuals), and the median follow-up time was 2.55Â years.ResultsThree different feature selection models, the penalized Lasso and Ridge regressions and the C-index boosting algorithm, were used to test the genomic data. The Ridge regression model incorporating both the conventional markers and transcripts outperformed the other models. The multivariate Cox regression model was used to adjust for the conventional mortality prediction markers, i.e., the body mass index, frailty index and cell-free DNA level, revealing that 331 transcripts were independently associated with survival. The final mortality-predicting transcriptomic signature derived from the Ridge regression model was mapped to a network that identified nuclear factor kappa beta (NF-ÎºB) as a central node.ConclusionsTogether with the loss of physiological reserves, the transcriptomic predictors centered around NF-ÎºB underscored the role of immunoinflammatory signaling, the control of the DNA damage response and cell cycle, and mitochondrial functions as the key determinants of old-age mortality.",2014,BMC Medical Genomics
Robust Inference on Average Treatment Effects with Possibly More Covariates than Observations,"This paper concerns robust inference on average treatment effects following model selection. Under selection on observables, we construct confidence intervals using a doubly-robust estimator that are robust to model selection errors and prove their uniform validity over a large class of models that allows for multivalued treatments with heterogeneous effects and selection amongst (possibly) more covariates than observations. The semiparametric efficiency bound is attained under appropriate conditions. Precise conditions are given for any model selector to yield these results, and we specifically propose the group lasso, which is apt for treatment effects, and derive new results for high-dimensional, sparse multinomial logistic regression. Both a simulation study and revisiting the National Supported Work demonstration show our estimator performs well in finite samples.",2015,Journal of Econometrics
Logistic regression paradigm for training a single-hidden layer feedforward neural network. Application to gene expression datasets for cancer research,"OBJECTIVE
The speed of the diagnosis process is vital in pursuing the trial of curing cancer. During the last decade, precision medicine evolved by detecting different types of cancer through microarrays (MA) of deoxyribonucleic acid (DNA) processed by machine learning (ML) algorithms. Personalized diagnosis, followed by personalized treatment, should imply personalized hyperparameters of the ML. The goal of this paper is to propose a novel adaptive ML method that embeds knowledge into the architecture of the algorithm and also filters the features in order to reduce their number, increase computational speed, and decrease computational cost and time.


MATERIALS AND METHODS
fLogSLFN is a novel two-fold theoretically effective ML that can be used in two-class decision problems that embeds the logistic regression in such a manner that the hidden nodes of a single-hidden layer feedforward neural network (SLFN) are problem dependent. A filtering module based on the significance of each attribute is embedded in order to avoid the 'curse of dimensionality' phenomenon. The proposed model has been tested on three publicly available high-dimensional cancer datasets that contain gene expressions provided by complementary DNA (cDNA) array, and DNA microarray. The proposed novel method filtered logistic SLFN (fLogSLFN) has been also compared and statistically benchmarked to four ML algorithms: extreme learning machine (ELM), radial basis function network (RBF), single-hidden layer feedforward neural network trained by the backpropagation algorithm (BPNN), logistic regression with the LASSO penalty, and the adaptive single-hidden layer feedforward network (aSLFN).


MAIN FINDINGS
The experimental results showed that the fLogSLFN is competitive to the other state-of-the-art models, obtaining accuracies between 64,70% and 98.66% depending on the dataset it had been applied on.


CONCLUSIONS
In contrast to other state-of-the-art ML algorithms, the fLogSLFN is capable to embed the knowledge extracted from the data into its architecture, making it problem dependent. The filtering module increases its computational speed, while decreasing computational cost and time. The statistical analysis revealed the fact that by filtering the features the performance is kept, making the algorithm more efficient.",2020,Journal of biomedical informatics
Random Subspace Method for high-dimensional regression with the R package regRSM,"Model selection and variable importance assessment in high-dimensional regression are among the most important tasks in contemporary applied statistics. In our procedure, implemented in the package regRSM, the Random Subspace Method (RSM) is used to construct a variable importance measure. The variables are ordered with respect to the measures computed in the first step using the RSM and then, from the hierarchical list of models given by the ordering, the final subset of variables is chosen using information criteria or validation set. Modifications of the original method such as the weighted Random Subspace Method and the version with initial screening of redundant variables are discussed. We developed parallel implementations which enable to reduce the computation time significantly. In this paper, we give a brief overview of the methodology, demonstrate the packageâ€™s functionality and present a comparative study of the proposed algorithm and the competitive methods like lasso or CAR scores. In the performance tests the computational times for parallel implementations are compared.",2016,Computational Statistics
Clinical Significance of Organic Anion Transporting Polypeptide Gene Expression in High-Grade Serous Ovarian Cancer,"High-grade serous ovarian cancer (HGSOC) is considered the most deadly and frequently occurring type of ovarian cancer and is associated with various molecular compositions and growth patterns. Evaluating the mRNA expression pattern of the organic anion transporters (OATPs) encoded by SLCO genes may allow for improved stratification of HGSOC patients for targeted invention. The expression of SLCO mRNA and genes coding for putative functionally related ABC-efflux pumps, enzymes, pregnane-X-receptor, ESR1 and ESR2 (coding for estrogen receptors ERÎ± and ERÃŸ) and HER-2 were assessed using RT-qPCR. The expression levels were assessed in a cohort of 135 HGSOC patients to elucidate the independent impact of the expression pattern on the overall survival (OS). For identification of putative regulatory networks, Graphical Gaussian Models were constructed from the expression data with a tuning parameter K varying between meaningful borders (Pils et al., 2012; Auer et al., 2015, 2017; Kurman and Shih Ie, 2016; Karam et al., 2017; Labidi-Galy et al., 2017; Salomon-Perzynski et al., 2017; Sukhbaatar et al., 2017). The final value used (K = 4) was determined by maximizing the proportion of explained variation of the corresponding LASSO Cox regression model for OS. The following two networks of directly correlated genes were identified: (i) SLCO2B1 with ABCC3 implicated in estrogen homeostasis; and (ii) two ABC-efflux pumps in the immune regulation (ABCB2/ABCB3) with ABCC3 and HER-2. Combining LASSO Cox regression and univariate Cox regression analyses, SLCO5A1 coding for OATP5A1, an estrogen metabolite transporter located in the cytoplasm and plasma membranes of ovarian cancer cells, was identified as significant and independent prognostic factor for OS (HR = 0.68, CI 0.49-0.93; p = 0.031). Furthermore, results indicated the benefits of patients with high expression by adding 5.1% to the 12.8% of the proportion of explained variation (PEV) for clinicopathological parameters known for prognostic significance (FIGO stage, age and residual tumor after debulking). Additionally, overlap with previously described signatures that indicated a more favorable prognosis for ovarian cancer patients was shown for SLCO5A1, the network ABCB2/ABCB3/ABCC4/HER2 as well as ESR1. Furthermore, expression of SLCO2A1 and PGDH, which are important for PGE2 degradation, was associated with the non-miliary peritoneal tumor spreading. In conclusion, the present findings suggested that SLCOs and the related molecules identified as potential biomarkers in HGSOC may be useful for the development of novel therapeutic strategies.",2018,Frontiers in Pharmacology
STATISTICAL ESTIMATION AND TESTING VIA THE ORDERED l 1 NORM By MaÅ‚gorzata,"We introduce a novel method for sparse regression and variable selection, which is inspired by modern ideas in multiple testing. Imagine we have observations from the linear model y = XÎ²+ z, then we suggest estimating the regression coefficients by means of a new estimator called the ordered lasso, which is the solution to minimize b 1 2â€–y âˆ’Xbâ€– 2 `2 + Î»1|b|(1) + Î»2|b|(2) + . . .+ Î»p|b|(p); here, Î»1 â‰¥ Î»2 â‰¥ . . . â‰¥ Î»p and |b|(1) â‰¥ |b|(2) â‰¥ . . . â‰¥ |b|(p) is the order statistic of the magnitudes of b. In short, the regularizer is an ordered `1 norm which penalizes the regression coefficients according to their rank: the higher the rankâ€”the closer to the topâ€”the larger the penalty. This is similar to the famous Benjamini-Hochberg procedure (BHq) [9], which compares the value of a test statistic taken from a family to a critical threshold that depends on its rank in the family. The ordered lasso is a convex program and we demonstrate an efficient algorithm for computing the solution. We prove that for orthogonal designs with p variables, taking Î»i = F âˆ’1(1âˆ’ qi) (F is the cumulative distribution function of the errors), qi = iq/(2p), controls the false discovery rate (FDR) for variable selection. This holds under the assumption that the errors are i.i.d. symmetric and continuous random variables. When the design matrix is nonorthogonal there are inherent limitations on the FDR level and the power which can be obtained with model selection methods based on `1-like penalties. However, whenever the columns of the design matrix are not strongly correlated, we demonstrate empirically that it is possible to select the parameters Î»i as to obtain FDR control at a reasonable level as long as the number of nonzero coefficients is not too large. At the same time, the procedure exhibits increased power over the lasso, which treats all coefficients equally. The paper illustrates further estimation properties of the new selection rule through comprehensive simulation studies.",2013,
Prediction of Benign and Malignant Thymic Tumors based on Radiomics Features,"The purpose of this study is to find out the radiomics features associated with differentiating benign and malignant thymic tumors from the quantitative CT features extracted by computer, and to establish a prediction mode, which can predict the benign and malignant thymic tumors, and improve the diagnostic efficiency of CT in thymic tumors. Retrospectively analyze the CT image data of 100 patients with pathologically confirmed thymic tumors. All selected images were preprocessed with A.K. (Artificial Intelligence Kit) software to extract the features of the lesions, and then we can use the Lasso algorithm to screen out CT radiomics features with diagnostic value and significant correlation with benign and malignant tumors. Logistic regression was used to construct a predictive model for the diagnosis of benign and malignant thymic tumors. The receiver operating characteristic(ROC) curve and area under curve (AUC) were used to evaluate the diagnostic efficacy of the constructed CT image features for benign and malignant thymic tumors. The most useful features of CT radiomics features were selected by Lasso algorithm: Surface Area, Inverse Difference Moment_AllDirection_offset1_SD, Voxel ValueSum, skewness, High Grey RuelnEmphasis_Allrection_offset7_SD. The area under ROC curve (AUC) was 0.752.The feature parameters screened on CT images have good diagnostic efficacy in differentiating benign from malignant thymoma.",2019,2019 IEEE International Conference on Mechatronics and Automation (ICMA)
Balanced Bayesian LASSO for heavy tails,"Regression procedures are not only hindered by large p and small n, but can also suffer in cases when outliers are present or the data generating mechanisms are heavy tailed. Since the penalized estimates like the least absolute shrinkage and selection operator (LASSO) are equipped to deal with the large p small n by encouraging sparsity, we combine a LASSO type penalty with the absolute deviation loss function, instead of the standard least squares loss, to handle the presence of outliers and heavy tails. The model is cast in a Bayesian setting and a Gibbs sampler is derived to efficiently sample from the posterior distribution. We compare our method to existing methods in a simulation study as well as on a prostate cancer data set and a base deficit data set from trauma patients.",2016,Journal of Statistical Computation and Simulation
Parsimonious additive logistic models,"Logistic regression is a standard tool in statistics for binary classification. The logistic model relates the logarithm of the odds-ratio to the predictors via a linear regression model. A generalization is the additive logistic model, which replaces each linear term by an unspecified smooth function, allowing for more flexibility while preserving interpretability. Another variant is penalized logistic regression, which shrinks coefficients to improve the accuracy of prediction. Ridge regression (L2-penalization) and lasso (L1-penalization) are the main penalization procedures. An attractive property of the later is that it performs parameter estimation and variable selection simultaneously. New theoretical results, efficient algorithms, and available software play a major role in the recent popularization of lasso. In this study, L1-penalization is adapted to additive logistic regression fitted by smoothing splines. Coefficients associated to predictors. Coefficients associated to predictors with little effect on the response may be shrunk (some of them to zero). This approach gives parsimonious models, removes irrelevant variables, and identifies non linear trends. The estimates are computed via the usual Newton-Raphson update, combined with the lars-lasso algorithm, to resolve the penalization problem, and the backfitting algorithm to fit additive models. Different criteria based on the effective degrees of freedom are proposed to choose the penalization parameters. Performance is illustrated with some examples.",2008,
Consistent Model Combination of Lasso via Regularization Path,"It is well-known that model combination can improve prediction performance of regression model. We investigate the model combination of Lasso with regularization path in this paper. We first define the prediction risk of Lasso estimator, and prove that Lasso regularization path contains at least one prediction consistent estimator. Then we establish the prediction consistency for convex combination of Lasso estimators, which gives the mathematical justification for model combination of Lasso on regularization path. With the inherent piecewise linearity of Lasso regularization path, we construct the initial candidate model set, then select the models for combination with Occamâ€™s Window method. Finally, we carry out the combination on the selected models using the Bayesian model averaging. Theoretical analysis and experimental results suggest the feasibility of the proposed method.",2016,
A generalization of regularized dual averaging and its dynamics,"Excessive computational cost for learning large data and streaming data can be alleviated by using stochastic algorithms, such as stochastic gradient descent and its variants. Recent advances improve stochastic algorithms on convergence speed, adaptivity and structural awareness. However, distributional aspects of these new algorithms are poorly understood, especially for structured parameters. To develop statistical inference in this case, we propose a class of generalized regularized dual averaging (gRDA) algorithms with constant step size, which improves RDA (Xiao, 2010; Flammarion and Bach, 2017). Weak convergence of gRDA trajectories are studied, and as a consequence, for the first time in the literature, the asymptotic distributions for online l1 penalized problems become available. These general results apply to both convex and non-convex differentiable loss functions, and in particular, recover the existing regret bound for convex losses (Nemirovski et al., 2009). As important applications, statistical inferential theory on online sparse linear regression and online sparse principal component analysis are developed, and are supported by extensive numerical analysis. Interestingly, when gRDA is properly tuned, support recovery and central limiting distribution (with mean zero) hold simultaneously in the online setting, which is in contrast with the biased central limiting distribution of batch Lasso (Knight and Fu, 2000). Technical devices, including weak convergence of stochastic mirror descent, are developed as by-products with independent interest. Preliminary empirical analysis of modern image data shows that learning very sparse deep neural networks by gRDA does not necessarily sacrifice testing accuracy.",2019,ArXiv
