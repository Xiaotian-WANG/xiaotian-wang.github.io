title,abstract,year,journal
"Seasonal relationships between foliar moisture content, heat content and biochemistry of lodgepole line and big sagebrush foliage","Wildland fires propagate by liberating energy contained within living and senescent plant biomass. The maximum amount of energy that can be generated by burning a given plant part can be quantified and is generally referred to as its heat content (HC). Many studies have examined heat content of wildland fuels but studies examining the seasonal variation in foliar HC among vegetation types are severely lacking. We collected foliage samples bi-weekly for five months from two common species in the western USA: lodgepole pine (Pinus contorta Douglas ex Loudon) and big sagebrush (Artemisia tridentata Nutt). We measured HC, live fuel moisture content (LFMC) and biochemical components in the leaf dry mass. Our results showed that HC increased for both species, coinciding with LFMC decrease during the growing season. Measured HC values were higher than the constant value in standard fuel models. Lasso regression models identified biochemical components for explaining temporal HC and LFMC variation in lodgepole pine (HC: R2adjâ€‰=â€‰0.55, root mean square error (RMSE)â€‰=â€‰0.35; LFMC: R2adjâ€‰=â€‰0.84, RMSEâ€‰=â€‰10.79), sagebrush (HC: R2adjâ€‰=â€‰0.90, RMSEâ€‰=â€‰0.13; LFMC: R2adjâ€‰=â€‰0.96, RMSEâ€‰=â€‰7.66) and combined data from both species (HC: R2adjâ€‰=â€‰0.77, RMSEâ€‰=â€‰0.33; LFMC: R2adjâ€‰=â€‰0.61, RMSEâ€‰=â€‰19.75). These results demonstrated the seasonal change in HC and LFMC resulted from temporal biochemical composition variation in dry mass. This new knowledge about HC seasonal change will ultimately lead to improved predictions of wildland fire spread and intensity.",2016,International Journal of Wildland Fire
Multivariate Statistical Process Control Using LASSO,"This article develops a new multivariate statistical process control (SPC) methodology based on adapting the LASSO variable selection method to the SPC problem. The LASSO method has the sparsity property of being able to select exactly the set of nonzero regression coefficients in multivariate regression modeling, which is especially useful in cases where the number of nonzero coefficients is small. In multivariate SPC applications, process mean vectors often shift in a small number of components. Our primary goals are to detect such a shift as soon as it occurs and to identify the shifted mean components. Using this connection between the two problems, we propose a LASSO-based multivariate test statistic, and then integrate this statistic into the multivariate EWMA charting scheme for Phase II multivariate process monitoring. We show that this approach balances protection against various shift levels and shift directions, and thus provides an effective tool for multivariate SPC applications. This article...",2009,Journal of the American Statistical Association
Adolescent age estimation using voice features.,"In this paper, a method for evaluating the chronological age of adolescents on the basis of their voice signal is presented. For every examined child, the vowels a, e, i, o and u were recorded in extended phonation. Sixty voice parameters were extracted from each recording. Voice recordings were supplemented with height measurement in order to check if it could improve the accuracy of the proposed solution. Predictor selection was performed using the LASSO (least absolute shrinkage and selection operator) algorithm. For age estimation, the random forest (RF) for regression method was employed and it was tested using a 10-fold cross-validation. The lowest absolute error (0.37 yearâ€‰Â±â€‰0.28) was obtained for boys only when all selected features were included into prediction. In all cases, the achieved accuracy was higher for boys than for girls, which results from the fact that the change of voice with age is larger for men than for women. The achieved results suggest that the presented approach can be employed for accurate age estimation during rapid development in children.",2020,Biomedizinische Technik. Biomedical engineering
Estimation of Cauliflower Weight Based on Multiple Linear Regression Modelling,"In view of the problem that the traditional electronic scale can't predict the prenatal yield of the cauliflower, which having the characteristics of irregular shape and uneven density distribution. In this paper a method is proposed to predict the weight of the cauliflower based on multiple linear regression models. Using Kinect scanning equipment to obtain 3D model of cauliflower, the length, width, height, max_area and volume of the cauliflower 3D model were selected as features. The optimal parameters of the two regularized models, ridge regression and LASSO were selected by cross-validation. In this paper, the K-fold cross-validation (K = 5) was used to divide and select the test set and the train set for small sample. Ridge Regression and LASSO models, as well as optimized models, were evaluated using decision coefficients and relative errors. The experimental results were compared and analyzed. The results show that the optimized LASSO model has the best prediction accuracy for the weight of cauliflower.",2018,
Model-driven parametric monitoring of high-dimensional nonlinear functional profiles,"In order to cope with system complexity and dynamic environments, modern industries are investing in a variety of sensor networks and data acquisition systems to increase information visibility. Multi-sensor systems bring the proliferation of high-dimensional functional profiles that capture rich information on the evolving dynamics of natural and engineered processes. This provides an unprecedented opportunity for online monitoring of operational quality and integrity of complex systems. However, the classical methodology of statistical process control is not concerned about high-dimensional sensor signals and is limited in the capability to perform multi-sensor fault diagnostics. It is not uncommon that multi-dimensional sensing capabilities are not fully utilized for decision making. This paper presents a new model-driven parametric monitoring strategy for the detection of dynamic fault patterns in high-dimensional functional profiles that are nonlinear and nonstationary. First, we developed a sparse basis function model of high-dimensional functional profiles, thereby reducing the large amount of data to a parsimonious set of model parameters (i.e., weight, shifting and scaling factors) while preserving the information. Further, we utilized the lasso-penalized logistic regression model to select a low-dimensional set of sensitive predictors for fault diagnostics. Experimental results on real-world data from patient monitoring showed that the proposed methodology outperforms traditional methods and effectively identify a sparse set of sensitive features from high-dimensional datasets for process monitoring and fault diagnostics.",2014,2014 IEEE International Conference on Automation Science and Engineering (CASE)
Stabilizing the lasso against cross-validation variability,"An abundance of high-dimensional data has meant that L""1 penalized regression, known as the lasso, has become an indispensable tool of the practitioner. A feature of the lasso is a ''tuning'' parameter that controls the amount of shrinkage applied to the coefficients. In practice, a value for the tuning parameter is chosen using the method of cross-validation. It is shown that the model that is selected by the lasso can be extremely sensitive to the fold assignment used for cross-validation. A consequence of this sensitivity is that the results from a lasso analysis can lack interpretability. To overcome this model-selection instability of the lasso, a method called the percentile-lasso is introduced. The model selected by the percentile-lasso corresponds to the model selected by the lasso, when the lasso is fitted using an appropriate percentile of the possible ''optimal'' tuning parameter values. It is demonstrated that the percentile-lasso can achieve substantial improvements in both model-selection stability and model-selection error compared to the lasso. Importantly, when applied to real data the percentile-lasso, unlike the lasso, produces interpretable results, that is, results that are robust to the assignment of observations to folds for cross-validation. The percentile-lasso is easily applied to extensions of the lasso and in the context of penalized generalized linear models.",2014,Comput. Stat. Data Anal.
Root Cause Analysis by a Combined Sparse Classification and Monte Carlo Approach,"Abstract Classification methods with embedded feature selection capability are very appealing for the analysis of complex processes since they allow the analysis of root causes even when the number of input variables is high. In this work, we investigate the performance of three techniques for classification within a Monte Carlo strategy with the aim of root cause analysis. We consider the naive bayes classifier and the logistic regression model with two different implementations for controlling model complexity, namely, a LASSO-like implementation with a l 1 norm regularization and a fully Bayesian implementation of the logistic model, the so called relevance vector machine. Several challenges can arise when estimating such models mainly linked to the characteristics of the data: a large number of input variables, high correlation among subsets of variables, the situation where the number of variables is higher than the number of available data points and the case of unbalanced datasets. Using an ecological and a semiconductor manufacturing dataset, we show advantages and drawbacks of each method, highlighting the superior performance in term of classification accuracy for the relevance vector machine with respect to the other classifiers. Moreover, we show how the combination of the proposed techniques and the Monte Carlo approach can be used to get more robust insights into the problem under analysis when faced with challenging modelling conditions.",2014,IFAC Proceedings Volumes
Near-infrared spectrum discriminant analysis based on information extraction by using the elastic net,"Elastic net method combines the merits of ridge regression and Lasso method. It reduces model prediction error by variable selection while not over-shrinking regression coefficients. In this paper, we take advantages of the elastic net's good properties of variable selection and simultaneous parameter estimation to select the important principal components, then establish discriminant model and apply it to near-infrared spectroscopy quantitative analysis. In the real data set analysis, 103 rhubarb samples were randomly split into two groups, one is viewed as training set which contains 35 samples, another group is considered as testing set which contains 68 samples. All of the samples' protein contents are measured by the national standard Kjeldahl method and the data were called chemical values. In order to testify feasibility and stability of the method, the training set and testing set were conducted random split and analyzed for ten times, respectively. According to these predicting results, the maximum number of false positives was 10, the minimum number of false positives is 5, and average false positive rate is 11.76%. These results showed a significant improvement compared to the results which derived by using ordinary principal component method directly.",2015,2015 11th International Conference on Natural Computation (ICNC)
Grouped variable selection in high dimensional partially linear additive Cox model,"In the analysis of survival outcome supplemented with both clinical information and high-dimensional gene expression data, traditional Cox proportional hazard model fails to meet some emerging needs in biological research. First, the number of covariates is generally much larger the sample size. Secondly, predicting an outcome with individual gene expressions is inadequate because a geneâ€™s expression is regulated by multiple biological processes and functional units. There is a need to understand the impact of changes at a higher level such as molecular function, cellular component, biological process, or pathway. The change at a higher level is usually measured with a set of gene expressions related to the biological process. That is, we need to model the outcome with gene sets as variable groups and the gene sets could be partially overlapped also. In this thesis work, we investigate the impact of a penalized Cox regression procedure on regularization, parameter estimation, variable group selection, and nonparametric modeling of nonlinear effects with a time-to-event outcome. We formulate the problem as a partially linear additive Cox model with high-dimensional data. We group genes into gene sets and approximate the nonparametric components by truncated series expansions with B-spline bases. After grouping and approximation, the problem of variable selection becomes that of selecting groups of coefficients in a gene set or in an approximation. We apply the group Lasso to obtain an initial solution path and reduce the dimension of",2019,
Machine Learning and Statistical Analysis for Materials Science: Stability and Transferability of Fingerprint Descriptors and Chemical Insights,"In the paradigm of virtual high-throughput screening for materials, we have developed a semiautomated workflow or â€œrecipeâ€ that can help a material scientist to start from a raw data set of materials with their properties and descriptors, build predictive models, and draw insights into the governing mechanism. We demonstrate our recipe, which employs machine learning tools and statistical analysis, through application to a case study leading to identification of descriptors relevant to catalysts for CO2 electroreduction, starting from a published database of 298 catalyst alloys. At the heart of our methodology lies the Bootstrapped Projected Gradient Descent (BoPGD) algorithm, which has significant advantages over commonly used machine learning (ML) and statistical analysis (SA) tools such as the regression coefficient shrinkage-based method (LASSO) or artificial neural networks: (a) it selects descriptors with greater stability and transferability, with a goal to understand the chemical mechanism rather ...",2017,Chemistry of Materials
Metabolomic Characterization of Hepatocellular Carcinoma in Patients with Liver Cirrhosis for Biomarker Discovery.,"Background: Metabolomics plays an important role in providing insight into the etiology and mechanisms of hepatocellular carcinoma (HCC). This is accomplished by a comprehensive analysis of patterns involved in metabolic alterations in human specimens. This study compares the levels of plasma metabolites in HCC cases versus cirrhotic patients and evaluates the ability of candidate metabolites in distinguishing the two groups. Also, it investigates the combined use of metabolites and clinical covariates for detection of HCC in patients with liver cirrhosis.Methods: Untargeted analysis of metabolites in plasma from 128 subjects (63 HCC cases and 65 cirrhotic controls) was conducted using gas chromatography coupled to mass spectrometry (GC-MS). This was followed by targeted evaluation of selected metabolites. LASSO regression was used to select a set of metabolites and clinical covariates that are associated with HCC. The performance of candidate biomarkers in distinguishing HCC from cirrhosis was evaluated through a leave-one-out cross-validation based on area under the receiver operating characteristics (ROC) curve.Results: We identified 11 metabolites and three clinical covariates that differentiated HCC cases from cirrhotic controls. Combining these features in a panel for disease classification using support vector machines (SVM) yielded better area under the ROC curve compared with alpha-fetoprotein (AFP).Conclusions: This study demonstrates the combination of metabolites and clinical covariates as an effective approach for early detection of HCC in patients with liver cirrhosis.Impact: Further investigation of these findings may improve understanding of HCC pathophysiology and possible implication of the metabolites in HCC prevention and diagnosis. Cancer Epidemiol Biomarkers Prev; 26(5); 675-83. Â©2016 AACR.",2017,"Cancer epidemiology, biomarkers & prevention : a publication of the American Association for Cancer Research, cosponsored by the American Society of Preventive Oncology"
Risk Prediction Using Genome-Wide Association Studies on Type 2 Diabetes,"The success of genome-wide association studies (GWASs) has enabled us to improve risk assessment and provide novel genetic variants for diagnosis, prevention, and treatment. However, most variants discovered by GWASs have been reported to have very small effect sizes on complex human diseases, which has been a big hurdle in building risk prediction models. Recently, many statistical approaches based on penalized regression have been developed to solve the ""large p and small n"" problem. In this report, we evaluated the performance of several statistical methods for predicting a binary trait: stepwise logistic regression (SLR), least absolute shrinkage and selection operator (LASSO), and Elastic-Net (EN). We first built a prediction model by combining variable selection and prediction methods for type 2 diabetes using Affymetrix Genome-Wide Human SNP Array 5.0 from the Korean Association Resource project. We assessed the risk prediction performance using area under the receiver operating characteristic curve (AUC) for the internal and external validation datasets. In the internal validation, SLR-LASSO and SLR-EN tended to yield more accurate predictions than other combinations. During the external validation, the SLR-SLR and SLR-EN combinations achieved the highest AUC of 0.726. We propose these combinations as a potentially powerful risk prediction model for type 2 diabetes.",2016,Genomics & Informatics
Penalized Regression Methods for Linear Models in SAS/STAT,"Regression problems with many potential candidate predictor variables occur in a wide variety of scientific fields and business applications. These problems require you to perform statistical model selection to find an optimal model, one that is as simple as possible while still providing good predictive performance. Traditional stepwise selection methods, such as forward and backward selection, suffer from high variability and low prediction accuracy, especially when there are many predictor variables or correlated predictor variables (or both). In the last decade, the higher prediction accuracy and computational efficiency of penalized regression methods have made them an attractive alternative to traditional selection methods. This paper first provides a brief review of the LASSO, adaptive LASSO, and elastic net penalized model selection methods. Then it explains how to perform model selection by applying these techniques with the GLMSELECT procedure, which includes extensive customization options and powerful graphs for steering statistical model selection.",2015,
Texts in Statistics An Introduction to Statistical Learning,"Statistics An Intduction to Stistical Lerning with Applications in R An Introduction to Statistical Learning provides an accessible overview of the fi eld of statistical learning, an essential toolset for making sense of the vast and complex data sets that have emerged in fi elds ranging from biology to fi nance to marketing to astrophysics in the past twenty years. Th is book presents some of the most important modeling and prediction techniques, along with relevant applications. Topics include linear regression, classifi cation, resampling methods, shrinkage approaches, tree-based methods, support vector machines, clustering, and more. Color graphics and real-world examples are used to illustrate the methods presented. Since the goal of this textbook is to facilitate the use of these statistical learning techniques by practitioners in science, industry, and other fi elds, each chapter contains a tutorial on implementing the analyses and methods presented in R, an extremely popular open source statistical soft ware platform. Two of the authors co-wrote Th e Elements of Statistical Learning (Hastie, Tibshirani and Friedman, 2nd edition 2009), a popular reference book for statistics and machine learning researchers. An Introduction to Statistical Learning covers many of the same topics, but at a level accessible to a much broader audience. Th is book is targeted at statisticians and non-statisticians alike who wish to use cutting-edge statistical learning techniques to analyze their data. Th e text assumes only a previous course in linear regression and no knowledge of matrix algebra. Gareth James is a professor of statistics at University of Southern California. He has published an extensive body of methodological work in the domain of statistical learning with particular emphasis on high-dimensional and functional data. Th e conceptual framework for this book grew out of his MBA elective courses in this area. Daniela Witten is an assistant professor of biostatistics at University of Washington. Her research focuses largely on high-dimensional statistical machine learning. She has contributed to the translation of statistical learning techniques to the fi eld of genomics, through collaborations and as a member of the Institute of Medicine committee that led to the report Evolution of Translational Omics. Trevor Hastie and Robert Tibshirani are professors of statistics at Stanford University, and are co-authors of the successful textbook Elements of Statistical Learning. Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie co-developed much of the statistical modeling soft ware and environment in R/S-PLUS and invented principal curves and surfaces. Tibshirani proposed the lasso and is co-author of the very successful An Introduction to the Bootstrap.",2017,
L1 Regularized Multiplicative Iterative Path Algorithm for Non-negative Generalized Linear Models,"In regression modeling, often a restriction that regression coefficients are non-negative is faced. The problem of model selection in non-negative generalized linear models (NNGLM) is considered using lasso, where regression coefficients in the linear predictor are subject to non-negative constraints. Thus, non-negatively constrained regression coefficient estimation is sought by maximizing the penalized likelihood (such as the l 1 -norm penalty). An efficient regularization path algorithm is proposed for generalized linear models with non-negative regression coefficients. The algorithm uses multiplicative updates which are fast and simultaneous. Asymptotic results are also developed for the constrained penalized likelihood estimates. Performance of the proposed algorithm is shown in terms of computational time, accuracy of solutions and accuracy of asymptotic standard deviations.",2016,Comput. Stat. Data Anal.
RÃ©gression : ThÃ©orie et applications,"Cet ouvrage expose de maniere detaillee, exemples a l'appui, lâ€™une des methodes statistiques les plus courantes : la regression. Les premiers chapitres sont consacres a la regression lineaire simple et multiple. Ils expliquent les fondements de la methode, tant au niveau des choix operes que des hypotheses et de leur utilite. Ensuite sont developpes les outils permettant de verifier les hypotheses de base mises en Å“uvre par la regression. Une presentation simple des modeles d'analyse de la covariance et de la variance est effectuee. Enfin, les derniers chapitres sont consacres au choix de modeles ainsi qu'a certaines extensions de la regression: lasso, PLS, PCR... La presentation temoigne d'un reel souci pedagogique des auteurs qui beneficient d'une experience d'enseignement aupres de publics tres varies. Les resultats exposes sont replaces dans la perspective de leur utilite pratique grÃ¢ce a l'analyse d'exemples concrets. Les commandes permettant le traitement des exemples sous le logiciel R figurent dans le corps du texte. Enfin chaque chapitre est complete par une suite d'exercices corriges. Le niveau mathematique requis le rend accessible aux etudiants des ecoles d'ingenieurs, de Masters et aux chercheurs dans les divers domaines des sciences appliquees.",2007,
Penalized Interaction Estimation for Ultrahigh Dimensional Quadratic Regression,"Quadratic regression goes beyond the linear model by simultaneously including main effects and interactions between the covariates. The problem of interaction estimation in high dimensional quadratic regression has received extensive attention in the past decade. In this article we introduce a novel method which allows us to estimate the main effects and interactions separately. Unlike existing methods for ultrahigh dimensional quadratic regressions, our proposal does not require the widely used heredity assumption. In addition, our proposed estimates have explicit formulas and obey the invariance principle at the population level. We estimate the interactions of matrix form under penalized convex loss function. The resulting estimates are shown to be consistent even when the covariate dimension is an exponential order of the sample size. We develop an efficient ADMM algorithm to implement the penalized estimation. This ADMM algorithm fully explores the cheap computational cost of matrix multiplication and is much more efficient than existing penalized methods such as all pairs LASSO. We demonstrate the promising performance of our proposal through extensive numerical studies.",2019,arXiv: Methodology
A penalized regression approach for DNA copy number study using the sequencing data.,"Modeling the high-throughput next generation sequencing (NGS) data, resulting from experiments with the goal of profiling tumor and control samples for the study of DNA copy number variants (CNVs), remains to be a challenge in various ways. In this application work, we provide an efficient method for detecting multiple CNVs using NGS reads ratio data. This method is based on a multiple statistical change-points model with the penalized regression approach, 1d fused LASSO, that is designed for ordered data in a one-dimensional structure. In addition, since the path algorithm traces the solution as a function of a tuning parameter, the number and locations of potential CNV region boundaries can be estimated simultaneously in an efficient way. For tuning parameter selection, we then propose a new modified Bayesian information criterion, called JMIC, and compare the proposed JMIC with three different Bayes information criteria used in the literature. Simulation results have shown the better performance of JMIC for tuning parameter selection, in comparison with the other three criterion. We applied our approach to the sequencing data of reads ratio between the breast tumor cell lines HCC1954 and its matched normal cell line BL 1954 and the results are in-line with those discovered in the literature.",2019,Statistical applications in genetics and molecular biology
A proposal for variable selection in the Cox model,"We propose a new method for variable selection and estimation in Cox's proportional hazards model. Our proposal minimizes the log partial likelihood subject to the sum of the absolute values of the parameters being bounded by a constant. Because of the nature of this constraint it tends to produce some coeecients that are exactly zero and hence gives interpretable models. The method is a variation of the \lasso"" proposal of Tibshirani (1994), designed for the linear regression context. Simulations indicate that the lasso can be more accurate than stepwise selection in this setting.",1997,
Consistent Robust Regression,"We present the first efficient and provably consistent estimator for the robust regression problem. The area of robust learning and optimization has generated a significant amount of interest in the learning and statistics communities in recent years owing to its applicability in scenarios with corrupted data, as well as in handling model mis-specifications. In particular, special interest has been devoted to the fundamental problem of robust linear regression where estimators that can tolerate corruption in up to a constant fraction of the response variables are widely studied. Surprisingly however, to this date, we are not aware of a polynomial time estimator that offers a consistent estimate in the presence of dense, unbounded corruptions. In this work we present such an estimator, called CRR. This solves an open problem put forward in the work of (Bhatia et al, 2015). Our consistency analysis requires a novel two-stage proof technique involving a careful analysis of the stability of ordered lists which may be of independent interest. We show that CRR not only offers consistent estimates, but is empirically far superior to several other recently proposed algorithms for the robust regression problem, including extended Lasso and the TORRENT algorithm. In comparison, CRR offers comparable or better model recovery but with runtimes that are faster by an order of magnitude.",2017,
