title,abstract,year,journal
Buckley-James Boosting for Survival Analysis with High-Dimensional Biomarker Data,"There has been increasing interest in predicting patients' survival after therapy by investigating gene expression microarray data. In the regression and classification models with high-dimensional genomic data, boosting has been successfully applied to build accurate predictive models and conduct variable selection simultaneously. We propose the Buckley-James boosting for the semiparametric accelerated failure time models with right censored survival data, which can be used to predict survival of future patients using the high-dimensional genomic data. In the spirit of adaptive LASSO, twin boosting is also incorporated to fit more sparse models. The proposed methods have a unified approach to fit linear models, non-linear effects models with possible interactions. The methods can perform variable selection and parameter estimation simultaneously. The proposed methods are evaluated by simulations and applied to a recent microarray gene expression data set for patients with diffuse large B-cell lymphoma under the current gold standard therapy.",2010,Statistical Applications in Genetics and Molecular Biology
Accuracy of genomic selection for alfalfa biomass yield in different reference populations,"BackgroundGenomic selection based on genotyping-by-sequencing (GBS) data could accelerate alfalfa yield gains, if it displayed moderate ability to predict parent breeding values. Its interest would be enhanced by predicting ability also for germplasm/reference populations other than those for which it was defined. Predicting accuracy may be influenced by statistical models, SNP calling procedures and missing data imputation strategies.ResultsLandrace and variety material from two genetically-contrasting reference populations, i.e., 124 elite genotypes adapted to the Po Valley (sub-continental climate; PV population) and 154 genotypes adapted to Mediterranean-climate environments (Me population), were genotyped by GBS and phenotyped in separate environments for dry matter yield of their dense-planted half-sib progenies. Both populations showed no sub-population genetic structure. Predictive accuracy was higher by joint rather than separate SNP calling for the two data sets, and using random forest imputation of missing data. Highest accuracy was obtained using Support Vector Regression (SVR) for PV, and Ridge Regression BLUP and SVR for Me germplasm. Bayesian methods (Bayes A, Bayes B and Bayesian Lasso) tended to be less accurate. Random Forest Regression was the least accurate model. Accuracy attained about 0.35 for Me in the range of 0.30-0.50 missing data, and 0.32 for PV at 0.50 missing data, using at least 10,000 SNP markers. Cross-population predictions based on a smaller subset of common SNPs implied a relative loss of accuracy of about 25Â % for Me and 30Â % for PV. Genome-wide association analyses based on large subsets of M. truncatula-aligned markers revealed many SNPs with modest association with yield, and some genome areas hosting putative QTLs. A comparison of genomic vs. conventional selection for parent breeding value assuming 1-year vs. 5-year selection cycles, respectively, indicated over three-fold greater predicted yield gain per unit time for genomic selection.ConclusionsGenomic selection for alfalfa yield is promising, based on its moderate prediction accuracy, moderate value of cross-population predictions, and lack of sub-population structure. There is limited scope for searching individual QTLs with overwhelming effect on yield. Some of our results can contribute to better design of genomic selection experiments for alfalfa and other crops with similar mating systems.",2015,BMC Genomics
Abstract 1401: A systems immunology analysis to detect prognostic biomarkers in patients with squamous cell carcinomas of the head and neck,"In 2015, an estimated 59,340 people will develop head and neck cancer and an estimated 12,290 deaths will occur. Recent evidence has demonstrated that the immune system plays a key role in the development, establishment, and progression of head and neck squamous cell carcinoma (HNSCC). Here, we conduct comprehensive immune profiling to better understand the immunophenotype of HNSCC patients, assess the applicability of immune-modulatory drugs, and identify prognostic biomarkers of response to cetuximab treatment. Our systems immunology approach is composed of three complimentary, high-dimensional technologies: time-of-flight mass cytometry (CyTOF), luminex, and the HIMChip microarray platform. Mass cytometry quantifies protein expression on a single-cell basis by utilizing transition element, isotope-tagged antibodies. The Luminex immunoassay measures plasma cytokine levels. The â€œHIMChipâ€ microarray is a custom Agilent SurePrint HD 8Ã—15k format array containing over 7,000 unique probes for over 4,274 human immune-related genes. We employed these technologies to study the global immune status of thirty HNSCC patients receiving treatment with cetuximab, an IgG1 monoclonal antibody (mAb) targeting the epidermal growth factor receptor (EGFR). Patients were consented to participate on the Phase 0 biomarker-focused clinical trial (NCT01114256) and peripheral blood was obtained before and after cetuximab treatment. Peripheral blood mononuclear cells and plasma were isolated from each time point and utilized in downstream analysis. Via manual gating, our CyTOF-based phenotyping measured 24 distinct populations and the expression of 5 activation markers and 5 checkpoint receptors. The results of manual gating were confirmed with an automated, unsupervised algorithmic analysis combining stochastic neighbor embedding and k-means clustering. The resulting 2D representation of our single-cell data preserved global geometries and facilitated exploration of unique subsets within the natural killer (NK) compartment. To assess the prognostic value of these NK cell subsets, we applied a lasso-regularized logistic regression model to stratify patients based on their clinical response to cetuximab therapy. We identified an NK cell subset that is associated with clinical benefit to therapy. Luminex analysis revealed a cetuximab-induced cytokine signature composed of VCAM1, IP-10, and VEGFD that may play a role in increasing NK cell trafficking to tumor (T-statistics of 1.77, 2.17, and 1.55 respectively). The HIMChip microarray results support our proteomic findings. These results provide the first known comprehensive immune profiling of HNSCC patients as they undergo treatment with cetuximab. Validation in a large cohort of patients is needed and future mechanistic studies will investigate the efficacy of our novel NK cell population in disease control. Citation Format: Cariad Chester, Ajay Fernandez, Atsushi Yonezawa, Xing Zhao, Naren Rajasekaran, Holbrook Kohrt. A systems immunology analysis to detect prognostic biomarkers in patients with squamous cell carcinomas of the head and neck. [abstract]. In: Proceedings of the 107th Annual Meeting of the American Association for Cancer Research; 2016 Apr 16-20; New Orleans, LA. Philadelphia (PA): AACR; Cancer Res 2016;76(14 Suppl):Abstract nr 1401.",2016,Cancer Research
Accuracy of Genomic Prediction under Different Genetic Architectures and Estimation Methods,"The accuracy of genomic breeding value prediction was investigated in various levels of reference population size, trait heritability and the number of quantitative trait locus (QTL). Five Bayesian methods, including Bayesian Ridge regression, BayesA, BayesB, BayesC and Bayesian LASSO, were used to estimate the marker effects for each of 27 scenarios resulted from combining three levels for heritability (0.1, 0.3 and 0.5), training population size (600, 1000 and 1600) and QTL numbers (50, 100 and 150). A finite locus model was used to simulate stochastically a historical population consisting 100 animals at first 100 generations. Through next 100 generations, the population size gradually increased to 1000 individuals. Then the animals in generations 201 and 202 having both known genotypic and phenotypic records were assigned as reference population, and individuals at generations 203 and 204 were considered as validation population. The genome comprised five chromosomes of 100 cM length and 500 single nucleotide polymorphism markers for each chromosome that distributed through the genome randomly. The QTLs and markers were bi-allelic. In this study, the heritability had great significant positive effect on the accuracy (P<0.001). By increasing the size of the reference population, the average genomic accuracy increased from 0.64Â±0.03 to 0.70 Â± 0.04 (P<0.001). The accuracy responded to increasing number of QTLs non-linearly. The highest and lowest accuracies of Bayesian methods were 0.40 Â± 0.04 and 0.84 Â± 0.05, respectively. The results showed having the greatest amount of information (i.e. highest heritability, highest contribution of gene action in phenotypic variation and large reference population size), the highest accuracy (0.84) was obtained, with all investigated methods of estimation.",2018,Iranian Journal of Applied Animal Science
"How Much Is My House Worth ? Predicting Housing Prices in McKinney , Texas","The goal was to give homeowners a prediction on their houseâ€™s closing price which would be put on the market. Classification and regression methods were used as well as natural language processing on each houseâ€™s remarks. The best set of models found used Lasso L1 regularization. Prediction performed remarkably better than classification, and predicted close prices with 95% confidence with this interval: $54, 400 to $51, 800 The goal of this project is to create a model that can best predict the closing price for a house before it has been placed on the market. This is important as it can help homeowners not only estimate a listing price but serve as a second opinion if need be. The project is based on data from the company Opendoor, which uses machine learning to predict housing prices and make selling your home easier. Our dataset is from home sales in the small town of McKenney Texas from late 2014 to mid 2015. Although it has only about 1100 observations, it provides many different features for the data such as area of the house, types of floors, number of bedrooms, etc. Dataset and Features Cleaning the data set included eliminating features that were relevant only once the house had been sold since we were only interested in predicting the price that a house should be given when entered into the market. We then converted categorical features such as the type of flooring of the house into binary ones. We also created new features such as binary features for the season in which the house was listed on the market and the age of the house. These modifications to our dataset largely increased the number of features in our training set so that at the end we had 83 features. Once we had cleaned our data set we looked at the shape of the distribution of the target feature, the closing price of the house, to understand how to best fit the data. The closing price seemed to follow a normal distribution skewed left, and the log norCopyright c 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. mal of the distribution looked closer to a normal distribution not skewed in any direction. We decided to test how our models did predicting the log of the closing price as well as just the closing price.",2016,
Development of an individualized risk calculator for poor functioning in young people victimized during childhood: A longitudinal cohort study,"BACKGROUND
Childhood victimization elevates the average risk of developing functional impairment in adulthood. However, not all victimized children demonstrate poor outcomes. Although research has described factors that confer vulnerability or resilience, it is unknown if this knowledge can be translated to accurately identify the most vulnerable victimized children.


OBJECTIVE
To build and internally validate a risk calculator to identify those victimized children who are most at risk of functional impairment at age 18 years.


PARTICIPANTS
We utilized data from the Environmental Risk (E-Risk) Longitudinal Twin Study, a nationally-representative birth cohort of 2232 UK children born in 1994-95.


METHODS
Victimization exposure was assessed repeatedly between ages 5 and 12 years along with a range of individual-, family- and community-level predictors. Functional outcomes were assessed at age 18 years. We developed and evaluated a prediction model for psychosocial disadvantage and economic disadvantage using the Least Absolute Shrinkage and Selection Operator (LASSO) regularized regression with nested 10-fold cross-validation.


RESULTS
The model predicting psychosocial disadvantage following childhood victimization retained 12 of 22 predictors, had an area under the curve (AUC) of 0.65, and was well-calibrated within the range of 40-70% predicted risk. The model predicting economic disadvantage retained 10 of 22 predictors, achieved excellent discrimination (AUCâ€‰=â€‰0.80), and a high degree of calibration.


CONCLUSIONS
Prediction modelling techniques can be applied to estimate individual risk for poor functional outcomes in young adulthood following childhood victimization. Such risk prediction tools could potentially assist practitioners to target interventions, which is particularly useful in a context of scarce resources.",2019,Child Abuse & Neglect
Robust Variable Selection Methods for Grouped Data,"When predictor variables possess an underlying grouping structure in multiple regression, selecting important groups of variables is an essential component of building a meaningful regression model. Some methods exist to perform group selection, but do not perform well when the data include outliers. Four methods for robust variable selection of grouped data, based on the group LASSO, are presented: two regular methods and two adaptive methods. For each of the two methods in the regular and adaptive groups, one method works well for data with outliers in the y-direction, and the other method works well for data with outliers in both the xand ydirections. The effectiveness of each of these methods is illustrated with an extensive simulation study and a real data example.",2015,
Cortico-cortical and cortico-muscular connectivity analysis : methods and application to Parkinson's disease,"The concept of brain connectivity provides a new perspective to the understanding of the mechanism underlying brain functions, complementing the traditional approach of analyzing neural activity of isolated regions. Among the existing connectivity analysis techniques, multivariate autoregressive (mAR)-based measures are of great interest for their ability to characterize both directionality and spectral property of cortical interactions. Yet, the direct estimation of mAR-based connectivity from scalp electroencephalogram (EEG) is confounded by volume conduction, statistical instability and inter-subject variability. In this thesis, we propose novel signal processing methods to enhance the existing mAR-based connectivity methods. First, we explore incorporating sparsity constraints into the mAR formulation at both subject level and group level using LASSO-based regression. We show by simulation that sparse mAR yields more stable and accurate connectivity estimates compared to the traditional, non-sparse approach. Furthermore, the group-wise sparsity simplifies the inference of group-level connectivity patterns from multi-subject data. To mitigate the effect of volume conduction, we investigate source-level connectivity and propose a state-space generalized mAR framework to jointly model the mixing effect of volume conduction and causal relationships between underlying neural sources. By jointly estimating the mixing process and mAR model parameters, the proposed technique demonstrates improved connectivity estimation performance. Finally, we expanded our connectivity analysis to cortico-muscular level by modeling the relationships between EEG and simultaneously recorded electromyography (EMG) data using a multiblock partial least square (mbPLS) framework. The hierarchical construction of the mbPLS framework provides a natural way to model multi-subject, multi-",2012,
A comparative study on the classification of engineering surfaces with dimension reduction and coefficient shrinkage methods,"As more automated and accurate surface inspection devices enter the manufacturing process, engineers collect a larger amount of surface inspection data, in terms of storage space and the number of parameters to characterize the surface, but sometimes smaller in terms of the number of coherent surface observations. In these cases, more features are preferable to characterize engineering surfaces for capturing the details of the surface finish patterns. When the number of surface parameters exceeds the number of collected surface observations, a difficulty with the dimensionality emerges in classification. This paper has researched the accuracy and interpretability of using the dimension reduction and coefficient shrinkage methods in combination with the logistic model to deal with this dimensionality problem in engineering surface classification. Five methods for dimension reduction and coefficient shrinkage are selected and compared. These are: subset selection (Sub), principal component analysis (PCA), partial least squares (PLS), ridge regression (Ridge), and least absolute and shrinkage and selection operator (Lasso). A case study is used to illustrate their effectiveness by classifying 30 pump body surfaces with 40 surface feature parameters. The obtained results show that the dimension reduction methods, PCA and PLS, could achieve higher classification accuracies but their results are not interpretable. Sub could achieve higher accuracy in this case, but the discrete parameter selection process is aggressive. Finally, the classification results of the coefficient shrinkage methods, Ridge and Lasso, are interpretable for process faults diagnosis purposes; however, the accuracies are lower than the other methods.",2006,Journal of Manufacturing Systems
Theory for â„“ 1 /â„“ 2 -penalty procedures,"We study four procedures for regression models with group structure in the parameter vector. The first two are for models with univariate response variable. They are the so-called group Lasso (see Chapter 4), and the smoothed group Lasso for the high-dimensional additive model (see Chapter 5). We also discuss multivariate extensions, namely for the linear model with time-varying coefficients, for multivariate regression, and multitask learning.",2011,
A risk score model for the prediction of osteosarcoma metastasis,"Osteosarcoma is the most common primary solid malignancy of the bone, and its high mortality usually correlates with early metastasis. In this study, we developed a risk score model to help predict metastasis at the time of diagnosis. We downloaded and mined four expression profile datasets associated with osteosarcoma metastasis from the Gene Expression Omnibus. After data normalization, we performed LASSO logistic regression analysis together with 10-fold cross validation using the GSE21257 dataset. A combination of eight genes (RAB1,CLEC3B,FCGBP,RNASE3,MDL1,ALOX5AP,VMO1 and ALPK3) were identified as being associated with osteosarcoma metastasis. These genes were put into a gene risk score model, and the prediction efficiency of the model was then validated using three independent datasets (GSE33383, GSE66673, and GSE49003) by plotting receiver operating characteristic curves. The expression levels of the eight genes in all datasets were shown as heatmaps, and gene ontology gene annotation and Kyoto Encyclopedia of Genes and Genomes pathway enrichment analysis were performed. These eight genes play a role in cancer-related biological processes, such as apoptosis and biosynthetic processes. Our results may aid in elucidating the possible mechanisms of osteosarcoma metastasis, and may help to facilitate the individual management of patients with osteosarcoma after treatment.",2019,FEBS Open Bio
Ensemble Feature Selection for Plant Phenotyping: A Journey From Hyperspectral to Multispectral Imaging,"Hyperspectral imaging is becoming an increasingly popular tool for high-throughput plant phenotyping, because it provides remarkable insights about the health status of plants. Feature selection is a key component in a hyperspectral image analysis, largely because a significant portion of spectral features are redundant and/or irrelevant, depending on the desired application. This paper presents an ensemble feature selection method to identify the most informative spectral features for practical applications in plant phenotyping. The hyperspectral data set contained the images of four wheat lines, each with a control and a salt (NaCl) treatment. To rank spectral features, six feature selection methods were used as the base for the ensemble: correlation-based feature selection, ReliefF, sequential feature selection, support vector machine-recursive feature elimination (SVM-RFE), LASSO logistic regression, and random forest. The best results were achieved by the ensemble of ReliefF, SVM-RFE, and random forest, which drastically reduced the dimension of the hyperspectral data set from 215 to 15 features, while improving the accuracy in classifying the salt-treated vegetation pixels from the control pixels by 8.5%. To transform the hyperspectral data set into a multispectral data set, six wavelengths as the center of broad multispectral bands around the most prominent features were determined by a clustering algorithm. The result of salt tolerance assessment of the four wheat lines using the derived multispectral data set was similar to that of the hyperspectral data set. This demonstrates that the proposed feature selection pipeline can be utilized for determining the most informative features and can be a valuable tool in the development of tailored multispectral cameras.",2018,IEEE Access
1485: Cardiopulmonary Resuscitation for In-hospital Cardiac Arrest in the Elderly Retrospective Analysis,"Learning Objectives: Clinicians rely on their knowledge of prognosis to support patient decision-making regarding whether to enact Do Not Attempt Resuscitation (DNAPCR) orders. The Good Outcome Following Attempted Resuscitation (GO-FAR) Score is a well-validated tool that predicts survival after In-Hospital Cardiac Arrest (IHCA) with no cerebral dysfunction (CPC â‰¤ 1). However, patients may consider survival with moderate cerebral dysfunction acceptable (CPC â‰¤ 2). Our objective was to validate the ability of the GO-FAR score to identify patients unlikely to survive IHCA with CPC â‰¤ 2 and to develop a modified GO-FAR score to accurately predict survival after IHCA with CPC â‰¤ 2. Methods: 54,425 inpatients with IHCA from 2012 2017. First, validation of the GO-FAR Score was attempted using survival with CPC â‰¤ 2. Next, the data were divided into training (44%), test (22%), and validation (34%) sets. Univariate analysis was used to identify variables with >3% absolute difference in survival with CPC â‰¤ 2, which were carried forward to a multivariate analysis. A final logistic regression model was created using Bayesian information criterion (BIC), and least absolute shrinkage and selection operator (LASSO). Candidate decision models were assessed using the test dataset to select the most parsimonious model that best classified patients as having a very poor (â‰¤5%), poor (6%10%), average (11%30%), or above average (>30%) likelihood of survival after CPR for IHCA with CPC â‰¤ 2. Results: The original GO FAR Score performed poorly for CPC â‰¤ 2. In the univariate model, age > 85, admission CPC < 2, and non-surgical admission had the largest association with outcome (-12.1%, -14.4%, and -18%, respectively). 13 variables were included in the logistic regression analysis. The final model accurately categorized 24.8% of patients with below average likelihood of CPC â‰¤ 2 in the training set, versus 21.5% and 22.4% in the testing and validation sets. Conclusions: The shared decision-making process must balance the patientâ€™s goals and values for medical treatment with an accurate estimate of a patientâ€™s chance of survival with an acceptable functional outcome. The GO-FARTHER Score extends the relevance of the GO-FAR Score to those patients with CPC â‰¤ 2.",2019,Critical Care Medicine
A Sparsity Inducing Nuclear-Norm Estimator (SpINNEr) for Matrix-Variate Regression in Brain Connectivity Analysis,"Classical scalar-response regression methods treat covariates as a vector and estimate a corresponding vector of regression coefficients. In medical applications, however, regressors are often in a form of multi-dimensional arrays. For example, one may be interested in using MRI imaging to identify which brain regions are associated with a health outcome. Vectorizing the two-dimensional image arrays is an unsatisfactory approach since it destroys the inherent spatial structure of the images and can be computationally challenging. We present an alternative approach - regularized matrix regression - where the matrix of regression coefficients is defined as a solution to the specific optimization problem. The method, called SParsity Inducing Nuclear Norm EstimatoR (SpINNEr), simultaneously imposes two penalty types on the regression coefficient matrix---the nuclear norm and the lasso norm---to encourage a low rank matrix solution that also has entry-wise sparsity. A specific implementation of the alternating direction method of multipliers (ADMM) is used to build a fast and efficient numerical solver. Our simulations show that SpINNEr outperforms other methods in estimation accuracy when the response-related entries (representing the brain's functional connectivity) are arranged in well-connected communities. SpINNEr is applied to investigate associations between HIV-related outcomes and functional connectivity in the human brain.",2020,ArXiv
Generalized spike-and-slab priors for Bayesian group feature selection using expectation propagation,"We describe a Bayesian method for group feature selection in linear regression problems. The method is based on a generalized version of the standard spike-and-slab prior distribution which is often used for individual feature selection. Exact Bayesian inference under the prior considered is infeasible for typical regression problems. However, approximate inference can be carried out efficiently using Expectation Propagation (EP). A detailed analysis of the generalized spike-and-slab prior shows that it is well suited for regression problems that are sparse at the group level. Furthermore, this prior can be used to introduce prior knowledge about specific groups of features that are a priori believed to be more relevant. An experimental evaluation compares the performance of the proposed method with those of group LASSO, Bayesian group LASSO, automatic relevance determination and additional variants used for group feature selection. The results of these experiments show that a model based on the generalized spike-and-slab prior and the EP algorithm has state-of-the-art prediction performance in the problems analyzed. Furthermore, this model is also very useful to carry out sequential experimental design (also known as active learning), where the data instances that are most informative are iteratively included in the training set, reducing the number of instances needed to obtain a particular level of prediction accuracy.",2013,J. Mach. Learn. Res.
Credit Scoring Menggunakan Regresi Logistik Lasso,"Credit scoring merupakan suatu metode berbasis analisis statistika yang digunakan untuk mengukur besaran resiko kredit. Metode klasifikasi yang paling populer digunakan untuk credit scoring adalah regresi logistik. Regresi logistik digunakan untuk memprediksi variabel respon yang biner dengan satu set variabel penjelas (prediktor). Regresi logistik mempunyai keterbatasan yaitu jika terdapat multikolinieritas (korelasi yang tinggi antar variabel bebas) membuat model regresi yang didapat menjadi tidak lagi efisien karena nilai standar error koefisien regresi menjadi sangat besar (overestimate) atau dengan kata lain mengurangi akurasi dari estimasi. Oleh karena itu, diusulkan metode Least Absolute Shrinkage and Selection Operator (LASSO) untuk mengatasi hal tersebut. LASSO akan menyusutkan koefisien (parameter IÂ²) yang berkorelasi, menjadi nol atau mendekati nol. Sehingga menghasilkan model akhir yang lebih representatif. Pada akhirnya, performa model regresi logistik LASSO dibandingkan dengan model regresi logistik. Dengan melihat nilai presentase ketepatan model, model regresi logistik LASSO dianggap lebih baik daripada model regresi logistik karena memiliki nilai presentase ketepatan model yang lebih besar. 
Credit scoring is a method based on a statistical analysis is used to measure the amount of credit risk. The most popular method of classification used for credit scoring is logistic regression. Logistic regression was used to predict the response variable is a binary with a set of explanatory variables (predictors). Logistic regression have limitations, if there is multicollinearity (a high correlation between independent variables) obtained makes regression model becomes more not efficient because the value of the standard error of regression coefficient becomes very large (overestimate), or in other words reduce the accuracy of the estimate. Therefore, Least Absolute Shrinkage and Selection Operator (LASSO) method proposed to overcome it. LASSO would shrink coefficient (parameter IÂ²), which correlated, to zero or close to zero. Resulting in a final model more representative. In the end, the performance of the logistic regression model LASSO compared with logistic regression model. By looking at the value of the percentage of accuracy from the model, the logistic regression model LASSO considered better than the logistic regression model because it has larger percentage of of models accuracy.",2016,
REMI: Regression with marginal information and its application in genome-wide association studies,"In this study, we consider the problem of variable selection and estimation in high-dimensional linear regression models when the complete data are not accessible, but only certain marginal information or summary statistics are available. This problem is motivated from the Genome-wide association studies (GWAS) that have been widely used to identify risk variants underlying complex human traits/diseases. With a large number of completed GWAS, statistical methods using summary statistics become more and more important because of restricted accessibility to individual-level data sets. Theoretically guaranteed methods are highly demanding to advance the statistical inference with a large amount of available marginal information. Here we propose an $\ell_1$ penalized approach, REMI, to estimate high dimensional regression coefficients with marginal information and external reference samples. We establish an upper bound on the error of the REMI estimator, which has the same order as that of the minimax error bound of Lasso with complete individual-level data. In particular, when marginal information is obtained from a large number of samples together with a small number of reference samples, REMI yields good estimation and prediction results, and outperforms the Lasso because the sample size of accessible individual-level data can be limited. Through simulation studies and real data analysis of the NFBC1966 GWAS data set, we demonstrate that REMI can be widely applicable. The developed R package and the codes to reproduce all the results are available at this https URL",2018,arXiv: Applications
Ott_a_235951 1087..1098,"Department of Neurosurgery, Tongji Hospital, Tongji Medical College, Huazhong University of Science and Technology, Wuhan, Hubei, Peopleâ€™s Republic of China Background: Long non-coding RNAs (lncRNAs) have been verified to have a vital role in the progression of glioblastoma multiforme (GBM). Our research was about to identify the potential lncRNAs which was closely associated with the pathogenesis and prognosis of glioblastoma multiforme. Methods: All RNA sequence profiling data from patients with GBM were obtained from The Genotype-Tissue Expression (GTEx) and The Cancer Genome Atlas (TCGA). Differently expressed genes identified from GBM and control samples were used to construct competing endogenous RNA (ceRNA) network and perform corresponding functional enrichment analysis. Univariate Cox regression followed by lasso regression and multivariate Cox was used to validate independent lncRNA factors and construct a risk prediction model. Quantitative polymerase chain reaction (qPCR) was performed to verify the expression levels of potential lncRNA biomarkers in human GBM clinical specimens. A gene set enrichment analysis (GSEA) was subsequently conducted to explore potential signaling pathways in which critical lncRNAs may be involved. Moreover, nomogram plot was applied based on our prediction model and significant clinical covariates to visualize the prognosis of GBM patients. Results: A total of 2023 differentially expressed genes (DEGs) including 56 lncRNAs, 1587 message RNAs (mRNAs) and 380 other RNAs were included. Based on predictive databases, 16lncRNAs, 32 microRNAs (miRNAs) and 99 mRNAs were used to construct a ceRNA network. Moreover, we performed a novel risk prediction model with 5 potential prognostic lncRNAs, in which 4 of them were newly identified in GBM, to predict the prognosis of GBM patients. Finally, a nomogram plot was constructed to illustrate the potential relationship between the prognosis of GBM and our risk prediction model and significant clinical covariates. Conclusion: In this study, we identified 4 novel potential lncRNA biomarkers and constructed a prediction model of GBM prognosis. A simple-to-use nomogram was provided for further clinical application.",2020,
Real-Time Predictive Modeling of Key Quality Characteristics Using Regularized Regression : SAS Â® Procedures GLMSELECT and LASSO,"This paper describes the merging of designed experiments and regularized regression to find the significant factors to use for a real-time predictive model. The real-time predictive model is used in a Weyerhaeuser modified fiber mill to estimate the value of several key quality characteristics. The modified fiber product is accepted or rejected based on the model prediction or the predicted values deviation from lab tests. To develop the model, a designed experiment was needed at the mill. The experiment was planned by a team of engineers, managers, operators, lab techs, and a statistician. The data analysis used the actual values of the process variables manipulated in the designed experiment. There were instances of the set point not being achieved or maintained for the duration of the run. The lab tests of the key quality characteristics were used as the responses. The experiment was designed with JMPÂ® 64-bit Edition 11.2.0 and estimated the two-way interactions of the process variables. It was thought that one of the process variables was curvilinear and this effect was also made estimable. The LASSO method, as implemented in the SASÂ® GLMSELECT procedure, was used to analyze the data after cleaning and validating the results. Cross validation was used as the variable selection operator. The resulting prediction model passed all the predetermined success criteria. The prediction model is currently being used real time in the mill for product quality acceptance.",2016,
Association Between Radiomics Signature and Disease-Free Survival in Conventional Papillary Thyroid Carcinoma,"Patients with papillary thyroid carcinoma (PTC) would benefit from risk stratification tools that can aid in planning personalized treatment and follow-up. The aim of this study was to develop a conventional ultrasound (US)-based radiomics signature to estimate disease-free survival (DFS) in patients with conventional PTC. Imaging features were extracted from the pretreatment US images of 768 patients with conventional PTC who were treated between January 2004 and February 2006. The median follow-up period was 117.3 months, with 85 (11.1%) events. A radiomics signature (Rad-score) was generated by using the least absolute shrinkage and selection operator (LASSO) method in Cox regression. The Rad-score was significantly associated with DFS (hazard ratio [HR], 3.087; Pâ€‰<â€‰0.001), independent of clinicopathologic risk factors. A radiomics model which incorporated the Rad-score demonstrated better performance in the estimation of DFS (C-index: 0.777; 95% confidence interval [CI]: 0.735, 0.829) than the clinicopathologic model (C-index: 0.721; 95% CI: 0.675, 0.780). In conclusion, radiomics features from pretreatment US may be potential imaging biomarkers for risk stratification in patients with conventional PTC.",2018,Scientific Reports
Automatic detection of frustration of novice programmers from contextual and keystroke logs,"Novice programmers exhibit a repertoire of affective states over time when they are learning computer programming. The modeling of frustration is important as it informs on the need for pedagogical intervention of the student who may otherwise lose confidence and interest in the learning. In this paper, contextual and keystroke features of the students within a Java tutoring system are used to detect frustration of student within a programming exercise session. As compared to psychological sensors used in other studies, the use of contextual and keystroke logs are less obtrusive and the equipment used (keyboard) is ubiquitous in most learning environment. The technique of logistic regression with lasso regularization is utilized for the modeling to prevent over-fitting. The results showed that a model that uses only contextual and keystroke features achieved a prediction accuracy level of 0.67 and a recall measure of 0.833. Thus, we conclude that it is possible to detect frustration of a student from distilling both the contextual and keystroke logs within the tutoring system with an adequate level of accuracy.",2015,2015 10th International Conference on Computer Science & Education (ICCSE)
Lasso tuning through the flexible-weighted bootstrap,"Regularized regression approaches such as the Lasso have been widely adopted for constructing sparse linear models in high-dimensional datasets. A complexity in fitting these models is the tuning of the parameters which control the level of introduced sparsity through penalization. The most common approach to select the penalty parameter is through $k$-fold cross-validation. While cross-validation is used to minimise the empirical prediction error, approaches such as the $m$-out-of-$n$ paired bootstrap which use smaller training datasets provide consistency in selecting the non-zero coefficients in the oracle model, performing well in an asymptotic setting but having limitations when $n$ is small. In fact, for models such as the Lasso there is a monotonic relationship between the size of training sets and the penalty parameter. We propose a generalization of these methods for selecting the regularization parameter based on a flexible-weighted bootstrap procedure that mimics the $m$-out-of-$n$ bootstrap and overcomes its challenges for all sample sizes. Through simulation studies we demonstrate that when selecting a penalty parameter, the choice of weights in the bootstrap procedure can be used to dictate the size of the penalty parameter and hence the sparsity of the fitted model. We empirically illustrate our weighted bootstrap procedure by applying the Lasso to integrate clinical and microRNA data in the modeling of Alzheimer's disease. In both the real and simulated data we find a narrow part of the parameter space to perform well, emulating an $m$-out-of-$n$ bootstrap, and that our procedure can be used to improve interpretation of other optimization heuristics.",2019,arXiv: Methodology
On the sparse Bayesian learning of linear models,"ABSTRACT This work is a re-examination of the sparse Bayesian learning (SBL) of linear regression models of Tipping (2001) in a high-dimensional setting with a sparse signal. We show that in general the SBL estimator does not recover the sparsity structure of the signal. To remedy this, we propose a hard-thresholded version of the SBL estimator that achieves, for orthogonal design matrices, the non asymptotic estimation error rate of , where n is the sample size, p is the number of regressors, Ïƒ is the regression model standard deviation, and s is the number of non zero regression coefficients. We also establish that with high probability the estimator recovers the sparsity structure of the signal. In our simulations we found that the performance of thresholded SBL depends on the strength of the signal. With a weak signal thresholded SBL performs poorly compared to least absolute shrinkage and selection operator (lasso) (Tibshirani, 1996), but outperforms lasso when the signal is strong.",2015,Communications in Statistics - Theory and Methods
Channel selection for simultaneous myoelectric prosthesis control,"To develop a clinically available prosthesis based on electromyography (EMG) signals, the number of recording electrodes should be as small as possible. In this study, we investigate the possibility of the least absolute shrinkage and selection operator (LASSO) for finding electrode subsets suitable for regression based myoelectric prosthesis control. EMG signals were recorded using 192 electrodes while ten subjects were performing two degree-of-freedom (DoF) wrist movements. Among the whole channels, we selected subsets consisting of 96, 64, 48, 32, 24, 16, 12, and 8 electrodes, respectively, using the LASSO method. As a baseline method, electrode subsets having the same numbers of electrodes were arbitrary selected with regular spacing (uniform selection method). The performance of decoding the movements was estimated using the r-square value. The electrode subsets selected by the LASSO method generally outperformed those chosen by the arbitrary selection method. In particular, the performance of the LASSO method was significantly higher than that of the arbitrary selection method when using the subsets of 8 electrodes. From the analysis results, we could confirm that the LASSO method can be used to select reasonable electrode subsets for regression based myoelectric prosthesis control.",2014,2014 International Winter Workshop on Brain-Computer Interface (BCI)
Sexual Dysfunction and Mood Stabilizers in Long-Term Stable Patients With Bipolar Disorder.,"BACKGROUND
In addition to factors intrinsic to bipolar disorder (BD), sexual functioning (SF) can be affected by extrinsic causes, such as psychotropic drugs. However, the effect of mood stabilizers on SF and quality of life (QoL) is an underexplored research area.


AIM
To analyze SF in BD outpatients in euthymia for at least 6Â months treated only with mood stabilizers and the association between SF and QoL.


METHODS
A multicenter cross-sectional study was conducted in 114 BD outpatients treated with (i) lithium alone (L group); (ii) anticonvulsants alone (valproate or lamotrigine; A group); (iii) lithium plus anticonvulsants (L+A group); or (iv) lithium plus benzodiazepines (L+B group). The Changes in Sexual Functioning Questionnaire Short Form (CSFQ-14) was used. Statistical analyses were performed to compare CSFQ-14 scores among the pharmacological groups. An adaptive lasso was used to identify potential confounding variables, and linear regression models were used to study the association of the CSFQ-14 with QoL.


MAIN OUTCOME MEASURES
Self-reports on phases of the sexual response cycle (ie, desire, arousal, and orgasm) and QoL were assessed.


RESULTS
The A group had better total SF scores than the L group and the L+B group. Relative to the A group, the L and L+B groups had worse sexual desire; the L group had worse sexual arousal; and the L+A group and the L+B group had worse sexual orgasm. Regarding sociodemographic factors, being female and older age were associated with worse total SF and all subscale scores. Among all subscales scores, higher sexual arousal scores were associated with better QoL.


CLINICAL IMPLICATIONS
Potential modified extrinsic factors such as psychotropic medication that can affect SF can be addressed and adjusted to lessen side effects on SF.


STRENGTHS & LIMITATIONS
Sample of patients with euthymic BD in treatment with mood stabilizers and no antipsychotics or antidepressants, substance use as an exclusion criterion, and use of a validated, gender-specific scale to evaluate SF. Major limitations were cross-sectional design, sample size, and lack of information about stability of relationship with partner.


CONCLUSIONS
Lithium in monotherapy or in combination with benzodiazepines is related to worse total SF and worse sexual desire than anticonvulsants in monotherapy. While the addition of benzodiazepines or anticonvulsants to lithium negatively affects sexual orgasm, sexual arousal (which plays a significant role in QoL) improves when benzodiazepines are added to lithium. Anticonvulsants in monotherapy have the least negative effects on SF in patients with BD. GarcÃ­a-Blanco A, GarcÃ­a-Portilla MP, Fuente-TomÃ¡s L de la, etÂ al. Sexual Dysfunction and Mood Stabilizers in Long-Term Stable Patients With Bipolar Disorder. J Sex Med 2020;XX:XXX-XXX.",2020,The journal of sexual medicine
Lipidomic analysis of plasma samples from women with polycystic ovary syndrome,"Abstract
Polycystic ovary syndrome (PCOS) is a common disorder affecting between 5 and 18Â % of females of reproductive age and can be diagnosed based on a combination of clinical, ultrasound and biochemical features, none of which on its own is diagnostic. A lipidomic approach using liquid chromatography coupled with accurate mass high-resolution mass-spectrometry (LC-HRMS) was used to investigate if there were any differences in plasma lipidomic profiles in women with PCOS compared with control women at different stages of menstrual cycle. Plasma samples from 40 women with PCOS and 40 controls aged between 18 and 40Â years were analysed in combination with multivariate statistical analyses. Multivariate data analysis (LASSO regression and OPLS-DA) of the sample lipidomics datasets showed a weak prediction model for PCOS versus control samples from the follicular and mid-cycle phases of the menstrual cycle, but a stronger model (specificity 85Â % and sensitivity 95Â %) for PCOS versus the luteal phase menstrual cycle controls. The PCOS vs luteal phase model showed increased levels of plasma triglycerides and sphingomyelins and decreased levels of lysophosphatidylcholines and phosphatidylethanolamines in PCOS women compared with controls. Lipid biomarkers of PCOS were tentatively identified which may be useful in distinguishing PCOS from controls especially when performed during the menstrual cycle luteal phase.",2014,Metabolomics
Robust Sparse Regression under Adversarial Corruption,"We consider high dimensional sparse regression with arbitrary - possibly, severe or coordinated - errors in the covariates matrix. We are interested in understanding how many corruptions we can tolerate, while identifying the correct support. To the best of our knowledge, neither standard outlier rejection techniques, nor recently developed robust regression algorithms (that focus only on corrupted response variables), nor recent algorithms for dealing with stochastic noise or erasures, can provide guarantees on support recovery. As we show, neither can the natural brute force algorithm that takes exponential time to find the subset of data and support columns, that yields the smallest regression error. 
 
We explore the power of a simple idea: replace the essential linear algebraic calculation - the inner product - with a robust counterpart that cannot be greatly affected by a controlled number of arbitrarily corrupted points: the trimmed inner product. We consider three popular algorithms in the uncorrupted setting: Thresholding Regression, Lasso, and the Dantzig selector, and show that the counterparts obtained using the trimmed inner product are provably robust.",2013,
Special issue on Model Selection and High Dimensional Data Reduction,"Nowadays we are often confronted with data sets containing many variables; in some cases the number of variables exceeds the sample size. Indeed, many modern scientific investigations require the analysis of high dimensional data. Examples include genomic and proteomic data, spatial-temporal data, network data, and many others. Modeling such data and in particular high-dimensional data poses many challenges, often involving complex data structures. Specifically, a range of different models with varying complexity can be considered and a model that is optimal in some sense needs to be selected from a set of candidate models. Simultaneous variable selection and parameter estimation play a central role in such investigations. There is now an immense literature on variable selection, and penalized regression methods are becoming increasingly popular. Many new developments have been published in recent years by leading statistical journals. The application of regression models for high-dimensional data analysis is a challenging task. Regularization techniques have attracted much attention in the literature. Penalized regression is a technique for mitigating difficulties arising from collinearity and high-dimensionality. This approach necessarily incurs an estimation bias, while reducing the variance of the estimator. A tuning parameter is needed to adjust the effect of the penalization so that a desirable balance between model parsimony and goodness-of-fit can be achieved. Different forms of penalty functions have been studied intensively over the last two decades. Examples include the LASSO and its many variants (such as adaptive LASSO, group LASSO, relaxed LASSO, and so on), the SCAD, the Dantzig selector, and the elastic net, to name just a few. More recently, some of these penalization/regularization techniques have been extended to deal with the estimation of large covariance matrices, and the analysis of complex dependence structures such as networks and graphs. This special issue focuses on computationally efficient strategies, methodology and applications concerning the analysis of complex, high dimensional data set with a focus on model selection and data reduction. The papers contained in this issue deal with submodel selection and parameter estimation for a host of statistical models. Most of the papers are applied in nature and have a computational statistics or data analytic component. We anticipate that the papers published in this special issue will represent a positive contribution to the development of new ideas in the high-dimensional data analysis, and will provide interesting applications. A brief description of the contents of each of the nine papers in this special â€¦",2014,Comput. Stat. Data Anal.
A Mathematical Framework for Feature Selection from Real-World Data with Non-Linear Observations,"In this paper, we study the challenge of feature selection based on a relatively small collection of sample pairs $\{(x_i, y_i)\}_{1 \leq i \leq m}$. The observations $y_i \in \mathbb{R}$ are thereby supposed to follow a noisy single-index model, depending on a certain set of signal variables. A major difficulty is that these variables usually cannot be observed directly, but rather arise as hidden factors in the actual data vectors $x_i \in \mathbb{R}^d$ (feature variables). We will prove that a successful variable selection is still possible in this setup, even when the applied estimator does not have any knowledge of the underlying model parameters and only takes the 'raw' samples $\{(x_i, y_i)\}_{1 \leq i \leq m}$ as input. The model assumptions of our results will be fairly general, allowing for non-linear observations, arbitrary convex signal structures as well as strictly convex loss functions. This is particularly appealing for practical purposes, since in many applications, already standard methods, e.g., the Lasso or logistic regression, yield surprisingly good outcomes. Apart from a general discussion of the practical scope of our theoretical findings, we will also derive a rigorous guarantee for a specific real-world problem, namely sparse feature extraction from (proteomics-based) mass spectrometry data.",2016,ArXiv
An integrative DNA methylation model for improved prognostication of postsurgery recurrence and therapy in prostate cancer patients.,"PURPOSE
Patients with clinically localized, high-risk prostate cancer are often treated with surgery, but exhibit variable prognosis requiring long-term monitoring. An ongoing challenge for such patients is developing optimal strategies and biomarkers capable of differentiating between men at risk of early recurrence (<3 years) that will benefit from adjuvant therapies and men at risk of late recurrence (>5 years) who will benefit from long-term monitoring and/or salvage therapies.


PATIENTS AND METHODS
DNA methylation changes for 12 genes associated with disease progression were analyzed in 453 prostate tumors. A 4-gene prognostic model (4-G model) for biochemical recurrence (BCR) was derived utilizing LASSO from Cohort 1 (nâ€¯=â€¯254) and validated in Cohort 2 (nâ€¯=â€¯199). Subsequently, the 4-G model was evaluated for its association with salvage radiotherapy (RT) and/or hormone therapy, and the additive potential to CAPRA-S to develop an integrative gene model was assessed.


RESULTS
The 4-G model was significantly associated with BCR in both cohorts (chi-squared analysis Pâ‰¤ 0.004) and specifically, with late recurrence at 5+ years (P < 0.001, Cohort 1; P= 0.028, Cohort 2). Multivariable Cox proportional regression analysis identified the 4-G model as significantly associated with salvage RT or hormone therapy in Cohort 1 (hazard ratio (HR) 1.64, 95% confidence interval (CI) 1.29-2.10, P< 0.001) and further validated in Cohort 2 (HR 1.63, 95% CI 1.18-2.25, P< 0.001). The integrative model outperformed prostate-specific antigen and the 4-G model alone for predicting BCR and was associated with patients who received hormone therapy 3+ years postsurgery.


CONCLUSIONS
We have identified and validated a novel integrative gene model as an independent prognosticator of BCR and demonstrated its association with late BCR. These patients require more long-term postsurgical monitoring and could be spared the comorbidities of adjuvant therapies.",2019,Urologic oncology
A short review of machine learning methods for classifying the outcome of Gestational Diabetes,"Diabetes mellitus is a growing problem, especially in developing countries. People suffering from diabetes have an increased risk of developing a number of serious health problems. Consistently high blood glucose levels can lead to serious diseases affecting the heart and blood vessels, eyes, kidney, etc. In addition, people with diabetes also have a higher risk of developing infections. This paper aims to use suitable data mining and classification techniques which include the Logit model, the Probit model, the Classification tree technique, Artificial Neural Networks, Support Vector Machines, Ridge Regression technique and the Least Absolute Shrinkage and Selection Operator(LASSO) in order to determine the best method which can be used to classify the patients as suffering from gestational diabetes or not. The misclassification rate is calculated for different methods and the method having the least misclassification rate is said to be the most suitable to be applied to the given data, which is the PIMA Indians diabetes dataset.",2019,bioRxiv
Differentiating minimally invasive and invasive adenocarcinomas in patients with solitary sub-solid pulmonary nodules with a radiomics nomogram.,"AIM
To evaluate the preoperative differentiation between the minimally invasive adenocarcinoma (MIA) and invasive adenocarcinoma (IAC) in patients with sub-solid pulmonary nodules using a radiomics nomogram.


MATERIALS AND METHODS
A total of 100 patients with sub-solid pulmonary nodules who had pathologically confirmed MIA (43 patients, 13 male and 30 female) or IAC (57 patients, 26 male and 31 female) were recruited retrospectively. Radiomics features were extracted from computed tomography (CT) images. A radiomics signature was constructed by the least absolute shrinkage and selection operator (LASSO) algorithm. Solid presence, lesion size, shape regularity, and margins of pulmonary nodules were assessed to construct a subjective finding model. An integrated model of radiomics signatures and CT-based subjective findings, which was presented as a radiomics nomogram, was developed based on a multivariate logistic regression. The nomogram performance was assessed by its calibration, discrimination, and clinical usefulness.


RESULTS
The radiomics signature, which consisted of 11 radiomics features, showed good discrimination accuracy. The radiomics nomogram showed good calibration and discrimination in the training set (AUC [area under the curve] 0.943; 95% confidence interval [CI]: 0.895-0.991) and validation set (AUC 0.912; 95% CI: 0.780-1.000). The radiomics nomogram was determined to be clinically useful in the decision curve analysis (DCA).


CONCLUSION
The proposed radiomics nomogram has the potential to preoperatively differentiate MIA and IAC in patients with sub-solid pulmonary nodules.",2019,Clinical radiology
"Anaemia during pregnancy in Burkina Faso, west Africa, 1995-96: prevalence and associated factors. DITRAME Study Group.","We report the results of a cross-sectional study carried out in 1995-96 on anaemia in pregnant women who were attending two antenatal clinics in Bobo-Dioulasso, Burkina Faso, as part of a research programme including a clinical trial of zidovudine (ZDV) in pregnancy (ANRS 049 Clinical Trial). For women infected with human immunodeficiency virus (HIV) in Africa, anaemia is of particular concern when considering the use of ZDV to decrease mother-to-child transmission of HIV. The objectives were to determine the prevalence of and risk factors for maternal anaemia in the study population, and the effect of HIV infection on the severity of maternal anaemia. HIV counselling and testing were offered to all women, and haemograms were determined for those women who consented to serological testing. Haemoglobin (Hb) levels were available for 2308 of the 2667 women who accepted HIV testing. The prevalence of HIV infection was 9.7% (95% confidence interval (CI): 8.6-10.8%). The overall prevalence of anaemia during pregnancy (Hb level < 11 g/dl) was 66% (95% CI: 64-68%). The prevalence of mild (10 g/dl < or = Hb < 11 g/dl), moderate (7 g/dl < or = Hb < 10 g/dl) and severe (Hb < 7 g/dl) anaemia was 30.8%, 33.5% and 1.7%, respectively. The prevalence of anaemia was 78.4% in HIV-infected women versus 64.7% in HIV-seronegative women (P < 0.001). Although the relative risk of HIV-seropositivity increased with the severity of anaemia, no significant association was found between degree of anaemia and HIV serostatus among the study women with anaemia. Logistic regression analysis showed that anaemia was significantly and independently related to HIV infection, advanced gestational age, and low socioeconomic status. This study confirms the high prevalence of anaemia during pregnancy in Burkina Faso. Antenatal care in this population must include iron supplementation. Although HIV-infected women had a higher prevalence of anaemia, severe anaemia was infrequent, possibly because few women were in the advanced stage of HIV disease. A short course regimen of ZDV should be well tolerated in this population.",1999,Bulletin of the World Health Organization
Efficient parameter identification and model selection in nonlinear dynamical systems via sparse Bayesian learning,"Bayesian inference plays a central role in many of today's system identification algorithms. However, one of its major drawbacks is that it often requires solutions to analytically intractable integrals, and this has led to solutions via Markov Chain Monte Carlo (MCMC). Today, few Bayesian system identification methods can compete with the robustness of the family of MCMC solutions. However, MCMC suffers severely from its computational cost. 
 
 
 
Partly fuelled by the field of Compressive Sensing (CS), an interest in the machine learning community has arisen in sparse linear regression. Its value in identification of dynamical systems has also recently started to receive some attention. The idea is to represent the range of possible candidate functional forms that have generated a specific data set using a dictionary, and to then apply standard Lasso regression to select the ""best"" basis, with sparsity constraints. A major problem in this approach is that Lasso regression (and its derivatives) requires tuning of the regularisation parameter. In this paper, a procedure for selecting the ""best basis"" (and thus performing both model selection and system identification) is presented through a sparse Bayesian learning approach. Sparsity is induced via a hierarchical Gaussian prior and an approximation to the posterior distribution is sought using an iterative optimisation scheme for finding the optimal hyper-priors that govern prior and hence, the level of sparsity in the solution. The method is applied to five systems of engineering interest, which include a baseline linear system, an additive quadratic damping term, cubic stiffness (Duffing oscillator), Coulomb damping and a Bouc-Wen hysteresis model. The results are shown using numerical simulations. It is shown that this approach can identify not only the correct model parameters, but whether a nonlinearity is present in the system as well its type. With the formulation being Bayesian, it also yields estimates of uncertainty over the selected basis functions and predicted responses.",2019,
Penalized MM regression estimation with LÎ³ penalty: a robust version of bridge regression,"ABSTRACTThe bridge regression estimator generalizes both ridge regression and least absolute shrinkage and selection operator (LASSO) estimators. Since it minimizes the sum of squared residuals with a LÎ³ penalty, this estimator is typically not robust against outliers in the data. There have been attempts to define robust versions of the bridge regression method, but while these proposed methods produce bridge regression estimators robust to vertical outliers and heavy-tailed errors, they are not robust against leverage points. We propose a robust bridge regression estimation method combining MM and bridge regression estimation methods. The MM bridge regression estimator obtained from the proposed method is robust against vertical outliers and leverage points. Furthermore, for appropriate choices of the penalty function, the proposed method is able to perform variable selection and parameter estimation simultaneously. Consistency, asymptotic normality, and sparsity of the MM bridge regression estimator ar...",2016,Statistics
Detection of seepages in flood embankments using the ElasticNET method,"The presented article discusses the proposition of using an algorithm based on the ElasticNET method to obtain accurate and reproducible results of reconstruction of tomographic images. In particular, the research concerned solving of the inverse problem in electrical tomography with reference to levees and dams. To enable the reconstruction of high resolution images using the impedance tomography, the ElasticNET algorithm was used, which is a combination of two methods: dorsal regression and LASSO. The results of the research have shown that thanks to the ElasticNET method you can obtain high resolution images that are faithful representation of the cross-section of the dam. Streszczenie. W zaprezentowanym artykule omÃ³wiono propozycjÄ™ uÅ¼ycia algorytmu opartego na metodzie Elastic net w celu uzyskania dokÅ‚adych i powtarzalnych wynikÃ³w rekonstrukcji obrazÃ³w tomograficznych. W szczegÃ³lnoÅ›ci badania dotyczyÅ‚y rozwiÄ…zywania problemu odwrotnego w tomografii elektrycznej w odniesieniu do waÅ‚Ã³w przeciwpowodziowych i zapÃ³r. Aby umoÅ¼liwiÄ‡ rekonstrukcje obrazÃ³w o wysokiej rozdzielczoÅ›ci stosujÄ…c tomografiÄ™ impedancyjnÄ… zastosowano algorytm ElasticNET, ktÃ³ry jest poÅ‚Ä…czeniem dwÃ³ch metod: regresji grzbietowej i LASSO. Rezultaty badaÅ„ wykazaÅ‚y, Å¼e dziÄ™ki metodzie ElasticNET moÅ¼na uzyskaÄ‡ obrazy o wysokiej rozdzielnoczÅ›ci, bÄ™dÄ…ce wiernym odwzorowaniem przekroju zapory wodnej. (Wykrywanie przeciekÃ³w w waÅ‚ach przeciwpowodziowych przy uÅ¼yciu metody ElasticNET).",2019,PrzeglÄ…d Elektrotechniczny
Covariance matrix estimation and variable selection in high dimension,"Author(s): Cai, Mu | Advisor(s): Bickel, Peter J | Abstract: First part of the thesis focuses on sparse covariance matrices estimation under the scenario of large dimension p and small sample size n. In particular, we consider a class of covariance matrices which are approximately block diagonal under unknown permutations. We propose a block recovery estimator and show it achieves minimax optimal convergence rate for the class, which is the same as if the permutation were known. The problem is also related to sparse PCA and k-densest subgraphs, where the spike model is a special case of their intersection. Simulations of the spike model and multiple block model, together with a real world application, confirm that the proposed estimator is both statistically and computationally efficient.Second part of the thesis focuses on variable selection in linear regression, also under the high dimensional scenario of large p and small n. We propose a general framework to search variables based on their covariance structures, with a specific variable selection algorithm called kForward which iteratively fits local/small linear models among relatively highly correlated variables. For simulation experiments and a real world data set, we compare kForward to other popular methods including the Lasso, Elastic Net, SCAD, MC+, FoBa for both variable selection and prediction.",2013,
Least Absolute Shrinkage is Equivalent to Quadratic Penalization,"Adaptive ridge is a special form of ridge regression, balancing the quadratic penalization on each parameter of the model. This paper shows the equivalence between adaptive ridge and lasso (least absolute shrinkage and selection operator). This equivalence states that both procedures produce the same estimate. Least absolute shrinkage can thus be viewed as a particular quadratic penalization.",1998,
Manifold elastic net: a unified framework for sparse dimension reduction,"It is difficult to find the optimal sparse solution of a manifold learning based dimensionality reduction algorithm. The lasso or the elastic net penalized manifold learning based dimensionality reduction is not directly a lasso penalized least square problem and thus the least angle regression (LARS) (Efron etÂ al., Ann Stat 32(2):407â€“499, 2004), one of the most popular algorithms in sparse learning, cannot be applied. Therefore, most current approaches take indirect ways or have strict settings, which can be inconvenient for applications. In this paper, we proposed the manifold elastic net or MEN for short. MEN incorporates the merits of both the manifold learning based dimensionality reduction and the sparse learning based dimensionality reduction. By using a series of equivalent transformations, we show MEN is equivalent to the lasso penalized least square problem and thus LARS is adopted to obtain the optimal sparse solution of MEN. In particular, MEN has the following advantages for subsequent classification: (1) the local geometry of samples is well preserved for low dimensional data representation, (2) both the margin maximization and the classification error minimization are considered for sparse projection calculation, (3) the projection matrix of MEN improves the parsimony in computation, (4) the elastic net penalty reduces the over-fitting problem, and (5) the projection matrix of MEN can be interpreted psychologically and physiologically. Experimental evidence on face recognition over various popular datasets suggests that MEN is superior to top level dimensionality reduction algorithms.",2010,Data Mining and Knowledge Discovery
