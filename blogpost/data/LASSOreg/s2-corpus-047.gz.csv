title,abstract,year,journal
Approximate Steepest Coordinate Descent,"We propose a new selection rule for the coordinate selection in coordinate descent methods for huge-scale optimization. The efficiency of this novel scheme is provably better than the efficiency of uniformly random selection, and can reach the efficiency of steepest coordinate descent (SCD), enabling an acceleration of a factor of up to $n$, the number of coordinates. In many practical applications, our scheme can be implemented at no extra cost and computational efficiency very close to the faster uniform selection. Numerical experiments with Lasso and Ridge regression show promising improvements, in line with our theoretical guarantees.",2017,ArXiv
Covariance Estimation: The GLM and Regularization Perspectives,"Finding an unconstrained and statistically interpretable reparameterization of a covariance matrix is still an open problem in statistics. Its solution is of central importance in covariance estimation, particularly in the recent high-dimensional data environment where enforcing the positive-definiteness constraint could be computationally expensive. We provide a survey of the progress made in modeling covariance matrices from two relatively complementary perspectives: (1) generalized linear models (GLM) or parsimony and use of covariates in low dimensions, and (2) regularization or sparsity for high-dimensional data. An emerging, unifying and powerful trend in both perspectives is that of reducing a covariance estimation problem to that of estimating a sequence of regression problems. We point out several instances of the regression-based formulation. A notable case is in sparse estimation of a precision matrix or a Gaussian graphical model leading to the fast graphical LASSO algorithm. Some advantages and limitations of the regression-based Cholesky decomposition relative to the classical spectral (eigenvalue) and variance-correlation decompositions are highlighted. The former provides an unconstrained and statistically interpretable reparameterization, and guarantees the positive-definiteness of the estimated covariance matrix. It reduces the unintuitive task of covariance estimation to that of modeling a sequence of regressions at the cost of imposing an a priori order among the variables. Elementwise regularization of the sample covariance matrix such as banding, tapering and thresholding has desirable asymptotic properties and the sparse estimated covariance matrix is positive definite with probability tending to one for large samples and dimensions.",2011,Statistical Science
Using GEV-regression to improve accuracy of probability of default in low default portfolios,"Calculating probability of default (PD) for low default portfolios in a 
statistically sound way can be a daunting task. For groups of coun- 
terparties with no or few defaults, e.g. large corporates and nancial 
institutions, standard approaches like logistic regression fail due to the 
low number of events. In this thesis, a dierent approach is used. The 
logit function in logistic regression is replaced with the inverse of the 
Generalized Extreme Value (GEV) distribution function, called GEV- 
regression. 
The LASSO regression technique provides a way to automatically 
nd the attributes that contain the most information in the data by 
including all attributes, and adding a penalty for complexity in the 
maximization of the likelihood function for the parameters. This ap- 
proach is used to make a strong predictive model. Data was provided 
by Danske Bank consisting of two portfolios of anonymized counterpar- 
ties. PD was calculated for each observation, and the accuracy of the 
predictions using GEV-regression and logistic regression was compared. 
The purpose of this thesis is twofold. First, to use the data provided 
to create a model for probability of default, using LASSO regression. 
Second, to investigate if GEV-regression is suitable for calculating PD 
for low default portfolios. 
The analysis suggests that it is possible to make a good predictive 
model using LASSO regression. It also supports that GEV-regression 
is a viable option to logistic regression, as it had a similar accuracy. It 
does not however support that GEV-regression is better than logistic 
regression in low default portfolios. Further studies with more data is 
needed to conclude which methodology is the optimal one.",2014,
High-Dimensional Classification Models with Applications to Email Targeting,"Email communication is valuable for any modern company, since it offers an easy mean for spreading important information or advertising new products, features or offers and much more. To be able to identify which customers that would be interested in certain information would make it possible to significantly improve a company's email communication and as such avoiding that customers start ignoring messages and creating unnecessary badwill. This thesis focuses on trying to target customers by applying statistical learning methods to historical data provided by the music streaming company Spotify.An important aspect was the high-dimensionality of the data, creating certain demands on the applied methods. A binary classification model was created, where the target was whether a customer will open the email or not. Two approaches were used for trying to target the costumers, logistic regression, both with and without regularization, and random forest classifier, for their ability to handle the high-dimensionality of the data. Performance accuracy of the suggested models were then evaluated on both a training set and a test set using statistical validation methods, such as cross-validation, ROC curves and lift charts.The models were studied under both large-sample and high-dimensional scenarios. The high-dimensional scenario represents when the number of observations, N, is of the same order as the number of features, p and the large sample scenario represents when N â‰« p. Lasso-based variable selection was performed for both these scenarios, to study the informative value of the features.This study demonstrates that it is possible to greatly improve the opening rate of emails by targeting users, even in the high dimensional scenario. The results show that increasing the amount of training data over a thousand fold will only improve the performance marginally. Rather efficient customer targeting can be achieved by using a few highly informative variables selected by the Lasso regularization.",2015,
Identification and Validation of Immune-Related Gene Prognostic Signature for Hepatocellular Carcinoma,"Immune-related genes (IRGs) have been identified as critical drivers of the initiation and progression of hepatocellular carcinoma (HCC). This study is aimed at constructing an IRG signature for HCC and validating its prognostic value in clinical application. The prognostic signature was developed by integrating multiple IRG expression data sets from TCGA and GEO databases. The IRGs were then combined with clinical features to validate the robustness of the prognostic signature through bioinformatics tools. A total of 1039 IRGs were identified in the 657 HCC samples. Subsequently, the IRGs were subjected to univariate Cox regression and LASSO Cox regression analyses in the training set to construct an IRG signature comprising nine immune-related gene pairs (IRGPs). Functional analyses revealed that the nine IRGPs were associated with tumor immune mechanisms, including cell proliferation, cell-mediated immunity, and tumorigenesis signal pathway. Concerning the overall survival rate, the IRGPs distinctly grouped the HCC samples into the high- and low-risk groups. Also, we found that the risk score based on nine IRGPs was related to clinical and pathologic factors and remained a valid independent prognostic signature after adjusting for tumor TNM, grade, and grade in multivariate Cox regression analyses. The prognostic value of the nine IRGPs was further validated by forest and nomogram plots, which revealed that it was superior to the tumor TNM, grade, and stage. Our findings suggest that the nine-IRGP signature can be effective in determining the disease outcomes of HCC patients.",2020,Journal of Immunology Research
Predicting tomato crop yield from weather data using statistical learning techniques,"Predicting crop harvest quantities accurately is important in managing a farming enterprise effectively, facilitating decisions regarding crop management, allocation of resources, anticipated delivery times and quantities to customers and produce pricing, to name but a few. The aim of this project is to develop statistical models for predicting harvest quantity of field-grown crops on a commercial tomato farm in South Africa using weather data. Planting and harvest data for a seven-year period were provided by the tomato farm, while daily and 10-daily weather data for the same period were obtained from a nearby weather station and from satellite data. The data sets were cleaned, and the time series data were summarised in the form of a single summary statistic for each weather variable over the growing period of each crop. Median and total harvest density (t/ha) were each modelled using multiple linear regression, the lasso, regression trees, bagged regression trees, random forests and boosted regression trees. All of the crop and weather variables turned out to be informative in predicting tomato yield, with the average of the daily average wind speed and the average of the daily maximum relative humidity readings over the cropsâ€™ growing periods emerging as the most important predictors of median and total harvest density, respectively. Random forests modelled median harvest density the most accurately with an estimated mean absolute prediction error of 0.37 t/ha, while bagged regression trees modelled total harvest density the most accurately with a mean absolute prediction error of 12.67 t/ha. The model parameter estimators of all of the modelling techniques tended to have low variances, and the sizes of the prediction errors are most likely due to factors such as the absence of important predictors (soil fertility, irrigation regimes, etc.) from the models and the summary of the weather time series over the cropsâ€™ growing periods into single values. ii Stellenbosch University https://scholar.sun.ac.za",2017,
Fishing Economic Growth Determinants Using Bayesian Elastic Nets,"We propose a method to deal simultaneously with model uncertainty and correlated regressors in linear regression models by combining elastic net specifications with a spike and slab prior. The estimation method nests ridge regression and the LASSO estimator and thus allows for a more flexible modelling framework than existing model averaging procedures. In particular, the proposed technique has clear advantages when dealing with datasets of (potentially highly) correlated regressors, a pervasive characteristic of the model averaging datasets used hitherto in the econometric literature. We apply our method to the dataset of economic growth determinants by Sala-i-Martin et al. (Sala-i-Martin, X., Doppelhofer, G., and Miller, R. I. (2004). Determinants of Long-Term Growth: A Bayesian Averaging of Classical Estimates (BACE) Approach. American Economic Review, 94: 813-835) and show that our procedure has superior out-of-sample predictive abilities as compared to the standard Bayesian model averaging methods currently used in the literature. (authors' abstract)",2011,
Predicting default of listed companies in mainland China via U-MIDAS Logit model with group lasso penalty,Abstract We introduce the group LASSO penalty into the U-MIDAS logistic regression context to develop a U-MIDAS-Logit-GL model. The U-MIDAS-Logit-GL model enables us to identify important variables at group level in high dimensional mixed frequency data analysis. We then apply it to a real-world application on studying the default of listed companies in mainland China. The U-MIDAS-Logit-GL model is able to effectively identify important determinants from high-frequency financial factors and low-frequency corporate governance profiles simultaneously. It also successfully predicts the default and outperforms the other competitive models for both in-sample and out-of-sample tests.,2020,Finance Research Letters
Inference in High-Dimensional Linear Regression Models,"We introduce an asymptotically unbiased estimator for the full high-dimensional parameter vector in linear regression models where the number of variables exceeds the number of available observations. The estimator is accompanied by a closed-form expression for the covariance matrix of the estimates that is free of tuning parameters. This enables the construction of confidence intervals that are valid uniformly over the parameter vector. Estimates are obtained by using a scaled Moore-Penrose pseudoinverse as an approximate inverse of the singular empirical covariance matrix of the regressors. The approximation induces a bias, which is then corrected for using the lasso. Regularization of the pseudoinverse is shown to yield narrower confidence intervals under a suitable choice of the regularization parameter. The methods are illustrated in Monte Carlo experiments and in an empirical example where gross domestic product is explained by a large number of macroeconomic and financial indicators.",2017,arXiv: Statistics Theory
Variance Reduced Stochastic Gradient Descent with Sufficient Decrease,"The sufficient decrease technique has been widely used in deterministic optimization, even for non-convex optimization problems, such as line-search techniques. Motivated by those successes, we propose a novel sufficient decrease framework for a class of variance reduced stochastic gradient descent (VR-SGD) methods such as SVRG and SAGA. In order to make sufficient decrease for stochastic optimization, we design a new sufficient decrease criterion. We then introduce a coefficient \theta to satisfy the sufficient decrease property, which takes the decisions to shrink, expand or move in the opposite direction (i.e., \theta x for the variable x), and give two specific update rules for Lasso and ridge regression. Moreover, we analyze the convergence properties of our algorithms for strongly convex problems, which show that both of our algorithms attain linear convergence rates. We also provide the convergence guarantees of both of our algorithms for non-strongly convex problems. Our experimental results further verify that our algorithms achieve better performance than their counterparts.",2017,ArXiv
Evaluation of Machine Learning Approaches for Android Energy Bugs Detection With Revision Commits,"Performances of smartphones are profoundly affected by battery life. Maximizing the amount of usage of energy is essential to extend battery life. However, developers might concentrate more on the functionality of applications while ignoring the energy bugs that drain the battery during the development process. There are no quantitative approaches to detect these energy bugs introduced in this fast-paced development process. In this paper, we employ a system-call-based approach to develop a power consumption model for Android devices. Data that measure the energy consumption of mobile devices under different testing scenarios with the number of triggered system calls are utilized in the model training process. A balanced recursive feature elimination with cross-validation approach is proposed to select and rank the importance of the different system calls. Seven machine learning models are trained over the selected features with cross-validation and hyper-parameter tuning technique, where linear regression with the Lasso regularization outperforms all the other models. Then, the model is evaluated on the data set that measures the energy consumption on different revision history of the selected apps. The results show that the optimized Lasso model could detect energy bugs in the revision history of various applications. Optimization strategies are provided based on the selected features.",2019,IEEE Access
The LASSO Method for Bilinear Time Series Models,"In this article we propose a method called GLLS for the fitting of bilinear time series models. The GLLS procedure is the combination of the LASSO method, the generalized cross-validation method, the least angle regression method, and the stepwise regression method. Compared with the traditional methods such as the repeated residual method and the genetic algorithm, GLLS has the advantage of shrinking the coefficients of the models and saving the computational time. The Monte Carlo simulation studies and a real data example are reported to assess the performance of the proposed GLLS method.",2016,Communications in Statistics - Simulation and Computation
Graphical modeling of binary data using the LASSO: a simulation study,"BackgroundGraphical models were identified as a promising new approach to modeling high-dimensional clinical data. They provided a probabilistic tool to display, analyze and visualize the net-like dependence structures by drawing a graph describing the conditional dependencies between the variables. Until now, the main focus of research was on building Gaussian graphical models for continuous multivariate data following a multivariate normal distribution. Satisfactory solutions for binary data were missing. We adapted the method of Meinshausen and BÃ¼hlmann to binary data and used the LASSO for logistic regression. Objective of this paper was to examine the performance of the Bolasso to the development of graphical models for high dimensional binary data. We hypothesized that the performance of Bolasso is superior to competing LASSO methods to identify graphical models.MethodsWe analyzed the Bolasso to derive graphical models in comparison with other LASSO based method. Model performance was assessed in a simulation study with random data generated via symmetric local logistic regression models and Gibbs sampling. Main outcome variables were the Structural Hamming Distance and the Youden Index.We applied the results of the simulation study to a real-life data with functioning data of patients having head and neck cancer.ResultsBootstrap aggregating as incorporated in the Bolasso algorithm greatly improved the performance in higher sample sizes. The number of bootstraps did have minimal impact on performance. Bolasso performed reasonable well with a cutpoint of 0.90 and a small penalty term. Optimal prediction for Bolasso leads to very conservative models in comparison with AIC, BIC or cross-validated optimal penalty terms.ConclusionsBootstrap aggregating may improve variable selection if the underlying selection process is not too unstable due to small sample size and if one is mainly interested in reducing the false discovery rate. We propose using the Bolasso for graphical modeling in large sample sizes.",2012,BMC Medical Research Methodology
Log-penalized linear regression,"Regularization penalties are commonly used in linear regression to reduce overfitting (l). We introduce a log regularization penalty, motivated by a minimum-description-length (MDL) perspective (2) and from ideas in algorithmic complexity (3), and com- pare it to the more commonly used penalties known as ridge regreesion and the lasso (l). I. DISCUSSION",2003,"IEEE International Symposium on Information Theory, 2003. Proceedings."
"Discussion of â€œ Boosting Algorithms : Regularization , Prediction and Model Fitting â€ by Peter BÃ¼hlmann and","We congratulate the authors (hereafter BH) for an interesting take on the boosting technology, and for developing a modular computational environment in R for exploring their models. Their use of low-degree-of-freedom smoothing splines as a base learner provides an interesting approach to adaptive additive modeling. The notion of â€œTwin Boostingâ€ is interesting as well; besides the adaptive lasso, we have seen the idea applied more directly for the lasso and Dantzig selector (James, Radchenko & Lv 2007). In this discussion we elaborate on the connections between L2-boosting of a linear model and infinitesimal forward stagewise linear regression (Section 5.2.1 in BH). We then take the authors to task on their definition of degrees of freedom (Section 5.3 of BH).",2007,
Statistical methods for predicting genetic regulation,"Transcriptional regulation of gene expression is essential for cellular differentiation and function, and defects in the process are associated with cancer. Transcription is regulated by the cis-acting regulatory regions and trans-acting regulatory elements. Transcription factors bind on enhancers and repressors and form complexes by interacting with each other to control the expression of the genes. Understanding the regulation of genes would help us to understand the biological system and can be helpful in identifying therapeutic targets for diseases such as cancer. The ENCODE project has mapped binding sites of many TFs in some important cell types and this project also has mapped DNase I hypersensitivity sites across the cell types. 
Predicting transcription factors mutual interactions would help us in finding the potential transcription regulatory networks. Here, we have developed two methods for prediction of transcription factors mutual interactions from ENCODE ChIP-seq data, and both methods generated similar results which tell us about the accuracy of the methods. It is known that functional regions of genome are conserved and here we identified that shared/overlapping transcription factor binding sites in multiple cell types and in transcription factors pairs are more conserved than their respective non-shared/non-overlapping binding sites. It has been also studied that co-binding sites influence the expression level of genes. Most of the genes mapped to the transcription factor co-binding sites have significantly higher level of expression than those genes which were mapped to the single transcription factor bound sites. The ENCODE data suggests a very large number of potential regulatory sites across the complete genome in many cell types and methods are needed to identify those that are most relevant and to connect them to the genes that they control. A penalized regression method, LASSO was used to build correlative models, and choose two regulatory regions that are predictive of gene expression, and link them to their respective gene. 
Here, we show that our identified regulatory regions accumulate significant number of somatic mutations that occur in cancer cells, suggesting that their effects may drive cancer initiation and development. Harboring of somatic mutations in these identified regulatory regions is an indication of positive selection, which has been also observed in cancer related genes.",2016,
Integrating remotely sensed data into forest resource inventories,"The past two decades have demonstrated a great potential for airborne Light Detection 
and Ranging (LiDAR) data to improve the efficiency of forest resource inventories (FRIs). 
In order to make efficient use of LiDAR data in FRIs, the data need to be related to 
observations taken in the field. Various modeling techniques are available that enable 
a data analyst to establish a link between the two data sources. While the choice for 
a modeling technique may have negligible effects on point estimates, different model 
techniques may deliver different estimates of precision. 
This study investigated the impact of various model and variable selection procedures 
on estimates of precision. The focus was on LiDAR applications in FRIs. The procedures considered included stepwise variable selection procedures such as the Akaike 
Information Criterion (AIC), the corrected Akaike Information Criterion (AICc), and 
the Bayesian (or Schwarz) Information Criterion. Variables have also been selected 
based on the condition number of the matrix of covariates (i.e., LiDAR metrics) and 
the variance inflation factor. Other modeling techniques considered in this study were 
ridge regression, the least absolute shrinkage and selection operator (Lasso), partial least 
squares regression, and the random forest algorithm. Stepwise variable selection procedures have been considered in both, the (design-based) model-assisted, as well as in 
the model-based (or model-dependent) inference framework. All other techniques were 
investigated only for the model-assisted approach. 
In a comprehensive simulation study, the effects of the different modeling techniques 
on the precision of population parameter estimates (mean aboveground biomass per 
hectare) were investigated. Five different datasets were used. Three artificial datasets 
were simulated; two further datasets were based on FRI data from Canada and Norway. 
Canonical vine copulas were employed to create synthetic populations from the FRI 
data. From all populations simple random samples of different size were repeatedly 
drawn and the mean and variance of the mean were estimated for each sample. While 
for the model-based approach only a single variance estimator was investigated, for the 
model-assisted approach three alternative estimators were examined. 
The results of the simulation studies suggest that blind application of stepwise variable 
selection procedures lead to overly optimistic estimates of precision in LiDAR-assisted 
FRIs. The effects were severe for small sample sizes (n = 40 and n = 50). For large 
samples (n = 400) overestimation of precision was negligible. Good performance in 
terms of empirical standard errors and coverage rates were obtained for ridge regression, 
Lasso, and the random forest algorithm. This study concludes that the use of the latter 
three modeling techniques may prove useful in future LiDAR-assisted FRIs.",2015,
Ensembling Variable Selectors by Stability Selection for the Cox Model,"As an effective tool to build interpretive models, variable selection plays an increasingly important role in high-dimensional data analysis. It has been proven that ensemble learning can significantly improve selection accuracy, alleviate the instability of traditional selection methods, and reduce false discovery rate (FDR). Therefore, variable selection ensembles (VSEs) have gained much interest in recent years. Stability selection [1], a VSE technique based on subsampling in combination with a base algorithm like lasso, is an effective method to control FDR and to improve selection accuracy in linear regression models. In this paper, we apply it to handle variable selection problems in a Cox model. Some simulated data with various censoring rates are used to study the influence of one parameter Amin in stability selection to its performance. In the meantime, stability selection is compared with other variable selection approaches. The experimental results demonstrate its good performance.",2017,Computational Intelligence and Neuroscience
Integrating prior biological knowledge and graphical LASSO for network inference,"Systems biology aims at unravelling the mechanisms of complex diseases by investigating how individual elements of the cell (e.g., genes, proteins, metabolites, etc.) interact with each other. Network-based methods provide an intuitive framework to model, characterize, and understand these interactions. To reconstruct a biological network, one can either query public databases for known interactions (knowledge-driven approach) or build a mathematical model to measure the associations from data (data-driven approach). In this paper, we propose a new network inference method, integrating knowledge and data-driven approaches. The method integrates prior biological knowledge (i.e., protein-protein interactions from BioGRID database) and a Gaussian graphical model (i.e., graphical LASSO algorithm) to construct robust and biologically relevant network. The network is then utilized to extract differential sub-networks between case and control groups using the result from a statistical analysis (e.g., logistic regression). We applied the proposed method on a proteomic dataset acquired by analysis of sera from hepatocellular carcinoma (HCC) cases and patients with liver cirrhosis. The differential sub-networks led to the identification of hub proteins and key pathways, whose relevance to HCC study has been confirmed by literature survey.",2015,2015 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)
Dynamic prediction of cancer-specific survival for primary hypopharyngeal squamous cell carcinoma,"This study investigated a large cohort of patients to construct a predictive nomogram and a web-based survival rate calculator for dynamically predicting the cancer-specific survival of patients with primary hypopharyngeal squamous cell carcinoma (HSCC). Patients (nâ€‰=â€‰2007) initially diagnosed with primary HSCC from 2004 to 2015 were extracted from the Surveillance, Epidemiology, and End Results (SEER) database. All patients were randomly divided into the training and validation cohorts (1:1). The Lasso Cox regression model was applied to identify independent risk factors of cancer-specific survival for a predictive nomogram and a web-based calculator. The model was evaluated by concordance index, calibration, and decision curve analysis. Cancer-specific survival rates decreased with time, while 3-year conditional survival increased. Cancer-specific deaths evolved from relatively high within the first 3 years to low thereafter. Age, race, T stage, N stage, M stage, surgery, radiotherapy, chemotherapy, and marital status were identified as independent risk factors. We constructed a predictive nomogram for survival and a web-based calculator (https://linzhongyang.shinyapps.io/Hypopharyngeal/). Additionally, a prognostic risk stratification was developed according to nomogram total points. Patients with primary HSCC were found at a high risk of cancer-specific death during the first 3 years, indicating that additional effective follow-up strategies should be implemented over the period. This is the first study to construct a predictive nomogram and a web-based calculator for all patients with HSCC.",2020,International Journal of Clinical Oncology
Model-Assisted Survey Regression Estimation With The Lasso,"In the U.S. Forest Serviceâ€™s Forest Inventory and Analysis (FIA) program, as in other natural resource surveys, many auxiliary variables are available for use in model-assisted inference about finite population parameters. Some of this auxiliary information may be extraneous, and therefore model selection is appropriate to improve the efficiency of the survey regression estimators of finite population totals. A model-assisted survey regression estimator using the lasso is presented and extended to the adaptive lasso. For a sequence of finite populations and probability sampling designs, asymptotic properties of the lasso survey regression estimator are derived, including design consistency and central limit theory for the estimator and design consistency of a variance estimator. To estimate multiple finite population quantities with the method, lasso survey regression weights are developed, using both a model calibration approach and a ridge regression approximation. The gains in efficiency of the lasso estimator over the full regression estimator are demonstrated through a simulation study estimating tree canopy cover for a region in Utah.",2017,Journal of Survey Statistics and Methodology
"Investigation of Genetic Variants, Birthweight and Hypothalamic-Pituitary-Adrenal Axis Function Suggests a Genetic Variant in the SERPINA6 Gene Is Associated with Corticosteroid Binding Globulin in the Western Australia Pregnancy Cohort (Raine) Study","BACKGROUND
The hypothalamic-pituitary-adrenal (HPA) axis regulates stress responses and HPA dysfunction has been associated with several chronic diseases. Low birthweight may be associated with HPA dysfunction in later life, yet human studies are inconclusive. The primary study aim was to identify genetic variants associated with HPA axis function. A secondary aim was to evaluate if these variants modify the association between birthweight and HPA axis function in adolescents.


METHODS
Morning fasted blood samples were collected from children of the Western Australia Pregnancy Cohort (Raine) at age 17 (n = 1077). Basal HPA axis function was assessed by total cortisol, corticosteroid binding globulin (CBG), and adrenocorticotropic hormone (ACTH). The associations between 124 tag single nucleotide polymorphisms (SNPs) within 16 HPA pathway candidate genes and each hormone were evaluated using multivariate linear regression and penalized linear regression analysis using the HyperLasso method.


RESULTS
The penalized regression analysis revealed one candidate gene SNP, rs11621961 in the CBG encoding gene (SERPINA6), significantly associated with total cortisol and CBG. No other candidate gene SNPs were significant after applying the penalty or adjusting for multiple comparisons; however, several SNPs approached significance. For example, rs907621 (p = 0.002) and rs3846326 (p = 0.003) in the mineralocorticoid receptor gene (NR3C2) were associated with ACTH and SERPINA6 SNPs rs941601 (p = 0.004) and rs11622665 (p = 0.008), were associated with CBG. To further investigate our findings for SERPINA6, rare and common SNPs in the gene were imputed from the 1,000 genomes data and 8 SNPs across the gene were significantly associated with CBG levels after adjustment for multiple comparisons. Birthweight was not associated with any HPA outcome, and none of the gene-birthweight interactions were significant after adjustment for multiple comparisons.


CONCLUSIONS
Our study suggests that genetic variation in the SERPINA6 gene may be associated with altered CBG levels during adolescence. Replication of these findings is required.",2014,PLoS ONE
"ENERGIES_9_621_CODES: MATLAB codes for computing electricity spot price forecasts from ""Automated variable selection and shrinkage for day-ahead electricity price forecasting""","These Matlab scripts and functions compute electricity spot price forecasts using the Naive model, 10 different expert autoregression models, four variable selection methods (single-step elimination, stepwise regression) and five shrinkage techniques (ridge regression, LASSO, elastic nets) used in B. Uniejewski, J. Nowotarski, R. Weron (2016) Automated variable selection and shrinkage for day-ahead electricity price forecasting, Energies 9(8), 621 (http://dx.doi.org/10.3390/en9080621). The zip file includes three scripts (main_*.m) to run all computations, three functions (forecast*.m) to perform the forecasts, two files with flags (*Flags.mat) and four data files (*.txt) with electricity day-ahead prices and holiday dummies from the GEFCom2014 competition and Nord Pool.",2018,
Assessment of multiphasic contrast-enhanced MR textures in differentiating small renal mass subtypes,"AbstractPurposeThis study seeks to evaluate the use of quantitative texture parameters extracted from multiphasic contrast-enhanced magnetic resonance (MR) imaging in differentiating between benign and malignant masses (oncocytoma vs. clear cell and papillary RCC) and between common subtypes of renal cell carcinoma (clear cell vs. papillary RCC) in small renal masses (<â€‰4Â cm).MethodOne-hundred and forty-two renal lesions (90 clear cell and 22 papillary RCCs; 30 oncocytomas) were identified in a cohort of 41 patients (18 men, 23 women: mean age, 52.8â€‰Â±â€‰14.4Â years) who underwent preoperative multiphasic contrast-enhanced MR with four phases (unenhanced, arterial, venous, and delayed) between 2015 and 2016. In this study, texture features were extracted from entire cross-sectional tumoral region in three consecutive slices containing the largest cross-sectional area from each of the four phases. The change in imaging feature between precontrast imaging and each postcontrast phase was calculated. Data dimension reduction and feature selection were performed by conducting (1) pairwise Wilcoxon rank test followed by modified false discovery rate adjustment, and (2) Lasso regression. Multivariate modeling incorporating the selected features was performed using random forest classification method.ResultsHistogram imaging features were informative variables in differentiating between benign and malignant masses, while textures imaging features were of added value in differentiating between subtypes of RCCs. Papillary RCCs were distinguished from clear cell RCCs (sensitivity 65.5%, specificity 88%, and accuracy 77.9%), oncocytomas from clear cell RCCs (sensitivity 67.3%, specificity 88.9%, and accuracy 79.3%), and oncocytomas from papillary and clear cell RCCs (sensitivity 64.7%, specificity 85.9%, and accuracy 77.9%).ConclusionsA combination of histogram and texture imaging features on multiphasic MR can help differentiate histologic cell types in common small renal masses (<â€‰4Â cm).
",2018,Abdominal Radiology
Estimation for misclassified data with ultra-high levels,"Outcome misclassification is widespread in classification problems, but methods to account for it are rarely used. In this paper, the problem of inference with misclassified multinomial logit data with a large number of multinomial parameters is addressed. We have had a significant swell of interest in the development of novel methods to infer misclassified data. One simulation study is shown regarding how seriously misclassification issue occurs if the number of categories increase. Then, using the group lasso regression, we will show how the best model should be fitted for that kind of multinomial regression problems comprehensively.",2016,
Data Assimilation in Large-scale Networks of Open Channels,"Author(s): Rafiee Jahromi, Mohammad | Advisor(s): Bayen, Alexandre M | Abstract: This dissertation is mainly focused on assimilation of data into hydrodynamic models of water flow in open channel networks which is motivated by the need for accurate flow models in various applications such as emergency response and flood monitoring systems, automated gate systems and hydrological studies. We investigate application of different data assimilation techniques in different scenarios to incorporate the available flow measurements obtained from sensors into flow models to improve their accuracy.Water flow in open channels is an instance of the so-called distributed parameters systems in which the dynamics of the system is described by a set of partial differential equations. As the flow model, the Saint-Venant equations, also known as shallow water equations, which are a set of first-order hyperbolic nonlinear partial differential equations are used. Different practical scenarios are considered. In a case in which streaming measurements of the flow are available and real-time estimation of the flow state is desired, we present how standard state estimation techniques such the Kalman filter, the Extended Kalman filter and the Unscented Kalman filter can be applied to integrate the available measurements into the shallow water equations. It is also shown how these techniques can be adapted to a case in which some of the model parameters are unknown to estimate unkown parameters along with the state of the system.For data assimilation in large-scale networks which lead to high dimensional models, application of two sequential Monte Carlo methods, the optimal sampling importance resampling and the implicit particle filters, is considered. The computational cost of propagating each particle is higher in implicit particle filters, however, they provide more accurate results with smaller number of particles by choosing the particles in a way that they belong to the high probability regions of the posterior density function. We also propose a maximum-a-posteriori-based method to perform the state estimation which is shown to perform better in terms of both accuracy and computational cost for the application of interest. For flow estimation in tidally influenced channels, an efficient estimation method which takes advantage of spectral decomposition of the state is proposed. The estimation problem is formulated as a least squares regression with an $l_1$-norm regularization, known as the LASSO, and a homotopy-based algorithm is implemented to solve the resulting optimization problem recursively as new measurements become available. Finally, we consider the problem of optimal topology design in multi-agent systems for efficient average consensus. The network design problem is posed in two different ways. (1) Assuming that the maximum communication cost, i.e. the maximum number of communication links, is known, the goal is to find the network topology which results in the fastest convergence to the consensus (in presence of communication time delays on the links). (2) If a minimum performance of the protocol is required, the design problem is posed as finding the network with lowest possible communication cost which fulfills the required performance. The design problem is formulated as an optimization problem which is finally transformed to a mixed integer semidefinite program.",2012,
Laplacian Black Box Variational Inference,"Black box variational inference (BBVI) is a recently proposed estimation method for parameters of statistical models. BBVI is an order of magnitude faster than Markov chain Monte Carlo (MCMC). The computation of BBVI is similar to maximum a posteriori estimation, but in addition to the point estimation given by the latter, BBVI also estimate another important statistic, i.e. the range of the parameters. Assuming normal distribution for the parameters, BBVI minimizes the KL divergence between the normal distribution and the statistical models. However if the parameters are not normal distributed, the estimation might not be precise. Similarly, if outliers are allowed then fat tail distributions are more suitable to model parameters. This paper discusses the problems of Cauchy and Student's t-distribution as a variational distribution. Inspired by Lasso, this paper replaces normal with Laplace distribution to implement BBVI. Experiment on linear regression shows, under non-ideal condition (e.g. with outlier data), BBSI based on Laplace provides more stable and correct estimation.",2017,
Texture Models Based on Probabilistic Graphical Models,"Texture is one of the visual features playing an important role in image analysis.Many applications have been discovered using texture models.Probabilistic graphical models Science,are promising tools for constructing texture models.The problem of learning the structure of GGM for texture classification is addressed.GGM are characterized by a neighborhood,a set of parameters,and a noise sequence due to the connection between the local Markov property and conditional regression of a Gaussian random variable.By use of the methods of model selection to choose an appropriate neighborhood and estimate the unknown parameters for modeling GGM,neighborhood selection and parameter estimation are conducted simultaneously.And then new texture features based on GGM for texture synthesis and texture classification are extracted.Experimental results show that adaptive Lasso estimators are more effective.",2011,
Sick leave and return to work after surgery for type II SLAP lesions of the shoulder: a secondary analysis of a randomised sham-controlled study,"OBJECTIVES
To compare days on sick leave and assess predictors of return to work following shoulder surgery.


DESIGN
A secondary analysis of a randomised controlled trial.


SETTING
Orthopaedic department.


PARTICIPANTS
114 patients with type II superior labral tear from anterior to posterior of the shoulder.


INTERVENTIONS
Labral repair, biceps tenodesis or sham surgery.


OUTCOME MEASURES
Sick leave was obtained from national registers for the last year before and 2â€‰years following surgery. Total and shoulder related number of days on sick leave were obtained, using international diagnostic codes. We applied the difference-in-difference approach to compare the differences in the change in mean work days on sick leave between groups over time, backwards logistic regression and lasso regression to evaluate predictors.


RESULTS
Mean total number of work days on sick leave during the 2 years after surgery was 148 (range 0-460) days. More than 80% of the sick leave days were taken by 22% of the patients. Days on sick leave classified as shoulder-related constituted 80% of the total. In all three treatment groups, the mean total number of days on sick leave doubled the year after surgery. Sham surgery and labral repair had fewer postoperative sickness absence days compared with biceps tenodesis but differences were not significant when adjusted for days of sick leave the year before surgery. Predictors of return to work at 2 years analysed by logistic regression were no sick leave (OR 8.0, 95%â€‰CI 2.4 to 26.0) and moderate symptoms of anxiety or depression (OR 0.16, 95%â€‰CI 0.05 to 0.5) at inclusion. Similar results were obtained by lasso regression but manual work was an additional predictor.


CONCLUSIONS
Change in mean work days on sick leave comparing sham surgery, labral repair and biceps tenodesis, was not significantly different. Sick leave, symptoms of anxiety and depression, and manual work at inclusion predicted work status 2 years after surgery.


TRIAL REGISTRATION NUMBER
NCT00586742.",2020,BMJ Open
Language recognition via sparse coding over learned dictionary,"In this work, we explore the use of sparse features derived using a learned dictionary for language recognition (LR). These sparse features are referred to as s-vector and are derived by sparse coding of the commonly used low-dimensional i-vector based representation of speech utterances over the learned dictionary. The orthogonal matching pursuit (OMP), least absolute shrinkage and selection operator (LASSO), and elastic net (ENet) based sparse coding algorithms have been investigated for deriving the s-vectors. Two classifiers namely cosine distance scoring (CDS) and support vector machine (SVM) have been applied on the s-vectors. Scores are calibrated using regularized multi-class logistic regression. The effectiveness of the proposed approach is empirically validated on NIST 2007 LRE data set in closed set condition on 30 seconds duration segments.",2017,2017 4th International Conference on Signal Processing and Integrated Networks (SPIN)
Hyperspectral unmixingwith sparse group lasso,"Sparse unmixing has been recently introduced as a mechanism to characterize mixed pixels in remotely sensed hyperspectral images. It assumes that the observed image signatures can be expressed in the form of linear combinations of a number of pure spectral signatures known in advance (e.g., spectra collected on the ground by a field spectroradiometer). Unmixing then amounts to finding the optimal subset of signatures in a (potentially very large) spectral library that can best model each mixed pixel in the scene. In available spectral libraries, it is observed that the spectral signatures appear organized in groups (e.g. different alterations of a single mineral in the U.S. Geological Survey spectral library). In this paper, we explore the potential of the sparse group lasso technique in solving hyperspectral unmixing problems. Our introspection in this work is that, when the spectral signatures appear in groups, this technique has the potential to yield better results than the standard sparse regression approach. Experimental results with both synthetic and real hyperspectral data are given to investigate this issue.",2011,2011 IEEE International Geoscience and Remote Sensing Symposium
Correlation between impulse oscillometry parameters and asthma control in an adult population,"Purpose: Impulse oscillometry (IOS) has been proposed as an alternative test to evaluate the obstruction of small airways and to detect changes in airways earlier than spirometry. In this study, we sought to determine the utility and association of IOS parameters with spirometry and asthma control in an adult population. Patients and methods: Adults 14-82 years of age with asthma were classified into uncontrolled asthma (n=48), partially controlled asthma (n=45), and controlled asthma (n=49) groups,Â and characterized with fractional exhaled nitric oxide (FENO), IOS, and spirometry in a transversal analysis planned as a one-visit study. The basic parameters evaluated in IOS are resistance at 5 Hz (R5), an index affected by the large and small airway; resistance at 20 Hz (R20), an index of the resistance of large airways; difference between R5 and R20 (R5-R20), indicative of the function of the small peripheral airways; reactance at 5 Hz (X5), indicative of the capacitive reactance in the small peripheral airways; resonance frequency (Fres), the intermediate frequency at which the reactance is null, and reactance area (XA), which represents the total reactance (area under the curve) at all frequencies between 5 Hz to Fres. Results: There were statistical differences between groups in standard spirometry and IOS parameters reflecting small peripheral airways (R5, R10, R5-R20, Fres, XA and X5) (P<0.001). Accuracy of IOS and/or spirometry to discriminate between controlled asthma vs partially controlled asthma and uncontrolled asthma was low (AUC=0.61). Using linear regression models, we found a good association between spirometry and IOS. In order to evaluate IOS as an alternative or supplementary method for spirometry, we designed a predictive model for spirometry from IOS applying a penalized regression model (Lasso). Then, we compared the original spirometry values with the values obtained from the predictive model using Bland-Altman plots, and the models showed an acceptable bias in the case of FEV1/FVC, FEV1%, and FVC%. Conclusion: IOS did not show a discriminative capacity to correctly classify patients according to the degree of asthma control. However, values of IOS showed good association with values of spirometry. IOS could be considered as an alternative and accurate complement to spirometry in adults. In a predictive model, spirometry values estimated from IOS tended to overestimate in low values of ""real"" spirometry and underestimate in high values.",2019,Journal of Asthma and Allergy
A nomogram for individual prediction of vascular invasion in primary breast cancer.,"OBJECTIVES
To explore the feasibility of preoperative prediction of vascular invasion (VI) in breast cancer patients using nomogram based on multiparametric MRI and pathological reports.


METHODS
We retrospectively collected 200 patients with confirmed breast cancer between January 2016 and January 2018. All patients underwent MRI examinations before the surgery. VI was identified by postoperative pathology. The 200 patients were randomly divided into training (nâ€‰=â€‰100) and validation datasets (nâ€‰=â€‰100) at a ratio of 1:1. Least absolute shrinkage and selection operator (LASSO) regression was used to select predictors most associated with VI of breast cancer. A nomogram was constructed to calculate the area under the curve (AUC) of receiver operating characteristics, sensitivity, specificity, accuracy, positive prediction value (PPV) and negative prediction value (NPV). We bootstrapped the data for 2000 times without setting the random seed to obtain corrected results.


RESULTS
VI was observed in 79 patients (39.5%). LASSO selected 10 predictors associated with VI. In the training dataset, the AUC for nomogram was 0.94 (95% confidence interval [CI]: 0.89-0.99, the sensitivity was 78.9% (95%CI: 72.4%-89.1%), the specificity was 95.3% (95%CI: 89.1%-100.0%), the accuracy was 86.0% (95%CI: 82.0%-92.0%), the PPV was 95.7% (95%CI: 90.0%-100.0%), and the NPV was 77.4% (95%CI: 67.8%-87.0%). In the validation dataset, the AUC for nomogram was 0.89 (95%CI: 0.83-0.95), the sensitivity was 70.3% (95%CI: 60.7%-79.2%), the specificity was 88.9% (95%CI: 80.0%-97.1%), the accuracy was 77.0% (95%CI: 70.0%-83.0%), the PPV was 91.8% (95%CI: 85.3%-98.0%), and the NPV was 62.7% (95%CI: 51.7%-74.0%). The nomogram calibration curve shows good agreement between the predicted probability and the actual probability.


CONCLUSION
The proposed nomogram could be used to predict VI in breast cancer patients, which was helpful for clinical decision-making.",2019,European journal of radiology
Review of Regression Analysis Models,"In statistics and data analysis, we often need to establish a relationship between the various parameters in a data set. This relationship is important for prediction and analysis. Regression Analysis is such a technique. This work mainly focuses on the different Regression Analysis models used nowadays and how they are used in context of different data sets. Picking the right model for analysis is often the most difficult task and therefore, these models are looked upon closely in this research. While a Linear Regression Analysis model is used to fit linear data, a Polynomial Regression Analysis model focuses on a data set representing polynomial relationship between data parameters. Logistic Regression model is used in a scenario where we need a binary type of prediction. When the data set becomes complex, these models may suffer from issues like Underfitting and Overfitting. Ridge and Lasso Regression are considered the best models to deal with this type of situation. Ridge regression is used when data suffers from multicollinearity, that is independent variables are highly correlated. Lasso regression differs from ridge regression in a way that it uses absolute values in the penalty function, instead of squares. Using these models in the right way and with right data set, Data Analysis and Prediction can produce the most accurate results. Keywordsâ€” Regression; Underfitting; Overfitting; Regularization",2017,International Journal of Engineering Research and
Variable Selection in High-dimensional Varying-coefficient Models with Global Optimality,"The varying-coefficient model is flexible and powerful for modeling the dynamic changes of regression coefficients. It is important to identify significant covariates associated with response variables, especially for high-dimensional settings where the number of covariates can be larger than the sample size. We consider model selection in the high-dimensional setting and adopt difference convex programming to approximate the L0 penalty, and we investigate the global optimality properties of the varying-coefficient estimator. The challenge of the variable selection problem here is that the dimension of the nonparametric form for the varying-coefficient modeling could be infinite, in addition to dealing with the high-dimensional linear covariates. We show that the proposed varying-coefficient estimator is consistent, enjoys the oracle property and achieves an optimal convergence rate for the non-zero nonparametric components for high-dimensional data. Our simulations and numerical examples indicate that the difference convex algorithm is efficient using the coordinate decent algorithm, and is able to select the true model at a higher frequency than the least absolute shrinkage and selection operator (LASSO), the adaptive LASSO and the smoothly clipped absolute deviation (SCAD) approaches.",2012,J. Mach. Learn. Res.
Group Guided Fused Laplacian Sparse Group Lasso for Modeling Alzheimer's Disease Progression,"As the largest cause of dementia, Alzheimer's disease (AD) has brought serious burdens to patients and their families, mostly in the financial, psychological, and emotional aspects. In order to assess the progression of AD and develop new treatment methods for the disease, it is essential to infer the trajectories of patients' cognitive performance over time to identify biomarkers that connect the patterns of brain atrophy and AD progression. In this article, a structured regularized regression approach termed group guided fused Laplacian sparse group Lasso (GFL-SGL) is proposed to infer disease progression by considering multiple prediction of the same cognitive scores at different time points (longitudinal analysis). The proposed GFL-SGL simultaneously exploits the interrelated structures within the MRI features and among the tasks with sparse group Lasso (SGL) norm and presents a novel group guided fused Laplacian (GFL) regularization. This combination effectively incorporates both the relatedness among multiple longitudinal time points with a general weighted (undirected) dependency graphs and useful inherent group structure in features. Furthermore, an alternating direction method of multipliers- (ADMM-) based algorithm is also derived to optimize the nonsmooth objective function of the proposed approach. Experiments on the dataset from Alzheimer's Disease Neuroimaging Initiative (ADNI) show that the proposed GFL-SGL outperformed some other state-of-the-art algorithms and effectively fused the multimodality data. The compact sets of cognition-relevant imaging biomarkers identified by our approach are consistent with the results of clinical studies.",2020,Computational and Mathematical Methods in Medicine
A note on the Lasso for Gaussian graphical model selection,"Inspired by the success of the Lasso for regression analysis, it seems attractive to estimate the graph of a multivariate normal distribution by l1-norm penalized likelihood maximization. We examine some properties of the estimator and show that care has to be taken with interpretation of results as the estimator is not consistent for some graphs.",2008,Statistics & Probability Letters
Ridge Regression and Lasso Estimators for Data Analysis,"An important problem in data science and statistical learning is to predict an outcome based on data collected on several predictor variables. This is generally known as a regression problem. In the field of big data studies, the regression model often depends on a large number of predictor variables. The data scientist is often dealing with the difficult task of determining the most appropriate set of predictor variables to be employed in the regression model. In this thesis we adopt a technique that constraints the coefficient estimates which in effect shrinks the coefficient estimates towards zero. Ridge regression and lasso are two well-known methods for shrinking the coefficients towards zero. These two methods are investigated in this thesis. Ridge regression and lasso techniques are compared by analyzing a real data set for a regression model with a large collection of predictor variables.",2019,
PAC-5 Gene Expression Signature for Predicting Prognosis of Patients with Pancreatic Adenocarcinoma,"Pancreatic adenocarcinoma (PAC) is one of the most aggressive malignancies. Intratumoural molecular heterogeneity impedes improvement of the overall survival rate. Current pathological staging system is not sufficient to accurately predict prognostic outcomes. Thus, accurate prognostic model for patient survival and treatment decision is demanded. Using differentially expressed gene analysis between normal pancreas and PAC tissues, the cancer-specific genes were identified. A prognostic gene expression model was computed by LASSO regression analysis. The PAC-5 signature (LAMA3, E2F7, IFI44, SLC12A2, and LRIG1) that had significant prognostic value in the overall dataset was established, independently of the pathological stage. We provided evidence that the PAC-5 signature further refined the selection of the PAC patients who might benefit from postoperative therapies. SLC12A2 and LRIG1 interacted with the proteins that were implicated in resistance of EGFR kinase inhibitor. DNA methylation was significantly involved in the gene regulations of the PAC-5 signature. The PAC-5 signature provides new possibilities for improving the personalised therapeutic strategies. We suggest that the PAC-5 genes might be potential drug targets for PAC.",2019,Cancers
A Novel DNA Methylation-Based Signature Can Predict the Responses of MGMT Promoter Unmethylated Glioblastomas to Temozolomide,"Glioblastoma (GBM) is the most malignant glioma, with a median overall survival (OS) of 14-16 months. Temozolomide (TMZ) is the first-line chemotherapy drug for glioma, but whether TMZ should be withheld from patients with GBMs that lack O6-methylguanine-DNA methyltransferase (MGMT) promoter methylation is still under debate. DNA methylation profiling holds great promise for further stratifying the responses of MGMT promoter unmethylated GBMs to TMZ. In this study, we studied 147 TMZ-treated MGMT promoter unmethylated GBM, whose methylation information was obtained from the HumanMethylation27 (HM-27K) BeadChips (n = 107) and the HumanMethylation450 (HM-450K) BeadChips (n = 40) for training and validation, respectively. In the training set, we performed univariate Cox regression and identified that 3,565 CpGs were significantly associated with the OS of the TMZ-treated MGMT promoter unmethylated GBMs. Functional analysis indicated that the genes corresponding to these CpGs were enriched in the biological processes or pathways of mitochondrial translation, cell cycle, and DNA repair. Based on these CpGs, we developed a 31-CpGs methylation signature utilizing the least absolute shrinkage and selection operator (LASSO) Cox regression algorithm. In both training and validation datasets, the signature identified the TMZ-sensitive GBMs in the MGMT promoter unmethylated GBMs, and only the patients in the low-risk group appear to benefit from the TMZ treatment. Furthermore, these identified TMZ-sensitive MGMT promoter unmethylated GBMs have a similar OS when compared with the MGMT promoter methylated GBMs after TMZ treatment in both two datasets. Multivariate Cox regression demonstrated the independent prognostic value of the signature in TMZ-treated MGMT promoter unmethylated GBMs. Moreover, we also noticed that the hallmark of epithelial-mesenchymal transition, ECM related biological processes and pathways were highly enriched in the MGMT unmethylated GBMs with the high-risk score, indicating that enhanced ECM activities could be involved in the TMZ-resistance of GBM. In conclusion, our findings promote our understanding of the roles of DNA methylation in MGMT umethylated GBMs and offer a very promising TMZ-sensitivity predictive signature for these GBMs that could be tested prospectively.",2019,Frontiers in Genetics
Variable selection in quantile regression when the models have autoregressive errors,"Abstract This paper considers a problem of variable selection in quantile regression with autoregressive errors. Recently, Wu and Liu (2009) investigated the oracle properties of the SCAD and adaptive-LASSO penalized quantile regressions under non identical but independent error assumption. We further relax the error assumptions so that the regression model can hold autoregressive errors, and then investigate theoretical properties for our proposed penalized quantile estimators under the relaxed assumption. Optimizing the objective function is often challenging because both quantile loss and penalty functions may be non-differentiable and/or non-concave. We adopt the concept of pseudo data by Oh etÂ al. (2007) to implement a practical algorithm for the quantile estimate. In addition, we discuss the convergence property of the proposed algorithm. The performance of the proposed method is compared with those of the majorization-minimization algorithm (Hunter and Li, 2005) and the difference convex algorithm (Wu and Liu, 2009) through numerical and real examples.",2014,Journal of The Korean Statistical Society
LASSO estimation of threshold autoregressive models,"This paper develops a novel approach for estimating a threshold autoregressive (TAR) model with multiple-regimes and establishes its large sample properties. By reframing the problem in a regression variable selection context, a least absolute shrinkage and selection operator (LASSO) procedure is proposed to estimate a TAR model with an unknown number of thresholds, where the computation can be performed efficiently. It is further shown that the number and the location of the thresholds can be consistently estimated. A near optimal convergence rate of the threshold parameters is also established. Simulation studies are conducted to assess the performance in finite samples. The results are illustrated with an application to the quarterly US real GNP data over the period 1947â€“2009.",2015,Journal of Econometrics
