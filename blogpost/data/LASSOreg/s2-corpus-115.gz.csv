title,abstract,year,journal
Feature Selection for Data Integration with Mixed Multi-view Data.,"Data integration methods that analyze multiple sources of data simultaneously can often provide more holistic insights than can separate inquiries of each data source. Motivated by the advantages of data integration in the era of ""big data"", we investigate feature selection for high-dimensional multi-view data with mixed data types (e.g. continuous, binary, count-valued). This heterogeneity of multi-view data poses numerous challenges for existing feature selection methods. However, after critically examining these issues through empirical and theoretically-guided lenses, we develop a practical solution, the Block Randomized Adaptive Iterative Lasso (B-RAIL), which combines the strengths of the randomized Lasso, adaptive weighting schemes, and stability selection. B-RAIL serves as a versatile data integration method for sparse regression and graph selection, and we demonstrate the effectiveness of B-RAIL through extensive simulations and a case study to infer the ovarian cancer gene regulatory network. In this case study, B-RAIL successfully identifies well-known biomarkers associated with ovarian cancer and hints at novel candidates for future ovarian cancer research.",2019,arXiv: Methodology
Logistic regression and Ising networks: prediction and estimation when violating lasso assumptions,"The Ising model was originally developed to model magnetisation of solids in statistical physics. As a network of binary variables with the probability of becoming â€™activeâ€™ depending only on direct neighbours, the Ising model appears appropriate for many other processes. For instance, it was recently applied in psychology to model co-occurrences of mental disorders. It has been shown that the connections between the variables (nodes) in the Ising network can be estimated with a series of logistic regressions. This naturally leads to questions of how well such a model predicts new observations and how well parameters of the Ising model can be estimated using logistic regressions. Here we focus on the high-dimensional setting with more parameters than observations and consider violations of assumptions of the lasso. In particular, we determine the consequences for both prediction and estimation when the sparsity and restricted eigenvalue assumptions are not satisfied. We explain using the idea of connected copies (extreme multicollinearity) the fact that prediction becomes better when either sparsity or multicollinearity is not satisfied. We illustrate these results with simulations.",2018,
A novel approach to estimate the Cox model with temporal covariates and its application to medical cost data,"We propose a novel approach to estimate the Cox model with temporal covariates. Our new approach treats the temporal covariates as arising from a longitudinal process which is modeled jointly with the event time. Different from the literature, the longitudinal process in our model is specified as a bounded variational process and determined by a family of Initial Value Problems associated with an Ordinary Differential Equation. Our specification has the advantage that only the observation of the temporal covariates at the time to event and the time to event itself are required to fit the model, while it is fine but not necessary to have more longitudinal observations. This fact makes our approach very useful for many medical outcome datasets, like the New York State Statewide Planning and Research Cooperative System and the National Inpatient Sample, where it is important to find the hazard rate of being discharged given the accumulative cost but only the total cost at the discharge time is available due to the protection of patient information. Our estimation procedure is based on maximizing the full information likelihood function. The resulting estimators are shown to be consistent and asymptotically normally distributed. Variable selection techniques, like Adaptive LASSO, can be easily modified and incorporated into our estimation procedure. The oracle property is verified for the resulting estimator of the regression coefficients. Simulations and a real example illustrate the practical utility of the proposed model. Finally, a couple of potential extensions of our approach are discussed.",2018,arXiv: Methodology
Estimating stellar atmospheric parameters based on Lasso features,"With the rapid development of large scale sky surveys like the Sloan Digital Sky Survey (SDSS), GAIA and LAMOST (Guoshoujing telescope), stellar spectra can be obtained on an ever-increasing scale. Therefore, it is necessary to estimate stellar atmospheric parameters such as Teff, log g and [Fe/H] automatically to achieve the scientific goals and make full use of the potential value of these observations. Feature selection plays a key role in the automatic measurement of atmospheric parameters. We propose to use the least absolute shrinkage selection operator (Lasso) algorithm to select features from stellar spectra. Feature selection can reduce redundancy in spectra, alleviate the influence of noise, improve calculation speed and enhance the robustness of the estimation system. Based on the extracted features, stellar atmospheric parameters are estimated by the support vector regression model. Three typical schemes are evaluated on spectral data from both the ELODIE library and SDSS. Experimental results show the potential performance to a certain degree. In addition, results show that our method is stable when applied to different spectra.",2014,Research in Astronomy and Astrophysics
Abstract 1955: LIN28 paralogs impact ovarian cancer predisposition and tumorigenicity via distinct molecular pathways.,"Proceedings: AACR 104th Annual Meeting 2013; Apr 6-10, 2013; Washington, DC

The RNA binding protein LIN28 and its paralog LIN28B function at a critical junction of pleuripotency, metabolism and metastasis. Reactivation of LIN28 has been recently reported in epithelial ovarian cancer (EOC), where it is hypothesized to play a key role in maintaining tumor stem cells. Associations between single nucleotide polymorphisms in the LIN28B promoter and ovarian cancer susceptibility have also been identified. However, the clinical significance of these observations and the mechanisms by which either LIN28 gene contributes to ovarian cancer have not been explored.

Using Western blot, qPCR and immunohistochemistry, we found that a) LIN28 but not LIN28B is highly expressed in epithelia lining the distal fallopian tube and b) reactivation of LIN28 (1/8 specimens) and dysregulated expression of LIN28B (4/8 specimens) occur in EOC. To assess clinical significance of these events, we interrogated the TCGA ovarian cancer database, examining correlations between LIN28, LIN28B expression and outcomes by Kaplan-Meier analysis (n=581). Our results indicate that LIN28 expression is associated with shorter disease free interval in women with optimally debulked high grade serous ovarian cancers (p<0.05). Significantly higher levels of LIN28B were also noted in young (age <50) women diagnosed with ovarian cancer. Using serial logistic regression with L1 normalization (Lasso analysis), we discovered LIN28 levels correlated most robustly with multiple genes rather than miRNAs: APOC3 (Î² =0.74), FGG (Î² =0.69), HBG1(Î²=0.67) and HEMGN (Î² =0.67). Although associations between LIN28 and miRNAs predicted by current models were not observed, robust correlations between LIN28B and multiple miRNAs were detected: let-7b (Î² =-0.47), let-7d (Î² =-0.34), let-7i (Î² =-0.31) as well as others [miR-222 (Î²=-0.29), let-7e, miR-222, miR-324-5p] not previously known to be regulated by either LIN28 gene. Lastly, we found that expression of LIN28B but not LIN28 was induced by culturing A2780 and TOV112D ovarian cancer cells in media that promotes self-assembly into spheroids. Knockdown of LIN28B expression confirms that LIN28B directly regulates the let-7 and other miRNAs identified by our analyses, promotes spheroid assembly and in vivo tumorigenicity of ovarian cancer cell lines.

Collectively, these observations indicate that LIN28 likely plays an important role in regenerating epithelia that normally line the distal fallopian tube. Although our data also suggest that reactivation of LIN28 plays a key role in promoting ovarian cancer recurrences, LIN28 does not contribute to this process by targeting the differentiation of pluripotent cells via a let-7-mediated mechanism. Rather, this role appears to be most robust for its paralog LIN28B. Future work will focus on further dissecting the unique roles of these two gene products in ovarian cancer initiation and metastasis as well as their response to treatment.

Citation Format: Claire Mach, Ying-Wooi Wan, Zhandong Liu, Matthew L. Anderson. LIN28 paralogs impact ovarian cancer predisposition and tumorigenicity via distinct molecular pathways. [abstract]. In: Proceedings of the 104th Annual Meeting of the American Association for Cancer Research; 2013 Apr 6-10; Washington, DC. Philadelphia (PA): AACR; Cancer Res 2013;73(8 Suppl):Abstract nr 1955. doi:10.1158/1538-7445.AM2013-1955",2013,Cancer Research
The Group Square-Root Lasso: Theoretical Properties and Fast Algorithms,"We introduce and study the group square-root lasso (GSRL) method for estimation in high dimensional sparse regression models with group structure. The new estimator minimizes the square root of the residual sum of squares plus a penalty term proportional to the sum of the Euclidean norms of groups of the regression parameter vector. The net advantage of the method over the existing group lasso-type procedures consists in the form of the proportionality factor used in the penalty term, which for GSRL is independent of the variance of the error terms. This is of crucial importance in models with more parameters than the sample size, when estimating the variance of the noise becomes as difficult as the original problem. We show that the GSRL estimator adapts to the unknown sparsity of the regression vector, and has the same optimal estimation and prediction accuracy as the GL estimators, under the same minimal conditions on the model. This extends the results recently established for the square-root lasso, for sparse regression without group structure. Moreover, as a new type of result for square-root lasso methods, with or without groups, we study correct pattern recovery, and show that it can be achieved under conditions similar to those needed by the lasso or group-lasso-type methods, but with a simplified tuning strategy. We implement our method via a new algorithm, with proved convergence properties, which, unlike existing methods, scales well with the dimension of the problem. Our simulation studies support strongly our theoretical findings.",2014,IEEE Transactions on Information Theory
Overlapping group screening for detection of gene-gene interactions: application to gene expression profiles with survival trait,"BackgroundThe development of a disease is a complex process that may result from joint effects of multiple genes. In this article, we propose the overlapping group screening (OGS) approach to determining active genes and gene-gene interactions incorporating prior pathway information. The OGS method is developed to overcome the challenges in genome-wide data analysis that the number of the genes and gene-gene interactions is far greater than the sample size, and the pathways generally overlap with one another. The OGS method is further proposed for patientsâ€™ survival prediction based on gene expression data.ResultsSimulation studies demonstrate that the performance of the OGS approach in identifying the true main and interaction effects is good and the survival prediction accuracy of OGS with the Lasso penalty is better than the ordinary Lasso method. In real data analysis, we identify several significant genes and/or epistasis interactions that are associated with clinical survival outcomes of diffuse large B-cell lymphoma (DLBCL) and non-small-cell lung cancer (NSCLC) by utilizing prior pathway information from the KEGG pathway and the GO biological process databases, respectively.ConclusionsThe OGS approach is useful for selecting important genes and epistasis interactions in the ultra-high dimensional feature space. The prediction ability of OGS with the Lasso penalty is better than existing methods. The OGS approach is generally applicable to various types of outcome data (quantitative, qualitative, censored event time data) and regression models (e.g. linear, logistic, and Coxâ€™s regression models).",2018,BMC Bioinformatics
Comparison of techniques for the estimation of daily global irradiation and a new technique for the estimation of hourly global irradiation,"Global irradiation and sunshine duration data recorded at Trieste (CNR, Istituto Talassografico di Trieste) during the 11-year period 1972â€“1982 are analyzed using the classical Angstrom equation H = H0 (a + bS/S0) and the equation Hâ€² = H0 (a + bS/Sâ€²0) suggested by Hay [7] for incorporating the effects of (i) multiple reflections, and (ii) not burning of the sunshine recorder chart for small elevation of the sun. The values of the regression constants and the correlation coefficients are calculated using each yearly data set separately. Correlation coefficients of 0Â·89 or more are obtained for the 11 years. Substantial unsystematic scatter is obtained in the values of a as well as b for different years. The use of the equation Hâ€² = H0 (a + bS/Sâ€²0) is not found to either decrease this scatter or to give better values of the correlation coefficients. 
 
Hourly global irradiation data is also analyzed. Eleven-year mean values of the ratio hourly/daily are plotted against the solar time for each of the 12 months of the year. The normal distribution curve 
P(t) = 12Ï€rexpâˆ’(tâˆ’12)22Ïƒ 
is found to fit the data closely. The mean of the normal distribution is taken at the solar noon and the Ïƒ values are obtained for each month by matching the experimental and the theoretical values at the solar noon. The Ïƒ values so obtained are found to bear an excellent linear correlation (r = 0Â·996) with S0, viz.Ïƒ = 0Â·461 + 0Â·192 S0. This provides a simple and elegant technique for estimating hourly irradiation from the daily values and may be of universal applicability. The technique enables the estimation of global irradiation for any smaller interval of time as well.",1984,Solar & Wind Technology
Comprehensible Predictive Modeling Using Regularized Logistic Regression and Comorbidity Based Features,"Different studies have demonstrated the importance of comorbidities to better understand the origin and evolution of medical complications. This study focuses on improvement of the predictive model interpretability based on simple logical features representing comorbidities. We use group lasso based feature interaction discovery followed by a post-processing step, where simple logic terms are added. In the final step, we reduce the feature set by applying lasso logistic regression to obtain a compact set of non-zero coefficients that represent a more comprehensible predictive model. The effectiveness of the proposed approach was demonstrated on a pediatric hospital discharge dataset that was used to build a readmission risk estimation model. The evaluation of the proposed method demonstrates a reduction of the initial set of features in a regression model by 72%, with a slight improvement in the Area Under the ROC Curve metric from 0.763 (95% CI: 0.755-0.771) to 0.769 (95% CI: 0.761-0.777). Additionally, our results show improvement in comprehensibility of the final predictive model using simple comorbidity based terms for logistic regression.",2015,PLoS ONE
Does Breastmilk Influence the Development of Bronchopulmonary Dysplasia?,"OBJECTIVE
To assess whether breastmilk feeding is associated with a reduced risk of bronchopulmonary dysplasia (BPD). Secondary outcome measures analyzed were retinopathy of prematurity (ROP) and necrotizing enterocolitis (NEC).


STUDY DESIGN
In an ongoing multicenter cohort study, the data of 1433 very low birth weight infants born before 32Â weeks of gestation and discharged in 2013 were analyzed. We compared growth and neonatal complications of infants who received breastmilk exclusively (NÂ =Â 223) with those who received formula feedings exclusively (NÂ =Â 239). Logistic regression models were estimated for BPD, ROP, and NEC using nutrition as an independent variable. The Firth logistic regression model and Lasso were used for sensitivity analyses.


RESULTS
Exclusively breastmilk-fed infants gained less weight compared with formula-fed infants. SDS for weight decreased between birth and discharge (median (Q1-Q3): formula -0.9 (-1.4 to [-0.5]) vs breastmilk -1.1 (-1.7 to [-0.6])). Exclusive formula feeding of very low birth weight infants was associated with increased risks of BPD (OR 2.6) as well as NEC (OR 12.6) and ROP (OR 1.80) after controlling for known risk factors.


CONCLUSIONS
Exclusive breastmilk feeding was associated with lower growth rates and a reduced risk of BPD as well as NEC and ROP.",2016,The Journal of pediatrics
Benchmarking of surgical complications in gynaecological oncology: prospective multicentre study.,"OBJECTIVE
To explore the impact of risk-adjustment on surgical complication rates (CRs) for benchmarking gynaecological oncology centres.


DESIGN
Prospective cohort study.


SETTING
Ten UK accredited gynaecological oncology centres.


POPULATION
Women undergoing major surgery on a gynaecological oncology operating list.


METHODS
Patient co-morbidity, surgical procedures and intra-operative (IntraOp) complications were recorded contemporaneously by surgeons for 2948 major surgical procedures. Postoperative (PostOp) complications were collected from hospitals and patients. Risk-prediction models for IntraOp and PostOp complications were created using penalised (lasso) logistic regression using over 30 potential patient/surgical risk factors.


MAIN OUTCOME MEASURES
Observed and risk-adjusted IntraOp and PostOp CRs for individual hospitals were calculated. Benchmarking using colour-coded funnel plots and observed-to-expected ratios was undertaken.


RESULTS
Overall, IntraOp CR was 4.7% (95% CI 4.0-5.6) and PostOp CR was 25.7% (95% CI 23.7-28.2). The observed CRs for all hospitals were under the upper 95% control limit for both IntraOp and PostOp funnel plots. Risk-adjustment and use of observed-to-expected ratio resulted in one hospital moving to the >95-98% CI (red) band for IntraOp CRs. Use of only hospital-reported data for PostOp CRs would have resulted in one hospital being unfairly allocated to the red band. There was little concordance between IntraOp and PostOp CRs.


CONCLUSION
The funnel plots and overall IntraOp (â‰ˆ5%) and PostOp (â‰ˆ26%) CRs could be used for benchmarking gynaecological oncology centres. Hospital benchmarking using risk-adjusted CRs allows fairer institutional comparison. IntraOp and PostOp CRs are best assessed separately. As hospital under-reporting is common for postoperative complications, use of patient-reported outcomes is important.


TWEETABLE ABSTRACT
Risk-adjusted benchmarking of surgical complications for ten UK gynaecological oncology centres allows fairer comparison.",2016,BJOG : an international journal of obstetrics and gynaecology
Recognizing surgical patterns,"In the Netherlands, each year over 1700 patients die from preventable surgical errors. Numerous initiatives to improve surgical practice have had some impact, but problems persist. Despite the introduction of checklists and protocols, patient safety in surgery remains a continuing challenge. This is complicated by some surgeons viewing their own work as an artistic manoeuver whose workflow cannot be captured. However, safeguarding patient safety is also a hospital's management responsibility and no longer only in the surgeon's hands. In spite of the inherent variations, surgeries of the same kind produce similar data, and are usually performed in similar workflows. Surgery is characterized by a peri-operative pipeline of pre-, intra- and post-operative processes. To both reduce errors and improve efficiency, the workflow in the peri-operative pipeline should be designed and planned as effectively as possible in terms of flow of patients and allocation of scarce resources such as operating rooms, instruments and personnel. Currently, planning is done on a very basic level, without using real-world data to learn and improve efficiency. Fortunately, there is lot of available, but unexploited data about surgical interventions that can be used for this purpose. The aim of this thesis is to use acquired and registered peri-operative data to support hospital management to improve safety and efficiency in surgery. The method of assessing safety and efficiency in surgery for individual patients needs to be tailored to each patient. As a result generalization of the results is difficult. We discuss how pattern recognition (PR) provides tools for the assessment of surgical outcome for individual patients. It also allows for handling of outliers and does not impose the same restrictions on data collection procedures as for randomized controlled trials. We show that PR is a pragmatic next step towards data intensive operating rooms with evidence based support for surgeries. Below the techniques as proposed in this thesis are brie y described. To support pre-operative planning of surgeries, assessment of surgical complexity is needed beforehand in order to prepare and possibly avoid complications and delays. This complexity assessment can also aid surgeons in decisions regarding how to proceed with the surgical procedure, for instance by taking extra precautions or making a referral to a more experienced surgeon when a complex surgery is predicted. We show how to use readily available patient data to predict surgical complexity. Classifiers are trained and evaluated using readily collected data from patients undergoing laparoscopic cholecystectomy (LAPCHOL). It is shown that complexity of LAPCHOL surgeries can be predicted in the pre-operative stage with accuracy up to 83% using an LDC or SVM classifier. We also derived the set of features that are relevant for predicting complexity including inflammation, wall thickening, sex and BMI score. To realize intra-operative safety and efficiency goals in surgery, hospitals are searching for autonomous systems for monitoring the surgical workflow in the operating room (OR). In this thesis we propose an autonomous registration technique for the OR. Registering the time of use of surgical instruments and the sequence in which they are used enables us to detect the surgical steps, including the duration of each step. By deploying this as a real-time system, dynamic support for the surgical team and dynamic planning of patients can be performed. For monitoring the usage of surgical instruments, signals from sensors which can detect video, motion and RFID tags can be used. For the application in the OR, it is necessary that these sensors are designed to meet the requirements of the OR environment, specifically with respect to sterilization and non-intrusiveness. We propose a tracking system to detect and track instruments in endoscopic video using biocompatible and sterilization-proof colour markers. The system tracks single and multiple instruments in the video. The output of the tracking tool is a log file with an identifier of the instrument used and the duration of its use for each entry. These instrument logs are then used for workflow mining and outlier detection in surgery. We derived a surgical consensus from multiple surgery logs using global multiple sequence alignment. We showed that the derived consensus conforms to the main steps of laparoscopic cholecystectomy as described in best practices. Using global pair-wise alignment, we showed that outliers from this consensus can be detected using the surgical log. These outliers are commonly simple variations in the execution of the surgical procedure, but can also represent serious complications or errors. To improve post-operative efficiency, accurate predictions of patients' length of stay (LOS) in the postanesthesia care unit (PACU) may lead to cost savings and a number of other efficiency benefits. We propose to use available perioperative data to predict the PACU LOS, using the features case demographics, intra-operative parameters, medications, patient co-morbidities, and surgeon. A linear regression method was used along with ordinary least square regression and `least absolute shrinkage and selection operator' (LASSO-) regression. A forward feature selection approach was then used to identify and rank factors that impact PACU LOS. We showed that PACU LOS can be predicted by perioperative factors with an improvement of 12-18 minutes compared to using the mean baseline. If this prediction is updated with online information, mainly by monitoring post-operative oxygen saturation, future work could lead to real-time LOS algorithms based on peri-operative factors to predict, manage and possibly intercept anticipated, prolonged PACU LOS. This thesis has proposed and demonstrated the application of pattern recognition tools to log, assess and predict surgical workflow parameters. Work in this thesis did not directly contribute to reduce errors and safety in the OR. However, the tools developed in the thesis can be used to support standardization of surgical workflow to both reduce errors and support surgical planning. Moreover, the proposed techniques for the operating room can be used in other medical domains such as the intensive care unit with only small contextual modifications.",2012,
A landscape-level analysis of yellow-cedar decline in coastal British Columbia,"Yellow-cedar (Chamaecyparis nootkatensis D. Don (Spach)) is currently undergoing a dramatic decline in western North America. Recent research suggests that site factors combined with a shift in climate have predisposed yellow-cedar trees to decline. We conducted the first landscape-level analysis of the decline in coastal British Columbia to assess relations between the decline and topographic variables. We used lasso-penalized logistic regression to model yellow-cedar decline presence and absence with topographic variables derived from a digital elevation model. Model results indicated that low elevation sites close to the coast, which are more exposed and have more variation in elevation, are more likely to show evidence of decline. The logistic model fit the data well (Nagelkerke R2 = 0.846) and had high predictive accuracy (AUC = 0.98). The topographic variables identified by the model influence degree of soil saturation, temperatures, and snowpack presence in a forest stand, supporting the proposed a...",2011,Canadian Journal of Forest Research
Inference with normal-gamma prior distributions in regression problems,"This paper considers the efiects of placing an absolutely continuous prior distribution on the regression coe-cients of a linear model. We show that the posterior expectation is a matrix-shrunken version of the least squares estimate where the shrinkage matrix depends on the derivatives of the prior predictive den- sity of the least squares estimate. The special case of the normal-gamma prior, which generalizes the Bayesian Lasso (Park and Casella 2008), is studied in depth. We discuss the prior interpretation and the posterior efiects of hyperparameter choice and suggest a data-dependent default prior. Simulations and a chemomet- ric example are used to compare the performance of the normal-gamma and the Bayesian Lasso in terms of out-of-sample predictive performance.",2010,Bayesian Analysis
Lass-0: sparse non-convex regression by local search,"We compute approximate solutions to L0 regularized linear regression using L1 regularization, also known as the Lasso, as an initialization step. Our algorithm, the Lass-0 (""Lass-zero""), uses a computationally efficient stepwise search to determine a locally optimal L0 solution given any L1 regularization solution. We present theoretical results of consistency under orthogonality and appropriate handling of redundant features. Empirically, we use synthetic data to demonstrate that Lass-0 solutions are closer to the true sparse support than L1 regularization models. Additionally, in real-world data Lass-0 finds more parsimonious solutions than L1 regularization while maintaining similar predictive accuracy.",2015,arXiv: Machine Learning
Bayesian methods for skewed response including longitudinal and heteroscedastic data,"Skewed response data are very popular in practice, especially in biomedical area. We begin our work from the skewed longitudinal response without heteroscedasticity. We extend the skewed error density to the multivariate response. Then we study the heterocedasticity. We extend the transform-both-sides model to the bayesian variable selection area to handle the univariate skewed response, where the variance of response is a function of the median. At last, we proposed a novel model to handle the skewed univariate response with a flexible heteroscedasticity. For longitudinal studies with heavily skewed continuous response, statistical model and methods focusing on mean response are not appropriate. In this paper, we present a partial linear model of median regression function of skewed longitudinal response. We develop a semiparametric Bayesian estimation procedure using an appropriate Dirichlet process mixture prior for the skewed error distribution. We provide justifications for using our methods including theoretical investigation of the support of the prior, asymptotic properties of the posterior and also simulation studies of finite sample properties. Ease of implementation and advantages of our model and method compared to existing methods are illustrated via analysis of a cardiotoxicity study of children of HIV infected mother. Our second aim is to develop a Bayesian simultaneous variable selection and estimation of median regression for skewed response variable. Our hierarchical Bayesian model can incorporate advantages of l0 penalty for skewed and heteroscedastic error. Some preliminary simulation studies have been conducted to compare the performance of proposed model and existing frequentist median lasso regression model. Considering the estimation bias and total square error, our proposed model performs as good as, or better than competing frequentist estimators. In biomedical studies, the covariates often affect the location, scale as well as the shape of the skewed response distribution. Existing biostatistical literature mainly focuses on the mean regression with a symmetric error distribution. While such modeling assumptions and methods are often deemed as restrictive and inappropriate for skewed response, the completely nonparametric methods may lack a physical interpretation of the covariate effects. Existing nonparametric methods also miss any easily implementable computational tool. For a skewed response, we develop a novel model accommodating a nonparametric error density that depends on the covariates. The advantages of our semiparametric associated Bayes method include the ease of prior elicitation/determination, an easily implementable posterior computation,",2013,
Statistical Methods for Drug Safety,Introduction Randomized Clinical Trials Observational Studies The Problem of Multiple Comparisons The Evolution of Available Data Streams The Hierarchy of Scientific Evidence Statistical Significance Summary Basic Statistical Concepts Relative Risk Odds Ratio Statistical Power Maximum Likelihood Estimation Non-Linear Regression Models Causal Inference Multi-Level Models Introduction Issues Inherent in Longitudinal Data Historical Background Statistical Models for the Analysis of Longitudinal and/or Clustered Data Causal Inference Introduction Propensity Score Matching Marginal Structural Models Instrumental Variables Differential Effects Analysis of Spontaneous Reports Proportional Reporting Ratio Bayesian Confidence Propagation Neural Network (BCPNN) Empirical Bayes Screening Multi-Item Gamma Poisson Shrinker Bayesian Lasso Logistic Regression Random-Effect Poisson Regression Discussion Meta-Analysis Fixed-Effect Meta-Analysis Random-Effect Meta-Analysis Maximum Marginal Likelihood/Empirical Bayes Method Bayesian Meta-Analysis Confidence Distribution Framework for Meta-Analysis Discussion Ecological Methods Time Series Methods State Space Model Change Point Analysis Mixed-Effects Poisson Regression Model Discrete-Time Survival Models Introduction Discrete-Time Ordinal Regression Model Discrete-Time Ordinal Regression Frailty Model Illustration Competing Risk Models Illustration Research Synthesis Introduction Three-Level Mixed-Effects Regression Models Analysis of Medical Claims Data Introduction Administrative Claims Observational Data Experimental Strategies Statistical Strategies Illustrations Conclusion Methods to Be Avoided Introduction Spontaneous Reports Vote Counting Simple Pooling of Studies Including Randomized and Non-Randomized Trials in Meta-Analysis Multiple Comparisons and Biased Reporting of Results Immortality Time Bias Summary and Conclusions Final Thoughts Bibliography Index,2015,
"Genomic prediction accuracies and abilities for growth and wood quality traits of Scots pine, using genotyping-by-sequencing (GBS) data","Higher genetic gains can be achieved through genomic selection (GS) by shortening time of progeny testing in tree breeding programs. Genotyping-by-sequencing (GBS), combined with two imputation methods, allowed us to perform the current genomic prediction study in Scots pine (Pinus sylvestris L.). 694 individuals representing 183 full-sib families were genotyped and phenotyped for growth and wood quality traits. 8719 SNPs were used to compare different genomic prediction models. In addition, the impact on the predictive ability (PA) and prediction accuracy to estimate genomic breeding values was evaluated by assigning different ratios of training and validation sets, as well as different subsets of SNP markers. Genomic Best Linear Unbiased Prediction (GBLUP) and Bayesian Ridge Regression (BRR) combined with expectation maximization (EM) imputation algorithm showed higher PAs and prediction accuracies than Bayesian LASSO (BL). A subset of approximately 4000 markers was sufficient to provide the same PAs and accuracies as the full set of 8719 markers. Furthermore, PAs were similar for both pedigree- and genomic-based estimations, whereas accuracies and heritabilities were slightly higher for pedigree-based estimations. However, prediction accuracies of genomic models were sufficient to achieve a higher selection efficiency per year, varying between 50-87% compared to the traditional pedigree-based selection.",2019,bioRxiv
Best Subset Selection via a Modern Optimization Lens,"In the last twenty-five years (1990-2014), algorithmic advances in integer optimization combined with hardware improvements have resulted in an astonishing 200 billion factor speedup in solving Mixed Integer Optimization (MIO) problems. We present a MIO approach for solving the classical best subset selection problem of choosing $k$ out of $p$ features in linear regression given $n$ observations. We develop a discrete extension of modern first order continuous optimization methods to find high quality feasible solutions that we use as warm starts to a MIO solver that finds provably optimal solutions. The resulting algorithm (a) provides a solution with a guarantee on its suboptimality even if we terminate the algorithm early, (b) can accommodate side constraints on the coefficients of the linear regression and (c) extends to finding best subset solutions for the least absolute deviation loss function. Using a wide variety of synthetic and real datasets, we demonstrate that our approach solves problems with $n$ in the 1000s and $p$ in the 100s in minutes to provable optimality, and finds near optimal solutions for $n$ in the 100s and $p$ in the 1000s in minutes. We also establish via numerical experiments that the MIO approach performs better than {\texttt {Lasso}} and other popularly used sparse learning procedures, in terms of achieving sparse solutions with good predictive power.",2015,arXiv: Methodology
A general framework for fast stagewise algorithms,"Forward stagewise regression follows a very simple strategy for constructing a sequence of sparse regression estimates: it starts with all coefficients equal to zero, and iteratively updates the coefficient (by a small amount e) of the variable that achieves the maximal absolute inner product with the current residual. This procedure has an interesting connection to the lasso: under some conditions, it is known that the sequence of forward stagewise estimates exactly coincides with the lasso path, as the step size e goes to zero. Furthermore, essentially the same equivalence holds outside of least squares regression, with the minimization of a differentiable convex loss function subject to an l1 norm constraint (the stagewise algorithm now updates the coefficient corresponding to the maximal absolute component of the gradient). 
 
Even when they do not match their l1-constrained analogues, stagewise estimates provide a useful approximation, and are computationally appealing. Their success in sparse modeling motivates the question: can a simple, effective strategy like forward stagewise be applied more broadly in other regularization settings, beyond the l1 norm and sparsity? The current paper is an attempt to do just this. We present a general framework for stagewise estimation, which yields fast algorithms for problems such as group-structured learning, matrix completion, image denoising, and more.",2015,J. Mach. Learn. Res.
Development and Validation of a 9-Gene Prognostic Signature in Patients With Multiple Myeloma,"Background: Multiple myeloma (MM) is one of the most common types of hematological malignance, and the prognosis of MM patients remains poor. Objective: To identify and validate a genetic prognostic signature in patients with MM. Methods: Co-expression network was constructed to identify hub genes related with International Staging System (ISS) stage of MM. Functional analysis of hub genes was conducted. Univariate Cox proportional hazard regression analysis was conducted to identify genes correlated with the overall survival (OS) of MM patients. Least absolute shrinkage and selection operator (LASSO) penalized Cox proportional hazards regression model was used to minimize overfitting and construct a prognostic signature. The prognostic value of the signature was validated in the test set and an independent validation cohort. Results: A total of 758 hub genes correlated with ISS stage of MM patients were identified, and these hub genes were mainly enriched in several GO terms and KEGG pathways involved in cell proliferation and immune response. Nine hub genes (HLA-DPB1, TOP2A, FABP5, CYP1B1, IGHM, FANCI, LYZ, HMGN5, and BEND6) with non-zero coefficients in the LASSO Cox regression model were used to build a 9-gene prognostic signature. Relapsed MM and ISS stage III MM was associated with high risk score calculated based on the signature. Patients in the 9-gene signature low risk group was significantly associated with better clinical outcome than those in the 9-gene signature high risk group in the training set, test, and validation set. Conclusions: We developed a 9-gene prognostic signature that might be an independent prognostic factor in patients with MM.",2019,Frontiers in Oncology
Penalized logistic regression for high-dimensional DNA methylation data with case-control studies,"MOTIVATION
DNA methylation is a molecular modification of DNA that plays crucial roles in regulation of gene expression. Particularly, CpG rich regions are frequently hypermethylated in cancer tissues, but not methylated in normal tissues. However, there are not many methodological literatures of case-control association studies for high-dimensional DNA methylation data, compared with those of microarray gene expression. One key feature of DNA methylation data is a grouped structure among CpG sites from a gene that are possibly highly correlated. In this article, we proposed a penalized logistic regression model for correlated DNA methylation CpG sites within genes from high-dimensional array data. Our regularization procedure is based on a combination of the l(1) penalty and squared l(2) penalty on degree-scaled differences of coefficients of CpG sites within one gene, so it induces both sparsity and smoothness with respect to the correlated regression coefficients. We combined the penalized procedure with a stability selection procedure such that a selection probability of each regression coefficient was provided which helps us make a stable and confident selection of methylation CpG sites that are possibly truly associated with the outcome.


RESULTS
Using simulation studies we demonstrated that the proposed procedure outperforms existing main-stream regularization methods such as lasso and elastic-net when data is correlated within a group. We also applied our method to identify important CpG sites and corresponding genes for ovarian cancer from over 20 000 CpGs generated from Illumina Infinium HumanMethylation27K Beadchip. Some genes identified are potentially associated with cancers.",2012,Bioinformatics
Information-theoretic limits on sparsity recovery in the high-dimensional and noisy setting,"The problem of sparsity pattern or support set recovery refers to estimating the set of nonzero coefficients of an unknown vector beta* isin Ropfp based on a set of n noisy observations. It arises in a variety of settings, including subset selection in regression, graphical model selection, signal denoising, compressive sensing, and constructive approximation. The sample complexity of a given method for subset recovery refers to the scaling of the required sample size n as a function of the signal dimension p, sparsity index k (number of non-zeroes in beta*), as well as the minimum value betamin of beta* over its support and other parameters of measurement matrix. This paper studies the information-theoretic limits of sparsity recovery: in particular, for a noisy linear observation model based on random measurement matrices drawn from general Gaussian measurement matrices, we derive both a set of sufficient conditions for exact support recovery using an exhaustive search decoder, as well as a set of necessary conditions that any decoder, regardless of its computational complexity, must satisfy for exact support recovery. This analysis of fundamental limits complements our previous work on sharp thresholds for support set recovery over the same set of random measurement ensembles using the polynomial-time Lasso method (lscr1-constrained quadratic programming).",2009,IEEE Trans. Inf. Theory
On gene Selection and Classification for Cancer microarray Data Using Multi-Step Clustering and Sparse Representation,"Microarray data profiles gene expression on a whole genome scale, and provides a good way to study associations between gene expression and occurrence or progression of cancer disease. Many researchers realized that microarray data is useful to predict cancer cases. However, the high dimension of gene expressions, which is significantly larger than the sample size, makes this task very difficult. It is very important to identify the significant genes causing cancer. Many feature selection algorithms have been proposed focusing on improving cancer predictive accuracy at the expense of ignoring the correlations between the features. In this work, a novel framework (named by SGS) is presented for significant genes selection and efficient cancer case classification. The proposed framework first performs a clustering algorithm to find the gene groups where genes in each group have higher correlation coefficient, and then selects (1) the significant (2) genes in each group using the Bayesian Lasso method and important gene groups using the group Lasso method, and finally builds a prediction model based on the shrinkage gene space with efficient classification algorithm (such as support vector machine (SVM), 1NN, and regression). Experimental results on public available microarray data show that the proposed framework often outperforms the existing feature selection and prediction methods such as SAM, information gain (IG), and Lasso-type prediction models.",2011,Advances in Adaptive Data Analysis
Deep learning applied to glacier evolution modelling,"Abstract. We present a novel approach to simulate and reconstruct annual glacier-wide surface mass balance (SMB) series based on a deep artificial neural network (ANN; i.e. deep learning). This method has been included as the SMB component of an open-source regional glacier evolution model. While most glacier models tend to incorporate more and more physical processes, here we take an alternative approach by creating a parameterized model based on data science. Annual glacier-wide SMBs can be simulated from topo-climatic predictors using either deep learning or Lasso (least absolute shrinkage and selection operator; regularized multilinear regression), whereas the glacier geometry is updated using a glacier-specific parameterization. We compare and cross-validate our nonlinear deep learning SMB model against other standard linear statistical methods on a dataset of 32 French Alpine glaciers. Deep learning is found to outperform linear methods, with improved explained variance (up to +64 â€‰% in space and +108 â€‰% in time) and accuracy (up to +47 â€‰% in space and +58 â€‰% in time), resulting in an estimated r2 of 0.77 and a root-mean-square error (RMSE) of 0.51â€‰mâ€‰w.e. Substantial nonlinear structures are captured by deep learning, with around 35â€‰% of nonlinear behaviour in the temporal dimension. For the glacier geometry evolution, the main uncertainties come from the ice thickness data used to initialize the model. These results should encourage the use of deep learning in glacier modelling as a powerful nonlinear tool, capable of capturing the nonlinearities of the climate and glacier systems, that can serve to reconstruct or simulate SMB time series for individual glaciers in a whole region for past and future climates.",2020,The Cryosphere
A novel genomic-clinicopathologic nomogram to improve prognosis prediction of hepatocellular carcinoma.,"There is a lack of precise and clinical accessible model to predict the prognosis of hepatocellular carcinoma (HCC) in clinic practice currently. Here, an inclusive nomogram was developed by integrating genomic markers and clinicopathologic factors for predicting the outcome of patients with HCC. A total of 365 samples of HCC were obtained from the Cancer Genome Atlas (TCGA) database. The LASSO analysis was carried out to identify HCC-related mRNAs, and the multivariate Cox regression analysis was used to construct a genomic-clinicopathologic nomogram. As results, 9 mRNAs were finally identified as prognostic indicators, including RGCC, CDH15, XRN2, RAB3IL1, THEM4, PIF1, MANBA, FKTN and GABARAPL1, and used to establish a 9-mRNA classifier. Additionally, an inclusive nomogram was built up by combining the 9-mRNA classifier (P < 0.001) and clinicopathologic factors including age (P = 0.006) and metastasis (P < 0.001) to predict the mortality of HCC patients. Time-dependent receiver operating characteristic, index of concordance and calibration analyses indicated favorable accuracy of the model. Decision curve analysis suggested that appropriate intervention according to the established nomogram will bring net benefit when threshold probability was above 25%. The genomic-clinicopathologic model could be a reliable tool for predicting the mortality, helping determining the individualized treatment and probably improving HCC survival.",2020,Clinica chimica acta; international journal of clinical chemistry
Hyperspectral Image Classification Based on Structured Sparse Logistic Regression and Three-Dimensional Wavelet Texture Features,"Hyperspectral remote sensing imagery contains rich information on spectral and spatial distributions of distinct surface materials. Owing to its numerous and continuous spectral bands, hyperspectral data enable more accurate and reliable material classification than using panchromatic or multispectral imagery. However, high-dimensional spectral features and limited number of available training samples have caused some difficulties in the classification, such as overfitting in learning, noise sensitiveness, overloaded computation, and lack of meaningful physical interpretability. In this paper, we propose a hyperspectral feature extraction and pixel classification method based on structured sparse logistic regression and 3-D discrete wavelet transform (3D-DWT) texture features. The 3D-DWT decomposes a hyperspectral data cube at different scales, frequencies, and orientations, during which the hyperspectral data cube is considered as a whole tensor instead of adapting the data to a vector or matrix. This allows the capture of geometrical and statistical spectral-spatial structures. After the feature extraction step, sparse representation/modeling is applied for data analysis and processing via sparse regularized optimization, which selects a small subset of the original feature variables to model the data for regression and classification purpose. A linear structured sparse logistic regression model is proposed to simultaneously select the discriminant features from the pool of 3D-DWT texture features and learn the coefficients of the linear classifier, in which the prior knowledge about feature structure can be mapped into the various sparsity-inducing norms such as lasso, group, and sparse group lasso. Furthermore, to overcome the limitation of linear models, we extended the linear sparse model to nonlinear classification by partitioning the feature space into subspaces of linearly separable samples. The advantages of our methods are validated on the real hyperspectral remote sensing data sets.",2013,IEEE Transactions on Geoscience and Remote Sensing
An Evaluation of Parameter Pruning Approaches for Software Estimation,"Model-based estimation often uses impact factors and historical data to predict the effort of new projects. Estimation accuracy of this approach is highly dependent on how well impact factors are selected. This paper comparatively assesses six methods for prune parameters of effort estimation models, including Stepwise regression, Lasso, constrained regression, GRASP, Tabu search, and PCA. Four data sets were used for evaluation, showing that estimation accuracy varies among the methods but no method consistently outperforms the rest. Stepwise regression prunes estimation model parameters the most while it does not sacrifice much estimation performance. Our study provides further evidence to support the use of Stepwise regression for selecting factors in effort estimation.",2019,Proceedings of the Fifteenth International Conference on Predictive Models and Data Analytics in Software Engineering
Stock market prediction using social media data and finding the covariance of the LASSO,"Stock market prediction has been a research topic for decades; recently, efforts to increase the accuracy by including data from social media like Google and Twitter received a lot of attention. Social media can be regarded as indicator for sentiments and sentiments are known to influence the stock market. Current models lack interpretation; it is difficult to determine what data is relevant for stock market prediction, since there is an abundance of social media data. A regression method that induces sparsity is thus required; data that is not useful is discarded automatically. The LASSO induces sparsity via L1-regularization; however, the covariance and confidence of the found regression coefficients cannot be derived easily, while this is important for interpretation. This thesis therefore reviews all known methods for approximating the covariance and confidence interval for the LASSO and determines their accuracy using numerical simulations. A new method is proposed based on the Unscented Transform, which outcompetes all methods in the underdetermined scenario, where there are more features than data points. Unfortunately, linear regression via the LASSO has limited use for stock markets as the achieved prediction accuracy is low. Nonlinear models are often applied for stock market prediction to achieve higher accuracies. Therefore a new feature selection method is proposed for the nonlinear Support Vector Regression (SVR) to select the correct data for stock market prediction using the SVR. This method yields accurate feature selection when the number of features to select from is low.",2014,
Development and validation of a radiomics nomogram for progression-free survival prediction in stage IV EGFR-mutant non-small cell lung cancer,"Accurately predict the risk of disease progression and benefit of tyrosine kinase inhibitors (TKIs) therapy for stage IV non-small cell lung cancer (NSCLC) patients with activing epidermal growth factor receptor (EGFR) mutations by current staging methods are challenge. We postulated that integrating a classifier consisted of multiple computed tomography (CT) phenotypic features, and other clinicopathological risk factors into a single model could improve risk stratification and prediction of progression-free survival (PFS) of EGFR TKIs for these patients. Patients confirmed as stage IV EGFR-mutant NSCLC received EGFR TKIs with no resection; pretreatment contrast enhanced CT performed at approximately 2 weeks before the treatment was enrolled. A six-CT-phenotypic-feature-based classifier constructed by the LASSO Cox regression model, and three clinicopathological factors: pathologic N category, performance status (PS) score, and intrapulmonary metastasis status were used to construct a nomogram in a training set of 115 patients. The prognostic and predictive accuracy of this nomogram was then subjected to an external independent validation of 107 patients. PFS between the training and independent validation set is no statistical difference by Mann-Whitney U test (P = 0.2670). PFS of the patients could be predicted with good consistency compared with the actual survival. C-index of the proposed individualized nomogram in the training set (0Â·707, 95%CI: 0Â·643, 0Â·771) and the independent validation set (0Â·715, 95%CI: 0Â·650, 0Â·780) showed the potential of clinical prognosis to predict PFS of stage IV EGFR-mutant NSCLC from EGFR TKIs. The individualized nomogram might facilitate patient counselling and individualise management of patients with this disease.",2017,
Pure additive contribution of genetic variants to a risk prediction model using propensity score matching: application to type 2 diabetes,"The achievements of genome-wide association studies have suggested ways to predict diseases, such as type 2 diabetes (T2D), using single-nucleotide polymorphisms (SNPs). Most T2D risk prediction models have used SNPs in combination with demographic variables. However, it is difficult to evaluate the pure additive contribution of genetic variants to classically used demographic models. Since prediction models include some heritable traits, such as body mass index, the contribution of SNPs using unmatched case-control samples may be underestimated. In this article, we propose a method that uses propensity score matching to avoid underestimation by matching case and control samples, thereby determining the pure additive contribution of SNPs. To illustrate the proposed propensity score matching method, we used SNP data from the Korea Association Resources project and reported SNPs from the genome-wide association study catalog. We selected various SNP sets via stepwise logistic regression (SLR), least absolute shrinkage and selection operator (LASSO), and the elastic-net (EN) algorithm. Using these SNP sets, we made predictions using SLR, LASSO, and EN as logistic regression modeling techniques. The accuracy of the predictions was compared in terms of area under the receiver operating characteristic curve (AUC). The contribution of SNPs to T2D was evaluated by the difference in the AUC between models using only demographic variables and models that included the SNPs. The largest difference among our models showed that the AUC of the model using genetic variants with demographic variables could be 0.107 higher than that of the corresponding model using only demographic variables.",2019,Genomics & Informatics
Kernel based methods for accelerated failure time model with ultra-high dimensional data,"BackgroundMost genomic data have ultra-high dimensions with more than 10,000 genes (probes). Regularization methods with L1 and Lp penalty have been extensively studied in survival analysis with high-dimensional genomic data. However, when the sample size n â‰ª m (the number of genes), directly identifying a small subset of genes from ultra-high (m > 10, 000) dimensional data is time-consuming and not computationally efficient. In current microarray analysis, what people really do is select a couple of thousands (or hundreds) of genes using univariate analysis or statistical tests, and then apply the LASSO-type penalty to further reduce the number of disease associated genes. This two-step procedure may introduce bias and inaccuracy and lead us to miss biologically important genes.ResultsThe accelerated failure time (AFT) model is a linear regression model and a useful alternative to the Cox model for survival analysis. In this paper, we propose a nonlinear kernel based AFT model and an efficient variable selection method with adaptive kernel ridge regression. Our proposed variable selection method is based on the kernel matrix and dual problem with a much smaller n Ã— n matrix. It is very efficient when the number of unknown variables (genes) is much larger than the number of samples. Moreover, the primal variables are explicitly updated and the sparsity in the solution is exploited.ConclusionsOur proposed methods can simultaneously identify survival associated prognostic factors and predict survival outcomes with ultra-high dimensional genomic data. We have demonstrated the performance of our methods with both simulation and real data. The proposed method performs superbly with limited computational studies.",2010,BMC Bioinformatics
