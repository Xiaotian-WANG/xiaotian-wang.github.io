title,abstract,year,journal
Square-root lasso: pivotal recovery of sparse signals via conic programming,"We propose a pivotal method for estimating high-dimensional sparse linear regression models, where the overall number of regressors p is large, possibly much larger than n, but only s regressors are significant. The method is a modification of the lasso, called the square-root lasso. The method is pivotal in that it neither relies on the knowledge of the standard deviation Ïƒ nor does it need to pre-estimate Ïƒ. Moreover, the method does not rely on normality or sub-Gaussianity of noise. It achieves near-oracle performance, attaining the convergence rate Ïƒl(s/n) log pr-super-1/2 in the prediction norm, and thus matching the performance of the lasso with known Ïƒ. These performance results are valid for both Gaussian and non-Gaussian errors, under some mild moment restrictions. We formulate the square-root lasso as a solution to a convex conic programming problem, which allows us to implement the estimator using efficient algorithmic methods, such as interior-point and first-order methods. Copyright 2011, Oxford University Press.",2011,Biometrika
Developing small-area predictions for smoking and obesity prevalence in the United States for use in Environmental Public Health Tracking.,"BACKGROUND
Globally and in the United States, smoking and obesity are leading causes of death and disability. Reliable estimates of prevalence for these risk factors are often missing variables in public health surveillance programs. This may limit the capacity of public health surveillance to target interventions or to assess associations between other environmental risk factors (e.g., air pollution) and health because smoking and obesity are often important confounders.


OBJECTIVES
To generate prevalence estimates of smoking and obesity rates over small areas for the United States (i.e., at the ZIP code and census tract levels).


METHODS
We predicted smoking and obesity prevalence using a combined approach first using a lasso-based variable selection procedure followed by a two-level random effects regression with a Poisson link clustered on state and county. We used data from the Behavioral Risk Factor Surveillance System (BRFSS) from 1991 to 2010 to estimate the model. We used 10-fold cross-validated mean squared errors and the variance of the residuals to test our model. To downscale the estimates we combined the prediction equations with 1990 and 2000 U.S. Census data for each of the four five-year time periods in this time range at the ZIP code and census tract levels. Several sensitivity analyses were conducted using models that included only basic terms, that accounted for spatial autocorrelation, and used Generalized Linear Models that did not include random effects.


RESULTS
The two-level random effects model produced improved estimates compared to the fixed effects-only models. Estimates were particularly improved for the two-thirds of the conterminous U.S. where BRFSS data were available to estimate the county level random effects. We downscaled the smoking and obesity rate predictions to derive ZIP code and census tract estimates.


CONCLUSIONS
To our knowledge these smoking and obesity predictions are the first to be developed for the entire conterminous U.S. for census tracts and ZIP codes. Our estimates could have significant utility for public health surveillance.",2014,Environmental research
Quelques questions de sÃ©lection de variables autour de l'estimateur LASSO,"Le probleme general etudie dans cette these est celui de la regression lineaire en grande dimension. On s'interesse particulierement aux methodes d'estimation qui capturent la sparsite du parametre cible, meme dans le cas ou la dimension est superieure au nombre d'observations. Une methode populaire pour estimer le parametre inconnu de la regression dans ce contexte est l'estimateur des moindres carres penalises par la norme l1 des coefficients, connu sous le nom de LASSO. Les contributions de la these portent sur l'etude de variantes de l'estimateur LASSO pour prendre en compte soit des informations supplementaires sur les variables d'entree, soit des modes semi-supervises d'acquisition des donnees. Plus precisement, les questions abordees dans ce travail sont : i) l'estimation du parametre inconnu lorsque l'espace des variables explicatives a une structure bien determinee (presence de correlations, structure d'ordre sur les variables ou regroupements entre variables) ; ii) la construction d'estimateurs adaptes au cadre transductif, pour lequel les nouvelles observations non etiquetees sont prises en consideration. Ces adaptations sont en partie deduites par une modification de la penalite dans la definition de l'estimateur LASSO. Les procedures introduites sont essentiellement analysees d'un point de vue non-asymptotique ; nous prouvons notamment que les estimateurs construits verifient des Inegalites de Sparsite Oracles. Ces inegalites ont pour particularite de dependre du nombre de composantes non-nulles du parametre cible. Un controle sur la probabilite d'erreur d'estimation du support du parametre de regression est egalement etabli. Les performances pratiques des methodes etudiees sont par ailleurs illustrees a travers des resultats de simulation.",2009,
An efficient Monte Carlo EM algorithm for Bayesian lasso,"The lasso is a popular technique of simultaneous estimation and variable selection in many research areas. The marginal posterior mode of the regression coefficients is equivalent to estimates given by the non-Bayesian lasso when the regression coefficients have independent Laplace priors. Because of its flexibility of statistical inferences, the Bayesian approach is attracting a growing body of research in recent years. Current approaches are primarily to either do a fully Bayesian analysis using Markov chain Monte Carlo (MCMC) algorithm or use Monte Carlo expectation maximization (MCEM) methods with an MCMC algorithm in each E-step. However, MCMC-based Bayesian method has much computational burden and slow convergence. Tan et al. [An efficient MCEM algorithm for fitting generalized linear mixed models for correlated binary data. J Stat Comput Simul. 2007;77:929â€“943] proposed a non-iterative sampling approach, the inverse Bayes formula (IBF) sampler, for computing posteriors of a hierarchical model in the structure of MCEM. Motivated by their paper, we develop this IBF sampler in the structure of MCEM to give the marginal posterior mode of the regression coefficients for the Bayesian lasso, by adjusting the weights of importance sampling, when the full conditional distribution is not explicit. Simulation experiments show that the computational time is much reduced with our method based on the expectation maximization algorithm and our algorithms and our methods behave comparably with other Bayesian lasso methods not only in prediction accuracy but also in variable selection accuracy and even better especially when the sample size is relatively large.",2014,Journal of Statistical Computation and Simulation
"The Stream Algorithm: Computationally Efficient Ridge-Regression via Bayesian Model Averaging, and Applications to Pharmacogenomic Prediction of Cancer Cell Line Sensitivity","Computational efficiency is important for learning algorithms operating in the ""large p, small n"" setting. In computational biology, the analysis of data sets containing tens of thousands of features (""large p""), but only a few hundred samples (""small n""), is nowadays routine, and regularized regression approaches such as ridge-regression, lasso, and elastic-net are popular choices. In this paper we propose a novel and highly efficient Bayesian inference method for fitting ridge-regression. Our method is fully analytical, and bypasses the need for expensive tuning parameter optimization, via cross-validation, by employing Bayesian model averaging over the grid of tuning parameters. Additional computational efficiency is achieved by adopting the singular value decomposition reparametrization of the ridge-regression model, replacing computationally expensive inversions of large p Ã— p matrices by efficient inversions of small and diagonal n Ã— n matrices. We show in simulation studies and in the analysis of two large cancer cell line data panels that our algorithm achieves slightly better predictive performance than cross-validated ridge-regression while requiring only a fraction of the computation time. Furthermore, in comparisons based on the cell line data sets, our algorithm systematically out-performs the lasso in both predictive performance and computation time, and shows equivalent predictive performance, but considerably smaller computation time, than the elastic-net.",2014,Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing
Robust Variable Selection in Functional Linear Models,"We consider the problem of selecting functional variables using the L1 regularization in a functional linear regression model with a scalar response and functional predictors in the presence of outliers. Since the LASSO is a special case of the penalized least squares regression with L1-penalty function it suffers from the heavy-tailed errors and/or outliers in data. Recently, the LAD regression and the LASSO methods have been combined (the LAD-LASSO regression method) to carry out robust parameter estimation and variable selection simultaneously for a multiple linear regression model. However variable selection of the functional predictor based on LASSO fails since multiple parameters exist for a functional predictor . Therefore group LASSO is used for selecting grouped variables rather than individual variables. In this study we extend the LADgroup LASSO to a functional linear regression model with a scalar response and functional predictors. We illustrate the LADgroup LASSO on both simulated and real data.",2014,
Full Quantification of Left Ventricle via Deep Multitask Learning Network Respecting Intra- and Inter-Task Relatedness,"Cardiac left ventricle (LV) quantification is among the most clinically important tasks for identification and diagnosis of cardiac diseases, yet still a challenge due to the high variability of cardiac structure and the complexity of temporal dynamics. Full quantification, i.e., to simultaneously quantify all LV indices including two areas (cavity and myocardium), six regional wall thicknesses (RWT), three LV dimensions, and one cardiac phase, is even more challenging since the uncertain relatedness intra and inter each type of indices may hinder the learning procedure from better convergence and generalization. In this paper, we propose a newly-designed multitask learning network (FullLVNet), which is constituted by a deep convolution neural network (CNN) for expressive feature embedding of cardiac structure; two followed parallel recurrent neural network (RNN) modules for temporal dynamic modeling; and four linear models for the final estimation. During the final estimation, both intra- and inter-task relatedness are modeled to enforce improvement of generalization: (1) respecting intra-task relatedness, group lasso is applied to each of the regression tasks for sparse and common feature selection and consistent prediction; (2) respecting inter-task relatedness, three phase-guided constraints are proposed to penalize violation of the temporal behavior of the obtained LV indices. Experiments on MR sequences of 145 subjects show that FullLVNet achieves high accurate prediction with our intra- and inter-task relatedness, leading to MAE of 190 mm\(^2\), 1.41 mm, 2.68 mm for average areas, RWT, dimensions and error rate of 10.4% for the phase classification. This endows our method a great potential in comprehensive clinical assessment of global, regional and dynamic cardiac function.",2017,ArXiv
"Mixtures, envelopes and hierarchical duality","Summary 
 
We develop a connection between mixture and envelope representations of objective functions that arise frequently in statistics. We refer to this connection by using the term â€˜hierarchical dualityâ€™. Our results suggest an interesting and previously underexploited relationship between marginalization and profiling, or equivalently between the Fenchelâ€“Moreau theorem for convex functions and the Bernsteinâ€“Widder theorem for Laplace transforms. We give several different sets of conditions under which such a duality result obtains. We then extend existing work on envelope representations in several ways, including novel generalizations to varianceâ€“mean models and to multivariate Gaussian location models. This turns out to provide an elegant missing data interpretation of the proximal gradient method, which is a widely used algorithm in machine learning. We show several statistical applications in which the framework proposed leads to easily implemented algorithms, including a robust version of the fused lasso, non-linear quantile regression via trend filtering and the binomial fused double-Pareto model. Code for the examples is available on GitHub at https://github.com/jgscott/hierduals.",2014,Journal of The Royal Statistical Society Series B-statistical Methodology
Spatial Lasso With Applications to GIS Model Selection,"Geographic information systems (GIS) organize spatial data in multiple two-dimensional arrays called layers. In many applications, a response of interest is observed on a set of sites in the landscape, and it is of interest to build a regression model from the GIS layers to predict the response at unsampled sites. Model selection in this context then consists not only of selecting appropriate layers, but also of choosing appropriate neighborhoods within those layers. We formalize this problem as a linear model and propose the use of Lasso to simultaneously select variables, choose neighborhoods, and estimate parameters. Spatially dependent errors are accounted for using generalized least squares and spatial smoothness in selected coefficients is incorporated through use of a priori spatial covariance structure. This leads to a modification of the Lasso procedure, called spatial Lasso. The spatial Lasso can be implemented by a fast algorithm and it performs well in numerical examples, including an applicat...",2010,Journal of Computational and Graphical Statistics
Pathwise least angle regression and a significance test for the elastic net,"Least angle regression (LARS) by Efron et al. (2004) is a novel method for constructing the piece-wise linear path of Lasso solutions. For several years, it remained also as the de facto method for computing the Lasso solution before more sophisticated optimization algorithms preceded it. LARS method has recently again increased its popularity due to its ability to find the values of the penalty parameters, called knots, at which a new parameter enters the active set of non-zero coefficients. Significance test for the Lasso by Lockhart et al. (2014), for example, requires solving the knots via the LARS algorithm. Elastic net (EN), on the other hand, is a highly popular extension of Lasso that uses a linear combination of Lasso and ridge regression penalties. In this paper, we propose a new novel algorithm, called pathwise (PW-)LARS-EN, that is able to compute the EN knots over a grid of EN tuning parameter Î± values. The developed PW-LARS-EN algorithm decreases the EN tuning parameter and exploits the previously found knot values and the original LARS algorithm. A covariance test statistic for the Lasso is then generalized to the EN for testing the significance of the predictors. Our simulation studies validate the fact that the test statistic has an asymptotic Exp(1) distribution.",2017,2017 25th European Signal Processing Conference (EUSIPCO)
Vanilla Lasso for sparse classification under single index models,"This paper study sparse classification problems. We show that under single-index models, vanilla Lasso could give good estimate of unknown parameters. With this result, we see that even if the model is not linear, and even if the response is not continuous, we could still use vanilla Lasso to train classifiers. Simulations confirm that vanilla Lasso could be used to get a good estimation when data are generated from a logistic regression model.",2015,arXiv: Statistics Theory
Restricted Eigenvalue from Stable Rank with Applications to Sparse Linear Regression,"High-dimensional settings, where the data dimension (d) far exceeds the number of observations (\(n\)), are common in many statistical and machine learning applications. Methods based on \(\ell_1\)-relaxation, such as Lasso, are very popular for sparse recovery in these settings. Restricted Eigenvalue (RE) condition is among the weakest, and hence the most general, condition in literature imposed on the Gram matrix that guarantees nice statistical properties for the Lasso estimator. It is natural to ask: what families of matrices satisfy the RE condition? Following a line of work in this area, we construct a new broad ensemble of dependent random design matrices that have an explicit RE bound. Our construction starts with a fixed (deterministic) matrix \(X \in \mathbb{R}^{n \times d}\) satisfying a simple stable rank condition, and we show that a matrix drawn from the distribution \(X \Phi^\top \Phi\), where \(\Phi \in \mathbb{R}^{m \times d}\) is a subgaussian random matrix, with high probability, satisfies the RE condition. This construction allows incorporating a fixed matrix that has an easily verifiable condition into the design process, and allows for generation of compressed design matrices that have a lower storage requirement than a standard design matrix. We give two applications of this construction to sparse linear regression problems, including one to a compressed sparse regression setting where the regression algorithm only has access to a compressed representation of a fixed design matrix \(X\).",2018,
On prediction with the LASSO when the design is not incoherent,"The LASSO estimator is an $\ell_1$-norm penalized least-squares estimator, which was introduced for variable selection in the linear model. When the design matrix satisfies, e.g. the Restricted Isometry Property, or has a small coherence index, the LASSO estimator has been proved to recover, with high probability, the support and sign pattern of sufficiently sparse regression vectors. Under similar assumptions, the LASSO satisfies adaptive prediction bounds in various norms. The present note provides a prediction bound based on a new index for measuring how favorable is a design matrix for the LASSO estimator. We study the behavior of our new index for matrices with independent random columns uniformly drawn on the unit sphere. Using the simple trick of appending such a random matrix (with the right number of columns) to a given design matrix, we show that a prediction bound similar to \cite[Theorem 2.1]{CandesPlan:AnnStat09} holds without any constraint on the design matrix, other than restricted non-singularity.",2012,arXiv: Statistics Theory
Serum microRNA-based prediction of responsiveness to eribulin in metastatic breast cancer,"The identification of biomarkers for predicting the responsiveness to eribulin in patients with metastatic breast cancer pretreated with an anthracycline and a taxane remains an unmet need. Here, we established a serum microRNA (miRNA)-based prediction model for the emergence of new distant metastases after eribulin treatment. Serum samples were collected from metastatic breast cancer patients prior to eribulin treatment and comprehensively evaluated by miRNA microarray. The prediction model for estimating eribulin efficacy was established using the logistic LASSO regression model. Serum samples were collected from 147 patients, of which 52 developed at least one new distant metastasis after eribulin monotherapy and 95 did not develop new distant metastases. A combination of eight serum miRNAs (miR-4483, miR-8089, miR-4755-3p, miR-296-3p, miR-575, miR-4710, miR-5698 and miR-3160-5p) predicted the appearance of new distant metastases with an area under the curve of 0.79, sensitivity of 0.69 and specificity of 0.82. The serum levels of miR-8089 and miR-5698 were significantly associated with overall survival after the initiation of eribulin treatment. The present study provides evidence that serum miRNA profiling may serve as a biomarker for the responsiveness to eribulin and for predicting the development of new distant metastases in metastatic breast cancer.",2019,PLoS ONE
Special Issue on INFORMS 2015 Annual Meeting,"E ach year, the Institute for Operations Research and the Management Sciences (INFORMS) Annual Meeting provides an excellent platform for researchers world-wide to present their state-of-the-art research in quality and reliability. The objective of this Special Issue is to introduce the recent advancements in quality and reliability methodological studies as well as applications in a form of full-length papers expanded from the presentations in the INFORMS 2015 Annual Meeting (November 1â€“4, Philadelphia, PA). We are very grateful for the contributions of the authors who submitted papers. After two to three rounds of rigorous reviews, 10 papers were accepted for publication. Among them, five papers focus on process monitoring and fault identification. The paper by Turkoz et al. proposes a new fault identification method when a high dimensional process is out-of-control. The proposed method combines the support vector data description-based test statistic with an adaptive step-down procedure to identify the faults. By using a nonparametric one-class classification method, the proposed approach does not rely on any distribution assumption. Compared with the existing distribution free methods, the proposed method has much more stable performance when the number of faults is more than one. The paper by Choe et al. focuses on the off-line change-point detection problem for time-series data. It adopts the Thresholded Least Absolute Shrinkage and Selection Operator (LASSO) techniques to control the false positives. The authors demonstrated the superior performance of the proposed method by comparing with several benchmark methods based on both simulations and a case study related to solar panel performance. The paper proposed by Zang, Wang, and Jin is handling with the issue of monitoring of processes based on unaligned profiles. While existing works focus on monitoring of well-aligned profiles, this paper develops new algorithms for monitoring unaligned profiles with varying sampling points. For this, they propose a robust dynamic time warping algorithm for profile alignment that are robust to noises and shift signals. And then, they propose a penalization-based charting algorithm that gives more effective performance in shift detection. In order to illustrate their proposed framework, they applied it for unaligned profile monitoring to the ingot growth process and monitoring heating power profiles. The paper by Abdella et al. proposes the double Exponentially Weighted Moving Average based procedure to evaluate the quality of a process based on polynomial quality profiles. The simulations studies have revealed the distinctive performance of their proposed double Exponentially Weighted Moving Average based control charts in quickly detecting changes in the second-order polynomial profiles. Their extensive simulation studies are based on two shift patterns in the polynomial quality profiles: changes in the coefficients of the regression parameters and changes in the process variability. Timely detection of whether a data stream reaches the steady state is critical in various fields. The paper by Hou, Wu, and Chen presents a new online steady state detection algorithm under the Bayesian framework based on a multiple change-point state space formulation and the sequential Monte Carlo methods. In order to reduce the variance of Monte Carlo estimation and enhance the computational efficiency, they propose a Rao-Blackwellization technique. Both artificially simulated signals and a real data example from the ultrasonic-cavitation based nanoparticle dispersion process are used to demonstrate the robustness of their proposed algorithms for various types of signals with different levels of noises. Five other papers address other important quality and reliability issues such as forecasting, maintenance, and robust design. The paper by Xiang and Coit proposes a multi-criteria optimization model to jointly minimize the burning and maintenance costs of a product with heterogeneous subpopulations. The preventive maintenance is allowed to be imperfect. Two papers in this special issue use hybrid models for forecasting. The paper by Xu et al. develops a hybrid Autoregressive integrated moving average-Linear regression (ARIMA-LR) method that combines the ARIMA model and the linear regression model in a sequential manner to capture both the seasonal trend and effects of predictors in time series data. Real-world case studies regarding two Chinese emergence departments are conducted to illustrate the forecasting performance of the developed method. The paper by Xin et al. proposes a hybrid model of singular spectrum analysis and support vector regression to predict failure time series. A stepwise grid search algorithm is used to find optimal tuning parameters in the hybrid model. The paper by Soh, Kim, and Yum develops a multivariate loss function approach to multi-characteristic robust design problems with an appropriately defined signal-to-noise (SN) ratio. Existing methods do not properly consider the varianceâ€“covariance structures among performance characteristics and/or do not preserve the original properties of the Taguchi SN ratio. The paper by Zhang, Yang, and Xin proposes a semi-parametric microstructure modeling method that can capture the variation across different microstructure samples. Existing methods only consider a single microstructure sample, while the unit-to-unit variability among different samples is ignored. Their proposed model can be used for both isotropic materials, and anisotropic materials in which the microstructure properties in the vertical direction and those in the horizontal direction are different. They illustrate their proposed methods by applying them for the quality control of real-life dual phase steels.",2016,Quality and Reliability Eng. Int.
Joint variable selection and network modeling for detecting eQTLs,"Abstract In this study, we conduct a comparison of three most recent statistical methods for joint variable selection and covariance estimation with application of detecting expression quantitative trait loci (eQTL) and gene network estimation, and introduce a new hierarchical Bayesian method to be included in the comparison. Unlike the traditional univariate regression approach in eQTL, all four methods correlate phenotypes and genotypes by multivariate regression models that incorporate the dependence information among phenotypes, and use Bayesian multiplicity adjustment to avoid multiple testing burdens raised by traditional multiple testing correction methods. We presented the performance of three methods (MSSL â€“ Multivariate Spike and Slab Lasso, SSUR â€“ Sparse Seemingly Unrelated Bayesian Regression, and OBFBF â€“ Objective Bayes Fractional Bayes Factor), along with the proposed, JDAG (Joint estimation via a Gaussian Directed Acyclic Graph model) method through simulation experiments, and publicly available HapMap real data, taking asthma as an example. Compared with existing methods, JDAG identified networks with higher sensitivity and specificity under row-wise sparse settings. JDAG requires less execution in small-to-moderate dimensions, but is not currently applicable to high dimensional data. The eQTL analysis in asthma data showed a number of known gene regulations such as STARD3, IKZF3 and PGAP3, all reported in asthma studies. The code of the proposed method is freely available at GitHub (https://github.com/xuan-cao/Joint-estimation-for-eQTL).",2020,Statistical Applications in Genetics and Molecular Biology
Sparse Regression Algorithm for Activity Estimation in $\gamma$ Spectrometry,"We consider the counting rate estimation of an unknown radioactive source, which emits photons at times modeled by an homogeneous Poisson process. A spectrometer converts the energy of incoming photons into electrical pulses, whose number provides a rough estimate of the intensity of the Poisson process. When the activity of the source is high, a physical phenomenon known as pileup effect distorts direct measurements, resulting in a significant bias to the standard estimators of the source activities used so far in the field. We show in this paper that the problem of counting rate estimation can be interpreted as a sparse regression problem. We suggest a post-processed, non-negative, version of the Least Absolute Shrinkage and Selection Operator (LASSO) to estimate the photon arrival times. The main difficulty in this problem is that no theoretical conditions can guarantee consistency in sparsity of LASSO, because the dictionary is not ideal and the signal is sampled. We therefore derive theoretical conditions and bounds which illustrate that the proposed method can none the less provide a good, close to the best attainable, estimate of the counting rate activity. The good performances of the proposed approach are studied on simulations and real datasets.",2010,IEEE Transactions on Signal Processing
-1 - Neighborhood Properties Are Important Determinants of 1 Temperature Sensitive Mutations 2 3 4 5,"20 Temperature-sensitive (TS) mutants are powerful tools to study gene function in 21 vivo. These mutants exhibit wild-type activity at permissive temperatures and reduced 22 activity at restrictive temperatures. Although random mutagenesis can be used to 23 generate TS mutants, the procedure is laborious and unfeasible in multicellular 24 organisms. Further, the underlying molecular mechanisms of the TS phenotype are 25 poorly understood. To elucidate TS mechanisms, we used a machine learning 26 methodï£§logistic regressionï£§to investigate a large number of sequence and structure 27 features. We developed and tested 133 features, describing properties of either the 28 mutation site or the mutation site neighborhood. We defined three types of 29 neighborhood using sequence distance, Euclidean distance, and topological distance. 30 We discovered that neighborhood features outperformed mutation site features in 31 predicting TS mutations. The most predictive features suggest that TS mutations tend 32 to occur at buried and rigid residues, and are located at conserved protein domains. 33 The environment of a buried residue often determines the overall structural stability of 34 a protein, thus may lead to reversible activity change upon temperature switch. We 35 developed TS prediction models based on logistic regression and the Lasso 36 regularized procedure. Through a tenfold cross-validation, we obtained the area 37 under the curve of 0.91 for the model using both sequence and structure features. 38 Testing on independent datasets suggested that the model predicted TS mutations with 39 a 50% precision. In summary, our study elucidated the mechanism of TS mutants and 40 suggested the importance of neighborhood properties in determining TS mutations. 41 We further developed models to predict TS mutations derived from single amino acid 42 substitutions. In this way, TS mutants can be efficiently obtained through 43 experimentally introducing the predicted mutations. 44 45-3",2011,
Cmar_a_186914 8731..8741,"*These authors contributed equally to this work Purpose: In recent years, there has been an increase in the incidence of small renal masses (SRMs) and nephrectomy was the standard management of this disease in the past. Currently, the use of active surveillance has been recommended as an alternative option in the case of some patients with SRMs due to its heterogenicity. However, limited studies focused on the regarding risk stratification. Therefore, in the current study, we developed a nomogram for the purpose of predicting the presence of high-grade SRMs on the basis of the patient information provided (clinical information, hematological indicators, and CT imaging data). Patients and methods: A total of 329 patients (consisting of development and validation cohort) who had undergone nephrectomy for SRMs between January 2013 and May 2016 retrospectively were recruited for the present study. All preoperative information, including clinical predictors, hematological indicators, and CT predictors, were obtained. Lasso regression model was used for data dimension reduction and feature selection. Multivariable logistic regression analysis was applied for the establishment of the predicting model. The performance of the nomogram was assessed with respect to its calibration and discrimination properties and externally validated. Results: The predictors used in the assessment of the nomogram included tumor size, CT tumor contour, CT necrosis, CT tumor exophytic properties, and CT collecting system oppression. Based on these parameters, the nomogram was evaluated to have an effective discrimination and calibration ability, and the C-index was found to be 0.883 after internal validation and 0.887 following external validation. Conclusion: Based on the aforementioned findings, it can be concluded that CT imagingâ€“ based preoperative nomogram is an effective predictor of SRMs and hence can be used in the preoperative evaluation of SRMs, due to its calibration and discrimination abilities.",2019,
"Investigating the association between stress. Anxiety and geophagy among pregnant women in mwanza, Tanzania","Geophagy, the craving and intentional consumption of soil, is common especially among pregnant women in some low- and middle-income settings. Soils may contain a variety of non-nutritive components such as heavy metals and microbes or substances that interfere with gastrointestinal absorptive processes, posing health risks to pregnant women. Several hypotheses regarding the practice have been proposed but very few have examined the role of maternal stress. The practice of geophagy may help to alleviate stress or anxiety during gestation from perceived dietary or other pregnancy-related concerns. In this study, we evaluated several measures of maternal stress (general anxiety, Pregnancy-Related Anxiety Scores (10-item revised) and Perceived Stress Scores) and other covariates in relation to geophagic behaviour in early pregnancy in 227 women (12-19 weeks gestation) recruited from two hospitals in the Nyamagana district of Mwanza City, Tanzania. Geophagy was reported by 24.7% of the pregnant women. LASSO regression identified the self-reported treatment of nausea or vomiting during pregnancy (adjusted ORâ€¯=â€¯3.12, 95%CI: 1.43 to 6.83), paternal education level (adjusted ORâ€¯=â€¯2.79, 95%CI: 1.32 to 5.87 for primary or lower education level), antenatal hospital site (adjusted ORâ€¯=â€¯3.71, 95%CI: 1.78 to 7.75), prescription drug use prior to pregnancy (adjusted ORâ€¯=â€¯1.76, 95%CI: 0.87 to 3.56) and general anxiety (feeling worried, tense or anxious in the past four weeks) (adjusted ORâ€¯=â€¯1.81, 95%CI: 0.88 to 3.72) as predictors of geophagic behaviour. Given that relatively little has been done to examine geophagy in relation to the public health risk it may pose to pregnant women, these findings suggest the need for further investigations regarding maternal stress.",2019,Appetite
Tissue-guided LASSO for prediction of clinical drug response using preclinical samples,"Prediction of clinical drug response (CDR) of cancer patients, based on their clinical and molecular profiles obtained prior to administration of the drug, can play a significant role in individualized medicine. Machine learning models have the potential to address this issue but training them requires data from a large number of patients treated with each drug, limiting their feasibility. While large databases of drug response and molecular profiles of preclinical in-vitro cancer cell lines (CCLs) exist for many drugs, it is unclear whether preclinical samples can be used to predict CDR of real patients. We designed a systematic approach to evaluate how well different algorithms, trained on gene expression and drug response of CCLs, can predict CDR of patients. Using data from two large databases, we evaluated various linear and non-linear algorithms, some of which utilized information on gene interactions. Then, we developed a new algorithm called TG-LASSO that explicitly integrates information on samples' tissue of origin with gene expression profiles to improve prediction performance. Our results showed that regularized regression methods provide better prediction performance. However, including the network information or common methods of including information on the tissue of origin did not improve the results. On the other hand, TG-LASSO improved the predictions and distinguished resistant and sensitive patients for 7 out of 13 drugs. Additionally, TG-LASSO identified genes associated with the drug response, including known targets and pathways involved in the drugs' mechanism of action. Moreover, genes identified by TG-LASSO for multiple drugs in a tissue were associated with patient survival. In summary, our analysis suggests that preclinical samples can be used to predict CDR of patients and identify biomarkers of drug sensitivity and survival.",2020,PLoS Computational Biology
Comparison of learning-based wastewater flow prediction methodologies for smart sewer management,"Abstract Situational awareness in sanitary sewer systems requires accurate flow information at different spatial locations in a city. It is especially desirable to predict flows across a wastewater network in response to heavy rainfall events in addition to regular consumption patterns. Typically, complicated hydraulic models that suffer from difficulties in parameter identification and high computational burden are used for flow prediction. Recently, data-driven approaches have been employed for flow prediction. In this paper, we first design and then compare the performance of three data-driven methods to predict flow: (1) Artificial Neural Network (ANN); (2) Long-Short Term Memory (LSTM); (3) Least Absolute Shrinkage and Selection Operator (LASSO). We test the performance of these machine and statistical learning techniques using data gathered from the City of Springfield. While, all three data-driven methodologies provide acceptable prediction performance, we observe that LSTM outperforms ANN due to the inherent memory integrated with a feedback structure. The statistical learning approach, i.e., LASSO regression not only offers good prediction performance, but also helps identify the key spatial and temporal features that influence flow at any specific location. This added information could aid in remediation activities. Another key contribution of this paper is that we quantify the value of groundwater data in flow prediction. Specifically, including groundwater data as an additional input enhances flow prediction performance in all three methods. Lastly, to better predict flows corresponding to rare events (e.g., 50 or 100 year rainfall events) we use a resampling approach (known as SmoteR) to modify the training dataset. Simulation results indicate that the resampling technique is effective in improving prediction performance.",2019,Journal of Hydrology
ClustOfVar : an R package for dimension reduction via clustering of variables. Application in supervised classification and variable selection in gene expressions data,"The main goal of this work is to tackle the problem of dimension reduction for high-dimensional supervised classication. The motivation is to handle gene expression data. The proposed method works in 2 steps. First, one eliminates redundancy using clustering of variables, based on the R-package ClustOfVar. This first step is only based on the exploratory variables (genes). Second, the synthetic variables (summarizing the clusters obtained at the first step) are used to construct a classifier (e.g. logistic regression, LDA, random forests). We stress that the first step reduces the dimension and gives linear combinations of original variables (synthetic variables). This step can be considered as an alternative to PCA. A selection of predictors (synthetic variables) in the second step gives a set of relevant original variables (genes). Numerical performances of the proposed procedure are evaluated on gene expression datasets. We compare our methodology with LASSO and sparse PLS discriminant analysis on these datasets.",2013,
Linear regression in the Bayesian framework,"These notes aim at clarifying different strategies to perform linear regression from given dataset. Methods like the weighted and ordinary least squares, ridge regression or LASSO are proposed in the literature. The present article is my understanding of these methods which are, according to me, better unified in the Bayesian framework. The formulas to address linear regression with these methods are derived. The KIC for model selection is also derived in the end of the document.",2019,arXiv: Methodology
18F-FDG PET image biomarkers improve prediction of late radiation-induced xerostomia.,"BACKGROUND AND PURPOSE
Current prediction of radiation-induced xerostomia 12months after radiotherapy (Xer12m) is based on mean parotid gland dose and baseline xerostomia (Xerbaseline) scores. The hypothesis of this study was that prediction of Xer12m is improved with patient-specific characteristics extracted from 18F-FDG PET images, quantified in PET image biomarkers (PET-IBMs).


PATIENTS AND METHODS
Intensity and textural PET-IBMs of the parotid gland were collected from pre-treatment 18F-FDG PET images of 161 head and neck cancer patients. Patient-rated toxicity was prospectively collected. Multivariable logistic regression models resulting from step-wise forward selection and Lasso regularisation were internally validated by bootstrapping. The reference model with parotid gland dose and Xerbaseline was compared with the resulting PET-IBM models.


RESULTS
High values of the intensity PET-IBM (90th percentile (P90)) and textural PET-IBM (Long Run High Grey-level Emphasis 3 (LRHG3E)) were significantly associated with lower risk of Xer12m. Both PET-IBMs significantly added in the prediction of Xer12m to the reference model. The AUC increased from 0.73 (0.65-0.81) (reference model) to 0.77 (0.70-0.84) (P90) and 0.77 (0.69-0.84) (LRHG3E).


CONCLUSION
Prediction of Xer12m was significantly improved with pre-treatment PET-IBMs, indicating that high metabolic parotid gland activity is associated with lower risk of developing late xerostomia. This study highlights the potential of incorporating patient-specific PET-derived functional characteristics into NTCP model development.",2018,Radiotherapy and oncology : journal of the European Society for Therapeutic Radiology and Oncology
A simulation based method for assessing the statistical significance of logistic regression models after common variable selection procedures,"ABSTRACT Classification models can demonstrate apparent prediction accuracy even when there is no underlying relationship between the predictors and the response. Variable selection procedures can lead to false positive variable selections and overestimation of true model performance. A simulation study was conducted using logistic regression with forward stepwise, best subsets, and LASSO variable selection methods with varying total sample sizes (20, 50, 100, 200) and numbers of random noise predictor variables (3, 5, 10, 15, 20, 50). Using our critical values can help reduce needless follow-up on variables having no true association with the outcome.",2017,Communications in Statistics - Simulation and Computation
A permutation approach for selecting the penalty parameter in penalized model selection.,"We describe a simple, computationally efficient, permutation-based procedure for selecting the penalty parameter in LASSO-penalized regression. The procedure, permutation selection, is intended for applications where variable selection is the primary focus, and can be applied in a variety of structural settings, including that of generalized linear models. We briefly discuss connections between permutation selection and existing theory for the LASSO. In addition, we present a simulation study and an analysis of real biomedical data sets in which permutation selection is compared with selection based on the following: cross-validation (CV), the Bayesian information criterion (BIC), scaled sparse linear regression, and a selection method based on recently developed testing procedures for the LASSO.",2015,Biometrics
Experimental study and Random Forest prediction model of microbiome cell surface hydrophobicity,"Experimental study and prediction model of microbiome cell surface hydrophobicity.Expected Measurement Moving Average Machine Learning model to predict CSH.Random Forest prediction model with 12 features and test R-squared of 0.992. The cell surface hydrophobicity (CSH) is an assessable physicochemical property used to evaluate the microbial adhesion to the surface of biomaterials, which is an essential step in the microbial biofilm formation and pathogenesis. For the present in vitro fermentation experiment, the CSH of ruminal mixed microbes was considered, along with other data records of pH, ammonia-nitrogen concentration, and neutral detergent fibre digestibility, conditions of surface tension and specific surface area in two different time scales. A dataset of 170,707 perturbations of input variables, grouped into two blocks of data, was constructed. Next, Expected Measurement Moving Average Machine Learning (EMMA-ML) models were developed in order to predict CSH after perturbations of all input variables. EMMA-ML is a Perturbation Theory method that combines the ideas of Expected Measurement, Box-Jenkins Operators/Moving Average, and Time Series Analysis. Seven regression methods have been tested: Multiple Linear regression, Generalized Linear Model with Stepwise Feature Selection, Partial Least Squares regression, Lasso regression, Elastic Net regression, Neural Networks regression, and Random Forests (RF). The best regression performance has been obtained with RF (EMMA-RF model) with an R-squared of 0.992. The model analysis has shown that CSH values were highly dependent on the in vitro fermentation parameters of detergent fibre digestibility, ammonia nitrogen concentration, and the expected values of cell surface hydrophobicity in the first time scale.",2017,Expert Syst. Appl.
