title,abstract,year,journal
A Fast Algorithm for Detecting Gene-gene Interactions in Genome-wide Association Studies.,"With the recent advent of high-throughput genotyping techniques, genetic data for genome-wide association studies (GWAS) have become increasingly available, which entails the development of efficient and effective statistical approaches. Although many such approaches have been developed and used to identify single-nucleotide polymorphisms (SNPs) that are associated with complex traits or diseases, few are able to detect gene-gene interactions among different SNPs. Genetic interactions, also known as epistasis, have been recognized to play a pivotal role in contributing to the genetic variation of phenotypic traits. However, because of an extremely large number of SNP-SNP combinations in GWAS, the model dimensionality can quickly become so overwhelming that no prevailing variable selection methods are capable of handling this problem. In this paper, we present a statistical framework for characterizing main genetic effects and epistatic interactions in a GWAS study. Specifically, we first propose a two-stage sure independence screening (TS-SIS) procedure and generate a pool of candidate SNPs and interactions, which serve as predictors to explain and predict the phenotypes of a complex trait. We also propose a rates adjusted thresholding estimation (RATE) approach to determine the size of the reduced model selected by an independence screening. Regularization regression methods, such as LASSO or SCAD, are then applied to further identify important genetic effects. Simulation studies show that the TS-SIS procedure is computationally efficient and has an outstanding finite sample performance in selecting potential SNPs as well as gene-gene interactions. We apply the proposed framework to analyze an ultrahigh-dimensional GWAS data set from the Framingham Heart Study, and select 23 active SNPs and 24 active epistatic interactions for the body mass index variation. It shows the capability of our procedure to resolve the complexity of genetic control.",2014,The annals of applied statistics
Machine Learning of Lithium Diffusivity Dataset by Molecular Dynamics Simulations for Inorganic Oxide Crystals,"Toward development of next-generation all solid rechargeable batteries, it is essential to improve lithium ion conductivity of solid electrolytes. Attempts have been made to discover good lithium ionic conductor for decades. However, most of good solid-state conductors are non-oxides, mainly sulfides. No oxides are known which have sufficient ionic conductivity and wide electric window for practical use [1]. It seems very difficult to explore good oxide-based lithium ion conductors just by conventional trials-and-error ways from huge variety of chemical composition and structures. It should therefore be useful to construct a prediction model of ionic diffusivity from fundamental information of the compounds, such as chemical composition and crystal structure. In the group of the present authorsâ€™, theoretical prediction of the lithium conductivity of the LISICON type oxides was systematically made by combining a large set of density functional theory (DFT) calculations and machine learning of experimental data [2]. In the present study, we aim to expand the chemical compositional and structural space of target materials. We use empirical-potential molecular-dynamics (MD) simulations for various oxides in order to construct a database of lithium ion diffusivity (DLi), which is then applied to machine learning to construct a prediction model. Firstly, 797 ternary and quaternary compounds containing lithium and oxygen were taken from ICSD database (FIZ Karlsruhe). MD simulations were performed using Buckingham-type potentials implemented in the GULP code [3]. Simulation temperature was as high as 1600 K in order to reduce any atomic ordering effects. Time step was 5 fs and the total number of steps was 200,000 (1ns). Diffusion coefficient of each ion was then calculated. Some compounds were excluded from the dataset when diffusivity of other ions than lithium was noticed. We found that many of well-known lithium ionic conductors, for example Li0.62La1.12TiO3 (LLTO) and Li7La3Zr2O12 (garnet type) [1], show high DLi(1600K) by the present calculations. Secondary, we apply machine learning technique to all data in order to construct a prediction model. Logarithm of the diffusion coefficient, log DLi(1600K), was used as the target variable. Two types of descriptor set are considered. One contains variables related to crystalline density and composition, for example volume per atom and fraction of lithium. The other contains variables generated from partial radial distribution functions in addition to the former descriptor set. LASSO (least absolute shrinkage and selection operator) [4] is chosen to construct the prediction model. Using LASSO, descriptors can be reasonably selected. Figure 1 compares regression results for two descriptor sets. Root mean square (RMS) error is 0.27 for log DLi(1600K) with the second model, while it is 0.41 with the first model. This means the inclusion of the structural information notably improves the quality of the model.",2018,
Chemometrics and direct injection analysis of volatile compounds by PTR-TOF-MS: a tool for metabolomic investigations,"Sample classification and â€œexplanatoryâ€ variable selection is a cutting edge problem in metabolomics. Among direct injection methods for the detection of volatile compounds, Proton transfer reaction-mass spectrometry (PTR-MS) is becoming more and more spread, especially after the coupling with time of flight detectors (PTR-TOF-MS). PTR-MS was proposed almost two decades ago for the rapid and high sensitivity monitoring of volatile compounds. It was immediately evident that food science and technology was one of its most interesting field of application given the role that volatile compounds play in food production, storage and consumption. We show that modern chemometric and data mining techniques, such as Random Forest, Partial Discriminant Analysis, Support Vector Machine, are well suited for addressing multiclass problems starting from fruit flavour profiles (by GC-MS) or fingerprints (by PTR-TOF-MS). Marker identification is successfully performed by recursive strategies such as Random Forest Recursive Feature Elimination. Moreover, regression methods, for instance LASSO and PLS, proved to be useful to link headspace, nose-space and sensory data from different analysis techniques. We present results from metabolomic studies by GC-MS and PTR-TOF-MS on i) raspberries: several cultivars having diverse levels of Botrytis susceptibility have been classified by the mentioned chemometric strategies and markers of Botrytis resistance have been identified; ii) apple cultivars and clones: markers for the discrimination of the apple clones based on their flavour profile have been identified; iii) grana cheeses and olive oils: we investigate the link between GC-MS profiles and PTR-TOF-MS fingerprint. Through these examples we will discuss the characteristics of the proposed strategy and show that it can provide a powerful tool for metabolomic.",2012,
Application of Multi-task Lasso Regression in the Parametrization of Stellar Spectraâ˜†â˜†â˜†,"The multi-task learning approaches have attracted the increasing attention in the ï¬elds of machine learning, computer vision, and artiï¬cial intelligence. By utilizing the correlations in tasks, learning multiple related tasks simultaneously is better than learning each task independently. An efficient multi-task Lasso (Least Absolute Shrinkage Selection and Operator) regression algorithm is proposed in this paper to estimate the physical parameters of stellar spectra. It not only can obtain the information about the common features of the different physical parameters, but also can preserve effectively their own peculiar features. Experiments were done based on the ELODIE synthetic spectral data simulated with the stellar atmospheric model, and on the SDSS data released by the American large-scale survey Sloan. The estimation precision of our model is better than those of the methods in the related literature, especially for the estimates of the gravitational acceleration (lg g) and the chemical abundance ([Fe/H]). In the experiments we changed the spectral resolution, and applied the noises with different signal-to-noise ratios (SNRs) to the spectral data, so as to illustrate the stability of the model. The results show that the model is inï¬‚uenced by both the resolution and the noise. But the inï¬‚uence of the noise is larger than that of the resolution. In general, the multi-task Lasso regression algorithm is easy to operate, it has a strong stability, and can also improve the overall prediction accuracy of the model.",2015,Chinese Astronomy and Astrophysics
PENALIZED SELECTION OF VARIABLE CONTRIBUTING TO ENHANCED SEED YIELD IN MUNGBEAN (Vigna radiata L.),"Penalized regression methods for simultaneous variable selection and coefficient estimation have received a great deal of attention in recent years. Especially those based on the least absolute shrinkage and selection operator (LASSO), that involves penalizing the absolute size of the regression coefficients. The ordinary least square and LASSO methods were used for selection of most significant traits contributing towards seed yield in mungbean plants with 18 morphological and yield associated traits and to develop the prediction model . Bayesian information criterion was applied to choose minimum tuning parameter. Results indicated that dry weight biomass and harvest index were highly significant characters towards seed yield while days to maturity, days to flowering, number of nodes per plant, pods per plant and degree of indetermination had a significant affect on response variable. Based on the results, it was rational to conclude that high yield of mungbean crop could be obtained by selecting the breading materials with these important characters on seed yield.",2014,Pakistan Journal of Agricultural Sciences
Signal Enhancement of Cadmium in Lettuce Using Laser-Induced Breakdown Spectroscopy Combined with Pyrolysis Process,"Fast detection of heavy metals in lettuce is significant for food market regulation and the control of heavy metal pollution. Advanced methods like laser-induced breakdown spectroscopy (LIBS) technology have been tried to determine the cadmium (Cd) content. To retard the negative effect of complex matrix composition from samples and improve quantitative performance of LIBS technology, the pyrolysis process combined with LIBS was adopted to determine the cadmium (Cd) content of lettuce. Adaptive iteratively reweighted penalized least squares (airPLS) was used to preprocess the LIBS spectra and solve the baseline drift. For multivariate linear regression based on the three selected Cd emission lines correlation coefficient in the prediction set Rp2 increased from 0.9154 to 0.9969, and the limit of detection (LOD) decreased from 9.1 mg/kg to 0.9 mg/kg after the pyrolysis process. The partial least squares (PLS) regression and support vector regression (SVR) were applied to construct calibration models based on full spectra. In addition, the least absolute shrinkage and selection operator (LASSO) was implemented to choose limited lines to predict the Cd content. The PLS model with the pyrolysis process obtained the best results with Rp2 = 0.9973 and LOD = 0.8 mg/kg. The results indicated that the pyrolysis method could enhance the spectral signal of cadmium and thus significantly improve the analysis results for all the models. It is shown in this experiment that proper sample preprocessing could effectively amplify the Cd signal in LIBS and make LIBS measurement an efficient method to assess Cd contamination in the vegetable industry.",2019,Molecules
2 Logistic Group Lasso 2 . 1 Model Setup,"The Group Lasso is an extension of the Lasso to do variable selection on (predefined) groups of variables in linear regression models. The estimates have the attractive property of being invariant under groupwise orthogonal reparametrizations. We extend the Group Lasso to logistic regression models and present an efficient algorithm, especially suitable for high-dimensional problems, which can also be applied to generalized linear models to solve the corresponding convex optimization problem. The Group Lasso estimator for logistic regression is shown to be statistically consistent even if the number of predictors is much larger than sample size but with sparse true underlying structure. We further use a two-stage procedure which aims for sparser models than the Group Lasso, leading to improved prediction performance for some cases. Moreover, due to the two-stage nature, the estimates can be constructed to be hierarchical. The methods are used on simulated and real datasets about splice site detection in DNA sequences.",2007,
A further analysis of robust regression modeling and data mining corrections testing in global stocks,"In this analysis of the risk and return of stocks in global markets, we build a reasonably large number of stock selection models and create optimized portfolios to outperform a global benchmark. We apply robust regression techniques, LAR regression, and LASSO regression modeling to estimate stock selection models. Markowitz-based optimization techniques is used in portfolio construction within a global stock universe. We apply the Markowitzâ€“Xu data mining corrections test to a global stock universe. We find that (1) robust regression applications are appropriate for modeling stock returns in global markets; (2) weighted latent root regression robust regression techniques work as well as LAR and LASSO-Regressions in building effective stock selection models; (3) meanâ€“variance techniques continue to produce portfolios capable of generating excess returns above transactions costs; and (4) our models pass several data mining tests such that regression models produce statistically significant asset selection for global stocks. Recent Sturdy-Regression modeling technique may offer the greatest potential for further research for statistically based stock selection modeling.",2020,Annals of Operations Research
Optimal decisions with multiple agents of varying performance,"In this dissertation, I look at four distinct systems that all embody a similar challenge to modeling complex scenarios from noisy multidimensional historical data. In many scenarios, it is important to form an opinion, make a prediction, implement a business decision, or make an investment based upon expected future system behavior. All systems embody an amount of uncertainty, and quantify- ing that uncertainty using statistical methods, allows for better decision making. Three distinct scenarios are discussed with novel application of statistical methods to best quantify the endogenous uncertainty and aid in optimal decision making. Two chapters focus on predicting the winners of a horse race, one on predicting movement of a stock index, and the fourth on molding response from an online advertising effort. The first horse racing chapter uses a hierarchical Bayesian approach to model- ing running speed, using a novel grouping of races into ""profiles"" and then pooling information across those profiles to improve predictive accuracy. The second horse racing chapter implements a novel conditional logistic regression that is modified by frailty parameter derived from winning payoff, and then regularized with a LASSO. High speed parallel algorithms, running on a GPU, were hand coded to optimize tuning LASSO parameters in rapid time. The chapter on stock index prediction explores the application of ensemble filters. I show how an ensemble of filters on individual member stocks is a better predictor of index direction than a filter directly on the index. The chapter on advertising explores how the clicks and sales from an Ad- Words campaign may be modeled with a re-paramaterized Beta distribution to better capture variance. Empirical data from a live campaign is studied, with a hierarchical Bayesian framework for brand features solved using a Metropolis within Gibbs algorithm.",2013,
"Soil Nutrient Detection for Precision Agriculture Using Handheld Laser-Induced Breakdown Spectroscopy (LIBS) and Multivariate Regression Methods (PLSR, Lasso and GPR)","Precision agriculture (PA) strongly relies on spatially differentiated sensor information. Handheld instruments based on laser-induced breakdown spectroscopy (LIBS) are a promising sensor technique for the in-field determination of various soil parameters. In this work, the potential of handheld LIBS for the determination of the total mass fractions of the major nutrients Ca, K, Mg, N, P and the trace nutrients Mn, Fe was evaluated. Additionally, other soil parameters, such as humus content, soil pH value and plant available P content, were determined. Since the quantification of nutrients by LIBS depends strongly on the soil matrix, various multivariate regression methods were used for calibration and prediction. These include partial least squares regression (PLSR), least absolute shrinkage and selection operator regression (Lasso), and Gaussian process regression (GPR). The best prediction results were obtained for Ca, K, Mg and Fe. The coefficients of determination obtained for other nutrients were smaller. This is due to much lower concentrations in the case of Mn, while the low number of lines and very weak intensities are the reason for the deviation of N and P. Soil parameters that are not directly related to one element, such as pH, could also be predicted. Lasso and GPR yielded slightly better results than PLSR. Additionally, several methods of data pretreatment were investigated.",2020,"Sensors (Basel, Switzerland)"
Inferring regulatory networks using a hierarchical Bayesian graphical Gaussian model,"In this paper, we propose a new formalism based on graphical Gaussian model (GGM) to infer genetic regulatory networks. A hierarchical Bayesian prior for the precision matrix of the GGM is introduced to impose a bias toward sparse graph structure. We show that the MAP estimation of the undirected graph can be readily obtained by a variant of the well-known Lasso regression algorithm. Then we integrate the estimated graph with the â€œCHIp-Chipâ€ protein-binding location data to infer the regulatory networks using a post-processing algorithm. Compared to extant Bayesian network (BN) models for similar tasks, our formalism captures statistical dependencies among genes that are more prevalent and plausible in the biological system. Our approach is also capable of modeling partial correlations between mRNA levels and therefore goes beyond clustering-based approaches. We applied our method to an expression microarray data (more than 6000 genes) together with a genome-wide location analysis data (more than 100 TFs). Evaluated on the consistency with the GO annotations, our method achieves a significantly better performance than clustering and BN learning algorithms in discovering genetic regulatory modules.",2005,
A generalized Dantzig selector with shrinkage tuning,"The Dantzig selector performs variable selection and model fitting in linear regression. It uses an L 1 penalty to shrink the regression coefficients towards zero, in a similar fashion to the lasso. While both the lasso and Dantzig selector potentially do a good job of selecting the correct variables, they tend to overshrink the final coefficients. This results in an unfortunate trade-off. One can either select a high shrinkage tuning parameter that produces an accurate model but poor coefficient estimates or a low shrinkage parameter that produces more accurate coefficients but includes many irrelevant variables. We extend the Dantzig selector to fit generalized linear models while eliminating overshrinkage of the coefficient estimates, and develop a computationally efficient algorithm, similar in nature to least angle regression, to compute the entire path of coefficient estimates. A simulation study illustrates the advantages of our approach relative to others. We apply the methodology to two datasets. Copyright 2009, Oxford University Press.",2009,Biometrika
Sparse Oracle Inequalities for Variable Selection via Regularized Quantization,"We give oracle inequalities on procedures which combines quantization and variable selection via a weighted Lasso $k$-means type algorithm. The results are derived for a general family of weights, which can be tuned to size the influence of the variables in different ways. Moreover, these theoretical guarantees are proved to adapt the corresponding sparsity of the optimal codebooks, if appropriate. Even if there is no sparsity assumption on the optimal codebooks, our procedure is proved to be close to a sparse approximation of the optimal codebooks, as has been done for the Generalized Linear Models in regression. If the optimal codebooks have a sparse support, we also show that this support can be asymptotically recovered, giving an asymptotic upper bound on the probability of misclassification. These results are illustrated with Gaussian mixture models in arbitrary dimension with sparsity assumptions on the means, which are standard distributions in model-based clustering.",2014,arXiv: Statistics Theory
A differential-geometric approach to generalized linear models with grouped predictors,"We propose an extension of the differential-geometric least angle regression method to perform sparse group inference in a generalized linear model. An efficient algorithm is proposed to compute the solution curve. The proposed group differential-geometric least angle regression method has important properties that distinguish it from the group lasso. First, its solution curve is based on the invariance properties of a generalized linear model. Second, it adds groups of variables based on a group equiangularity condition, which is shown to be related to score statistics. An adaptive version, which includes weights based on the Kullbackâ€“Leibler divergence, improves its variable selection features and is shown to have oracle properties when the number of predictors is fixed.",2016,Biometrika
High-dimensional regression with unknown variance,"We review recent results for high-dimensional sparse linear re- gression in the practical case of unknown variance. Different sparsity settings are covered, including coordinate-sparsity, group-sparsity and variation- sparsity. The emphasis is put on nonasymptotic analyses and feasible pro- cedures. In addition, a small numerical study compares the practical perfor- mance of three schemes for tuning the lasso estimator and some references are collected for some more general models, including multivariate regres- sion and nonparametric regression.",2011,Statistical Science
Analyses and applications of optimization methods for complex network reconstruction,"Abstract Inferring the topology of a network from observable dynamics is a key topic in the research of complex network. With the observation error considered, the topology inferring is formulated as a connectivity reconstruction problem that can be solved through optimization estimation. It is found that the different optimization methods should be selected to deal with the different degrees of noise, different scales of observable time series and such other situations when it comes to the problem of connectivity reconstruction, which has not been analyzed and discussed before yet. In this paper, four regression methods, namely least squares, ridge, lasso and elastic net, are used to solve the problem of network reconstruction in different situations. In particular, a further analysis is made of the effects of each regression method on the network reconstruction problem in detail. Through simulation of a variety of artificial and real networks, as it has turned out, the four regression methods are effective in respect to network reconstruction when certain conditions are respectively satisfied. Based on the experimental results, it is possible to reach some interesting conclusions that can guide our readers to know the internal mechanisms for network reconstruction and choose the appropriate regression method in accordance with the actual situation and existing knowledge.",2020,Knowl. Based Syst.
Development of a statistical model for discrimination of rupture status in posterior communicating artery aneurysms,"BackgroundIntracranial aneurysms at the posterior communicating artery (PCOM) are known to have high rupture rates compared to other locations. We developed and internally validated a statistical model discriminating between ruptured and unruptured PCOM aneurysms based on hemodynamic and geometric parameters, angio-architectures, and patient age with the objective of its future use for aneurysm risk assessment.MethodsA total of 289 PCOM aneurysms in 272 patients modeled with image-based computational fluid dynamics (CFD) were used to construct statistical models using logistic group lasso regression. These models were evaluated with respect to discrimination power and goodness of fit using tenfold nested cross-validation and a split-sample approach to mimic external validation.ResultsThe final model retained maximum and minimum wall shear stress (WSS), mean parent artery WSS, maximum and minimum oscillatory shear index, shear concentration index, and aneurysm peak flow velocity, along with aneurysm height and width, bulge location, non-sphericity index, mean Gaussian curvature, angio-architecture type, and patient age. The corresponding area under the curve (AUC) was 0.8359. When omitting data from each of the three largest contributing hospitals in turn, and applying the corresponding model on the left-out data, the AUCs were 0.7507, 0.7081, and 0.5842, respectively.ConclusionsStatistical models based on a combination of patient age, angio-architecture, hemodynamics, and geometric characteristics can discriminate between ruptured and unruptured PCOM aneurysms with an AUC of 84%. It is important to include data from different hospitals to create models of aneurysm rupture that are valid across hospital populations.",2018,Acta Neurochirurgica
Inference of Gene Regulatory Networks from Genetic Perturbations with Linear Regression Model,"It is an effective strategy to use both genetic perturbation data and gene expression data to infer regulatory networks that aims to improve the detection accuracy of the regulatory relationships among genes. Based on both types of data, the genetic regulatory networks can be accurately modeled by Structural Equation Modeling (SEM). In this paper, a linear regression (LR) model is formulated based on the SEM, and a novel iterative scheme using Bayesian inference is proposed to estimate the parameters of the LR model (LRBI). Comparative evaluations of LRBI with other two algorithms, the Adaptive Lasso (AL-Based) and the Sparsity-aware Maximum Likelihood (SML), are also presented. Simulations show that LRBI has significantly better performance than AL-Based, and overperforms SML in terms of power of detection. Applying the LRBI algorithm to experimental data, we inferred the interactions in a network of 35 yeast genes. An open-source program of the LRBI algorithm is freely available upon request.",2013,PLoS ONE
Feature selection and validated predictive performance in the domain of Legionella pneumophila: a comparative study,"BackgroundGenetic comparisons of clinical and environmental Legionella strains form an essential part of outbreak investigations. DNA microarrays often comprise many DNA markers (features). Feature selection and the development of prediction models are particularly challenging in this domain with many variables and comparatively few subjects or data points. We aimed to compare modeling strategies to develop prediction models for classifying infections as clinical or environmental.MethodsWe applied a bootstrap strategy for preselecting important features to a database containing 222 Legionella pneumophila strains with 448 continuous markers and a dichotomous outcome (clinical or environmental). Feature selection was done with 50 bootstrap samples resulting in a top 10 of most important features for each of four modeling techniques: classification and regression trees (CART), random forests (RF), support vector machines (SVM) and least absolute shrinkage and selection operator (LASSO). Validation was done in a second bootstrap re-sampling loop (200Ã—) for evaluation of discriminatory model performance according to the AUC.ResultsThe top 5 of selected features differed considerably between the various modeling techniques, with only one common feature (â€œLePn.007B8â€). The mean validated AUC-values of the SVM model and the CART model were 0.859 and 0.873 respectively. The LASSO and the RF model showed higher validated AUC-values (0.925 and 0.975 respectively).ConclusionsIn the domain of Legionella pneumophila, which comprises many potential features for classifying of infections as clinical or environmental, the RF and LASSO techniques provide good prediction models. The identification of potentially biologically relevant features is highly dependent on the technique used, and should hence be interpreted with caution.",2016,BMC Research Notes
The Derivation and Validation of a Geriatric-Sensitive Perioperative Cardiac Risk Index (GSCRI),"Author(s): Alrezk, Rami | Advisor(s): Elashoff, David; Hu, Perry | Abstract: Surgical patients age 65 and over face a high risk of cardiac complications. The Revised Cardiac Risk Index (RCRI) and the Gupta Myocardial Infarction or Cardiac Arrest (MICA) are widely used to predict perioperative cardiac risk but they are not specifically designed to predict that risk in geriatric patients. The objective of this study is to develop and validate a geriatric-sensitive cardiac risk index (GSCRI). Three variables were selected using Lasso regression in the National Surgical Quality Improvement Program (NSQIP) 2013 with the addition of four clinically significant variables. The model was developed using the NSQIP 2013 geriatric cohort (N=485,426) (172,905 age â‰¥ 65) and validated on the NSQIP 2012 geriatric cohort (N=485,426) (210,914 age â‰¥ 65). The Area under the Curve (AUC) for the NSQIP 2012 geriatric cohort for three indices was compared. Gupta MICA had an AUC of 0.70 and the RCRI had an AUC of 0.63. Our GSCRI model showed better performance with an AUC of 0.76",2016,
The Benefit of Group Sparsity in Group Inference with De-biased Scaled Group Lasso,"We study confidence regions and approximate chi-squared tests for variable groups in high-dimensional linear regression. When the size of the group is small, low-dimensional projection estimators for individual coefficients can be directly used to construct efficient confidence regions and p-values for the group. However, the existing analyses of low-dimensional projection estimators do not directly carry through for chi-squared-based inference of a large group of variables without inflating the sample size by a factor of the group size. We propose to de-bias a scaled group Lasso for chi-squared-based statistical inference for potentially very large groups of variables. We prove that the proposed methods capture the benefit of group sparsity under proper conditions, for statistical inference of the noise level and variable groups, large and small. Such benefit is especially strong when the group size is large.",2014,arXiv: Statistics Theory
Bi-level variable selection via adaptive sparse group Lasso,"Penalization has been extensively adopted for variable selection in regression. In some applications, covariates have natural grouping structures, where those in the same group have correlated measurements or related functions. Under such settings, variable selection should be conducted at both the group-level and within-group-level, that is, a bi-level selection. In this study, we propose the adaptive sparse group Lasso (adSGL) method, which combines the adaptive Lasso and adaptive group Lasso (GL) to achieve bi-level selection. It can be viewed as an improved version of sparse group Lasso (SGL) and uses data-dependent weights to improve selection performance. For computation, a block coordinate descent algorithm is adopted. Simulation shows that adSGL has satisfactory performance in identifying both individual variables and groups and lower false discovery rate and mean square error than SGL and GL. We apply the proposed method to the analysis of a household healthcare expenditure data set.",2015,Journal of Statistical Computation and Simulation
Collaborative hierarchical sparse modeling,"Sparse modeling is a powerful framework for data analysis and processing. Traditionally, encoding in this framework is done by solving an â„“1-regularized linear regression problem, usually called Lasso. In this work we first combine the sparsity-inducing property of the Lasso model, at the individual feature level, with the block-sparsity property of the group Lasso model, where sparse groups of features are jointly encoded, obtaining a sparsity pattern hierarchically structured. This results in the hierarchical Lasso, which shows important practical modeling advantages. We then extend this approach to the collaborative case, where a set of simultaneously coded signals share the same sparsity pattern at the higher (group) level but not necessarily at the lower one. Signals then share the same active groups, or classes, but not necessarily the same active set. This is very well suited for applications such as source separation. An efficient optimization procedure, which guarantees convergence to the global optimum, is developed for these new models. The underlying presentation of the new framework and optimization approach is complemented with experimental examples and preliminary theoretical results.",2010,2010 44th Annual Conference on Information Sciences and Systems (CISS)
Development and validation of the immune signature to predict distant metastasis in patients with nasopharyngeal carcinoma.,"BACKGROUND
The tumor immune microenvironment has clinicopathological significance in predicting prognosis and therapeutic efficacy. We aimed to develop an immune signature to predict distant metastasis in patients with nasopharyngeal carcinoma (NPC).


METHODS
Using multiplexed quantitative fluorescence, we detected 17 immune biomarkers in a primary screening cohort of 54 NPC tissues presenting with/without distant metastasis following radical therapy. The LASSO (least absolute shrinkage and selection operator) logistic regression model used statistically significant survival markers in the training cohort (n=194) to build an immune signature. The prognostic and predictive accuracy of it was validated in an external independent group of 304 patients.


RESULTS
Eight statistically significant markers were identified in the screening cohort. The immune signature consisting of four immune markers (PD-L1+ CD163+, CXCR5, CD117) in intratumor was adopted to classify patients into high and low risk in the training cohort and it showed a high level of reproducibility between different batches of samples (r=0.988 for intratumor; p<0.0001). High-risk patients had shorter distant metastasis-free survival (HR 5.608, 95%â€‰CI 2.619 to 12.006; p<0.0001) and progression-free survival (HR 2.798, 95%â€‰CI 1.498 to 5.266; p=0Â·001). The C-indexes which reflected the predictive capacity in training and validation cohort were 0.703 and 0.636, respectively. Low-risk patients benefited from induction chemotherapy plus concurrent chemoradiotherapy (IC+CCRT) (HR 0.355, 95%â€‰CI 0.147 to 0.857; p=0Â·021), while high-risk patients did not (HR 1.329, 95% CI 0.543 to 3.253; p=0Â·533). To predict the individual risk of distant metastasis, nomograms with the integration of both immune signature and clinicopathological risk factors were developed.


CONCLUSIONS
The immune signature provided a reliable estimate of distant metastasis risk in patients with NPC and might be applied to identify the cohort which benefit from IC+CCRT.",2020,Journal for immunotherapy of cancer
Speaker adaptation based on regularized speaker-dependent eigenphone matrix estimation,"Eigenphone-based speaker adaptation outperforms conventional maximum likelihood linear regression (MLLR) and eigenvoice methods when there is sufficient adaptation data. However, it suffers from severe over-fitting when only a few seconds of adaptation data are provided. In this paper, various regularization methods are investigated to obtain a more robust speaker-dependent eigenphone matrix estimation. Element-wise l1 norm regularization (known as lasso) encourages the eigenphone matrix to be sparse, which reduces the number of effective free parameters and improves generalization. Squared l2 norm regularization promotes an element-wise shrinkage of the estimated matrix towards zero, thus alleviating over-fitting. Column-wise unsquared l2 norm regularization (known as group lasso) acts like the lasso at the column level, encouraging column sparsity in the eigenphone matrix, i.e., preferring an eigenphone matrix with many zero columns as solution. Each column corresponds to an eigenphone, which is a basis vector of the phone variation subspace. Thus, group lasso tries to prevent the dimensionality of the subspace from growing beyond what is necessary. For nonzero columns, group lasso acts like a squared l2 norm regularization with an adaptive weighting factor at the column level. Two combinations of these methods are also investigated, namely elastic net (applying l1 and squared l2 norms simultaneously) and sparse group lasso (applying l1 and column-wise unsquared l2 norms simultaneously). Furthermore, a simplified method for estimating the eigenphone matrix in case of diagonal covariance matrices is derived, and a unified framework for solving various regularized matrix estimation problems is presented. Experimental results show that these methods improve the adaptation performance substantially, especially when the amount of adaptation data is limited. The best results are obtained when using the sparse group lasso method, which combines the advantages of both the lasso and group lasso methods. Using speaker-adaptive training, performance can be further improved.",2014,"EURASIP Journal on Audio, Speech, and Music Processing"
