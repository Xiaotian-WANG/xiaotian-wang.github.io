title,abstract,year,journal
Evaluation of random forest regression for prediction of breeding value from genomewide SNPs,"Genomic prediction is meant for estimating the breeding value using molecular marker data which has turned out to be a powerful tool for efficient utilization of germplasm resources and rapid improvement of cultivars. Model-based techniques have been widely used for prediction of breeding values of genotypes from genomewide association studies. However, application of the random forest (RF), a model-free ensemble learning method, is not widely used for prediction. In this study, the optimum values of tuning parameters of RF have been identified and applied to predict the breeding value of genotypes based on genomewide single-nucleotide polymorphisms (SNPs), where the number of SNPs (P variables) is much higher than the number of genotypes (n observations) (P > > n). Further, a comparison was made with the model-based genomic prediction methods, namely, least absolute shrinkage and selection operator (LASSO), ridge regression (RR) and elastic net (EN) under P > > n. It was found that the correlations between the predicted and observed trait response were 0.591, 0.539, 0.431 and 0.587 for RF, LASSO, RR and EN, respectively, which implies superiority of the RF over the model-based techniques in genomic prediction. Hence, we suggest that the RF methodology can be used as an alternative to the model-based techniques for the prediction of breeding value at genome level with higher accuracy.",2015,Journal of Genetics
Sparse estimation based on a validation criterion,"A sparse estimator with close ties with the LASSO (least absolute shrinkage and selection operator) is analysed. The basic idea of the estimator is to relax the least-squares cost function to what the least-squares method would achieve on validation data and then use this as a constraint in the minimization of the â„“1-norm of the parameter vector. In a linear regression framework, exact conditions are established for when the estimator is consistent in probability and when it possesses sparseness. By adding a re-estimation step, where least-squares is used to re-estimate the non-zero elements of the parameter vector, the so called Oracle property can be obtained, i.e. the estimator achieves the asymptotic CramÃ©r-Rao lower bound corresponding to when it is known which regressors are active. The method is shown to perform favourably compared to other methods on a simulation example.",2011,IEEE Conference on Decision and Control and European Control Conference
Complete hazard ranking to analyze right-censored data: An ALS survival study,"Survival analysis represents an important outcome measure in clinical research and clinical trials; further, survival ranking may offer additional advantages in clinical trials. In this study, we developed GuanRank, a non-parametric ranking-based technique to transform patients' survival data into a linear space of hazard ranks. The transformation enables the utilization of machine learning base-learners including Gaussian process regression, Lasso, and random forest on survival data. The method was submitted to the DREAM Amyotrophic Lateral Sclerosis (ALS) Stratification Challenge. Ranked first place, the model gave more accurate ranking predictions on the PRO-ACT ALS dataset in comparison to Cox proportional hazard model. By utilizing right-censored data in its training process, the method demonstrated its state-of-the-art predictive power in ALS survival ranking. Its feature selection identified multiple important factors, some of which conflicts with previous studies.",2017,PLoS Computational Biology
Quantification of differences in resistance to gastrointestinal nematode infections in sheep using a multivariate blood parameter.,"Breeding for resistance to gastrointestinal nematodes (GIN) in sheep relies largely on the use of worm egg counts (WEC) to identify animals that are able to resist infection. As an alternative to such measures of parasite load we aimed to develop a method to identify animals showing resistance to GIN infection based on the impact of the infection on blood parameters. We hypothesized that blood parameters may provide a measure of infection level with a blood-feeding parasite through perturbation of red blood cell parameters due to feeding behaviour of the parasite, and white blood cell parameters through the mounting of an immune response in the host animal. We measured a set of blood parameters in 390 sheep that had been exposed to an artificial regime of repeated challenges with Trichostrongylus colubriformis followed by Haemonchus contortus. A simple analysis revealed strong relationships between single blood parameters and WECs with correlation coefficients -0.54 to -0.60. We then used more complex multi-variate methods based on supervised classifier models (including Bayesian Network) as well as regression models (Lasso and Elastic Net) to study the relationships between WECs and blood parameters, and derived algorithms describing the relationships. The ability of these algorithms to classify sheep GIN resistance status was tested using the WEC and blood parameters collected from a different group of 418 sheep that had acquired natural infections of H. contortus from pasture. We identified the most resistant and most susceptible animals (10% percentiles) of this group based on WECs, and then compared the identities of these animals to the identities of animals that were predicted to be most resistant and most susceptible by our algorithms. The models showed varying abilities to predict susceptible and resistant sheep, with up to 65% of the most susceptible animals and 30% of the most resistant animals identified by the Elastic Net model algorithms. The prediction algorithms derived from female sheep data performed better than those for male sheep in some cases, with the predicted animals accounting for up to 50-60% of the actual resistant and susceptible female animals. Heritability values were calculated for blood parameters and the aggregate trait descriptions defined by the novel prediction algorithms. The aggregate trait descriptions were moderately heritable and may therefore be suitable for use in genetic selection strategies. The present study indicates that multivariate models based on blood parameter data showed some ability to predict the resistance status of sheep to infection with H. contortus.",2019,Veterinary parasitology
Extensions of Morse-Smale Regression with Application to Actuarial Science,"The problem of subgroups is ubiquitous in scientific research (ex. disease heterogeneity, spatial distributions in ecology...), and piecewise regression is one way to deal with this phenomenon. Morse-Smale regression offers a way to partition the regression function based on level sets of a defined function and that function's basins of attraction. This topologically-based piecewise regression algorithm has shown promise in its initial applications, but the current implementation in the literature has been limited to elastic net and generalized linear regression. It is possible that nonparametric methods, such as random forest or conditional inference trees, may provide better prediction and insight through modeling interaction terms and other nonlinear relationships between predictors and a given outcome. 
This study explores the use of several machine learning algorithms within a Morse-Smale piecewise regression framework, including boosted regression with linear baselearners, homotopy-based LASSO, conditional inference trees, random forest, and a wide neural network framework called extreme learning machines. Simulations on Tweedie regression problems with varying Tweedie parameter and dispersion suggest that many machine learning approaches to Morse-Smale piecewise regression improve the original algorithm's performance, particularly for outcomes with lower dispersion and linear or a mix of linear and nonlinear predictor relationships. On a real actuarial problem, several of these new algorithms perform as good as or better than the original Morse-Smale regression algorithm, and most provide information on the nature of predictor relationships within each partition to provide insight into differences between dataset partitions.",2017,arXiv: Machine Learning
A four-DNA methylation signature as a novel prognostic biomarker for survival of patients with gastric cancer,"Gastric cancer (GC) is the fifth most frequently diagnosed cancer and the third leading cause of cancer-related mortality. Lack of prognostic indicators for patient survival hinders GC treatment and survival. Methylation profile data of patients with GC obtained from The Cancer Genome Atlas (TCGA) database were analyzed to identify methylation sites as biomarkers for GC prognosis. The cohort was divided into training and validation sets. Univariate Cox, LASSO regression,and multivariate Cox analyses revealed a close correlation of a four-DNA methylation signature as a risk score model with the overall survival of patients with GC. The survival between high-risk and low-risk score patients with GC was significantly different. Analyses of receiver operating characteristics revealed a high prognostic accuracy of the four-DNA methylation signature in patients with GC. The subgroup analysis indicated that the accuracy included that for anatomical region, histologic grade, TNM stage, pathological stage, and sex. The GC prognosis based on the four-DNA methylation signature was more precise than that based on known biomarkers. The four-DNA methylation signature could serve as a novel independent prognostic factor that could be an important tool to predict the prognostic outcome of GC patients. This potential must be verified in a large-scale population cohort study and through basic research studies.",2020,Cancer Cell International
"Essays on Forecasting Financial Markets Using Decomposition, Constraints and Extreme Learning Machines","Chapter 1 and 2 discuss how to use a decomposition model to make a density forecast of the financial return and how to improve this density forecast by imposing matching moment constraints. The density forecast model is based on a decomposition of financial returns into the absolute return and the sign of the return. We also use the maximum entropyprinciple for the out-of-sample density forecast subject to the constraintthat matches the mean forecasts from the decomposition model and a simple regression model. In Chapter 1 (joint with Professor Tea-Hwy Lee),We show that when the mean forecast from the decomposition model deviates from that of the mean return, imposing the matching mean forecast constraint will tilt the density forecast of the decomposition model and improve over the density forecast of the original decomposition model. In Chapter 2 (joint with Professor Tae-Hwy Lee and Ru Zhang), we further improve the decomposition model by using dependent copula functions, and we show that the risk forecast produced by the decomposition density forecast model is superior to RiskMetrics in terms of giving higher coverage probability and lower predictive quanitle loss in extreme events of large loss for monthly returns.Chapter 3 and 4 (joint with Professor Tae-Hwy Lee and Ru Zhang) deal with the testing of nonlinearity of time series data by using artificial neural network (ANN). In Chapter 3, we find that the original Lee, White and Granger (LWG, 1993) test is sensitive to the randomly generated activation parameters since they consider a fairly small number (10 or 20) of random hidden unit activations. To solve this problem, we simply increase the number of randomized hidden unit activations to a very large number (e.g., 1000). We show that using many randomly generated activation parameters can robustify the performance of the ANN test when it is applied to a real empirical data. This robustification is reliable and useful in practice, and can be achieved at no cost as increasing the number of random activations is almost costless given todayihs computer technology. In Chapter 4, we further consider different types of regularization of the dimensionality, such as principal component analysis (PCA), Lasso, Pretest, partial least squares (PLS), among others. We demonstrate that while these supervised regularization methods such as Lasso, Pretest, PLS, may be useful for forecasting, they may not be used for testing because the supervised regularization would create the post-sample inference or post-selection inference (PoSI) problem. Our Monte Carlo simulation shows that the PoSI problem is especially severe with PLS and Pretest while it seems relatively mild or even negligible with Lasso.",2013,
Measuring Visual Field Progression in the Central 10 Degrees Using Additional Information from Central 24 Degrees Visual Fields and â€˜Lasso Regressionâ€™,"PURPOSE
To measure progression of the visual field (VF) mean deviation (MD) index in longitudinal 10-2 VFs more accurately, by adding information from 24-2 VFs using Lasso regression.


METHODS
A training dataset consisted of 138 eyes from 97 patients with glaucoma or ocular hypertension and a testing dataset consisted of 40 eyes from 34 patients with glaucoma or ocular hypertension. The Lasso method was used to predict total deviation (TD) values in training patients' 10-2 VFs based on information from their 24-2 VFs (52 TD values, foveal sensitivity and mean deviation MD). Then, the MD of each patient's 10-2 VF was estimated as the average of these Lasso-predicted TD values (10-2 VF 'Lasso MD'; LMD). Finally, linear regression was applied to each testing patient's series of longitudinal 10-2 VF MDs with and without additional Lasso-derived LMDs in order to predict future MDs not included in the regression analysis. Absolute prediction errors were compared when only actual 10-2 MDs were regressed against when a combination of actual 10-2 MDs and LMDs were regressed.


RESULTS
THE AVERAGE ABSOLUTE PREDICTION ERROR WAS SIGNIFICANTLY SMALLER FOR THE NOVEL METHOD INCORPORATING LMDS (RANGE: 1.6 to 1.8 dB) compared with the standard approach (range: 1.7 to 3.4 dB) (p<0.05, ANOVA test).


CONCLUSIONS
Deriving 10-2 VF MD values from 24-2 VFs improves the prediction accuracy of progression. This approach will help clinicians to predict patients' visual function in the parafoveal area.",2013,PLoS ONE
Nearest neighbor ensembles for functional data with interpretable feature selection,"Abstract Functional data becomes increasingly common in many fields of application. Although much research has been done on functional regression and clustering approaches for chemometric data, so far few classification methods exist. This paper introduces an ensemble method for classification that inherently provides automatic and interpretable feature selection. It is designed for single as well as multiple functional (and non-functional) covariates. The ensemble members are posterior probability estimates that are based on a k-nearest-neighbor approach. The ensemble allows for feature selection by including members that are calculated from various semi-metrics used in the k-nearest-neighbor approach, where a particular semi-metric represents a specific curve feature. Each ensemble member, and thus each curve feature, is weighted by an unknown coefficient. These coefficients are estimated using a proper scoring rule with implicit Lasso-type penalty, such that some coefficients can be estimated to be exactly zero. Thus, the ensemble automatically provides feature selection, and also, in the case of multiple functional (and non-functional) covariates, variable selection. The selection performance and the interpretability of the coefficients are investigated in simulation studies. Data of a cell chip used for water quality monitoring experiments is examined. Here, the relevance of especially the feature selection aspect of the ensemble is illustrated.",2015,Chemometrics and Intelligent Laboratory Systems
Compressed sensing of time-varying signals,"Compressed sensing (CS) lowers the number of measurements required for reconstruction and estimation of signals that are sparse when expanded over a proper basis. Traditional CS approaches deal with time-invariant sparse signals, meaning that, during the measurement process, the signal of interest does not exhibit variations. However, many signals encountered in practice are varying with time as the observation window increases (e.g., video imaging, where the signal is sparse and varies between different frames). The present paper develops CS algorithms for time-varying signals, based on the least-absolute shrinkage and selection operator (Lasso) that has been popular for sparse regression problems. The Lasso here is tailored for smoothing time-varying signals, which are modeled as vector valued discrete time series. Two algorithms are proposed: the Group-Fused Lasso, when the unknown signal support is time-invariant but signal samples are allowed to vary with time; and the Dynamic Lasso, for the general class of signals with time-varying amplitudes and support. Performance of these algorithms is compared with a sparsity-unaware Kalman smoother, a support-aware Kalman smoother, and the standard Lasso which does not account for time variations. The numerical results amply demonstrate the practical merits of the novel CS algorithms.",2009,2009 16th International Conference on Digital Signal Processing
The biglasso Package: A Memory- and Computation-Efficient Solver for Lasso Model Fitting with Big Data in R,"Penalized regression models such as the lasso have been extensively applied to analyzing high-dimensional data sets. However, due to memory limitations, existing R packages like glmnet and ncvreg are not capable of fitting lasso-type models for ultrahigh-dimensional, multi-gigabyte data sets that are increasingly seen in many areas such as genetics, genomics, biomedical imaging, and high-frequency finance. In this research, we implement an R package called biglasso that tackles this challenge. biglasso utilizes memory-mapped files to store the massive data on the disk, only reading data into memory when necessary during model fitting, and is thus able to handle out-of-core computation seamlessly. Moreover, it's equipped with newly proposed, more efficient feature screening rules, which substantially accelerate the computation. Benchmarking experiments show that our biglasso package, as compared to existing popular ones like glmnet, is much more memory- and computation-efficient. We further analyze a 31 GB real data set on a laptop with only 16 GB RAM to demonstrate the out-of-core computation capability of biglasso in analyzing massive data sets that cannot be accommodated by existing R packages.",2017,arXiv: Computation
Derivation of a simple postoperative delirium incidence and severity prediction model,"Background: Delirium is an important postoperative complication, yet a simple and effective delirium prediction model remains elusive. We hypothesized that the combination of the National Surgical Quality Improvement Program (NSQIP) risk calculator for serious complications (NSQIP-SC) or risk of death (NSQIP-D), and cognitive tests of executive function (Trail Making Test A and B [TMTA, TMTB]), could provide a parsimonious model to predict postoperative delirium incidence or severity. Methods: Data were collected from 100 adults (>65yo) undergoing major non-cardiac surgery. In addition to NSQIP-SC, NSQIP-D, TMTA and TMTB, we collected participant age, sex, ASA score, tobacco use, type of surgery, depression, Framingham risk score, and preoperative blood pressure. Delirium was diagnosed with the Confusion Assessment Method (CAM), and the Delirium Rating Scale-R-98 (DRS) was used to assess symptom severity. LASSO and Best Subsets logistic and linear regression were employed in line with TRIPOD guidelines. Results: Three participants were excluded due to intraoperative deaths (2) and alcohol withdrawal (1). Ninety-seven participants with a mean age of 71.68+4.55, 55% male (31/97 CAM+, 32%) and a mean Peak DRS of 21.5+6.40 were analyzed. Of the variables included, only NSQIP-SC and TMTB were identified to be predictors of postoperative delirium incidence (p<0.001, AUROC 0.81, 95% CI: 0.72, 0.90) and severity (p<0.001, Adj. R2: 0.30). Conclusions: In this cohort, preoperative NSQIP-SC and TMTB were identified as predictors of postoperative delirium incidence and severity. Future studies should verify whether this two- factor model could be used for accurate delirium prediction. Keywords: aging, delirium, perioperative, prediction, surgical risk. Clinical Trial Registration ID #NCT03124303 and #NCT01980511",2018,bioRxiv
Circulating MicroRNAs in Delayed Cerebral Infarction After Aneurysmal Subarachnoid Hemorrhage,"BACKGROUND
Delayed cerebral infarction (DCI) is a major cause of morbidities after aneurysmal subarachnoid hemorrhage (SAH) and typically starts at day 4 to 7 after initial hemorrhage. MicroRNAs (miRNAs) play an important role in posttranscriptional gene expression control, and distinctive patterns of circulating miRNA changes have been identified for some diseases. We aimed to investigate miRNAs that characterize SAH patients with DCI compared with those without DCI.


METHODS AND RESULTS
Circulating miRNAs were collected on day 7 after SAH in healthy, SAH-free controls (n=20), SAH patients with DCI (n=20), and SAH patients without DCI (n=20). We used the LASSO (least absolute shrinkage and selection operator) method of regression analysis to characterize miRNAs associated with SAH patients with DCI compared with those without DCI. In the 28 dysregulated miRNAs associated with DCI and SAH, we found that a combination of 4 miRNAs (miR-4532, miR-4463, miR-1290, and miR-4793) could differentiate SAH patients with DCI from those without DCI with an area under the curve of 100% (95% CI 1.000-1.000, P<0.001). This 4-miRNA combination could also distinguish SAH patients with or without DCI from healthy controls with areas under the curve of 99.3% (95% CI 0.977-1.000, P<0.001) and 82.0% (95% CI 0.685-0.955, P<0.001), respectively.


CONCLUSIONS
We found a 4-miRNA combination that characterized SAH patients with DCI. The findings could guide futureÂ mechanistic study to develop therapeutic targets.",2017,Journal of the American Heart Association: Cardiovascular and Cerebrovascular Disease
On the Consistency of Feature Selection using Greedy Least Squares Regression,"This paper studies the feature selection problem using a greedy least squares regression algorithm. We show that under a certain irrepresentable condition on the design matrix (but independent of the sparse target), the greedy algorithm can select features consistently when the sample size approaches infinity. The condition is identical to a corresponding condition for Lasso. 
 
Moreover, under a sparse eigenvalue condition, the greedy algorithm can reliably identify features as long as each nonzero coefficient is larger than a constant times the noise level. In comparison, Lasso may require the coefficients to be larger than O(âˆšs) times the noise level in the worst case, where s is the number of nonzero coefficients.",2009,J. Mach. Learn. Res.
Son preference and contraceptive practice among tribal groups in rural South India,"This paper examines the son preference and contraceptive practice among tribal groups in rural south India. Parentsâ€™ preferences for the sex of their children have constituted an important theme in population and social research over the past three decades. Data were collected from a household survey of 398 currently married women of reproductive age group (15-49) from four taluks in the Nilgiris District of rural Tamilnadu are selected with respect to the different tribal communities. Cross tabulation and logistic regression analysis was carried out for finding out relationships between the socio-economic, demographic variables on contraceptive practice. The use of contraceptive practice by tribal groups in rural areas is strongly linked to individual and household socio-economic and demographic variables. Findings shows that the expectation that a son will provide financial support in old age is strongly associated with the response that a son is important. Son preference is slightly more among the tribal women, particularly among the users of spacing method who are more among those preferring the sons. Some of the socio-economic variables like education of husband and occupation have shown negative influence on higher fertility and positive influence on contraceptive use among the tribal women. It is proposed that there is need for more comprehensive on tribes in different areas in state and in the Indian nation to explicitly bring out the son preference attitudes of tribal people, which have an impact on their fertility and family planning practices. A. SATHIYA SUSUMAN 32 pucca houses to them as early as possible. As the social participation is low, suitable steps should be taken to increase their participation in public bodies and institutions (Gurusamy, 1988). Fertility differentials, child mortality and family planning practices between two tribal groups in Luknow, India, a majority of the tribal groups 87 per cent and 67 per cent were living in joint family units; two-third of the tribal women were without schooling and the extent of indebtedness was high among both groups. Lower mean age at marriage among the wives of the two groups. Lower mean age at first birth, an average women gave a higher fertility level children and a higher ideal family size (Saxena, 1982). The acceptance and practice of family planning by the eligible couples depends to a very large extent on their awareness and knowledge regarding family planning methods, availability of services and favorable attitude towards family planning practices (Gurumurthy, 1986; Karkal Pause, 1979; Nag, 1991). The validity of the concept those tribes had a higher birth rate than the general population and the socio-cultural factors were responsible for the higher fertility among tribes. Two indicators of sex preference were used in the study such as the ideal number of sons and ideal number of daughters in a family. More balanced view concerning the ideal number of sons and daughters among the tribal women. The women do not have any special preference for either boys or girls (Repetto, 1972). The tribal society does not suffer from the compulsion to get married and have children, preferably a son. The tribal neither believe in rebirth nor believe in the theory of transmigration of soul. A number of studies in India showed that parents like to depend on their sons for old age security than their daughters. The cost of bringing up children up to age 15 does not seem to vary much between sons and daughters. However, the expectations from sons appeared to be more as compared to the daughters. The review of related literature leads us to conclude that the theoretical explanations advanced for some expected relationship between level of development socio-economic and cultural and value of children are sound . But the empirical relationship obtained in various setting always does not support the expected relationships. It is more so in the case of individual characteristics of the parents and value of children. The differentials in the (fertility differentials) level are due to differentials in the value of sons and daughters child, which are likely to be different under Indian context (Kuriyan, 1982; Sarma et al., 1974). Both the boys and girls were perceived as contributing rather differently to these basic values. Respondents of both sexes perceived boys contribute to the satisfaction of a greater number of values and needs than girls. Financial security, being accepted by others, having a happy home, personal achievement and carrying on the family name were all values that respondents associated more with boys than with girls (Sathiya Susuman, 2000). The patterns observed, however, are by no means uniform throughout India and they can be expected to continue to decline. The preference for sons that has been observed to varying degrees in every part of India has been showed to have an adverse effect on both fertility behavior and sex differentials in child mortality (Krishnamoorthy, 1974). The desire for sons discourages some couples from discontinuing childbearing after reaching their desired number of children because they have not yet had their minimum desired number of sons (Vlassoff et al., 1980). Tribal communities of various countries and societies are using contraception for limit their family size. While the same, couples has been desired more number of children, irrespective of the sex. Most of the tribal couples want at least on son for various reasons. This might be traditional or cultural system. When couples have achieved desired number of sex particularly sons, they adopt sterilization from government hospital or health centre or other temporary methods or usually tribal women are using herbal medicine or home made contraception (Sathiya Susuman, 2000). It is necessary to study the son preference and contraceptive practice among rural south Indian tribal groups. The major aim of the study is to examine the son preference and contraceptive practice among various groups of tribal communities, it is important that we consider with under developed community groups. To this study, the tribal women were compared with respect to their influence of socio-economic and demographic variables on son preference and contraceptive practice in rural areas.",2006,
Edge detection in sparse Gaussian graphical models,"In this paper, we consider the problem of detecting edges in a Gaussian graphical model. The problem is equivalent to the identification of non-zero entries of the concentration matrix of a normally distributed random vector. Following the methodology initiated in Meinshausen and Buhlmann (2006), we tackle the problem through regression models where each component of the random vector is regressed on the remaining components. We adapt a method called SLasso cum EBIC (sequential LASSO cum extended Bayesian information criterion) recently developed in Luo and Chen (2011) for feature selection in sparse regression models to suit the special nature of the concentration matrix, and propose two approaches, dubbed SR-SLasso and JR-SLasso, for the identification of non-zero entries of the concentration matrix. Comprehensive numerical studies are conducted to compare the proposed approaches with other available competing methods. The numerical studies demonstrate that the proposed approaches are more accurate than the other methods for the identification of non-zero entries of the concentration matrix.",2014,Comput. Stat. Data Anal.
"The Influence Of Maternal Nutrition Intake On The Birth Weight Of Newborns In Lanzhou, China","Background: The objective of this paper is to evaluate the role of maternal nutrition intake before and during pregnancy on infant birth weight. Self-reported data on intake of 94 nutrients during four time periods (one year before pregnancy and during the three trimesters of pregnancy) was collected from over 7,000 women during 2010-2012 in Gansu Provincial Maternity and Child Care Hospital in Lanzhou, China. Methods: Hierarchical clustering and k-means clustering were performed to detect any potentially influential nutrients. Principal Component Analysis (PCA) was used to reduce the dimension of predictors and linear regression analysis was subsequently conducted to fit new nutrient components and confounders on birth weight. Since the Globaltest showed significant influence of nutrient intake in participant group with low prepregnancy maternal Body Mass Index (BMI), stepwise selection and Least Absolute Shrinkage and Selection Operator (Lasso) methods were then performed directly over the 94 nutrients in low BMI group. Results: Hierarchical clustering and k-means clustering both resulted in six clusters, but the averages of birth weight in the six clusters were not significantly different from each other based on ANOVA result (p-value=0.1229 and 0.1032, for hierarchical and k-means clustering, respectively). Principal Components Analysis (PCA) was then performed and selected 10 new nutrient components to represent the original 94 nutrients. The following step was to fit the 10 new components and 9 confounders on birth weight in a linear regression model. This procedure was repeated for the four time periods. For each time period, there were two components showing strong association with birth weight. Comparing to the base model with only confounders, full models with nutrient components had very small R-squared increase (around 0.01). According to the Globaltest result, the overall effect of 94 nutrients showed strong association with birth weight in low pre-pregnancy maternal BMI group in all four time periods (pvalues=0.0439, 0.0033, 0.0017, and 0.0024, for four time periods, respectively). In the model selection part, stepwise selection resulted in two significant nutrient variables out of 94 variables for each time period. Specifically, variable â€˜insoluble dietary fiberâ€™ showed strong association with birth weight (p-value=0.0022, 0.0051 and 0.0062, for pre-pregnancy, the 2 and 3 trimester, respectively). In addition, a few vitamin nutrients showed strong association with birth weight (p-values < 0.0001), but the estimated coefficients were very small. Lasso method showed similar results as stepwise selection. Few nutrients showed significant influence but with very small estimated coefficients, and R-squared of the full models had very small increase compared to the base model with only confounders. Conclusions: Several methods have been tried to test the association between maternal nutrition intake and birth weight of newborns, such as clustering on predictors and observations, Globaltest, stepwise selection and Lasso method in linear regression. These methods showed consistent results that overall maternal nutrition intake was significantly associated with infant birth weight in low maternal BMI group, and a few individual nutrients showed significant association. It is suggested that instead of focusing on altered consumption of individual nutrients, overall maternal nutrition intake should be improved to help control birth weight in the normal range.",2013,
Additive Models with Sparse Convexity Patterns,"In nonparametric statistics, additive modeling is an efficient and interpretable technique for multivariate models, even when the true model is not additive. Shape constraints, such as requiring functions to be convex or monotone, also allow tractable methods that apply to numerous examples. Here, we propose a novel estimation technique that combines additive models with shape constraints. Specifically, we consider a regression model in which the true function is additive and each component is either convex, concave, or identically zero. This model extends the idea of sparse additive models (SpAM) proposed in [17]. We first show that the problem can be expressed as a 0-1 mixed-integer secondorder cone program (MISOCP), for which there exist solvers based on heuristics. Then, we describe our recently discovered quadratic program (QP) formulation which extends the idea of Lasso [20]. We present examples as well as simulations comparing the different formulations. âˆ—This work is done as part of the Chicago Center for the Theory of Computing and Allied Areas (â€œTheory Centerâ€) Summer 2014 Research Education for Undergraduates (REU) Program, which is jointly run by the University of Chicago and Toyota Technical Institute at Chicago. This paper contains both original research and reviews of established ideas studied during the program duration. The research is a joint work with Sabyasachi Chatterjee, Min Xu, and Professor John Lafferty.",2014,
LASSO-Driven Inference in Time and Space,"We consider the estimation and inference in a system of high-dimensional regression equations allowing for temporal and cross-sectional dependency in covariates and error processes, covering rather general forms of weak dependence. A sequence of large-scale regressions with LASSO is applied to reduce the dimensionality, and an overall penalty level is carefully chosen by a block multiplier bootstrap procedure to account for multiplicity of the equations and dependencies in the data. Correspondingly, oracle properties with a jointly selected tuning parameter are derived. We further provide high-quality de-biased simultaneous inference on the many target parameters of the system. We provide bootstrap consistency results of the test procedure, which are based on a general Bahadur representation for the Z-estimators with dependent data. Simulations demonstrate good performance of the proposed inference procedure. Finally, we apply the method to quantify spillover effects of textual sentiment indices in a financial market and to test the connectedness among sectors. JEL classification: C12, C22, C51, C53",2018,
"Predictive factors of immune tolerance treatment response in severe haemophilia A patients with inhibitors: A realâ€world report from a single centre, mixed retrospectiveâ€prospective longâ€term study","Dear Editor, The appropriate management of patients with inhibitors repâ€ resents the main challenge for physicians who specialize in haemoâ€ philia. Immune tolerance induction (ITI) is the primary therapeutic strategy for achieving inhibitor eradication.1 ITI represents an intenâ€ sive and continuous exposure to FVIII until a patient gains complete or partial tolerance against the factor. To contribute to the data from an experienced centre in managing haemophilic patients, the presâ€ ent mixed retrospective and prospective study was aimed to analyse the association between the ITI success rate with a series of cliniâ€ cal variables. The medical records of severe haemophilia A patients from a Congenital Coagulopathies Unit, who started an ITI regimen between March 1980 and July 2015, were reviewed. The study was conducted in accordance with the Declaration of Helsinki and Good Clinical Practice. Eligible subjects included both children and adults diagnosed with severe haemophilia A (FVIII:C <1%) and treated for primary or rescue ITI with plasmaâ€derived FVIII concentrates (pdFVIII), either purified FVIII or von Willebrand factor (VWF)â€containing (pdFVIII/ VWF), or with recombinant (rFVIII) concentrates. Rescue ITI was deâ€ fined as the ITI treatments undergone after failure of the primary ITI course. The definitions of ITI success and failure were generally consistent with those currently in use.1 The time to outcome was measured from initiation of ITI until achievement of success (comâ€ plete or partial), failure or rescue ITI. The decision of whether ITI was a failure, or to continue ITI treatment longer, was made according to the physician's discretion. For pharmacokinetic measurements, after 3 days of infused FVIII washing, FVIII:C levels were determined (â€œpreâ€). Then, 50 IU/kg FVIII was administered and FVIII:C was determined again after 15 minâ€ utes (â€œpostâ€), 1, 2, 6, 24 and 48 hours (although last 7 years, meaâ€ surements were taken at three time points: pre, post and 48 hours). For association analyses, a L1â€penalized logistic regression model, LASSO (Least Absolute Shrinkage and Selection Operator), was used. Those variables not penalized to zero were considered as being associated with ITI success. Peak titre was logâ€transformed before the analyses because of its high right skewness. The use of penalized models is required in cases such as in this study, in which the number of variables was high in relation to the number of assessments. Kaplanâ€Meier survival analysis was constructed for the time elapsed in the percentage of patients: (a) reaching inhibitor elimiâ€ nation; (b) reaching a normal FVIII recovery and (c) reaching a normal halfâ€life of infused FVIII. Discrimination by infused FVIII dose (<100 IU/kg/d; â‰¥100 IU/kg/d) was also made. The generalâ€ ized Wilcoxon test was used for comparison. Software R (The R Foundation, Vienna, Austria) version 3.2.1 was used for calculations and analysis. Results showed that 26 patients started an ITI course during the study period. Of these, three patients are still under treatment while 23 ended primary ITI and were therefore evaluated. Data collected were retrospective in patients who started ITI up to year 2000 (n = 11) and prospective after that year (n = 12 patients). Details of patient characteristics and ITI data are shown in Table 1. Just over half of the patients (n = 13; 57%) started ITI within 1 year after inhibitor diagnostic (47% of complete success [CS]), while in nine patients (39%) the lapse took between 1 and 5 years, and one patient showed an extreme value of 11 years. Eighteen paâ€ tients (78%) were <5 years old at the time of ITI initiation (50% of CS). The majority of the patients (n = 18; 78%) showed an inhibitor titre <10 BU/mL at start of ITI (55% of CS) but five of them had titre â‰¥10 BU/mL (60% of CS). Overall, primary ITI success was 57% (13/23 patients), which was lower than that shown in previous studies ranging 63%â€100%.2â€7 A possible reason may be the inclusion of all screened patients in the study, thus mimicking a group with intention of treatment, which has shown a lower success rate. Similarly, preâ€ mature changes of product type or dose regimen in some patients as well as the high historical inhibitor peak titre in our population could have a role in reducing the chances of success. Nevertheless, less strict criteria than ours for reporting an outcome of successful ITI have been described in other studies6,8 and in a registry.9 The median time of ITI treatment was 11.4 months (Q1, Q3: 9, 24). Fourteen patients (61%) received pdFVIII (13 pdFVIII/VWF and 1 purified FVIII) for ITI. There was a higher rate of CS in patients treated with pdFVIII (10/14; 71%) rather than those treated with rFVIII (3/9; 33%). Interestingly, the percentage of patients with at",2019,Haemophilia
A Fast Em Algorithm for Quadratic Optimization Subject to Convex Constraints,"Convex constraints (CCs) such as box constraints and linear inequality constraints appear frequently in statistical inference and in applications. The problems of quadratic optimization (QO) subject to CCs occur in isotonic regression, shape-restricted non-parametric regression, variable selection (via the lasso algorithm and bridge regression), limited dependent variables models, image reconstruction, and so on. Existing packages for QO are not generally applicable to CCs. Although EM-type algorithms may be applied to such problems (Tian, Ng and Tan (2005)), the convergence rate/speed of these algorithms is painfully slow, especially for high-dimensional data. This paper develops a fast EM algorithm for QO with CCs. We construct a class of data augmentation schemes indexed by a â€˜working parameterâ€™ r (r âˆˆ R), and then optimize r over R under several convergence criteria. In addition, we use Cholesky decomposition to reduce both the number of latent variables and the dimension, leading to further acceleration of the EM. Standard errors of the restricted estimators are calculated using a non-parametric bootstrapping procedure. Simulation and comparison are performed and a complex multinomial dataset is analyzed to illustrate the proposed methods.",2007,
EfficientRepresentation ofMachine Learning Models â€“ investigating optimization options for decision trees in embedded systems Resurseffektiv Representation av MaskininlÃ¤rningsmodeller,"Combining embedded systems and machine learning models is an exciting prospect. However, to fully target any embedded system, with the most stringent resource requirements, the models have to be designed with care not to overwhelm it. Decision tree ensembles are targeted in this thesis. A benchmark model is created with LightGBM, a popular framework for gradient boosted decision trees. This model is first transformed and regularized with RuleFit, a LASSO regression framework. Then it is further optimized with quantization and weight sharing, techniques used when compressing neural networks. The entire process is combined into a novel framework, called ESRule. The data used comes from the domain of frequency measurements in cellular networks. There is a clear use-case where embedded systems can use the produced resource optimized models. Compared with LightGBM, ESRule uses 72Ë† less internal memory on average, simultaneously increasing predictive performance. The models use 4 kilobytes on average. The serialized variant of ESRule uses 104Ë† less hard disk space than LightGBM. ESRule is also clearly faster at predicting a single sample.",2019,
Survival Prediction Based on Compound Covariate under Cox Proportional Hazard Models,"Survival prediction from a large number of covariates is a current focus of statistical and medical research. In this paper, we study a methodology known as the compound covariate prediction performed under univariate Cox proportional hazard models. We demonstrate via simulations and real data analysis that the compound covariate method generally competes well with ridge regression and Lasso methods, both already well-studied methods for predicting survival outcomes with a large number of covariates. Furthermore, we develop a refinement of the compound covariate method by incorporating likelihood information from multivariate Cox models. The new proposal is an adaptive method that borrows information contained in both the univariate and multivariate Cox regression estimators. We show that the new proposal has a theoretical justification from a statistical large sample theory and is naturally interpreted as a shrinkage-type estimator, a popular class of estimators in statistical literature. Two datasets, the primary biliary cirrhosis of the liver data and the non-small-cell lung cancer data, are used for illustration. The proposed method is implemented in R package ""compound.Cox"" available in CRAN at http://cran.r-project.org/.",2012,PLoS ONE
L2Boosting for Economic Applications,"In the recent years more and more highdimensional data sets, where the number of parameters p is high compared to the number of observations n or even larger, are available for applied researchers. Boosting algorithms represent one of the major advances in machine learning and statistics in recent years and are suitable for the analysis of such data sets. While Lasso has been applied very successfully for highdimensional data sets in Economics, boosting has been underutilized in this field, although it has been proven very powerful in fields like Biostatistics and Pattern Recognition. We attribute this to missing theoretical results for boosting. The goal of this paper is to fill this gap and show that boosting is a competitive methods for inference of a treatment effect or instrumental variable (IV) estimation in a high-dimensional setting. First, we present the L2Boosting with componentwise least squares algorithm and variants which are tailored for regression problems which is the workhorse for most Econometric problems. Then we present how L2Boosting can be used for estimation of treatment effects and IV estimation. We highlight the methods and illustrate them with simulations and empirical examples. For further results and technical details we refer to (Luo and Spindler 2016) and (Luo and Spindler 2017) and to the online supplement of the paper.",2017,
"A Tool to Early Predict Severe 2019-Novel Coronavirus Pneumonia (COVID-19) : A Multicenter Study using the Risk Nomogram in Wuhan and Guangdong, China","Background Severe cases of coronavirus disease 2019 (COVID-19) rapidly develop acute respiratory distress leading to respiratory failure, with high short-term mortality rates. At present, there is no reliable risk stratification tool for non-severe COVID-19 patients at admission. We aimed to construct an effective model for early identifying cases at high risk of progression to severe COVID-19. Methods SARS-CoV-2 infected patients from one center in Wuhan city and two centers in Guangzhou city, China were included retrospectively. All patients with non-severe COVID-19 during hospitalization were followed for more than 15 days after admission. Patients who deteriorated to severe or critical COVID-19 and patients who kept non-severe state were assigned to the severe and non-severe group, respectively. We compared the demographic, clinical, and laboratory data between severe and non-severe group. Based on baseline data, least absolute shrinkage and selection operator (LASSO) algorithm and logistic regression model were used to construct a nomogram for risk prediction in the train cohort. The predictive accuracy and discriminative ability of nomogram were evaluated by area under the curve (AUC) and calibration curve. Decision curve analysis (DCA) and clinical impact curve analysis (CICA) were conducted to evaluate the clinical applicability of our nomogram. Findings The train cohort consisted of 189 patients, while the two independent validation cohorts consisted of 165 and 18 patients. Among all cases, 72 (19.35%) patients developed severe COVID-19 and 107 (28.76%) patients had one of the following basic disease, including hypertension, diabetes, coronary heart disease, chronic respiratory disease, tuberculosis disease. We found one demographic and six serological indicators (age, serum lactate dehydrogenase, C-reactive protein, the coefficient of variation of red blood cell distribution width (RDW), blood urea nitrogen, albumin, direct bilirubin) are associated with severe COVID-19. Based on these features, we generated the nomogram, which has remarkably high diagnostic accuracy in distinguishing individuals who exacerbated to severe COVID-19 from non-severe COVID-19 (AUC 0.912 [95% CI 0.846-0.978]) in the train cohort with a sensitivity of 85.71 % and specificity of 87.58% ; 0.853 [0.790-0.916] in validation cohort with a sensitivity of 77.5 % and specificity of 78.4%. The calibration curve for probability of severe COVID-19 showed optimal agreement between prediction by nomogram and actual observation. DCA and CICA further indicated that our nomogram conferred significantly high clinical net benefit. Interpretation Our nomogram could help clinicians to early identify patients who will exacerbate to severe COVID-19. And this risk stratification tool will enable better centralized management and early treatment of severe patients, and optimal use of medical resources via patient prioritization and thus significantly reduce mortality rates. The RDW plays an important role in predicting severe COVID-19, implying that the role of RBC in severe disease is underestimated.",2020,medRxiv
Role of Metabolic Obesity and Body Mass Index in Patients with Coronary Artery Diseases,"Obesity and its related consequences are some of the most alarming health problems the world is currently facing. Studies have shown that a little more than a fourth of the adults all over the world are overweight and a little more than a tenth are obese. Also, Obesity is considered as an independent risk factor for determining the severity of a Coronary Artery Disease (CAD). The aim of this paper is to determine the role of Metabolic Obesity and Body Mass Index in Patients with Coronary Artery Diseases. We train a model capable of predicting the GENSINI score which determines the severity of CAD in four health groups, namely Metabolically Healthy and Normal Weight (MHNW), Metabolically Obese Normal Weight (MONW), Metabolically Healthy Obese (MHO) and Metabolically Abnormally Obese (MAO). Several factors like Glycated Hemoglobin (HBA1C), Triglyceride and Insulin Resistance are considered. Ridge Regression, Lasso regression, and Neural Networks are the mechanisms used to train a model for predicting the GENSINI score based upon health statistics of patients",2018,
Factors Associated with Nutritional Risk among Homebound Older Adults with Depressive Symptoms.,"OBJECTIVES
This study used the Evans model of public health determinants to identify factors associated with nutritional risk in older adults.


DESIGN
The Evans model domains (physical and mental well-being, social/environmental statuses, individual choice, and economic security) were measured in a sample of homebound older adults. Regularized logistic regression analysis with LASSO penalty function was used to determine the strongest domain of the Evans model. Using traditional logistic regression, individual variables across all domains were compared to identify the significant predictors.


SETTING
Older adults receiving home meal services were referred to the study by community program staff.


PARTICIPANTS
Participants included 164 homebound older adults (age â‰¥ 60) who endorsed at least one gateway symptom of depression.


MEASUREMENTS
Nutritional risk was determined using the Mini Nutritional Assessment. Domains of the Evans model were measured using the MAI Medical Condition Checklist, items from the IADL scale, the Structured Clinical Interview for DSM-IV Axis I Disorders, the Duke Social Support Index, living arrangements, marital status, the Alcohol Use Disorders Identification Test, items from the SCID Screening Module, and a self-report of perceived financial security.


RESULTS
Poor mental well-being, defined by a diagnosis of major depressive disorder, was identified as the strongest Evans model domain in the prediction of nutritional risk. When each variable was independently evaluated across domains, instrumental support (Wald's Z=-2.24, p=0.03) and a history of drug use (Wald's Z=-2.40, p=0.02) were significant predictors.


CONCLUSIONS
The Evans model is a useful conceptual framework for understanding nutritional health, with the mental domain found to be the strongest domain predictor of nutritional risk. Among individual variables across domains, having someone to help with shopping and food preparation and a history of drug use were associated with lower nutritional risk. These analyses highlight potential targets of intervention for nutritional risk among older adults.",2016,The Journal of frailty & aging
Determination of General Circulation Model Grid Resolution to Improve Accuracy of Rainfall Prediction in West Java,The statistical downscaling technique is used to predict rainfall using general circulation model (GCM) data. One important factor to improve prediction accuracy is selection of GCM grid resolution. The purpose of this study is to determine the best GCM grid resolution to predict rainfall. The data used was GCM data from CFSRv2 with grid resolutions of 0.312Â°Ã—0.312Â°; 0.5Â°Ã—0.5Â°; 2.5Â°Ã—2.5Â° and local rainfall data in West Java. The determination of best grid resolution began with determining the GCM domain in each rainfall observation station based on minimum correlation value of 0.3 between GCM data and local rainfall data. The domains were evaluated by LASSO regression based on the smallest RMSEP and the largest correlation. The results showed that the best grid resolution is 2.5Â°Ã—2.5Â°.,2019,International journal of scientific and research publications
"Regularization, sparse recovery, and median-of-means tournaments","A regularized risk minimization procedure for regression function estimation is introduced that achieves near optimal accuracy and confidence under general conditions, including heavy-tailed predictor and response variables. The procedure is based on median-of-means tournaments, introduced by the authors in [8]. It is shown that the new procedure outperforms standard regularized empirical risk minimization procedures such as lasso or slope in heavy-tailed problems.",2017,arXiv: Statistics Theory
A FITM1-Related Methylation Signature Predicts the Prognosis of Patients With Non-Viral Hepatocellular Carcinoma,"Although great progress has been made in treatment against hepatitis virus infection, the prognosis of hepatocellular carcinoma (HCC) remains unsatisfied. Therefore, there is an unmet need to explore biomarkers or prognostic models for monitoring non-viral hepatocellular carcinoma. Accumulating evidence indicates that DNA methylation participates in carcinogenesis of malignancies. In the present study, we analyzed 101 non-viral HCC patients from TCGA database to figure out methylation-driven genes (MDGs) that might get involved in non-viral HCC pathogenesis using MethyMix algorithm. Then we picked out 8 key genes out of 137 MDGs that could affect the overall survival (OS) of both methylation and expression level. Using PCA, Uni-variate, Multi-variate, and LASSO cox regression analyses, we confirmed the potential prognostic value of these eight epigenetic genes. Ultimately, combined with immunohistochemistry (IHC), ROC, OS, and GSEA analyses, fat storage-inducing transmembrane protein1 (FITM1) was identified as a novel tumor suppressor gene in non-viral HCC and an applicable FITM1-methylation-based signature was built in a training set and validated in a testing set. Briefly, our work provides several potential biomarkers, especially FITM1, as well as a new method for disease surveillance and treatment strategy development.",2020,Frontiers in Genetics
University of Groningen (18)F-FDG PET image biomarkers improve prediction of late radiation-induced xerostomia,"Background and purpose: Current prediction of radiation-induced xerostomia 12 months after radiotherapy (Xer12m) is based on mean parotid gland dose and baseline xerostomia (Xerbaseline) scores. The hypothesis of this study was that prediction of Xer12m is improved with patient-specific characteristics extracted from F-FDG PET images, quantified in PET image biomarkers (PET-IBMs). Patients and methods: Intensity and textural PET-IBMs of the parotid gland were collected from pretreatment F-FDG PET images of 161 head and neck cancer patients. Patient-rated toxicity was prospectively collected. Multivariable logistic regression models resulting from step-wise forward selection and Lasso regularisation were internally validated by bootstrapping. The reference model with parotid gland dose and Xerbaseline was compared with the resulting PET-IBM models. Results: High values of the intensity PET-IBM (90th percentile (P90)) and textural PET-IBM (Long Run High Grey-level Emphasis 3 (LRHG3E)) were significantly associated with lower risk of Xer12m. Both PET-IBMs significantly added in the prediction of Xer12m to the reference model. The AUC increased from 0.73 (0.65â€“0.81) (reference model) to 0.77 (0.70â€“0.84) (P90) and 0.77 (0.69â€“0.84) (LRHG3E). Conclusion: Prediction of Xer12m was significantly improved with pre-treatment PET-IBMs, indicating that high metabolic parotid gland activity is associated with lower risk of developing late xerostomia. This study highlights the potential of incorporating patient-specific PET-derived functional characteristics into NTCP model development. 2017 The Author(s). Published by Elsevier Ireland Ltd. Radiotherapy and Oncology 126 (2018) 89â€“95 This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-ncnd/4.0/). F-FDG PET imaging provides functional information about the metabolic activity of tissue. This makes F-FDG PET a powerful and widely used diagnostic modality in oncology. In head and neck oncology, F-FDG PET can complement other image modalities in tumour staging and delineation for radiotherapy [1,2]. The common clinical use of F-FDG PET allows for the possibility to extract large amounts of patient-specific functional information that could contribute to prognosis for head and neck cancer (HNC) patients. Several studies have shown that PET image characteristics of the tumour can contribute to predicting overall, disease-free or event-free survival [3â€“6]. However, patient-specific image characteristics for predicting normal tissue radiation toxicities are less explored, while these are also crucial in supporting treatment decisions. Additionally, new radiation techniques (e.g. proton therapy [7] and magnetic resonance imaging (MRI) guided radiation [8]) may allow for better sparing of normal tissue. These new techniques demand improved prediction models, to select patients most at risk of developing toxicities [9]. Radiation-induced xerostomia is a major and frequent side effect for HNC patients, and has a considerable impact on these patientsâ€™ quality of life [10]. Conventional Normal Tissue Complication Probability (NTCP) models that predict patient-rated xerostomia are based on doseâ€“volume parameters and baseline complaints [11,12]. However, there is still a significant, unexplained variance in predicting xerostomia with these models. Therefore, the demand persists to improve the identification of patients at risk. Previous work showed that patient-specific CT characteristics of the parotid glands could significantly improve the prediction of patient-rated xerostomia, however, model performance improvement was marginal [13]. The hypothesis was that the predictive CT characteristic is related to the ratio of nonfunction to functional parotid tissue. It can be expected that this ratio would be better represented by image characteristics from functional imaging (i.e. PET or MR images). 90 F-FDG-PET-image biomarkers improve xerostomia prediction In this study, the relationship was tested between metabolic activity of the parotid gland and late xerostomia. Consequently, the patient-specific response to radiation in developing this toxicity was investigated. The purpose was to determine whether functional information from F-FDG PET images, which is quantified in PET-image biomarkers (PET-IBMs), was associated with patient-rated moderate-to-severe xerostomia 12 months after radiotherapy (Xer12m). Since current NTCP prediction models are based on parotid gland dose and baseline complaints, the study subsequently addressed whether PET-IBMs could improve on the current prediction of Xer12m Materials and methods Patient demographics and treatment F-FDG PET/CT scans were acquired of 161 HNC patients in treatment position before the start of radiotherapy. The patients were treated with definitive radiotherapy either with or without concurrent chemotherapy or cetuximab, between November 2010 and August 2015. Patients without follow-up data 12 months after radiotherapy were excluded from this study. Patients were also excluded if they underwent surgery in the head and neck area before or within one year after treatment. A detailed description of the radiotherapy protocols is given in previous studies [13,14]. In summary, all patients were treated with IMRT or VMAT using a simultaneous integrated boost (SIB) technique. The parotid glands and the swallowing structures were spared as much as possible without compromising the dose to the target volumes [14,15]. Patients received a total dose of 70 Gy (2 Gy per fraction, 5 or 6 times a week) to the primary tumour and, if present, pathological lymph nodes. A radiation dose of 54.25 Gy (1.55 Gy per fraction, 5 or 6 times a week) was delivered to the elective lymph node levels.",2018,
Inflammatory markers in late pregnancy in association with postpartum depressionâ€”A nested case-control study,"Recent studies indicate that the immune system adaptation during pregnancy could play a significant role in the pathophysiology of perinatal depression. The aim of this study was to investigate if inflammation markers in a late pregnancy plasma sample can predict the presence of depressive symptoms at eight weeks postpartum. Blood samples from 291 pregnant women (median and IQR for days to delivery, 13 and 7-23days respectively) comprising 63 individuals with postpartum depressive symptoms, as assessed by the Edinburgh postnatal depression scale (EPDSâ‰¥12) and/or the Mini International Neuropsychiatric Interview (M.I.N.I.) and 228 controls were analyzed with an inflammation protein panel using multiplex proximity extension assay technology, comprising of 92 inflammation-associated markers. A summary inflammation variable was also calculated. Logistic regression, LASSO and Elastic net analyses were implemented. Forty markers were lower in late pregnancy among women with depressive symptoms postpartum. The difference remained statistically significant for STAM-BP (or otherwise AMSH), AXIN-1, ADA, ST1A1 and IL-10, after Bonferroni correction. The summary inflammation variable was ranked as the second best variable, following personal history of depression, in predicting depressive symptoms postpartum. The protein-level findings for STAM-BP and ST1A1 were validated in relation to methylation status of loci in the respective genes in a different population, using openly available data. This explorative approach revealed differences in late pregnancy levels of inflammation markers between women presenting with depressive symptoms postpartum and controls, previously not described in the literature. Despite the fact that the results do not support the use of a single inflammation marker in late pregnancy for assessing risk of postpartum depression, the use of STAM-BP or the novel notion of a summary inflammation variable developed in this work might be used in combination with other biological markers in the future.",2017,Psychoneuroendocrinology
How Useful Is Bagging in Forecasting Economic Time Series? A Case Study of U.S. Consumer Price Inflation,"This article focuses on the widely studied question of whether the inclusion of indicators of real economic activity lowers the prediction mean squared error of forecasting models of U.S. consumer price inflation. We propose three variants of the bagging algorithm specifically designed for this type of forecasting problem and evaluate their empirical performance. Although bagging predictors in our application are clearly more accurate than equally weighted forecasts, median forecasts, ARM forecasts, AFTER forecasts, or Bayesian forecast averages based on one extra predictor at a time, they are generally about as accurate as the Bayesian shrinkage predictor, the ridge regression predictor, the iterated LASSO predictor, or the Bayesian model average predictor based on random subsets of extra predictors. Our results show that bagging can achieve large reductions in prediction mean-squared errors even in such challenging applications as inflation forecasting; however, bagging is not the only method capable of...",2005,Journal of the American Statistical Association
Sparse principal component analysis for multiblock data and its extension to sparse multiple correspondence analysis,"Two new methods to select groups of variables have been developed for multiblock data: ""Group Sparse Principal Component Analysis"" (GSPCA) for continuous variables and ""Sparse Multiple Correspondence Analysis"" (SMCA) for categorical variables. GSPCA is a compromise between Sparse PCA method of Zou, Hastie and Tibshirani and the method ""group Lasso"" of Yuan and Lin. PCA is formulated as a regression-type optimization problem and uses the constraints of the group Lasso on regression coe cients to produce modi ed principal
components with sparse loadings. It leads to reduce the number of nonzero coe cients, i.e. the number of selected groups. SMCA is a straightforward extension of GSPCA to groups of indicator variables, with the chi-square metric. Two real examples will be used to illustrate each method. The fi rst one is a data set on 25 trace elements measured in three tissues of 48 crabs (25 blocks of 3 variables). The second one is a data set of 502 women aimed at the identi cation of genes a ecting skin aging with more than 370.000 blocks, each block corresponding to SNPs (Single Nucleotide Polymorphisms) coded into 3 categories.",2012,
Assessment of the cardiovascular adverse effects of drug-drug interactions through a combined analysis of spontaneous reports and predicted drug-target interactions,"Adverse drug effects (ADEs) are one of the leading causes of death in developed countries and are the main reason for drug recalls from the market, whereas the ADEs that are associated with action on the cardiovascular system are the most dangerous and widespread. The treatment of human diseases often requires the intake of several drugs, which can lead to undesirable drug-drug interactions (DDIs), thus causing an increase in the frequency and severity of ADEs. An evaluation of DDI-induced ADEs is a nontrivial task and requires numerous experimental and clinical studies. Therefore, we developed a computational approach to assess the cardiovascular ADEs of DDIs. This approach is based on the combined analysis of spontaneous reports (SRs) and predicted drug-target interactions to estimate the five cardiovascular ADEs that are induced by DDIs, namely, myocardial infarction, ischemic stroke, ventricular tachycardia, cardiac failure, and arterial hypertension. We applied a method based on least absolute shrinkage and selection operator (LASSO) logistic regression to SRs for the identification of interacting pairs of drugs causing corresponding ADEs, as well as noninteracting pairs of drugs. As a result, five datasets containing, on average, 3100 ADE-causing and non-ADE-causing drug pairs were created. The obtained data, along with information on the interaction of drugs with 1553 human targets predicted by PASS Targets software, were used to create five classification models using the Random Forest method. The average area under the ROC curve of the obtained models, sensitivity, specificity and balanced accuracy were 0.838, 0.764, 0.754 and 0.759, respectively. The predicted drug targets were also used to hypothesize the potential mechanisms of DDI-induced ventricular tachycardia for the top-scoring drug pairs. The created five classification models can be used for the identification of drug combinations that are potentially the most or least dangerous for the cardiovascular system. Author summary Assessment of adverse drug effects as well as the influence of drug-drug interactions on their manifestation is a nontrivial task that requires numerous experimental and clinical studies. We developed a computational approach for the prediction of adverse effects that are induced by drug-drug interactions, which are based on a combined analysis of spontaneous reports and predicted drug-target interactions. Importantly, the approach requires only structural formulas to predict adverse effects, and, therefore, may be applied for new, insufficiently studied drugs. We applied the approach to predict five of the most important cardiovascular adverse effects, because they are the most dangerous and widespread. These effects are myocardial infarction, ischemic stroke, ventricular tachycardia, arterial hypertension and cardiac failure. The accuracies of predictive models were relatively high, in the range of 73-81%; therefore, we performed a prediction of the five cardiovascular adverse effects for the large number of drug pairs and revealed the combinations that are the most dangerous for the cardiovascular system. We consider that the developed approach can be used for the identification of pairwise drug combinations that are potentially the most or least dangerous for the cardiovascular system.",2019,bioRxiv
Bootstrap Methods for Lasso-Type Estimators Under A Moving-Parameter Framework,"We study the distributions of Lasso-type regression estimators in a moving-parameter asymptotic framework, and consider various bootstrap methods for estimating them accordingly. We show, in particular, that the distribution functions of Lasso-type estimators, including even those possessing the oracle properties such as the adaptive Lasso and the SCAD, cannot be consistently estimated by the bootstraps uniformly over the space of the regression parameters, especially when some of the regression coefficients lie close to the origin. Such lack of uniform consistency poses difficulties in practical applications of the bootstraps for making Lasso-based inferences. In the light of this seemingly negative result, we seek, however, to develop criteria for assessing the relative risks, phrased in terms of their uniform consistency properties, of the various bootstrap methods, based on which an optimal bootstrap strategy may be formulated in an adaptive manner. A simulation study is provided to demonstrate the non-normal nature of the distributions of Lasso-type estimators, and to assess the performances of various bootstrap estimates of such distributions across different values of regression parameters.",2012,
Using New Models to Analyze Complex Regularities of the World,"This commentary to the recent article by Musso et al. (2013) discusses issues related to model fitting, comparison of classification accuracy of generative and discriminative models, and two (or more) cultures of data modeling. We start by questioning the extremely high classification accuracy with an empirical data from a complex domain. There is a risk that we model perfect nonsense perfectly. Our second concern is related to the relevance of comparing multilayer perceptron neural networks and linear discriminant analysis classification accuracy indices. We find this problematic, as it is like comparing apples and oranges. It would have been easier to interpret the model and the variable (group) importanceâ€™s if the authors would have compared MLP to some discriminative classifier, such as group lasso logistic regression. Finally, we conclude our commentary with a discussion about the predictive properties of the adopted data modeling approach.",2014,
IsoLasso: A LASSO Regression Approach to RNA-Seq Based Transcriptome Assembly,"The new second generation sequencing technology revolutionizes many biology-related research fields and poses various computational biology challenges. One of them is transcriptome assembly based on RNA-Seq data, which aims at reconstructing all full-length mRNA transcripts simultaneously from millions of short reads. In this article, we consider three objectives in transcriptome assembly: the maximization of prediction accuracy, minimization of interpretation, and maximization of completeness. The first objective, the maximization of prediction accuracy, requires that the estimated expression levels based on assembled transcripts should be as close as possible to the observed ones for every expressed region of the genome. The minimization of interpretation follows the parsimony principle to seek as few transcripts in the prediction as possible. The third objective, the maximization of completeness, requires that the maximum number of mapped reads (or ?expressed segments? in gene models) be explained by (i.e., contained in) the predicted transcripts in the solution. Based on the above three objectives, we present IsoLasso, a new RNA-Seq based transcriptome assembly tool. IsoLasso is based on the well-known LASSO algorithm, a multivariate regression method designated to seek a balance between the maximization of prediction accuracy and the minimization of interpretation. By including some additional constraints in the quadratic program involved in LASSO, IsoLasso is able to make the set of assembled transcripts as complete as possible. Experiments on simulated and real RNA-Seq datasets show that IsoLasso achieves, simultaneously, higher sensitivity and precision than the state-of-art transcript assembly tools.",2011,Journal of computational biology : a journal of computational molecular cell biology
"Regulation Techniques for Multicollinearity : Lasso , Ridge , and Elastic Nets","Multicollinearity can be briefly described as the phenomenon in which two or more identified predictor variables are linearly related, or codependent. The presence of this phenomenon can have a negative impact on an analysis as a whole and can severely limit the conclusions of a research study. In this paper, we will briefly review how to detect multicollinearity, and once it is detected, which regularization techniques would be the most appropriate to combat it. The nuances and assumptions of R1 (Lasso), R2 (Ridge Regression), and Elastic Nets will be covered in order to provide adequate background for appropriate analytic implementation. This paper is intended for any level of SASÂ® user. This paper is also written to an audience with a background in theoretical and applied statistics, though the information within will be presented in such a way that any level of statistics/mathematical knowledge will be able to understand the content.",2018,
