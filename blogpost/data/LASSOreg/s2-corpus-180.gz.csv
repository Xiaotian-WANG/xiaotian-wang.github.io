title,abstract,year,journal
Statistical methods for the testing and estimation of linear dependence structures on paired high-dimensional data : application to genomic data,"This thesis provides novel methodology for statistical analysis of paired high-dimensional genomic data, with the aim to identify gene interactions specific to each group of samples as well as the gene connections that change between the two classes of observations. An example of such groups can be patients under two medical conditions, in which the estimation of gene interaction networks is relevant to biologists as part of discerning gene regulatory mechanisms that control a disease process like, for instance, cancer. We construct these interaction networks from data by considering the nonzero structure of correlation matrices, which measure linear dependence between random variables, and their inverse matrices, which are commonly known as precision matrices and determine linear conditional dependence instead. In this regard, we study three statistical problems related to the testing, single estimation and joint estimation of (conditional) dependence structures. Firstly, we develop hypothesis testing methods to assess the equality of two correlation matrices, and also two correlation sub-matrices, corresponding to two classes of samples, and hence the equality of the underlying gene interaction networks. We consider statistics based on the average of squares, maximum and sum of exceedances of sample correlations, which are suitable for both independent and paired observations. We derive the limiting distributions for the test statistics where possible and, for practical needs, we present a permuted samples based approach to find their corresponding non-parametric distributions. Cases where such hypothesis testing presents enough evidence against the null hypothesis of equality of two correlation matrices give rise to the problem of estimating two correlation (or precision) matrices. However, before that we address the statistical problem of estimating conditional dependence between random variables in a single class of samples when data are high-dimensional, which is the second topic of the thesis. We study the graphical lasso method which employs an L1 penalized likelihood expression to estimate the precision matrix and its underlying non-zero graph structure. The lasso penalization term is given by the L1 norm of the precision matrix elements scaled by a regularization parameter, which determines the trade-off between sparsity of the graph and fit to the data, and its selection is our main focus of investigation. We propose several procedures to select the regularization parameter in the graphical lasso optimization problem that rely on network characteristics such as clustering or connectivity of the graph. Thirdly, we address the more general problem of estimating two precision matrices that are 5 expected to be similar, when datasets are dependent, focusing on the particular case of paired observations. We propose a new method to estimate these precision matrices simultaneously, a weighted fused graphical lasso estimator. The analogous joint estimation method concerning two regression coefficient matrices, which we call weighted fused regression lasso, is also developed in this thesis under the same paired and high-dimensional setting. The two joint estimators maximize penalized marginal log likelihood functions, which encourage both sparsity and similarity in the estimated matrices, and that are solved using an alternating direction method of multipliers (ADMM) algorithm. Sparsity and similarity of the matrices are determined by two tuning parameters and we propose to choose them by controlling the corresponding average error rates related to the expected number of false positive edges in the estimated conditional dependence networks. These testing and estimation methods are implemented within the R package ldstatsHD, and are applied to a comprehensive range of simulated data sets as well as to high-dimensional real case studies of genomic data. We employ testing approaches with the purpose of discovering pathway lists of genes that present significantly different correlation matrices on healthy and unhealthy (e.g., tumor) samples. Besides, we use hypothesis testing problems on correlation sub-matrices to reduce the number of genes for estimation. The proposed joint estimation methods are then considered to find gene interactions that are common between medical conditions as well as interactions that vary in the presence of unhealthy tissues.",2018,
Variable Selection with Big Data based on Zero Norm and via Sequential Monte Carlo,"Selecting a subset from many potential explanatory variables in linear regressions has long been the subject of research interest, and the matter is made more important in the era of big data when many more variables become available/accessible. Of late, the l1-norm penalty based techniques such as Lasso of Tibshirani (1996) have become very popular. However, the variable selection problem in its natural setting is a zero-norm penalty problem, i.e., a penalty on the number of variables as opposed to the l1-norm of the regression coefficients. The popularity of the l1-norm penalty or its variants has more to do with computational considerations, because selection with the zero-norm penalty is a highly demanding combinatory optimization problem when the number of potential variables becomes large. We devise a sequential Monte Carlo (SMC) method as a practical and reliable tool for zero-norm variable selection problems, and selecting, say, best 20 out of 1,000 potential variables can, for example, be completed with a typical multi-core desktop computer in a couple of minutes. The methodological essence is to understand that the selection problem is equivalent to the task of sampling from a discrete probability function defined over all possible combinations comprising, say, k regressors out of p â‰¥ k potential variables, where the peak of this function corresponds to the optimal combination. The solution technique sets out to sequentially generate samples, and after a while the final sample represents the target probability function. With the final SMC sample in place, we deploy the extreme value theory to assess how likely and to what extent the maximum R2 has been achieved. We also demonstrate through a simulation study the methodâ€™s reliability and superiority vis-a-vis the adaptive Lasso. âˆ—Duan is with the National University of Singapore (Business School, Risk Management Institute and Department of Economics). E-mail: bizdjc@nus.edu.sg. The author thanks Yu-Hung Chien, Shuping Li, Kaican Kang and Qiqi Zou for their able research assistance.",2019,
"Summary and discussion of : â€œ Sequential selection procedures and false discovery rate control â€ Statistics Journal Club , 36-825","Suppose we have a sequence of null hypotheses H1, . . . ,Hm and that we want to reject some of these hypotheses while controlling the expected number of â€˜mistakesâ€™ (in the sense of FDR, to be explained later). We also have a constraint on the way in which we can reject the hypotheses. Namely, the hypotheses have to be rejected in an ordered fashion: if the i-th hypothesis Hi is rejected, then all the preceding ones, H1, . . . , Hiâˆ’1, must be rejected as well (think of step-wise regression, but also lasso or hierarchical clustering for example). We want to guarantee that the rule used to perform this ordered sequence of hypothesis tests controls the proportion of false discoveries, which is defined as",2014,
"Applied the additive hazard model to predict the survival time of patient with diffuse large B- cell lymphoma and determine the effective genes, using microarray data","Background: Recent studies have shown that effective genes on survival time of cancer patients play an important role as a risk factor or preventive factor. Present study was designed to determine effective genes on survival time for diffuse large B-cell lymphoma patients and predict the survival time using these selected genes. Materials & Methods: Present study is a cohort study was conducted on 40 patients with diffuse large Bcell lymphoma. For these patients, 2042 gene expression was measured. In order to predict the survival time, the composition of the semi-parametric additive survival model with two gene selection methods elastic net and lasso were used. Two methods were evaluated by plotting area under the ROC curve over time and calculating the integral of this curve. Results: Based on our findings, the elastic net method identified 10 genes, and Lasso-Cox method identified 7 genes. GENE3325X increased the survival time (P=0.006), Whereas GENE3980X and GENE377X reduced the survival time (P=0.004). These three genes were selected as important genes in both methods. Conclusion: This study showed that the elastic net method outperformed the common Lasso method in terms of predictive power. Moreover, apply the additive model instead Cox regression and using microarray data is usable way for predict the survival time of patients.",2015,
Fast Newton methods for the group fused lasso,"We present a new algorithmic approach to the group fused lasso, a convex model that approximates a multi-dimensional signal via an approximately piecewise-constant signal. This model has found many applications in multiple change point detection, signal compression, and total variation denoising, though existing algorithms typically using first-order or alternating minimization schemes. In this paper we instead develop a specialized projected Newton method, combined with a primal active set approach, which we show to be substantially faster that existing methods. Furthermore, we present two applications that use this algorithm as a fast subroutine for a more complex outer loop: segmenting linear regression models for time series data, and color image denoising. We show that on these problems the proposed method performs very well, solving the problems faster than state-of-the-art methods and to higher accuracy.",2014,
"Development and internal validation of an aneurysm rupture probability model based on patient characteristics and aneurysm location, morphology, and hemodynamics","PurposeUnruptured cerebral aneurysms pose a dilemma for physicians who need to weigh the risk of a devastating subarachnoid hemorrhage against the risk of surgery or endovascular treatment and their complications when deciding on a treatment strategy. A prediction model could potentially support such treatment decisions. The aim of this study was to develop and internally validate a model for aneurysm rupture based on hemodynamic and geometric parameters, aneurysm location, and patient gender and age.MethodsCross-sectional data from 1061 patients were used for image-based computational fluid dynamics and shape characterization of 1631 aneurysms for training an aneurysm rupture probability model using logistic group Lasso regression. The modelâ€™s discrimination and calibration were internally validated based on the area under the curve (AUC) of the receiver operating characteristic and calibration plots.ResultsThe final model retained 11 hemodynamic and 12 morphological variables, aneurysm location, as well as patient age and gender. An adverse hemodynamic environment characterized by a higher maximum oscillatory shear index, higher kinetic energy and smaller low shear area as well as a more complex aneurysm shape, male gender and younger age were associated with an increased rupture risk. The corresponding AUC of the model was 0.86 (95% CI [0.85, 0.86], after correction for optimism 0.84).ConclusionThe model combining variables from various domains was able to discriminate between ruptured and unruptured aneurysms with an AUC of 86%. Internal validation indicated potential for the application of this model in clinical practice after evaluation with longitudinal data.",2018,International Journal of Computer Assisted Radiology and Surgery
Block Regularized Lasso for Multivariate Multi-Response Linear Regression,"The multivariate multi-response (MVMR) linear regression problem is investigated, in which design matrices are Gaussian with covariance matrices (1:K) = (1) ;:::; (K) for K linear regressions. The support union of K p-dimensional regression vectors (collected as columns of matrix B ) are recovered using l1=l2-regularized Lasso. Sucient and necessary conditions to guarantee successful recovery of the support union are characterized via a threshold. More specifically, it is shown that under certain conditions on the distributions of design matri",2013,
Day ahead electricity price forecast by NARX model with LASSO based features selection,"The availability of accurate day-ahead price forecasts is crucial to achieve an effective participation to electricity markets. Starting from available state of the art, we propose a forecast technique exploiting a nonlinear auto regressive model with exogenous input, including a feature selection mechanism based on the Least Absolute Shrinkage and Selection Operator (LASSO). The rationale behind such a choice is twofold. On the one hand, we aim to target potential increase of forecast accuracy by learning complex non-linear mappings. On the other hand, we want to increase the interpretability of the resulting model and minimize the effort needed to properly set up the forecaster. A framework such as the LASSO, capable to self-extract features from spot price multi-variate time series, might represent a very useful tool for industrial practitioners. Experiments have been performed on Italian market dataset, demonstrating that the proposed method can extract useful features and achieve robust performance. Moreover, we show how the proposed method can support interpretation of forecaster structure and it can reveal interesting correlations within the regression set.",2019,2019 IEEE 17th International Conference on Industrial Informatics (INDIN)
Exploration of the immune-related signature and immune infiltration analysis for breast ductal and lobular carcinoma.,"Background
In this study, we aimed to explore the tumour associated immune signature of breast cancer (BC) and conduct integrative analyses with immune infiltrates in BC.


Methods
We downloaded the transcriptome profiling and clinical data of BC from The Cancer Genome Atlas (TCGA) database. The list of immune-related signatures was from the Innate database. The limma package was utilized to conduct the normalization, and we screened the differential immune signatures in BC. A univariate Cox regression model and the LASSO method were used to find the hub prognostic immune genes. The TAIG risk model was calculated based on the multivariate Cox regression results, and a receiver operating characteristic (ROC) curve was generated to assess the predictive power of TAIG. Moreover, we also conducted a correlation analysis between TAIG and the clinical characteristics. Additionally, we utilized the METABRIC cohort as the validation data set. The TIMER database is a comprehensive resource for performing systematic analyses of immune infiltrates across various malignancies. We evaluated the associations of immune signatures with several immune cells based on TIMER. Furthermore, we used the CIBERSORT algorithm to determine the fractions of immune cells in each sample and compared the differential distributions of immune infiltrates between two TAIG groups using the Wilcoxon rank-sum test.


Results
A total of 1,178 samples were obtained from the TCGA-BRCA database, but only 1,045 breast tumour samples were matched with complete transcriptome expression data. Meanwhile, we collected a total of 1,094 BC patients from the METABRIC cohort. We found a list of 1,399 differential immune signatures associated with survival, and functional analysis revealed that these genes participated in cytokine-cytokine receptor interactions, Th1 and Th2 cell differentiation and the JAK-STAT signalling pathway. The TAIG risk model was established from the multivariate Cox analysis, and we observed that high TAIG levels correlated with poor survival outcomes based on Kaplan-Meier analysis. The Kruskal-Wallis test suggested that high TAIG levels correlated with high AJCC-TNM stages and advanced pathological stages (P<0.01). We validated the well robustness of TAIG in METABRIC cohort and 5-year AUC reached up to 0.829. Moreover, we further uncovered the associations of hub immune signatures with immune cells and calculated the immune cell fractions in specific tumour samples based on gene signature expression. Last, we used the Wilcoxon rank-sum test to compare the differential immune density in the two groups and found that several immune cells had a significantly lower infiltrating density in the high TAIG groups, including CD8+ T cells (P=0.031), memory resting CD4+ T cells (P=0.026), M0 macrophages (P=0.023), and M2 macrophages (P=0.048).


Conclusions
In summary, we explored the immune signature of BC and constructed a TAIG risk model to predict prognosis. Moreover, we integrated the identified immune signature with tumour-infiltrating immune cells and found adverse associations between the TAIG levels and immune cell infiltrating density.",2019,Annals of translational medicine
Exploring the use of machine learning for risk adjustment: A comparison of standard and penalized linear regression models in predicting health care costs in older adults,"BACKGROUND
Payers and providers still primarily use ordinary least squares (OLS) to estimate expected economic and clinical outcomes for risk adjustment purposes. Penalized linear regression represents a practical and incremental step forward that provides transparency and interpretability within the familiar regression framework. This study conducted an in-depth comparison of prediction performance of standard and penalized linear regression in predicting future health care costs in older adults.


METHODS AND FINDINGS
This retrospective cohort study included 81,106 Medicare Advantage patients with 5 years of continuous medical and pharmacy insurance from 2009 to 2013. Total health care costs in 2013 were predicted with comorbidity indicators from 2009 to 2012. Using 2012 predictors only, OLS performed poorly (e.g., R2 = 16.3%) compared to penalized linear regression models (R2 ranging from 16.8 to 16.9%); using 2009-2012 predictors, the gap in prediction performance increased (R2:15.0% versus 18.0-18.2%). OLS with a reduced set of predictors selected by lasso showed improved performance (R2 = 16.6% with 2012 predictors, 17.4% with 2009-2012 predictors) relative to OLS without variable selection but still lagged behind the prediction performance of penalized regression. Lasso regression consistently generated prediction ratios closer to 1 across different levels of predicted risk compared to other models.


CONCLUSIONS
This study demonstrated the advantages of using transparent and easy-to-interpret penalized linear regression for predicting future health care costs in older adults relative to standard linear regression. Penalized regression showed better performance than OLS in predicting health care costs. Applying penalized regression to longitudinal data increased prediction accuracy. Lasso regression in particular showed superior prediction ratios across low and high levels of predicted risk. Health care insurers, providers and policy makers may benefit from adopting penalized regression such as lasso regression for cost prediction to improve risk adjustment and population health management and thus better address the underlying needs and risk of the populations they serve.",2019,PLoS ONE
The Iterated Lasso for High-Dimensional Logistic Regression By,"We consider an iterated Lasso approach for variable selection and estimation in sparse, high-dimensional logistic regression models. In this approach, we use the Lasso (Tibshirani 1996) to obtain an initial estimator and reduce the dimension of the model. We then use the Lasso as the initial estimator in the adaptive Lasso (Zou 2006) to obtain the final selection and estimation results. We provide conditions under which this two-step approach possesses asymptotic oracle selection and estimation properties. One important aspect of our results is that the total number of covariates can be larger than the sample size. Simulation studies indicate that the iterated Lasso has superior performance in variable selection relative to the standard Lasso. A data example is used to illustrate the proposed approach.",2008,
Meaningful Prediction Parameters for Evaluating the Suitability of Power Tools for Usage,"Abstract During the usage of power tools, user and power tool are interacting strongly with each other, which is why the working result depends heavily on the usability of a power tool depending on the application (Suitability of Usage (SoU)). The optimization of power tools, in terms of the user-centered design, therefore aims at an increase of the SoU. So far, the acquisition of the SoU is done within broad application tests, with several users and different application situations. Hereby, problems occur because it is often difficult to find an adequate mix of professional trained participants, which are able to evaluate the SoU objectively. To investigate the differences between professional power tool users and trained non-professional users, a study has been performed. In this study, 20 professional and 19 non-professional users tested power tools (cordless screwdrivers and impact wrenches, three different manufacturer each) and evaluated the handles of these systems according to fourteen test items, divided into an observation and a usage phase. By the use of statistical variance analysis, the captured data has been analyzed to investigate the influence on the evaluation through the usage experience of the users. Results indicates a strong influence within the evaluation categories where evaluations are done which rely on long term experiences, like the evaluation of the distributed force on the fingers. To produce further increase in efficiency and objectivity, a categorical regression by the use of Lasso models has been performed to identify the most meaningful influencing predictors for the SoU level. The investigation is carried out using the example of the evaluation of discomfort on the handle. Results indicates that the most meaningful predictor for the evaluation of the handle is the circumference of the handle at the position of the middle finger.",2018,Procedia CIRP
Active Set Algorithms for the LASSO,"This thesis disserts on the computation of the Least Absolute Shrinkage and Selection Operator (LASSO) and derivate problems, in regression analysis. This operator has drawn increasing attention since its introduction by Robert Tibshirani in 1996, for its ability to provide or recover sparse linear models from noisy observations, sparsity meaning that only a few of possibly many explaining variables are selected to appear in the model. The selection is a result of adding to the least-squares method a constraint or minimization on the sum of absolute values of the linear coeffcients, otherwise called the l1 norm of the coefficient vector. After recounting the motivations, principles and problematics of regression analysis, linear estimators, least-squares minimization, model selection, and regularization, the two equivalent formulations of the LASSO constrained or regularized are presented, that both define a non-trivial computation problem to associate an estimator to a set of observations and a selection parameter. A brief history of algorithms for solving these problems is given, as well as the two possible approaches for handling the non differentiability of the l1 norm, and the equivalence to a quadratic program is explained. The second part focuses on practical algorithms for solving the LASSO. An algorithm proposed in 2000 by Michael Osborne is reformulated. This reformulation consists in giving a general definition and explanation of the active set method, that generalizes the simplex algorithm to convex programming, then specifying it to the LASSO program, and separately addressing linear algebra optimizations. Although it describes the same algorithm in essence, the presentation given here aims at exhibiting clearly its mechanisms, and uses different variables. In addition to helping understand and use this algorithm that seemed to be underrated, the alternative view taken here brings light on the possibility and advantages, not foreseen by the authors, to use the method for the regularized (and more practical) problem, as well as for the constrained one. The popular homotopy (or LAR-LASSO) method is then derived from this active set method, yelding also an alternative and somewhat simplifed view of this algorithm that can compute the operator for all values of its parameter (LASSO path). Practical implementations following these formulations are shown to be the most efficient methods of LASSO-path computation, contrasting with a recent study of Jerome H. Friedman suggesting that a coordinate descent method improves by far the state-of-the-art results of homotopy, interms of speed. The third part examines how these three algorithms (active set, homotopy, and coordinate descent) can handle some limit cases, and can be applied to extended problems. The limit cases include degeneracies, like duplicated or lin- early dependent variables, or simultaneous selections/deselections of variables. The latter issue, that was dismissed in previous works, is explained and given a simple solution. Another limit case is the use of a very large, possibly infinite number of variables to select from, where the active set method presents a major advantage over the homotopy. A first extension to the LASSO is its transposition in online learning settings, where it is necessary or desirable to solve for a growing or changing observation set. Again, the lack of flexibility of the homotopy method discards it in profit of the other two. The second extension is the use of l1 penalization with other loss function than the squared residual, or together with other penalization terms, and we summarize or state to which extent and how each algorithm can be transposed for these problems.",2011,
Genomic-Enabled Prediction Based on Molecular Markers and Pedigree Using the Bayesian Linear Regression Package in R.,"The availability of dense molecular markers has made possible the use of genomic selection in plant and animal breeding. However, models for genomic selection pose several computational and statistical challenges and require specialized computer programs, not always available to the end user and not implemented in standard statistical software yet. The R-package BLR (Bayesian Linear Regression) implements several statistical procedures (e.g., Bayesian Ridge Regression, Bayesian LASSO) in a unifi ed framework that allows including marker genotypes and pedigree data jointly. This article describes the classes of models implemented in the BLR package and illustrates their use through examples. Some challenges faced when applying genomic-enabled selection, such as model choice, evaluation of predictive ability through cross-validation, and choice of hyper-parameters, are also addressed.",2010,The plant genome
Network estimation in econometrics,This work proposes novel network analysis techniques for time series. We define the network of a multivariate time series as an undirected graph where nodes denote the components of the process and edges denote nonzero long run partial correlation between two components. Long run partial correlation is a comprehensive measure of cross-sectional conditional dependence for time series capturing contemporaneous as well as lead/lag relations. The procedure is based on a two step LASSO regression of the VAR approximation of the process.,2013,"Advances in Latent Variables - Methods, Models and Applications"
Ultrahigh Dimensional Variable Selection for Mapping Soil Carbon,"Modern soil mapping is characterised by the need to interpolate samples of geostatistical response observations and the availability of relatively large numbers of environmental characteristics for consideration as covariates to aid this interpolation. We demonstrate the efficiency of the Least Angle Regression algorithm for Least Absolute Shrinkage and Selection Operator (LASSO) penalized multiple linear regression at selecting covariates to aid the spatial interpolation of geostatistical soil carbon observations under an ultrahigh dimensional scenario. Where an exhaustive search of the models that could be constructed from 800 potential covariate terms and 60 observations would be prohibitively demanding, LASSO variable selection is accomplished with trivial computational investment.",2016,
Group lassoing change-points in piecewise-constant AR processes,"Regularizing the least-squares criterion with the total number of coefficient changes, it is possible to estimate time-varying (TV) autoregressive (AR) models with piecewise-constant coefficients. Such models emerge in various applications including speech segmentation, biomedical signal processing, and geophysics. To cope with the inherent lack of continuity and the high computational burden when dealing with high-dimensional data sets, this article introduces a convex regularization approach enabling efficient and continuous estimation of TV-AR models. To this end, the problem is cast as a sparse regression one with grouped variables, and is solved by resorting to the group least-absolute shrinkage and selection operator (Lasso). The fresh look advocated here permeates benefits from advances in variable selection and compressive sampling to signal segmentation. An efficient block-coordinate descent algorithm is developed to implement the novel segmentation method. Issues regarding regularization and uniqueness of the solution are also discussed. Finally, an alternative segmentation technique is introduced to improve the detection of change instants. Numerical tests using synthetic and real data corroborate the merits of the developed segmentation techniques in identifying piecewise-constant TV-AR models.",2012,EURASIP Journal on Advances in Signal Processing
Group LASSO for Structural Break Time Series,"Consider a structural break autoregressive (SBAR) process where j = 1, â€¦, m + 1, {t1, â€¦, tm} are change-points, 1 = t0 < t1 < â‹…â‹…â‹… < tm + 1 = n + 1, Ïƒ( Â· ) is a measurable function on , and {Ïµt} are white noise with unit variance. In practice, the number of change-points m is usually assumed to be known and small, because a large m would involve a huge amount of computational burden for parameters estimation. By reformulating the problem in a variable selection context, the group least absolute shrinkage and selection operator (LASSO) is proposed to estimate an SBAR model when m is unknown. It is shown that both m and the locations of the change-points {t1, â€¦, tm} can be consistently estimated from the data, and the computation can be efficiently performed. An improved practical version that incorporates group LASSO and the stepwise regression variable selection technique are discussed. Simulation studies are conducted to assess the finite sample performance. Supplementary materials for this article are av...",2014,Journal of the American Statistical Association
Characterization of Telemedicine Use Among US Emergency Departments: 3EMF,"Study Objectives: Acute kidney injury (AKI) is strongly associated with adverse clinical outcomes including prolonged hospitalization, progression to CKD, and death. Diagnosis of AKI relies on detection of changes in serum creatinine (sCr) and urine output, both of which lag days behind renal injury and are unreliable at initial presentation. Here, we utilized data mining and machine learning methods to develop a predictive model for AKI with capacity for identifying ED patients at high risk for development of AKI within 7 days of their ED visit. Methods: A retrospective cross-sectional cohort of ED visits from 3 hospitals over 2 years was generated and used for model derivation and out-of-sample validation. Clinical data for all adult ED visits where initial sCr measurements were available at index visit and again within 7 days of EDdeparturewere extracted froma relational database that underlies our electronic health record (EHR) by an experienced data user. Primary outcome for prediction was Stage 2 AKI within 7 days of ED visit, defined according to sCr-based Kidney Disease Improving Global Outcomes (KDIGO) criteria (sCr increase to 2 times baseline). Secondary outcomes included KDIGO Stage 1 AKI (sCr increase of 0.3mg/dl above baseline or 1.5 times baseline) and Stage 3 AKI (sCr increase to 3 times baseline or to 4.0 mg/dl). Predictor variables extracted from the EHR included vital signs, laboratory results, chief complaints, demographics, past medical history, active problems, home medications and EDmedication administrations. Only EHR data available prior to prediction, made at time of first metabolic panel result, was included. Predictor variables were normalized as follows: ED vital signs and laboratory results were processed to minimum and maximum values, nephrotoxic and nephroprotective medications were grouped by pharmacologic class and least absolute shrinkage and selection operator (LASSO) feature selection processing applied to chief complaints and active problems identify variables with predictive value for AKI.Multiple machine learning models (logistic regression, decision tree, linear discriminant analysis, support vector machine, and random forest) were generated and tested in the prediction of our primary outcome. All were developed using a training dataset comprised of 90% of encounters and evaluated in the remaining encounters using 10-fold cross validation. Performance of each model was assessed using binary classification measures and receiver operator curve (ROC) analyses. Results: Our final cohort included 127,183 ED visits by 72,539 unique patients. Median age was 58 years (IQR: 43-71) and most common high-risk comorbidities were hypertension (51.8%) and heart failure (9.8%). Incidence of AKI in our cohort was as follows: Stage 1: 12.4%, Stage 2: 1.5%, Stage 3: 1.0%. Predictive model performance as measured by area under the ROC analysis ranged from 0.661 (95% CI: 0.637 0.685) using decision tree to 0.771 (95% CI: 0.759 0.783) using random forest. Conclusions: Machine learning methods applied to EHR data identified ED patients at high risk for AKI well before patients met diagnostic criteria. The model developed here, when paired with nephroprotective point-of-care clinical decision support, has potential to improve outcomes for this patient population.",2018,Annals of Emergency Medicine
Bayesian model selection in logistic regression for the detection of adverse drug reactions.,"Spontaneous adverse event reports have a high potential for detecting adverse drug reactions. However, due to their dimension, the analysis of such databases requires statistical methods. In this context, disproportionality measures can be used. Their main idea is to project the data onto contingency tables in order to measure the strength of associations between drugs and adverse events. However, due to the data projection, these methods are sensitive to the problem of coprescriptions and masking effects. Recently, logistic regressions have been used with a Lasso type penalty to perform the detection of associations between drugs and adverse events. On different examples, this approach limits the drawbacks of the disproportionality methods, but the choice of the penalty value is open to criticism while it strongly influences the results. In this paper, we propose to use a logistic regression whose sparsity is viewed as a model selection challenge. Since the model space is huge, a Metropolis-Hastings algorithm carries out the model selection by maximizing the BIC criterion. Thus, we avoid the calibration of penalty or threshold. During our application on the French pharmacovigilance database, the proposed method is compared to well-established approaches on a reference dataset, and obtains better rates of positive and negative controls. However, many signals (i.e., specific drug-event associations) are not detected by the proposed method. So, we conclude that this method should be used in parallel to existing measures in pharmacovigilance. Code implementing the proposed method is available at the following url: https://github.com/masedki/MHTrajectoryR.",2016,Biometrical journal. Biometrische Zeitschrift
LASSO based stimulus frequency recognition model for SSVEP BCIs,"Abstract Steady-state visual evoked potential (SSVEP) has been increasingly used for the study of brainâ€“computer interface (BCI). How to recognize SSVEP with shorter time and lower error rate is one of the key points to develop a more efficient SSVEP-based BCI. To achieve this goal, we make use of the sparsity constraint of the least absolute shrinkage and selection operator (LASSO) for the extraction of more discriminative features of SSVEP, and then we propose a LASSO model using the linear regression between electroencephalogram (EEG) recordings and the standard square-wave signals of different frequencies to recognize SSVEP without the training stage. In this study, we verified the proposed LASSO model offline with the EEG data of nine healthy subjects in contrast to canonical correlation analysis (CCA). In the experiment, when a shorter time window was used, we found that the LASSO model yielded better performance in extracting robust and detectable features of SSVEP, and the information transfer rate obtained by the LASSO model was significantly higher than that of the CCA. Our proposed method can assist to reduce the recording time without sacrificing the classification accuracy and is promising for a high-speed SSVEP-based BCI.",2012,Biomed. Signal Process. Control.
Feature-specific inference for penalized regression using local false discovery rates,"Penalized regression methods, most notably the lasso, are a popular approach to analyzing high-dimensional data. An attractive property of the lasso is that it naturally performs variable selection. An important area of concern, however, is the reliability of these variable selections. Motivated by local false discovery rate methodology from the large-scale hypothesis testing literature, we propose a method for calculating a local false discovery rate for each variable under consideration by the lasso model. These rates can be used to assess the reliability of an individual feature, or to estimate the model's overall false discovery rate. The method can be used for all values of $\lambda$. This is particularly useful for models with a few highly significant features but a high overall Fdr, which are a relatively common occurrence when using cross validation to select $\lambda$. It is also flexible enough to be applied to many varieties of penalized likelihoods including GLM and Cox models, and a variety of penalties, including MCP and SCAD. We demonstrate the validity of this approach and contrast it with other inferential methods for penalized regression as well as with local false discovery rates for univariate hypothesis tests. Finally, we show the practical utility of our method by applying it to two case studies involving high dimensional genetic data.",2018,arXiv: Methodology
Development of a recommender system for dental care using machine learning,"Resource mismanagement along with the underutilization of dental care has led to serious health and economic consequences. Artificial intelligence was applied to a national health database to develop recommendations for dental care. The data were obtained from the 2013â€“2014 National Health and Nutrition Examination Survey to perform machine learning. Feature selection was done using LASSO in R to determine the best regression model. Prediction models were developed using several supervised machine learning algorithms, including logistic regression, support vector machine, random forest, and classification and regression tree. Feature selection by LASSO along with the inclusion of additional clinically relevant variables identified 8 top features associated with recommendation for dental care. The top 3 features include gum health, number of prescription medications taken, and race. Gum health shows a significantly higher relative importance compared to other features. Demographics, healthcare access, and general health variables were identified as top features related to receiving additional dental care, consistent with prior research. Practicing dentists and other healthcare professionals can follow this model to enable precision dentistry through the incorporation of our algorithms into computerized screening tool or decision tree diagram to achieve more efficient and personalized preventive strategies and treatment protocols in dental care.",2019,SN Applied Sciences
Metabolomic profiling of breast tumors using ductal fluid,"Identification of new biomarkers for breast cancer remains critical in order to enhance early detection of the disease and improve its prognosis. Towards this end, we performed an untargeted metabolomic analysis of breast ductal fluid using an ultra-performance liquid chromatography coupled with a quadrupole time-of-light (UPLC-QTOF) mass spectrometer. We investigated the metabolomic profiles of breast tumors using ductal fluid samples collected by ductal lavage (DL). We studied fluid from both the affected breasts and the unaffected contralateral breasts (as controls) from 43 women with confirmed unilateral breast cancer. Using this approach, we identified 1560 ions in the positive mode and 538 ions in the negative mode after preprocessing of the UPLCâ€‘QTOF data. Paired t-tests applied on these data matrices identified 209 ions (positive and negative modes combined) with significant change in intensity level between affected and unaffected control breasts (adjusted p-values <0.05). Among these, 83 ions (39.7%) showed a fold change (FC) >1.2 and 66 ions (31.6%) were identified with putative compound names. The metabolites that we identified included endogenous metabolites such as amino acid derivatives (N-Acetyl-DL-tryptophan) or products of lipid metabolism such as N-linoleoyl taurine, trans-2-dodecenoylcarnitine, lysophosphatidylcholine LysoPC(18:2(9Z,12Z)), glycerophospholipids PG(18:0/0:0), and phosphatidylserine PS(20:4(5Z,8Z,11Z,14Z). Generalized LASSO regression further selected 21 metabolites when race, menopausal status, smoking, grade and TNM stage were adjusted for. A predictive conditional logistic regression model, using the LASSO selected 21 ions, provided diagnostic accuracy with the area under the curve of 0.956 (sensitivity/specificity of 0.907/0.884). This is the first study that shows the feasibility of conducting a comprehensive metabolomic profiling of breast tumors using breast ductal fluid to detect changes in the cellular microenvironment of the tumors and shows the potential for this approach to be used to improve detection of breast cancer.",2016,International Journal of Oncology
