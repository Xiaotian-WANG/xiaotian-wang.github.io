title,abstract,year,journal
Debiasing the Debiased Lasso with Bootstrap,"In this paper, we prove that under proper conditions, bootstrap can further debias the debiased Lasso estimator for statistical inference of low-dimensional parameters in high-dimensional linear regression. We prove that the required sample size for inference with bootstrapped debiased Lasso, which involves the number of small coefficients, can be of smaller order than the existing ones for the debiased Lasso. Therefore, our results reveal the benefits of having strong signals. Our theory is supported by results of simulation experiments, which compare coverage probabilities and lengths of confidence intervals with and without bootstrap, with and without debiasing.",2017,arXiv: Statistics Theory
Regression Analysis and Prediction of Mini-Mental State Examination Score in Alzheimer's Disease Using Multi-granularity Whole-Brain Segmentations,"We presented and evaluated three sparsity learning based regression models with application to the automated prediction of the Mini-Mental State Examination (MMSE) scores in Alzheimerâ€™s disease(AD) using T1-weight magnetic resonance images (MRIs) from 678 subjects, including 190 healthy control (HC) subjects, 331 mild cognitive impairment (MCI) subjects, and 157 AD subjects. The raw features were obtained from a validated multi-granularity whole-brain analysis pipeline, providing multi-level whole-brain segmentation volumes. We employed the ridge, lasso, and elastic-net as our regression algorithms, with the whole-brain volumes at each level being the independent variables and the MMSE score being the dependent variable. We used 10-fold cross-validation to evaluate the prediction performance and another 10-fold inner loop to estimate the optimal parameters in each model. According to our results, the combination of elastic-net and the second level of whole-brain segmentation volumes (a total of 137 volumes) worked the best compared to all other possible combinations. The work presented in this paper provides a potentially powerful and novel non-invasive biomarker for AD.",2017,
Credit Scoring Analysis using LASSO Logistic Regression and Support Vector Machine (SVM),"Credit scoring analysis using logistic regression is less effective, as it requires hypothesis and assumption testings that must be fulfilled. Therefore, LASSO logistic regression analysis can be used to overcome this problem because it does not require hypothesis and assumption testings. In addition, Support Vector Machine (SVM) method is also a classification method that has good classification capability as well as can ignore all assumptions such as logistic regression. This study aims to study the factors that affect the smoothness of debtors in paying motorcycle credits and compare the goodness of both methods used. Explanatory variables that affect the smoothness of credit payments are the phone ownership, down payment, loan term, occupation, age, marital status, gender, education level, number of dependents, motorcycle type, interaction between motorcycle type with phone ownership and down payment, interaction between phone ownership with occupation, loan terms, and phone number ownership, interaction between downpayment with loan terms, installments, gender and educational level, and interaction between occupation with tyype of income. Application of LASSO logistic regression and SVM in this case have mostly the same classification accuracy. However, the results of the classification performance using SVM method is relatively stable compared with LASSO logistic regression Keywords-logistic regression, LASSO, classification, support vector machine",2017,
A generalised OMP algorithm for feature selection with application to gene expression data,"Feature selection for predictive analytics is the problem of identifying a minimal-size subset of features that is maximally predictive of an outcome of interest. To apply to molecular data, feature selection algorithms need to be scalable to tens of thousands of available features. In this paper, we propose gOMP, a highly-scalable generalisation of the Orthogonal Matching Pursuit feature selection algorithm to several directions: (a) different types of outcomes, such as continuous, binary, nominal, and time-to-event, (b) different types of predictive models (e.g., linear least squares, logistic regression), (c) different types of predictive features (continuous, categorical), and (d) different, statistical-based stopping criteria. We compare the proposed algorithm against LASSO, a prototypical, widely used algorithm for high-dimensional data. On dozens of simulated datasets, as well as, real gene expression datasets, gOMP is on par, or outperforms LASSO for case-control binary classification, quantified outcomes (regression), and (censored) survival times (time-to-event) analysis. gOMP has also several theoretical advantages that are discussed. While gOMP is based on quite simple and basic statistical ideas, easy to implement and to generalize, we also show in an extensive evaluation that it is also quite effective in bioinformatics analysis settings.",2020,ArXiv
"Folded concave penalized sparse linear regression: sparsity, statistical performance, and algorithmic theory for local solutions","This paper concerns the folded concave penalized sparse linear regression (FCPSLR), a class of popular sparse recovery methods. Although FCPSLR yields desirable recovery performance when solved globally, computing a global solution is NP-complete. Despite some existing statistical performance analyses on local minimizers or on specific FCPSLR-based learning algorithms, it still remains open questions whether local solutions that are known to admit fully polynomial-time approximation schemes (FPTAS) may already be sufficient to ensure the statistical performance, and whether that statistical performance can be non-contingent on the specific designs of computing procedures. To address the questions, this paper presents the following threefold results: (1) Any local solution (stationary point) is a sparse estimator, under some conditions on the parameters of the folded concave penalties. (2) Perhaps more importantly, any local solution satisfying a significant subspace second-order necessary condition (S$$^3$$3ONC), which is weaker than the second-order KKT condition, yields a bounded error in approximating the true parameter with high probability. In addition, if the minimal signal strength is sufficient, the S$$^3$$3ONC solution likely recovers the oracle solution. This result also explicates that the goal of improving the statistical performance is consistent with the optimization criteria of minimizing the suboptimality gap in solving the non-convex programming formulation of FCPSLR. (3) We apply (2) to the special case of FCPSLR with minimax concave penalty and show that under the restricted eigenvalue condition, any S$$^3$$3ONC solution with a better objective value than the Lasso solution entails the strong oracle property. In addition, such a solution generates a model error (ME) comparable to the optimal but exponential-time sparse estimator given a sufficient sample size, while the worst-case ME is comparable to the Lasso in general. Furthermore, to guarantee the S$$^3$$3ONC admits FPTAS.",2017,Mathematical Programming
Building machine learning systems with Python : master the art of machine learning with Python and build effective machine learning systems with this intensive hands-on guide,"Preface Chapter 1: Getting Started with Python Machine Learning Chapter 2: Learning How to Classify with Real-world Examples Chapter 3: Clustering Finding Related Posts Chapter 4: Topic Modeling Chapter 5: Classification Detecting Poor Answers Chapter 6: Classification II Sentiment Analysis Chapter 7: Regression Recommendations Chapter 8: Regression Recommendations Improved Chapter 9: Classification III Music Genre Classification Chapter 10: Computer Vision Pattern Recognition Chapter 11: Dimensionality Reduction Chapter 12: Big(ger) Data Appendix: Where to Learn More about Machine Learning Index Preface Up Chapter 1: Getting Started with Python Machine Learning Machine learning and Python the dream team What the book will teach you (and what it will not) What to do when you are stuck Getting started Introduction to NumPy, SciPy, and Matplotlib Installing Python Chewing data efficiently with NumPy and intelligently with SciPy Learning NumPy Indexing Handling non-existing values Comparing runtime behaviors Learning SciPy Our first (tiny) machine learning application Reading in the data Preprocessing and cleaning the data Choosing the right model and learning algorithm Before building our first model Starting with a simple straight line Towards some advanced stuff Stepping back to go forward another look at our data Training and testing Answering our initial question Summary Up Chapter 2: Learning How to Classify with Real-world Examples The Iris dataset The first step is visualization Building our first classification model Evaluation holding out data and cross-validation Building more complex classifiers A more complex dataset and a more complex classifier Learning about the Seeds dataset Features and feature engineering Nearest neighbor classification Binary and multiclass classification Summary Up Chapter 3: Clustering Finding Related Posts Measuring the relatedness of posts How not to do it How to do it Preprocessing similarity measured as similar number of common words Converting raw text into a bag-of-words Counting words Normalizing the word count vectors Removing less important words Stemming Installing and using NLTK Extending the vectorizer with NLTK's stemmer Stop words on steroids Our achievements and goals Clustering KMeans Getting test data to evaluate our ideas on Clustering posts Solving our initial challenge Another look at noise Tweaking the parameters Summary Up Chapter 4: Topic Modeling Latent Dirichlet allocation (LDA) Building a topic model Comparing similarity in topic space Modeling the whole of Wikipedia Choosing the number of topics Summary Up Chapter 5: Classification Detecting Poor Answers Sketching our roadmap Learning to classify classy answers Tuning the instance Tuning the classifier Fetching the data Slimming the data down to chewable chunks Preselection and processing of attributes Defining what is a good answer Creating our first classifier Starting with the k-nearest neighbor (kNN) algorithm Engineering the features Training the classifier Measuring the classifier's performance Designing more features Deciding how to improve Bias-variance and its trade-off Fixing high bias Fixing high variance High bias or low bias Using logistic regression A bit of math with a small example Applying logistic regression to our postclassification problem Looking behind accuracy precision and recall Slimming the classifier Ship it! Summary Up Chapter 6: Classification II Sentiment Analysis Sketching our roadmap Fetching the Twitter data Introducing the Naive Bayes classifier Getting to know the Bayes theorem Being naive Using Naive Bayes to classify Accounting for unseen words and other oddities Accounting for arithmetic underflows Creating our first classifier and tuning it Solving an easy problem first Using all the classes Tuning the classifier's parameters Cleaning tweets Taking the word types into account Determining the word types Successfully cheating using SentiWordNet Our first estimator Putting everything together Summary Up Chapter 7: Regression Recommendations Predicting house prices with regression Multidimensional regression Cross-validation for regression Penalized regression L1 and L2 penalties Using Lasso or Elastic nets in scikit-learn P greater than N scenarios An example based on text Setting hyperparameters in a smart way Rating prediction and recommendations Summary Up Chapter 8: Regression Recommendations Improved Improved recommendations Using the binary matrix of recommendations Looking at the movie neighbors Combining multiple methods Basket analysis Obtaining useful predictions Analyzing supermarket shopping baskets Association rule mining More advanced basket analysis Summary Up Chapter 9: Classification III Music Genre Classification Sketching our roadmap Fetching the music data Converting into a wave format Looking at music Decomposing music into sine wave components Using FFT to build our first classifier Increasing experimentation agility Training the classifier Using the confusion matrix to measure accuracy in multiclass problems An alternate way to measure classifier performance using receiver operator characteristic (ROC) Improving classification performance with Mel Frequency Cepstral Coefficients Summary Up Chapter 10: Computer Vision Pattern Recognition Introducing image processing Loading and displaying images Basic image processing Thresholding Gaussian blurring Filtering for different effects Adding salt and pepper noise Putting the center in focus Pattern recognition Computing features from images Writing your own features Classifying a harder dataset Local feature representations Summary Up Chapter 11: Dimensionality Reduction Sketching our roadmap Selecting features Detecting redundant features using filters Correlation Mutual information Asking the model about the features using wrappers Other feature selection methods Feature extraction About principal component analysis (PCA) Sketching PCA Applying PCA Limitations of PCA and how LDA can help Multidimensional scaling (MDS) Summary Up Chapter 12: Big(ger) Data Learning about big data Using jug to break up your pipeline into tasks About tasks Reusing partial results Looking under the hood Using jug for data analysis Using Amazon Web Services (AWS) Creating your first machines Installing Python packages on Amazon Linux Running jug on our cloud machine Automating the generation of clusters with starcluster Summary Up Appendix: Where to Learn More about Machine Learning Online courses Books Q&A sites Blogs Data sources Getting competitive What was left out Summary",2013,
Studies in the econometrics of panel data with applications to,"The main objective of this thesis was to develop new methodologies that can inform researchers about some aspects of their model that are not informed by economic theory. To this end I have combined model selection features with more traditional panel data econometric models. The different methodologies are useful in different economic settings. I have shown the capabilities of each of the new methodologies by using them in specific economic questions with real data. In all of them I have obtained novel results that add to the different economic literatures. In the case of grouped patterns of heterogeneity I have studied the link between income and democracy of countries and found that the unobserved heterogeneity found among countries is consistent with Huntingtonâ€™s 3rd wave of democracy theories. In terms of recovering the structure of interactions I have shown how the methodology is able to uncover the structure of knowledge spillovers resulting from R&D investments among firms in the US. Finally, I have explored the suitability of the Epstein Zin model and found that there is evidence of overspecification. In what follows I summarize the conclusions reached for each of the methodologies, outline the main findings, and discuss future lines of research that I think are worth exploring.In Chapters 2 and 3 I have proposed, jointly with Stephane Bonhomme, the Grouped fixed-effects (GFE) framework, that offers a flexible yet parsimonious approach to model unobserved heterogeneity. The approach delivers estimates of common regression parameters, together with interpretable estimates of group-specific time patterns and group membership. The framework allows for strictly exogenous covariates and lagged outcomes. It also easily accommodates unit-specific fixed-effects in addition to the time-varying grouped patterns, and grouped heterogeneity in coefficients. Importantly, the relationship between group membership and observed covariates is left unrestricted. The GFE approach should be useful in applications where time-varying grouped effects may be present in the data. As a first example, the empirical analysis of the evolution of democracy shows evidence of a clustering of political regimes and transitions. More generally, GFE should be well-suited in difference-in-difference designs, as a way to relax parallel trend assumptions. Other potential applications include models of social interactions and spatial dependence where the reference groups or the spatial weights matrix are estimated from the panel data. The extension to nonlinear models is a natural next step. While it is possible to define GFE estimators in more general models (see for example equation (2.9)), the analysis raises statistical challenges. One area of applications is static or dynamic discrete choice modelling, where a discrete specification of unobserved heterogeneity may be appealing (Kasahara and Shimotsu, 2009, Browning and Carro, 2013). See Saggio (2012) for a first attempt in this direction. Lastly, another interesting extension is to relax the assumption that there is a finite number of well-separated groups in the population. As an alternative approach, one could view the grouped model as an approximation to the underlying data generating process, and characterize the statistical properties of GFE as the number of groups G increases with the two dimensions of the panel. In Chapter 4 I present a methodology to estimate both the structure of interactions and the spillover effects when the structure of interactions is not observable to the econometrician. This method is useful when the structure of interactions is sparse and persistent over time. Both of these assumptions can be partially relaxed: Sparsity can be relaxed by adding a priori information on the structure of interactions. Persistence over time can be relaxed by splitting the sample, parametrizing the spillover effects as a function of time, or by augmenting the number of regressors as explained in Appendix C.1. Spillovers arise when characteristics have an impact on the outcome of other individuals in the sample. This model is useful in at least two cases: First, in the context of randomized treatment experiments, when the treatment is subject to generate externalities. Second, in production function frameworks, where productivity generates spillovers. I propose a new estimator, the Pooled Lasso estimator, that can be seen as a panel data counterpart of the Lasso estimator. I provide an iterative computation method that combines the Lasso estimator with OLS pooled regression. Computation is fast, in relation to the large number of potential structures of interactions, given the global convex nature of the criterion. I analyze the properties of the Pooled Lasso estimator in a simplified model with no common parameters under assumptions of Gaussian and independent errors, both in the time and cross-sectional dimension. Based on a recent paper by Lam and Souza (2013), these strong conditions on the errors are likely to be relaxed. First, gaussianity can be replaced by conditions on the tail probability of errors. Second, limited time-series dependence can also be incorporated using Nagaev-type inequalities. Finally, mild cross-sectional dependence in the errors is also likely to be incorporated. I study the rate of convergence of cross-sectional spillover effects and, more generally, aggregate spillover effects. These quantities can be interpreted as relevant policy parameters depending on the application. Under conditions of cross-sectional independence on the error in estimation of the spillover effects, average spillover effects are estimated at a much better rate than individual spillover effects. Inference methods in the context of the Lasso are hard to derive due to the non-differentiability of the criterion. However, recent work in econometrics and statistics show good progress in this direction. For instance, Belloni et al.. (2013), show how to conduct inference in a post-lasso setting. That is, after using Lasso as a model selection devise, an ex-post OLS regression is run conditional on the estimated model. In particular, after using a Lasso-type estimator twice to select relevant controls in a treatment effect framework, they derive the asymptotic distribution of the treatment effect estimator and provide a formula for confidence intervals. One of the main features of their work is that, even in spite of imperfect model selection, their results hold uniformly for a large class of DGPs. Another recent work, by Lockhart et al., (2013) focus in developing a significance test of the predictor variable that enters the current lasso model, in the sequence of models visited along the lasso solution path of LARS (e.g. Efron et al., 2004). The Lasso solution path are the different solutions that the Lasso delivers when the penalty parameter decreases.",2015,
The quantile process under random censoring,"In this paper we discuss the asymptotic properties of quantile processes under random censoring. In contrast to most work in this area we prove weak convergence of an appropriately standardized quantile process under the assumption that the quantile regression model is only linear in the region, where the process is investigated. Additionally, we also discuss properties of the quantile process in sparse regression models including quantile processes obtained from the Lasso and adaptive Lasso. The results are derived by a combination of modern empirical process theory, classical martingale methods and a recent result of Kato (2009).",2012,Mathematical Methods of Statistics
Sparse alternatives to ridge regression: a random effects approach,"In a calibration of near-infrared (NIR) instrument, we regress some chemical compositions of interest as a function of their NIR spectra. In this process, we have two immediate challenges: first, the number of variables exceeds the number of observations and, second, the multicollinearity between variables are extremely high. To deal with the challenges, prediction models that produce sparse solutions have recently been proposed. The term â€˜sparseâ€™ means that some model parameters are zero estimated and the other parameters are estimated naturally away from zero. In effect, a variable selection is embedded in the model to potentially achieve a better prediction. Many studies have investigated sparse solutions for latent variable models, such as partial least squares and principal component regression, and for direct regression models such as ridge regression (RR). However, in the latter, it mainly involves an L1 norm penalty to the objective function such as lasso regression. In this study, we investigate new sparse alternative models for RR within a random effects model framework, where we consider Cauchy and mixture-of-normals distributions on the random effects. The results indicate that the mixture-of-normals model produces a sparse solution with good prediction and better interpretation. We illustrate the methods using NIR spectra datasets from milk and corn specimens.",2014,Journal of Applied Statistics
Biomaker patterns of fatty acids and other fat-soluble biocompounds in blood to indicate nuts intake in the European Prospective Investigation into Cancer and Nutrition (EPIC) study,"Summary Introduction Nuts are nutrient dense foods, rich in unsaturated fatty acids and other fat-soluble biocompounds. Epidemiological and intervention studies have shown that a high intake of nuts is associated with a reduced risk of coronary heart disease; however, inaccurate estimation of nut intake using self-reported methods might limit detection of other health benefits such as reduced risk of cancer, where potential strengths of associations are likely smaller. This study aimed to identify biomarker patterns including fatty acids (FAs) and other fat-soluble compounds (carotenoids and tocopherols) in blood to indicate nut intake by applying a statistical algorithm combining dimension reduction and variable selection methods. Methods The study included 2324 subjects (44% men) aged 35â€“70 years from 8 European countries in EPIC cross-sectional study. Recent and habitual nut intakes were assessed with 24-h dietary recalls (24-HDR) and dietary questionnaire (DQ), respectively. Potential biomarkers related to nut intake included 22 plasma FAs and serum fat-soluble compounds (7 carotenoids and 2 tocopherols). They were measured by gas chromatography and high-performance liquid chromatography, respectively. In order to identify patterns of these biomarkers, maximizing the explained variability in nut intake, reduced rank regression (RRR) models were used with optimal subsets of biomarkers selected by two different variable selection methods; RRR-based variable importance in projection (RRR-VIP) and least absolute shrinkage and selection operator (LASSO). Prior to the main analysis, nut intake and biomarker levels were log-transformed and adjusted for energy intake (nut intake only) and country taking residuals in linear models. The performance of RRR models was evaluated by Pearson correlation coefficients of biomarker pattern scores with recent and habitual nut intakes through internal two-fold cross-validation. Results Higher performance was observed in biomarker patterns on habitual nut intake reported in the DQ. The selected subsets of biomarkers slightly differed depending on which selection method was used, but arachidic acid, linoleic acid and tocopherols, known as abundant compounds in nuts, were commonly selected as biomarkers to explain habitual nut intake in both methods. Pearson correlation coefficients of biomarker pattern scores with habitual nut intake (rÂ =Â 0.27) were two to three times stronger than those of single biomarkers (linoleic acid rÂ =Â 0.08; arachidic acid rÂ =Â 0.11). Neither biomarker patterns nor single biomarkers performed well in explaining recent nut intake reported in the 24-HDR. Conclusions Biomarker patterns consisting of fatty acids or fat-soluble biocompounds better indicated nut intakes compared to any of these single biomarkers evaluated, especially for habitual nut intake. Whether these identified biomarker patterns improve assessment of nut intake in diet-disease studies needs to be evaluated in future studies.",2018,Revue D Epidemiologie Et De Sante Publique
A Sparse Regression Method for Group-Wise Feature Selection with False Discovery Rate Control,"The method of Sorted L-One Penalized Estimation, or SLOPE, is a sparse regression method recently introduced by Bogdan et. al. [1] . It can be used to identify significant predictor variables in a linear model that may have more unknown parameters than observations. When the correlations between predictor variables are small, the SLOPE method is shown to successfully control the false discovery rate (the expected proportion of the irrelevant among all selected predictors) at a user specified level. However, the requirement for nearly uncorrelated predictors is too restrictive for genomic data, as demonstrated in our recent study [2] by an application of SLOPE to realistic simulated DNA sequence data. A possible solution is to divide the predictor variables into nearly uncorrelated groups, and to modify the procedure to select entire groups with an overall significant group effect, rather than individual predictors. Following this motivation, we extend SLOPE in the spirit of Group LASSO to Group SLOPE, a method that can handle group structures between the predictor variables, which are ubiquitous in real genomic data. Our theoretical results show that Group SLOPE controls the group-wise false discovery rate (gFDR), when groups are orthogonal to each other. For use in non-orthogonal settings, we propose two types of Monte Carlo based heuristics, which lead to gFDR control with Group SLOPE in simulations based on real SNP data. As an illustration of the merits of this method, an application of Group SLOPE to a dataset from the Framingham Heart Study results in the identification of some known DNA sequence regions associated with bone health, as well as some new candidate regions. The novel methods are implemented in the R package grpSLOPEMC , which is publicly available at https://github.com/agisga/grpSLOPEMC.",2018,IEEE/ACM Transactions on Computational Biology and Bioinformatics
Persistent severe acute respiratory distress syndrome for the prognostic enrichment of trials,"BACKGROUND
Acute respiratory distress syndrome (ARDS) is heterogeneous. As an indication of the heterogeneity of ARDS, there are patients whose syndrome improves rapidly (i.e., within 24 hours), others whose hypoxemia improves gradually and still others whose severe hypoxemia persists for several days. The latter group of patients with persistent severe ARDS poses challenges to clinicians. We attempted to assess the baseline characteristics and outcomes of persistent severe ARDS and to identify which variables are useful to predict it.


METHODS
A secondary analysis of patient-level data from the ALTA, EDEN and SAILS ARDSNet clinical trials was conducted. We defined persistent severe ARDS as a partial pressure of arterial oxygen to fraction of inspired oxygen ratio (PaO2:FiO2) of equal to or less than 100 mmHg on the second study day following enrollment. Regularized logistic regression with an L1 penalty [Least Absolute Shrinkage and Selection Operator (LASSO)] techniques were used to identify predictive variables of persistent severe ARDS.


RESULTS
Of the 1531 individuals with ARDS alive on the second study day after enrollment, 232 (15%) had persistent severe ARDS. Of the latter, 100 (43%) individuals had mild or moderate hypoxemia at baseline. Usage of vasopressors was greater [144/232 (62%) versus 623/1299 (48%); p<0.001] and baseline severity of illness was higher in patients with versus without persistent severe ARDS. Mortality at 60 days [95/232 (41%) versus 233/1299 (18%); p<0.001] was higher, and ventilator-free (p<0.001), intensive care unit-free [0 (0-14) versus 19 (7-23); p<0.001] and non-pulmonary organ failure-free [3 (0-21) versus 20 (1-26); p<0.001] days were fewer in patients with versus without persistent severe ARDS. PaO2:FiO2, FiO2, hepatic failure and positive end-expiratory pressure at enrollment were useful predictive variables.


CONCLUSIONS
Patients with persistent severe ARDS have distinct baseline characteristics and poor prognosis. Identifying such patients at enrollment may be useful for the prognostic enrichment of trials.",2020,PLoS ONE
Efficient sparse Hessian based algorithms for the clustered lasso problem,"We focus on solving the clustered lasso problem, which is a least squares problem with the $\ell_1$-type penalties imposed on both the coefficients and their pairwise differences to learn the group structure of the regression parameters. Here we first reformulate the clustered lasso regularizer as a weighted ordered-lasso regularizer, which is essential in reducing the computational cost from $O(n^2)$ to $O(n\log (n))$. We then propose an inexact semismooth Newton augmented Lagrangian (SSNAL) algorithm to solve the clustered lasso problem or its dual via this equivalent formulation, depending on whether the sample size is larger than the dimension of the features. An essential component of the SSNAL algorithm is the computation of the generalized Jacobian of the proximal mapping of the clustered lasso regularizer. Based on the new formulation, we derive an efficient procedure for its computation. Comprehensive results on the global convergence and local linear convergence of the SSNAL algorithm are established. For the purpose of exposition and comparison, we also summarize/design several first-order methods that can be used to solve the problem under consideration, but with the key improvement from the new formulation of the clustered lasso regularizer. As a demonstration of the applicability of our algorithms, numerical experiments on the clustered lasso problem are performed. The experiments show that the SSNAL algorithm substantially outperforms the best alternative algorithm for the clustered lasso problem.",2018,arXiv: Optimization and Control
On Ising models and algorithms for the construction of symptom networks in psychopathological research.,"During the past 5 to 10 years, an estimation method known as eLasso has been used extensively to produce symptom networks (or, more precisely, symptom dependence graphs) from binary data in psychopathological research. The eLasso method is based on a particular type of Ising model that corresponds to binary pairwise Markov random fields, and its popularity is due, in part, to an efficient estimation process that is based on a series of lâ‚-regularized logistic regressions. In this article, we offer an unprecedented critique of the Ising model and eLasso. We provide a careful assessment of the conditions that underlie the Ising model as well as specific limitations associated with the eLasso estimation algorithm. This assessment leads to serious concerns regarding the implementation of eLasso in psychopathological research. Some potential strategies for eliminating or, at least, mitigating these concerns include (a) the use of partitioning or mixture modeling to account for unobserved heterogeneity in the sample of respondents, and (b) the use of co-occurrence measures for symptom similarity to either replace or supplement the covariance/correlation measure associated with eLasso. Two psychopathological data sets are used to highlight the concerns that are raised in the critique. (PsycINFO Database Record (c) 2019 APA, all rights reserved).",2019,Psychological methods
Behavior of Lasso and Lasso-based inference under limited variability,"We study the nonasymptotic behavior of Lasso and Lasso-based inference when the covariates exhibit limited variability, which does not appear to have been considered in the literature, despite its prevalence in applied research. In settings that are generally considered favorable to Lasso, we show that, if the absolute value of a nonzero regression coefficient is smaller or equal to a threshold, Lasso fails to select the corresponding covariate with high probability (approaching to 1 asymptotically). In particular, limited variability can render Lasso unable to select even those covariates with coefficients that are well-separated from zero. Moreover, based on simple theoretical examples, we show that post double Lasso and debiased Lasso can exhibit size distortions under limited variability. Monte Carlo simulations corroborate our theoretical results and further demonstrate that, under limited variability, the performance of Lasso and Lasso-based inference methods is very sensitive to the choice of the penalty parameter. This begs the question of how to make statistical inference (e.g., constructing confidence intervals) under limited variability. In moderately high-dimensional problems, where the number of covariates is large but still smaller than the sample size, OLS constitutes a natural alternative to Lasso-based inference methods. In empirically relevant settings, our simulation results show that, under limited variability, OLS with recently developed standard errors, which are proven robust to many covariates, demonstrates a superior finite sample performance relative to Lasso-based inference methods.",2019,arXiv: Statistics Theory
Sub-region based radiomics analysis for survival prediction in oesophageal tumours treated by definitive concurrent chemoradiotherapy,"BACKGROUND
Evaluating clinical outcome prior to concurrent chemoradiotherapy remains challenging for oesophageal squamous cell carcinoma (OSCC) as traditional prognostic markers are assessed at the completion of treatment. Herein, we investigated the potential of using sub-region radiomics as a novel tumour biomarker in predicting overall survival of OSCC patients treated by concurrent chemoradiotherapy.


METHODS
Independent patient cohorts from two hospitals were included for training (nâ€¯=â€¯87) and validation (n =â€¯46). Radiomics features were extracted from sub-regions clustered from patients' tumour regions using K-means method. The LASSO regression for 'Cox' method was used for feature selection. The survival prediction model was constructed based on the sub-region radiomics features using the Cox proportional hazards model. The clinical and biological significance of radiomics features were assessed by correlation analysis of clinical characteristics and copy number alterations(CNAs) in the validation dataset.


FINDINGS
The overall survival prediction model combining with seven sub-regional radiomics features was constructed. The C-indexes of the proposed model were 0.729 (0.656-0.801, 95% CI) and 0.705 (0.628-0.782, 95%CI) in the training and validation cohorts, respectively. The 3-year survival receiver operating characteristic (ROC) curve showed an area under the ROC curve of 0.811 (0.670-0.952, 95%CI) in training and 0.805 (0.638-0.973, 95%CI) in validation. The correlation analysis showed a significant correlation between radiomics features and CNAs.


INTERPRETATION
The proposed sub-regional radiomics model could predict the overall survival risk for patients with OSCC treated by definitive concurrent chemoradiotherapy. FUND: This work was supported by the Zhejiang Provincial Foundation for Natural Sciences, National Natural Science Foundation of China.",2019,EBioMedicine
Motor Imagery ECoG Signal Classification Using Sparse Representation with Elastic Net Constraint,"In recent years, the brain-computer interface (BCI) technology based on the motor imagery has provided a new method for people to communicate with the outside world. How to effectively extract features and improve the recognition rate of EEG signals is one of the hot problems in this field. This study is based on the motor imagery ECoG signals, in which the common spatial pattern (CSP) algorithm is used for feature extraction, and then the extracted energy features are classified by the classification algorithms. In order to improve the classification accuracy of the ECoG signals, this study introduces the sparse representation-based classification (SRC) algorithm with the elastic network constraint. Then the accelerated proximal gradient (APG) algorithm and the least angle regression (LARS) algorithm are respectively applied to sparse coding for the ECoG signals. The elastic network which combines the L1 norm and the L2 norm not only avoids the over-fitting problem, but also has a higher prediction ability than the Lasso algorithm. The experimental results demonstrate that the proposed method can achieve better classification performance than other algorithms, such as the sparse representation algorithms with L1 minimization, SVM, KNN, Adaboost, and Naive Bayes.",2018,2018 IEEE 7th Data Driven Control and Learning Systems Conference (DDCLS)
Abstract 2738: Early detection of colorectal cancer applying a combination of serum markers,"Background: Fecal occult blood testing (FOBT) is the recommended first line screening for the detection of colorectal cancer (CRC). To improve the detection of CRC we evaluated serum markers and combinations of serum markers as an alternative approach. Methods: Applying Lasso Regression, a specialized form of penalized logistic regression, we selected six markers for an evaluation in a collective of 857 patients including 301 CRC patients, 143 patients with adenoma, 266 healthy controls and 147 disease controls. For each marker and marker combination the performance was assessed. Results: We tested a total of 22 biomarkers for the detection of CRC from serum. Of these six markers were selected for a marker combination by Lasso Regression. Included were the well-known tumor markers CEA and CYFRA21-1 as well as novel markers or markers that are less routinely used for the detection of CRC: ferritin, osteopontin, anti-p53 and seprase. CEA showed the best sensitivity of all markers with 43.9 % at 95% specificity, followed by seprase (42.4%), CYFRA21-1 (35.5%), osteopontin (30.2%), ferritin (23.9%) and anti-p53 (20.0%). When these markers were combined a sensitivity of 72.3% was reached at a corresponding specificity of 95% and of 62.1% at 98% specificity. Focusing on more screening relevant stages, UICC stages 0-III, reduced the sensitivity slightly to 68.0% and 53.3%, respectively. In a sub-collective where matched stool samples were available (75 CRC cases and 234 controls) the sensitivity of the marker combination was comparable to fecal immunochemical testing (FIT) with 82.4% and 68.9% vs. 81.8% and 72.7% at 95% and 98% specificity, respectively. Conclusion: When six markers were combined to detect CRC from serum, the combination reached a performance that was comparable to FIT. This provides a novel tool for CRC screening to trigger a follow-up colonoscopy for a final diagnosis. Citation Format: {Authors}. {Abstract title} [abstract]. In: Proceedings of the 101st Annual Meeting of the American Association for Cancer Research; 2010 Apr 17-21; Washington, DC. Philadelphia (PA): AACR; Cancer Res 2010;70(8 Suppl):Abstract nr 2738.",2010,Cancer Research
Study of model evaluation method and of selection method of tuning parameter in Lasso,"High-dimensional data sets have been used extensively in recent years. However, their use is computationally expensive, and model features are difficult to capture. Thus, it is necessary to construct a simple and appropriate model. The least absolute shrinkage and selection operator (Lasso) can estimate several regression coefficients as exactly zero, making it possible to estimate a model and select variables at the same time. However, several problems exist. The decision procedure for the tuning parameter, which is the coefficient of the penalty term, is not established, and it is difficult to determine the value of the estimated model. In this paper, we consider a model evaluation method and a selection method for the tuning parameter. We investigate selection methods for the most appropriate tuning parameter and model evaluation methods using the model evaluation indexes RSS, AIC, BIC, and Mallowsâ€™s Cp. Let several regression coefficients be zero and vary the number of zeroes. Then, we observe the performances of the evaluation indexes. Furthermore, we observe their performances when the values of the regression coefficients are close to zero and confirm them by simulation.",2016,
Evaluating Variable Selection Techniques for Multivariate Linear Regression,"The purpose of variable selection techniques is to select a subset of relevant variables for a particular learning algorithm in order to improve the accuracy of prediction model and improve the efficiency of the model. We conduct an empirical analysis to evaluate and compare seven well-known variable selection techniques for multiple linear regression model, which is one of the most commonly used regression model in practice. The variable selection techniques we apply are forward selection, backward elimination, stepwise selection, genetic algorithm (GA), ridge regression, lasso (Least Absolute Shrinkage and Selection Operator) and elastic net. Based on the experiment with 49 regression data sets, it is found that GA resulted in the lowest error rates while lasso most significantly reduces the number of variables. In terms of computational efficiency, forward/backward elimination and lasso requires less time than the other techniques.",2016,
A Comparison of Penalized Regressions for Estimating Directed Acyclic Networks,"Network models can be classified into two large groups: undirected and directed. Directed network graphs that can represent causal relationships are likely more appropriate in bio-medical data. There have been many studies to estimate DAGs(Directed Acyclic Graphs), of which the two-stage approach using lasso effectively. Find the edges between the nodes in the first step and find the direction in the second step. In this paper, we try to compare which penalized regression is better to find neighborhoods through simulations. We present the result of the simulations that shows which penalized regression is the best.",2018,2018 Tenth International Conference on Ubiquitous and Future Networks (ICUFN)
SÃ©lection de variables pour la classification non supervisÃ©e en grande dimension,"Il existe des situations de modelisation statistique pour lesquelles le probleme classique de classification non supervisee (c'est-a-dire sans information a priori sur la nature ou le nombre de classes a constituer) se double d'un probleme d'identification des variables reellement pertinentes pour determiner la classification. Cette problematique est d'autant plus essentielle que les donnees dites de grande dimension, comportant bien plus de variables que d'observations, se multiplient ces dernieres annees : donnees d'expression de genes, classification de courbes... Nous proposons une procedure de selection de variables pour la classification non supervisee adaptee aux problemes de grande dimension. Nous envisageons une approche par modeles de melange gaussien, ce qui nous permet de reformuler le probleme de selection des variables et du choix du nombre de classes en un probleme global de selection de modele. Nous exploitons les proprietes de selection de variables de la regularisation l1 pour construire efficacement, a partir des donnees, une collection de modeles qui reste de taille raisonnable meme en grande dimension. Nous nous demarquons des procedures classiques de selection de variables par regularisation l1 en ce qui concerne l'estimation des parametres : dans chaque modele, au lieu de considerer l'estimateur Lasso, nous calculons l'estimateur du maximum de vraisemblance. Ensuite, nous selectionnons l'un des ces estimateurs du maximum de vraisemblance par un critere penalise non asymptotique base sur l'heuristique de pente introduite par Birge et Massart. D'un point de vue theorique, nous etablissons un theoreme de selection de modele pour l'estimation d'une densite par maximum de vraisemblance pour une collection aleatoire de modeles. Nous l'appliquons dans notre contexte pour trouver une forme de penalite minimale pour notre critere penalise. D'un point de vue pratique, des simulations sont effectuees pour valider notre procedure, en particulier dans le cadre de la classification non supervisee de courbes. L'idee cle de notre procedure est de n'utiliser la regularisation l1 que pour constituer une collection restreinte de modeles et non pas aussi pour estimer les parametres des modeles. Cette etape d'estimation est realisee par maximum de vraisemblance. Cette procedure hybride nous est inspiree par une etude theorique menee dans une premiere partie dans laquelle nous etablissons des inegalites oracle l1 pour le Lasso dans les cadres de regression gaussienne et de melange de regressions gaussiennes, qui se demarquent des inegalites oracle l0 traditionnellement etablies par leur absence totale d'hypothese.",2012,
Penalized methods in genome-wide association studies,"Penalized regression methods are becoming increasingly popular in genomewide association studies (GWAS) for identifying genetic markers associated with disease. However, standard penalized methods such as the LASSO do not take into account the possible linkage disequilibrium between adjacent markers. We propose a novel penalized approach for GWAS using a dense set of single nucleotide polymorphisms (SNPs). The proposed method uses the minimax concave penalty (MCP) for marker selection and incorporates linkage disequilibrium (LD) information by penalizing the difference of the genetic effects at adjacent SNPs with high correlation. A coordinate descent algorithm is derived to implement the proposed method. This algorithm is efficient and stable in dealing with a large number of SNPs. A multi-split method is used to calculate the p-values of the selected SNPs for assessing their significance. We refer to the proposed penalty function as the smoothed minimax concave penalty (SMCP) and the proposed approach as the SMCP method. Performance of the proposed SMCP method and its comparison with a LASSO approach are evaluated through simulation studies, which demonstrate that the proposed method is more accurate in selecting associated SNPs. Its applicability to real data is illustrated using data from a GWAS on rheumatoid arthritis. Based on the idea of SMCP, we propose a new penalized method for group variable selection in GWAS with respect to the correlation between adjacent groups. The proposed method uses the group LASSO for encouraging group sparsity and a",2011,
MRI-based Radiomics nomogram to detect primary rectal cancer with synchronous liver metastases,"Synchronous liver metastasis (SLM) remains a major challenge for rectal cancer. Early detection of SLM is a key factor to improve the survival rate of rectal cancer. In this radiomics study, we predicted the SLM based on the radiomics of primary rectal cancer. A total of 328 radiomics features were extracted from the T2WI images of 194 patients. The least absolute shrinkage and selection operator (LASSO) regression was used to reduce the feature dimension and to construct the radiomics signature. after LASSO, principal component analysis (PCA) was used to sort the features of the surplus characteristics, and selected the features of the total contribution of 85%. Then the prediction model was built by linear regression, and the decision curve analysis was used to judge the net benefit of LASSO and PCA. In addition, we used two independent cohorts for training (nâ€‰=â€‰135) and validation (nâ€‰=â€‰159). We found that the model based on LASSO dimensionality construction had the maximum net benefit (in the training set (AUC [95% confidence interval], 0.857 [0.787â€“0.912]) and in the validation set (0.834 [0.714â€“0.918]). The radiomics nomogram combined with clinical risk factors and LASSO features showed a good predictive performance in the training set (0.921 [0.862â€“0.961]) and validation set (0.912 [0.809â€“0.97]). Our study indicated that radiomics based on primary rectal cancer could provide a non-invasive way to predict the risk of SLM in clinical practice.",2019,Scientific Reports
On the Prediction Performance of the Lasso,"Although the Lasso has been extensively studied, the relationship between its prediction performance and the correlations of the covariates is not fully understood. In this paper, we give new insights into this relationship in the context of multiple linear regression. We show, in particular, that the incorporation of a simple correlation measure into the tuning parameter leads to a nearly optimal prediction performance of the Lasso even for highly correlated covariates. However, we also reveal that for moderately correlated covariates, the prediction performance of the Lasso can be mediocre irrespective of the choice of the tuning parameter. For the illustration of our approach with an important application, we deduce nearly optimal rates for the least-squares estimator with total variation penalty",2017,Bernoulli
Enforcing Group Structure through the Group Fused Lasso,"We introduce the Group Total Variation (GTV) regularizer, a modification of Total Variation that uses the l 2,1 norm instead of the l 1 one to deal with multidimensional features. When used as the only regularizer, GTV can be applied jointly with iterative convex optimization algorithms such as FISTA. This requires to compute its proximal operator which we derive using a dual formulation. GTV can also be combined with a Group Lasso (GL) regularizer, leading to what we call Group Fused Lasso (GFL) whose proximal operator can now be computed combining the GTV and GL proximals through proximal Dykstra algorithm.We will illustrate how to apply GFL in strongly structured but ill-posed regression problems as well as the use of GTV to denoise colour images.",2015,
Regularized rare variant enrichment analysis for case-control exome sequencing data.,"Rare variants have recently garnered an immense amount of attention in genetic association analysis. However, unlike methods traditionally used for single marker analysis in GWAS, rare variant analysis often requires some method of aggregation, since single marker approaches are poorly powered for typical sequencing study sample sizes. Advancements in sequencing technologies have rendered next-generation sequencing platforms a realistic alternative to traditional genotyping arrays. Exome sequencing in particular not only provides base-level resolution of genetic coding regions, but also a natural paradigm for aggregation via genes and exons. Here, we propose the use of penalized regression in combination with variant aggregation measures to identify rare variant enrichment in exome sequencing data. In contrast to marginal gene-level testing, we simultaneously evaluate the effects of rare variants in multiple genes, focusing on gene-based least absolute shrinkage and selection operator (LASSO) and exon-based sparse group LASSO models. By using gene membership as a grouping variable, the sparse group LASSO can be used as a gene-centric analysis of rare variants while also providing a penalized approach toward identifying specific regions of interest. We apply extensive simulations to evaluate the performance of these approaches with respect to specificity and sensitivity, comparing these results to multiple competing marginal testing methods. Finally, we discuss our findings and outline future research.",2014,Genetic epidemiology
Regression-based sparse polynomial chaos for uncertainty quantification of subsurface flow models,"Abstract Surrogate-modelling techniques including Polynomial Chaos Expansion (PCE) is commonly used for statistical estimation (aka. Uncertainty Quantification) of quantities of interests obtained from expensive computational models. PCE is a data-driven regression-based technique that relies on spectral polynomials as basis-functions. In this technique, the outputs of few numerical simulations are used to estimate the PCE coefficients within a regression framework combined with regularization techniques where the regularization parameters are estimated using standard cross-validation as applied in supervised machine learning methods. In the present work, we introduce an efficient method for estimating the PCE coefficients combining Elastic Net regularization with a data-driven feature ranking approach. Our goal is to increase the probability of identifying the most significant PCE components by assigning each of the PCE coefficients a numerical value reflecting the magnitude of the coefficient and its stability with respect to perturbations in the input data. In our evaluations, the proposed approach has shown high convergence rate for high-dimensional problems, where standard feature ranking might be challenging due to the curse of dimensionality. The presented method is implemented within a standard machine learning library (scikit-learn [1] ) allowing for easy experimentation with various solvers and regularization techniques (e.g. Tikhonov, LASSO, LARS, Elastic Net) and enabling automatic cross-validation techniques using a widely used and well tested implementation. We present a set of numerical tests on standard analytical functions, a two-phase subsurface flow model and a simulation dataset for CO2 sequestration in a saline aquifer. For all test cases, the proposed approach resulted in a significant increase in PCE convergence rates.",2019,J. Comput. Phys.
Gene Expression Profiling Stratifies IDH-Wildtype Glioblastoma With Distinct Prognoses,"Objectives: In the present study, we aimed to determine the candidate genes that may function as biomarkers to further distinguish patients with isocitrate dehydrogenase (IDH)-wildtype glioblastoma (GBM), which are heterogeneous with respect to clinical outcomes. Materials and Methods: We selected 41 candidate genes associated with overall survival (OS) using univariate Cox regression from IDH-wildtype GBM patients based on RNA sequencing (RNAseq) expression data from the Chinese Glioma Genome Atlas (CGGA, n = 105) and The Cancer Genome Atlas (TCGA, n = 139) cohorts. Next, a seven-gene-based risk signature was formulated according to Least Absolute Shrinkage and Selection Operator (LASSO) regression algorithm in the CGGA RNAseq database as a training set, while another 525 IDH-wildtype GBM patient TCGA datasets, consisting of RNA sequencing and microarray data, were used for validation. Patient survival in the low- and high-risk groups was calculated using Kaplan-Meier survival curve analysis and the log-rank test. Uni-and multivariate Cox regression analysis was used to assess the prognosis value. Gene oncology (GO) and gene set enrichment analysis (GSEA) were performed for the functional analysis of the seven-gene-based risk signature. Results: We developed a seven-gene-based signature, which allocated each patient to a risk group (low or high). Patients in the high-risk group had dramatically shorter overall survival than their low-risk counterparts in three independent cohorts. Univariate and multivariate analysis showed that the seven-gene signature remained an independent prognostic factor. Moreover, the seven-gene risk signature exhibited a striking prognostic validity, with AUC of 78.4 and 73.9%, which was higher than for traditional ""age"" (53.7%, 62.4%) and ""GBM sub-type"" (57.7%, 52.9%) in the CGGA- and TCGA-RNAseq databases, respectively. Subsequent bioinformatics analysis predicted that the seven-gene signature was involved in the inflammatory response, immune response, cell adhesion, and apoptotic process. Conclusions: Our findings indicate that the seven-gene signature could be a potential prognostic biomarker. This study refined the current classification system of IDH-wildtype GBM and may provide a novel perspective for the research and individual therapy of IDH-wildtype GBM.",2019,Frontiers in Oncology
"Feature network models for proximity data : statistical inference, model selection, network representations and links with related models","Feature Network Models (FNM) are graphical structures that represent
 proximity data in a discrete space with the use of features. A 
statistical inference theory is introduced, based on the additivity 
properties of networks and the linear regression framework. Considering 
features as predictor variables leads in a natural way to a univariate 
multiple regression problem with positivity restrictions on the 
parameters, which represent edge lengths in the network representation. 
Theoretical standard errors and confidence intervals are obtained for 
the parameters and their performance is evaluated by Monte Carlo 
simulation. When the feature structure is not known in advance, a 
strategy is proposed to select an adequate subset of features that takes
 into account a good compromise between model fit and model complexity 
using Gray codes and the positive lasso. The same statistical inference 
theory also holds for additive trees that are special cases of FNM. 
Standard errors and confidence intervals, model tests and prediction 
error are obtained for the estimates of the branch lengths of additive 
trees. The dissertation concludes by demonstrating that there exists a 
universal network representation of city-block models based on key 
elements of the network representation consisting of betweenness, metric
 segmental additivity and internal nodes.",2006,
Component-based regularisation of multivariate generalised linear mixed models,"We address the componentâ€“based regularisation of a multivariate Generalised Linear Mixed Model (GLMM) in the framework of grouped data. A set Y of random responses is modelled with a multivariate GLMM, based on a set X of explanatory variables, a setA of additional explanatory variables, and random effects to introduce the withinâ€“group dependence of observations. Variables in X are assumed many and redundant so that regression demands regularisation. This is not the case for A, which contains few and selected variables. Regularisation is performed building an appropriate number of orthogonal components that both contribute to model Y and capture relevant structural information in X. To estimate the model, we propose to maximise a criterion specific to the Supervised Componentâ€“based Generalised Linear Regression (SCGLR) within an adaptation of Schallâ€™s algorithm. This extension of SCGLR is tested on both simulated and real grouped data, and compared to ridge and LASSO regularisations. Supplementary material for this article is available online.",2019,
Rapid compositional analysis of sawdust using sparse method and near infrared spectroscopy,"This paper proposes to measure the components of sawdust by combining a new sparse method with near infrared (NIR) spectroscopy technology. The spectroscopic data of sawdust samples are acquired by the means of Fourier transform near-infrared (FT-NIR) spectrometer. Wavelet filter is used to remove undesired noises from the spectroscopic data, and multivariate statistical methods, such as principal component regression (PCR), partial least squares regression (PLS) and least absolute shrinkage and selection operator (LASSO) are used to model the relationship between the spectroscopic data and sawdust composition. The constructed model is then tested on a set of new samples. Compared with PCR and PLS, it is shown that LASSO, a sparse method, is capable of constructing a sparse model with stronger ability in interpretation while retaining good modeling accuracy.",2014,The 26th Chinese Control and Decision Conference (2014 CCDC)
Linear Convergence of SVRG in Statistical Estimation,"The last several years has witness the huge success on the stochastic variance reduction method in the finite sum problem. However assumption on strong convexity to have linear rate limits its applicability. In particular, it does not include several important formulations such as Lasso, group Lasso, logistic regression, and some nonconvex models including corrected Lasso and SCAD. In this paper, we prove that, for a class of statistical M-estimators covering examples mentioned above, SVRG solves the formulation with a linear convergence rate without strong convexity or even convexity. Our analysis makes use of restricted strong convexity, under which we show that SVRG converges linearly to the fundamental statistical precision of the model, i.e., the difference between true unknown parameter Î¸âˆ— and the optimal solution Î¸Ì‚ of the model.",2016,ArXiv
Cirrus: An Automated Mammography-Based Measure of Breast Cancer Risk Based on Textural Features,"Background
We applied machine learning to find a novel breast cancer predictor based on information in a mammogram.


Methods
Using image-processing techniques, we automatically processed 46Â 158 analog mammograms for 1345 cases and 4235 controls from a cohort and case-control study of Australian women, and a cohort study of Japanese American women, extracting 20 textural features not based on pixel brightness threshold. We used Bayesian lasso regression to create individual- and mammogram-specific measures of breast cancer risk, Cirrus. We trained and tested measures across studies. We fitted Cirrus with conventional mammographic density measures using logistic regression, and computed odds ratios (OR) per standard deviation adjusted for age and body mass index.


Results
Combining studies, almost all textural features were associated with case-control status. The ORs for Cirrus measures trained on one study and tested on another study ranged from 1.56 to 1.78 (all Pâ€‰<â€‰10-6). For the Cirrus measure derived from combining studies, the OR was 1.90 (95% confidence interval [CI] = 1.73 to 2.09), equivalent to a fourfold interquartile risk ratio, and was little attenuated after adjusting for conventional measures. In contrast, the OR for the conventional measure was 1.34 (95% CI = 1.25 to 1.43), and after adjusting for Cirrus it became 1.16 (95% CI = 1.08 to 1.24; Pâ€‰=â€‰4 Ã— 10-5).


Conclusions
A fully automated personal risk measure created from combining textural image features performs better at predicting breast cancer risk than conventional mammographic density risk measures, capturing half the risk-predicting ability of the latter measures. In terms of differentiating affected and unaffected women on a population basis, Cirrus could be one of the strongest known risk factors for breast cancer.",2018,JNCI Cancer Spectrum
