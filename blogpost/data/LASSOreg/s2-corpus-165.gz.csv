title,abstract,year,journal
Union support recovery in high-dimensional multivariate regression,"In the problem of multivariate regression, a K-dimensional response vector is regressed upon a common set of p covariates, with a matrix B* isin RopfptimesK of regression coefficients. We study the behavior of the group Lasso using lscr1/lscr2 regularization for the union support problem, meaning that the set of s rows for which B* is non-zero is recovered exactly. Studying this problem under high-dimensional scaling, we show that group Lasso recovers the exact row pattern with high probability over the random design and noise for scalings of (n, p, s) such that the sample complexity parameter given by thetas(n, p, s) := n/[2psi(B*) log(p - s)] exceeds a critical threshold. Here n is the sample size, p is the ambient dimension of the regression model, s is the number of non-zero rows, and psi(B*) is a sparsity-overlap function that measures a combination of the sparsities and overlaps of the K-regression coefficient vectors that constitute the model. This sparsity-overlap function reveals that, if the design is uncorrelated on the active rows, block lscr1/lscr2 regularization for multivariate regression never harms performance relative to an ordinary Lasso approach, and can yield substantial improvements in sample complexity (up to a factor of K) when the regression vectors are suitably orthogonal. For more general designs, it is possible for the ordinary Lasso to outperform the group Lasso.",2008,"2008 46th Annual Allerton Conference on Communication, Control, and Computing"
Vijverberg et al. 1 Grey matter network differences between behavioral variant Frontotemporal Dementia and Alzheimerâ€™s disease,"We set out to study whether single-subject grey matter (GM) networks show disturbances that are specific for Alzheimerâ€™s disease (AD) (n=90) or behavioral variant Frontotemporal dementia (bvFTD) (n=59), and whether such disturbances would be related to cognitive deficits measured with Mini-mental state examination (MMSE) and a neuropsychological battery, using subjective cognitive decline subjects (SCD) as reference. AD and bvFTD patients had a lower degree, connectivity density, clustering, path length, betweenness centrality and small world values compared to SCD. AD patients had a lower connectivity density than bvFTD patients (F = 5.79, p = 0.02; MeanÂ±SD bvFTD 16.10% Â± 1.19; MeanÂ±SD AD 15.64% Â± 1.02). Lasso logistic regression showed that connectivity differences between bvFTD and AD were specific to 23 anatomical areas, in terms of local GM volume, degree and clustering. Lower clustering values and lower degree values were specifically associated with worse MMSE scores and lower performance on the neuropsychological tests. GM showed disease-specific alterations, when comparing bvFTD with AD patients, and these alterations were associated with cognitive deficits.",2017,
An L1 Representer Theorem for Multiple-Kernel Regression,"The theory of RKHS provides an elegant framework for supervised learning. It is the foundation of all kernel methods in machine learning. Implicit in its formulation is the use of a quadratic regularizer associated with the underlying inner product which imposes smoothness constraints. In this paper, we consider instead the generalized total-variation (gTV) norm as the sparsity-promoting regularizer. This leads us to propose a new Banach-space framework that justifies the use of generalized LASSO, albeit in a slightly modified version. We prove a representer theorem for multiple-kernel regression (MKR) with gTV regularization. The theorem states that the solutions of MKR have kernel expansions with adaptive positions, while the gTV norm enforces an $\ell_1$ penalty on the coefficients. We discuss the sparsity-promoting effect of the gTV norm which prevents redundancy in the multiple-kernel scenario.",2018,ArXiv
"Simultaneous regression shrinkage , variable selection and clustering of predictors with OSCAR","In this paper, a new method called the OSCAR (Octagonal Shrinkage and Clustering Algorithm for Regression) is proposed to simultaneously select variables and perform supervised clustering in the context of linear regression. The technique is based on penalized least squares with a geometrically intuitive penalty function that, like the LASSO penalty, shrinks some coefficients to exactly zero. Additionally, this penalty yields exact equality of some coefficients, encouraging correlated predictors that have a similar effect on the response to form clusters represented by a single coefficient. These resulting clusters can then be investigated further to discover what contributes to the group having a similar behavior. The OSCAR then enjoys sparseness in terms of the number of unique coefficients in the model. The proposed procedure is shown to compare favorably to the existing shrinkage and variable selection techniques in terms of both prediction error and reduced model complexity.",2006,
Controlling false discoveries in Bayesian gene networks with lasso regression p-values,"Motivation Bayesian networks can represent directed gene regulations and therefore are favored over co-expression networks. However, hardly any Bayesian network study concerns the false discovery control (FDC) of network edges, leading to low accuracies due to systematic biases from inconsistent false discovery levels in the same study. Results We design four empirical tests to examine the FDC of Bayesian networks from three p-value based lasso regression variable selections â€” two existing and one we originate. Our method, lassopv, computes p-values for the critical regularization strength at which a predictor starts to contribute to lasso regression. Using null and Geuvadis datasets, we find that lassopv obtains optimal FDC in Bayesian gene networks, whilst existing methods have defective p-values. The FDC concept and tests extend to most network inference scenarios and will guide the design and improvement of new and existing methods. Our novel variable selection method with lasso regression also allows FDC on other datasets and questions, even beyond network inference and computational biology. Availability Lassopv is implemented in R and freely available at https://github.com/lingfeiwang/lassopv and https://cran.r-project.org/package=lassopv. Contact Lingfei.Wang@roslin.ed.ac.uk Supplementary information Supplementary data are available at Bioinformatics online.",2017,bioRxiv
Homotopy Parametric Simplex Method for Sparse Learning,"High dimensional sparse learning has imposed a great computational challenge to large scale data analysis. In this paper, we are interested in a broad class of sparse learning approaches formulated as linear programs parametrized by a {\em regularization factor}, and solve them by the parametric simplex method (PSM). Our parametric simplex method offers significant advantages over other competing methods: (1) PSM naturally obtains the complete solution path for all values of the regularization parameter; (2) PSM provides a high precision dual certificate stopping criterion; (3) PSM yields sparse solutions through very few iterations, and the solution sparsity significantly reduces the computational cost per iteration. Particularly, we demonstrate the superiority of PSM over various sparse learning approaches, including Dantzig selector for sparse linear regression, LAD-Lasso for sparse robust linear regression, CLIME for sparse precision matrix estimation, sparse differential network estimation, and sparse Linear Programming Discriminant (LPD) analysis. We then provide sufficient conditions under which PSM always outputs sparse solutions such that its computational performance can be significantly boosted. Thorough numerical experiments are provided to demonstrate the outstanding performance of the PSM method.",2017,ArXiv
Slope meets Lasso: improved oracle bounds and optimality,"We show that two polynomial time methods, a Lasso estimator with adaptively chosen tuning parameter and a Slope estimator, adaptively achieve the exact minimax prediction and $\ell_2$ estimation rate $(s/n)\log (p/s)$ in high-dimensional linear regression on the class of $s$-sparse target vectors in $\mathbb R^p$. This is done under the Restricted Eigenvalue (RE) condition for the Lasso and under a slightly more constraining assumption on the design for the Slope. The main results have the form of sharp oracle inequalities accounting for the model misspecification error. The minimax optimal bounds are also obtained for the $\ell_q$ estimation errors with $1\le q\le 2$ when the model is well-specified. The results are non-asymptotic, and hold both in probability and in expectation. The assumptions that we impose on the design are satisfied with high probability for a large class of random matrices with independent and possibly anisotropically distributed rows. We give a comparative analysis of conditions, under which oracle bounds for the Lasso and Slope estimators can be obtained. In particular, we show that several known conditions, such as the RE condition and the sparse eigenvalue condition are equivalent if the $\ell_2$-norms of regressors are uniformly bounded.",2016,arXiv: Statistics Theory
Non-Invasive Fuhrman Grading of Clear Cell Renal Cell Carcinoma Using Computed Tomography Radiomics Features and Machine Learning,"Purpose: To identify optimal classification methods for computed tomography (CT) radiomics-based preoperative prediction of clear cells renal cell carcinoma (ccRCC) grade. Methods and material: Seventy one ccRCC patients were included in the study. Three image preprocessing techniques (Laplacian of Gaussian, wavelet filter, and discretization of the intensity values) were applied on tumor volumes. In total, 2530 radiomics features (tumor shape and size, intensity statistics, and texture) were extracted from each segmented tumor volume. Univariate analysis was performed to assess the association of each feature with the histological condition. In the case of multivariate analysis, the following was implemented: three feature selection including the least absolute shrinkage and selection operator (LASSO), students t-test and minimum Redundancy Maximum Relevance (mRMR) algorithms. These selected features were then used to construct three classification models (SVM, random forest, and logistic regression) to discriminate the high from low-grade ccRCC at nephrectomy. Lastly, multivariate model performance was evaluated on the bootstrapped validation cohort using the area under receiver operating characteristic curve (AUC). Results: Univariate analysis demonstrated that among different image sets, 128 bin discretized images have statistically significant different (q-value < 0.05) texture parameters with a mean of AUC 0.74 (q-value < 0.05). The three ML-based classifier shows proficient discrimination of the high from low-grade ccRCC. The AUC was 0.78 in logistic regression, 0.62 in random forest, and 0.83 in SVM model, respectively. Conclusion: Radiomics features can be a useful and promising non-invasive method for preoperative evaluation of ccRCC Fuhrman grades. Key words: RCC, Radiomics, Machine Learning, Computed Tomography",2019,arXiv: Medical Physics
Regularized Ordinal Regression and the ordinalNet R Package,"Regularization techniques such as the lasso (Tibshirani 1996) and elastic net (Zou and Hastie 2005) can be used to improve regression model coefficient estimation and prediction accuracy, as well as to perform variable selection. Ordinal regression models are widely used in applications where the use of regularization could be beneficial; however, these models are not included in many popular software packages for regularized regression. We propose a coordinate descent algorithm to fit a broad class of ordinal regression models with an elastic net penalty. Furthermore, we demonstrate that each model in this class generalizes to a more flexible form, for instance to accommodate unordered categorical data. We introduce an elastic net penalty class that applies to both model forms. Additionally, this penalty can be used to shrink a non-ordinal model toward its ordinal counterpart. Finally, we introduce the R package ordinalNet, which implements the algorithm for this model class.",2017,arXiv: Computation
Corpus Callosum Radiomics-Based Classification Model in Alzheimer's Disease: A Case-Control Study,"Background: Alzheimer's disease (AD) is a progressive neurodegenerative disease that causes the decline of some cognitive impairments. The present study aimed to identify the corpus callosum (CC) radiomic features related to the diagnosis of AD and build and evaluate a classification model. Methods: Radiomics analysis was applied to the three-dimensional T1-weighted magnetization-prepared rapid gradient echo (MPRAGE) images of 78 patients with AD and 44 healthy controls (HC). The CC, in each subject, was segmented manually and 385 features were obtained after calculation. Then, the feature selection were carried out. The logistic regression model was constructed and evaluated according to identified features. Thus, the model can be used for distinguishing the AD from HC subjects. Results: Eleven features were selected from the three-dimensional T1-weighted MPRAGE images using the LASSO model, following which, the logistic regression model was constructed. The area under the receiver operating characteristic curve values (AUC), sensitivity, specificity, accuracy, precision, and positive and negative predictive values were 0.720, 0.792, 0.500, 0.684, 0.731, 0.731, and 0.583, respectively. Conclusion: The results demonstrated the potential of CC texture features as a biomarker for the diagnosis of AD. This is the first study showing that the radiomics model based on machine learning was a valuable method for the diagnosis of AD.",2018,Frontiers in Neurology
"Adaptive estimation of the baseline hazard function in the Cox model by model selection, with high-dimensional covariates","The purpose of this article is to provide an adaptive estimator of the baseline function in the Cox model with high-dimensional covariates. We consider a two-step procedure : first, we estimate the regression parameter of the Cox model via a Lasso procedure based on the partial log-likelihood, secondly, we plug this Lasso estimator into a least-squares type criterion and then perform a model selection procedure to obtain an adaptive penalized contrast estimator of the baseline function. Using non-asymptotic estimation results stated for the Lasso estimator of the regression parameter, we establish a non-asymptotic oracle inequality for this penalized contrast estimator of the baseline function, which highlights the discrepancy of the rate of convergence when the dimension of the covariates increases.",2015,arXiv: Statistics Theory
Screening for Cognitive Impairment by Model-Assisted Cerebral Blood Flow Estimation,"Objective: Alzheimer's disease (AD) is a progressive and debilitating neurodegenerative disease; a major health concern in the ageing population with an estimated prevalence of 46 million dementia cases worldwide. Early diagnosis is therefore crucial so mitigating treatments can be initiated at an early stage. Cerebral hypoperfusion has been linked with blood-brain barrier dysfunction in the early stages of AD, and screening for chronic cerebral hypoperfusion in individuals has been proposed for improving the early diagnosis of AD. However, ambulatory measurements of cerebral blood flow are not routinely carried out in the clinical setting. In this study, we combine physiological modeling with Holter blood pressure monitoring and carotid ultrasound imaging to predict 24-h cerebral blood flow (CBF) profiles in individuals. One hundred and three participants [53 with mild cognitive impairment (MCI) and 50 healthy controls] underwent model-assisted prediction of 24-h CBF. Model-predicted CBF and neuropsychological tests were features in lasso regression models for MCI diagnosis. Results: A CBF-enhanced classifier for diagnosing MCI performed better, area-under-the-curve (AUC) = 0.889 (95%-CI: 0.800 to 0.978), than a classifier based only on the neuropsychological test scores, AUC = 0.818 (95%-CI: 0.643 to 0.992). An additional cohort of 25 participants (11 MCI and 14 healthy) was recruited to perform model validation by arterial spin-labeling magnetic resonance imaging, and to establish a link between measured CBF that predicted by the model. Conclusion: Ultrasound imaging and ambulatory blood pressure measurements enhanced with physiological modeling can improve MCI diagnosis accuracy.",2018,IEEE Transactions on Biomedical Engineering
CancerClock: A DNA Methylation Age Predictor to Identify and Characterize Aging Clock in Pan-Cancer,"Many biological indicators related to chronological age have been proposed. Recent studies found that epigenetic clock or DNA methylation age is highly correlated with chronological age. In particular, a significant difference between DNA methylation age (m-age) and chronological age was observed in cancers. However, the prediction and characterization of m-age in pan-cancer remains an explored area. In this study, 1,631 age-related methylation sites in normal tissues were discovered and analyzed. A comprehensive computational model named CancerClock was constructed to predict the m-age for normal samples based on methylation levels of the extracted methylation sites. LASSO linear regression model was used to screen and train the CancerClock model in normal tissues. The accuracy of CancerClock has proved to be 81%, and the correlation value between chronological age and m-age was 0.939 (P < 0.01). Next, CancerClock was used to evaluate the difference between m-age and chronological age for 33 cancer types from TCGA. There were significant differences between predicted m-age and chronological age in large number of cancer samples. These cancer samples were defined as ""age-related cancer samples"" and they have some differential methylation sites. The differences between predicted m-age and chronological age may contribute to cancer development. Some of these differential methylation sites were associated with cancer survival. CancerClock provided assistance in estimating the m-age in normal and cancer samples. The changes between m-age and chronological age may improve the diagnosis and prognosis of cancers.",2019,Frontiers in Bioengineering and Biotechnology
The predictive power of earnings conference calls : predicting stock price movement with earnings call transcripts,"Earnings conference calls are considered a valuable text based information source for investors. This paper investigates the possibility to predict the direction of stock prices by analyzing the transcripts of earnings conference calls. The paper investigates 29 339 different earnings call transcript from 2014 to 2017 and classify the individual documents to either be part of class up or down. Four different machine learning algorithms are used to classify and predict based on the bag of words method. These machine learning algorithms are Naive Bayes, Logistic regression with lasso regularization, Stochastic Gradient Boosting, and Support Vector Machine. All models are compared to a benchmarks based on S&P 500. The model with best performance is logistic regression with a classification error of 43,8%. In total, 2 of 4 models beats the benchmark significantly, namely logistic regression and gradient boosting. With these results, the paper concludes that earnings calls contain predicting power for next dayâ€™s stock price direction.",2018,
Molecular Predictors of Long-Term Survival in Glioblastoma Multiforme Patients,"Glioblastoma multiforme (GBM) is the most common and aggressive adult primary brain cancer, with <10% of patients surviving for more than 3 years. Demographic and clinical factors (e.g. age) and individual molecular biomarkers have been associated with prolonged survival in GBM patients. However, comprehensive systems-level analyses of molecular profiles associated with long-term survival (LTS) in GBM patients are still lacking. We present an integrative study of molecular data and clinical variables in these long-term survivors (LTSs, patients surviving >3 years) to identify biomarkers associated with prolonged survival, and to assess the possible similarity of molecular characteristics between LGG and LTS GBM. We analyzed the relationship between multivariable molecular data and LTS in GBM patients from the Cancer Genome Atlas (TCGA), including germline and somatic point mutation, gene expression, DNA methylation, copy number variation (CNV) and microRNA (miRNA) expression using logistic regression models. The molecular relationship between GBM LTS and LGG tumors was examined through cluster analysis. We identified 13, 94, 43, 29, and 1 significant predictors of LTS using Lasso logistic regression from the somatic point mutation, gene expression, DNA methylation, CNV, and miRNA expression data sets, respectively. Individually, DNA methylation provided the best prediction performance (AUC = 0.84). Combining multiple classes of molecular data into joint regression models did not improve prediction accuracy, but did identify additional genes that were not significantly predictive in individual models. PCA and clustering analyses showed that GBM LTS typically had gene expression profiles similar to non-LTS GBM. Furthermore, cluster analysis did not identify a close affinity between LTS GBM and LGG, nor did we find a significant association between LTS and secondary GBM. The absence of unique LTS profiles and the lack of similarity between LTS GBM and LGG, indicates that there are multiple genetic and epigenetic pathways to LTS in GBM patients.",2016,PLoS ONE
"Quantitative Structure-Activity Relationship Modeling of Juvenile Hormone Mimetic Compounds for Culex Pipiens Larvae, with a Discussion of Descriptor-Thinning Methods","Quantitative structure-activity relationship (QSAR) modelers often encounter the problem of multicollinearity owing to the availability of large numbers of computable molecular descriptors. Sparsity of the variables while using descriptors such as atom pairs increases the complexity. Three different predictor-thinning methods, namely, a modified Gram-Schmidt algorithm, a marginal soft thresholding algorithm, and LASSO (least absolute shrinkage and selection operator), were utilized to reduce the number of descriptors prior to developing linear models. Juvenile hormone (JH) activity of 304 compounds on Culex pipiens larvae was taken as the model data set, and predictor trimming of a large number of diverse descriptors comprising 268 global molecular descriptors (topostructural, topochemical, and geometrical), 13 quantum chemical descriptors, and 915 atom pairs (substructural counts) was applied prior to linear regression by the ridge regression method. The data set (N = 304) was split into five calibration data sets of random samples of sizes 60/110/160/210/260, and the remaining 244/194/144/94/44 compounds were used for validations. LASSO was not found to be a very effective method in handling a large set of descriptors because the number of predictors retained could not exceed the number of observations. The results indicated that the modified Gram-Schmidt algorithm could be used to trim the number of predictors in the global molecular descriptor set where collinearity of the descriptors was the major concern. On the contrary, the soft thresholding approach was found to be an effective tool in subset selection from a diverse set of descriptors having both sparsity and multicollinearity, as in the case of the combined set of atom pairs and global molecular descriptors. The final model developed after variable selection was dominated more by atom pairs, which indicated the important structural moieties that affect JH activity of the compounds. The success of the method reiterates the fact that QSAR or quantitative structure-property relationship (QSPR) models can be developed for a diverse set of compounds using properly parametrized and diverse sets of descriptors, of course, with the selection of the appropriate statistical tools.",2006,Journal of chemical information and modeling
A link-free sparse group variable selection method for single-index model,"ABSTRACT For regression problems with grouped covariates, we adapt the idea of sparse group lasso (SGL) [10] to the framework of the sufficient dimension reduction. Assuming that the regression falls into a single-index structure, we propose a method called the sparse group sufficient dimension reduction to conduct group and within-group variable selections simultaneously without assuming a specific link function. Simulation studies show that our method is comparable to the SGL under the regular linear model setting and outperforms SGL with higher true positive rates and substantially lower false positive rates when the regression function is nonlinear. One immediate application of our method is to the gene pathway data analysis where genes naturally fall into groups (pathways). An analysis of a glioblastoma microarray data is included for illustration of our method.",2017,Journal of Applied Statistics
Regression analysis of locality preserving projections via sparse penalty,"Recent studies have shown that linear subspace algorithms, such as Principal Component Analysis, Linear Discriminant Analysis and Locality Preserving Projections, have attracted tremendous attention in many fields of information processing. However, the projection results obtained by these algorithms are linear combination of the original features, which is difficult to be interpreted psychologically and physiologically. Motivated by Compressive Sensing theory, we formulate the generalized eigenvalue problem under CS framework, which then allows us to apply a sparsity penalty and minimization procedure to locality preserving projections. The proposed algorithm is called sparse locality preserving projections, which performs locality preserving projections in the lasso regression framework that dimensionality reduction, feature selection and classification are merged into one analysis. The method is also extended to its regularized form to improve its generalization. The proposed algorithm is a combination of locality preserving with sparse penalty. Additionally, the algorithm can be performed in either supervised or unsupervised tasks. Experimental results on toy and real data sets show that our methods are effective and demonstrate much higher performance.",2015,Inf. Sci.
Semiparametric Bayesian inference for accelerated failure time models with errors-in-covariates and doubly censored data,"This paper proposes a Bayesian semiparametric accelerated failure time model for doubly censored data with errors-in-covariates. The authors model the distributions of the unobserved covariates and the regression errors via the Dirichlet processes. Moreover, the authors extend the Bayesian Lasso approach to our semiparametric model for variable selection. The authors develop the Markov chain Monte Carlo strategies for posterior calculation. Simulation studies are conducted to show the performance of the proposed method. The authors also demonstrate the implementation of the method using analysis of PBC data and ACTG 175 data.",2017,Journal of Systems Science and Complexity
Magnetic resonance imaging and molecular features associated with tumor-infiltrating lymphocytes in breast cancer,"BackgroundWe sought to investigate associations between dynamic contrast-enhanced (DCE) magnetic resonance imaging (MRI) features and tumor-infiltrating lymphocytes (TILs) in breast cancer, as well as to study if MRI features are complementary to molecular markers of TILs.MethodsIn this retrospective study, we extracted 17 computational DCE-MRI features to characterize tumor and parenchyma in The Cancer Genome Atlas cohort (nâ€‰=â€‰126). The percentage of stromal TILs was evaluated on H&E-stained histological whole-tumor sections. We first evaluated associations between individual imaging features and TILs. Multiple-hypothesis testing was corrected by the Benjamini-Hochberg method using false discovery rate (FDR). Second, we implemented LASSO (least absolute shrinkage and selection operator) and linear regression nested with tenfold cross-validation to develop an imaging signature for TILs. Next, we built a composite prediction model for TILs by combining imaging signature with molecular features. Finally, we tested the prognostic significance of the TIL model in an independent cohort (I-SPY 1; nâ€‰=â€‰106).ResultsFour imaging features were significantly associated with TILs (Pâ€‰<â€‰0.05 and FDRâ€‰<â€‰0.2), including tumor volume, cluster shade of signal enhancement ratio (SER), mean SER of tumor-surrounding background parenchymal enhancement (BPE), and proportion of BPE. Among molecular and clinicopathological factors, only cytolytic score was correlated with TILs (Ïâ€‰=â€‰0.51; 95% CI, 0.36â€“0.63; Pâ€‰=â€‰1.6E-9). An imaging signature that linearly combines five features showed correlation with TILs (Ïâ€‰=â€‰0.40; 95% CI, 0.24â€“0.54; Pâ€‰=â€‰4.2E-6). A composite model combining the imaging signature and cytolytic score improved correlation with TILs (Ïâ€‰=â€‰0.62; 95% CI, 0.50â€“0.72; Pâ€‰=â€‰9.7E-15). The composite model successfully distinguished low vs high, intermediate vs high, and low vs intermediate TIL groups, with AUCs of 0.94, 0.76, and 0.79, respectively. During validation (I-SPY 1), the predicted TILs from the imaging signature separated patients into two groups with distinct recurrence-free survival (RFS), with log-rank Pâ€‰=â€‰0.042 among triple-negative breast cancer (TNBC). The composite model further improved stratification of patients with distinct RFS (log-rank Pâ€‰=â€‰0.0008), where TNBC with no/minimal TILs had a worse prognosis.ConclusionsSpecific MRI features of tumor and parenchyma are associated with TILs in breast cancer, and imaging may play an important role in the evaluation of TILs by providing key complementary information in equivocal cases or situations that are prone to sampling bias.",2018,Breast Cancer Research : BCR
Determinants of ICT infrastructure: A cross-country statistical analysis,"We investigate economic and institutional determinants of ICT infrastructure for a broad cross section ofmore than 100 countries. The ICT variable is constructed from a principal components analysis. The explanatory variables are selected by variants of the Lasso estimator from the machine learning literature.In addition to least squares, we also apply robust and semiparametric regression estimators. The results show that the regressions are able to explain ICT infrastructure very well. Major determinants identified are real income per capita, the availability of electricity, the extent of urbanization and indicators for the quality of the institutional environment. We also find evidence of conditional convergence of the ICT infrastructure across countries.",2016,
Detecting Epistasis by LASSO-Penalized-Model Search Algorithm in Human Genome-Wide Association Studies,"Extensive studies have shown that many complex diseases are influenced by interaction of certain genes, while due to the limitations and drawbacks of adopting logistic regression (LR) to detect epistasis in human Genome-Wide Association Studies (GWAS), we propose a new method named LASSO-penalized-model search algorithm (LPMA) by restricting it to a tuning constant and combining it with a penalization of the L1-norm of the complexity parameter, and it is implemented utilizing the idea of multi-step strategy. LASSO penalized regression particularly shows advantageous properties when the number of factors far exceeds the number of samples. We compare the performance of LPMA with its competitors. Through simulated data experiments, LPMA performs better regarding to the identification of epistasis and prediction accuracy.",2014,Advanced Materials Research
Simple one-pass algorithm for penalized linear regression with cross-validation on MapReduce,"In this paper, we propose a one-pass algorithm on MapReduce for penalized linear regression 
\[f_\lambda(\alpha, \beta) = \|Y - \alpha\mathbf{1} - X\beta\|_2^2 + p_{\lambda}(\beta)\] where $\alpha$ is the intercept which can be omitted depending on application; $\beta$ is the coefficients and $p_{\lambda}$ is the penalized function with penalizing parameter $\lambda$. $f_\lambda(\alpha, \beta)$ includes interesting classes such as Lasso, Ridge regression and Elastic-net. Compared to latest iterative distributed algorithms requiring multiple MapReduce jobs, our algorithm achieves huge performance improvement; moreover, our algorithm is exact compared to the approximate algorithms such as parallel stochastic gradient decent. Moreover, what our algorithm distinguishes with others is that it trains the model with cross validation to choose optimal $\lambda$ instead of user specified one. 
Key words: penalized linear regression, lasso, elastic-net, ridge, MapReduce",2013,ArXiv
A comparative study of variable selection procedures applied in high dimensional medical problems,"In health studies, many potential factors are usually introduced to determine an outcome variable. In our study, different statistical methods are applied to analyze trauma annual data, collected by 30 General Hospitals in Greece. The first dataset consists of 1681 observations and 76 factors and the second of 6334 observations and 131 factors, that include demographic, transport and intrahospital data. The statistical methods employed in this work were the nonconcave penalized likelihood methods, SCAD, LASSO, and Hard, the generalized linear logistic regression, and the best subset variable selection, used to detect possible risk factors of death. A variety of different statistical models are considered, with respect to the combinations of factors and the number of observations. A comparative survey reveals differences between results and execution times of each method, and the analysis produces models that identify the significant prognostic factors affecting death from trauma.",2008,
"Package â€˜ LBLGXE â€™ August 30 , 2019 Type Package Title Logistic Bayesian Lasso for Rare ( or Common ) Haplotype Association Version 1","Type Package Title Logistic Bayesian Lasso for Rare (or Common) Haplotype Association Version 1.4 Author Xiaochen Yuan, Yuan Zhang, Shuang Xia, Swati Biswas, and Shili Lin Maintainer Xiaochen Yuan <xxy142030@utdallas.edu> Description This function takes a dataset of haplotypes and environmental covariates with one binary phenotype in which rows for individuals of uncertain phase have been augmented by ``pseudoindividuals'' who carry the possible multilocus genotypes consistent with the single-locus phenotypes. Bayesian lasso is used to find the posterior distributions of logistic regression coefficients, which are then used to calculate Bayes Factor and credible set to test for association with haplotypes, environmental covariates and interactions. The model can handle complex sampling data, in particular, frequency matched cases and controls with controls obtained using stratified sampling. This version can also be applied to a dataset with no environmental covariate and two correlated binary phenotypes. Zhang Y, Hofmann J, Purdue M, Lin S, and Biswas S (2017) <doi:10.1038/jhg.2017.43>. Zhang Y, Lin S, and Biswas S (2017) <doi:10.1111/biom.12567>. Zhang Y and Biswas S (2015) <doi:10.4137/CIN.S17290>. Biswas S, Xia S and Lin S (2014) <doi:10.1002/gepi.21773>. Biswas S, Lin S (2012) <doi:10.1111/j.1541-0420.2011.01680.x>. Burkett K, Graham J and McNeney B (2006) <doi:10.18637/jss.v016.i02>.",2019,
