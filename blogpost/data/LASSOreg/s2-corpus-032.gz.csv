title,abstract,year,journal
Penalized Least Squares methods for solving the EEG Inverse Problem,"Most of the known solutions (linear and nonlinear) of the ill-posed EEG Inverse Problem can be interpreted as the estimated coefficients in a penalized regression framework. In this work we present a general formulation of this problem as a Multiple Penalized Least Squares model, which encompasses many of the previously known methods as particular cases (e.g., Minimum Norm, LORETA). New types of inverse solutions arise since recent advances in the field of penalized regression have made it possible to deal with non-convex penalty functions, which provide sparse solutions (Fan and Li (2001)). Moreover, a generalization of this approach allows the use of any combination of penalties based on l1 or l2-norms, leading to solutions with combined properties such as smoothness and sparsity. Synthetic data is used to explore the benefits of non-convex penalty functions (e.g., LASSO, SCAD and LASSO Fusion) and mixtures (e.g., Elastic Net and LASSO Fused) by comparing them with known solutions in terms of localization error, blurring and visibility. Real data is used to show that a mixture model (Elastic Net) allows for tuning the spatial resolution of the solution to range from very concentrated to very blurred sources.",2008,Statistica Sinica
New Error Analysis for Lasso,"The Lasso is one of the most important approaches for parameter estimation and variable selection in high dimensional linear regression. At the heart of its success is the attractive rate of convergence result even when $p$, the dimension of the problem, is much larger than the sample size $n$. In particular, Bickel et al. (2009) showed that this rate, in terms of the $\ell_1$ norm, is of the order $s\sqrt{(\log p)/n}$ for a sparsity index $s$. In this paper, we obtain a new bound on the convergence rate by taking advantage of the distributional information of the model. Under the normality or sub-Gaussian assumption, the rate can be improved to nearly $s/\sqrt{n}$ for certain design matrices. We further outline a general partitioning technique that helps to derive sharper convergence rate for the Lasso. The result is applicable to many covariance matrices suitable for high-dimensional data analysis.",2011,arXiv: Statistics Theory
Active odor cancellation,"Noise cancellation is a traditional problem in statistical signal processing that has not been studied in the olfactory domain for unwanted odors. In this paper, we use the newly discovered olfactory white signal class to formulate optimal active odor cancellation using both nuclear norm-regularized multivariate regression and simultaneous sparsity or group lasso-regularized non-negative regression. As an example, we show the proposed technique on real-world data to cancel the odor of durian, katsuobushi, sauerkraut, and onion.",2014,2014 IEEE Workshop on Statistical Signal Processing (SSP)
How powerful are summary-based methods for identifying expression-trait associations under different genetic architectures?,"Transcriptome-wide association studies (TWAS) have recently been employed as an approach that can draw upon the advantages of genome-wide association studies (GWAS) and gene expression studies to identify genes associated with complex traits. Unlike standard GWAS, summary level data suffices for TWAS and offers improved statistical power. Two popular TWAS methods include either (a) imputing the cis genetic component of gene expression from smaller sized studies (using multi-SNP prediction or MP) into much larger effective sample sizes afforded by GWAS - TWAS-MP or (b) using summary-based Mendelian randomization - TWAS-SMR. Although these methods have been effective at detecting functional variants, it remains unclear how extensive variability in the genetic architecture of complex traits and diseases impacts TWAS results. Our goal was to investigate the different scenarios under which these methods yielded enough power to detect significant expression-trait associations. In this study, we conducted extensive simulations based on 6000 randomly chosen, unrelated Caucasian males from Geisinger's MyCode population to compare the power to detect cis expression-trait associations (within 500 kb of a gene) using the above-described approaches. To test TWAS across varying genetic backgrounds we simulated gene expression and phenotype using different quantitative trait loci per gene and cis-expression /trait heritability under genetic models that differentiate the effect of causality from that of pleiotropy. For each gene, on a training set ranging from 100 to 1000 individuals, we either (a) estimated regression coefficients with gene expression as the response using five different methods: LASSO, elastic net, Bayesian LASSO, Bayesian spike-slab, and Bayesian ridge regression or (b) performed eQTL analysis. We then sampled with replacement 50,000, 150,000, and 300,000 individuals respectively from the testing set of the remaining 5000 individuals and conducted GWAS on each set. Subsequently, we integrated the GWAS summary statistics derived from the testing set with the weights (or eQTLs) derived from the training set to identify expression-trait associations using (a) TWAS-MP (b) TWAS-SMR (c) eQTL-based GWAS, or (d) standalone GWAS. Finally, we examined the power to detect functionally relevant genes using the different approaches under the considered simulation scenarios. In general, we observed great similarities among TWAS-MP methods although the Bayesian methods resulted in improved power in comparison to LASSO and elastic net as the trait architecture grew more complex while training sample sizes and expression heritability remained small. Finally, we observed high power under causality but very low to moderate power under pleiotropy.",2018,Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing
A Computed Tomography-Based Radiomic Prognostic Marker of Advanced High-Grade Serous Ovarian Cancer Recurrence: A Multicenter Study,"Objectives: We used radiomic analysis to establish a radiomic signature based on preoperative contrast enhanced computed tomography (CT) and explore its effectiveness as a novel recurrence risk prognostic marker for advanced high-grade serous ovarian cancer (HGSOC). Methods: This study had a retrospective multicenter (two hospitals in China) design and a radiomic analysis was performed using contrast enhanced CT in advanced HGSOC (FIGO stage III or IV) patients. We used a minimum 18-month follow-up period for all patients (median 38.8 months, range 18.8-81.8 months). All patients were divided into three cohorts according to the timing of their surgery and hospital stay: training cohort (TC) and internal validation cohort (IVC) were from one hospital, and independent external validation cohort (IEVC) was from another hospital. A total of 620 3-D radiomic features were extracted and a Lasso-Cox regression was used for feature dimension reduction and determination of radiomic signature. Finally, we combined the radiomic signature with seven common clinical variables to develop a novel nomogram using a multivariable Cox proportional hazards model. Results: A final 142 advanced HGSOC patients were enrolled. Patients were successfully divided into two groups with statistically significant differences based on radiomic signature, consisting of four radiomic features (log-rank test P = 0.001, <0.001, <0.001 for TC, IVC, and IEVC, respectively). The discrimination accuracies of radiomic signature for predicting recurrence risk within 18 months were 82.4% (95% CI, 77.8-87.0%), 77.3% (95% CI, 74.4-80.2%), and 79.7% (95% CI, 73.8-85.6%) for TC, IVC, and IEVC, respectively. Further, the discrimination accuracies of radiomic signature for predicting recurrence risk within 3 years were 83.4% (95% CI, 77.3-89.6%), 82.0% (95% CI, 78.9-85.1%), and 70.0% (95% CI, 63.6-76.4%) for TC, IVC, and IEVC, respectively. Finally, the accuracy of radiomic nomogram for predicting 18-month and 3-year recurrence risks were 84.1% (95% CI, 80.5-87.7%) and 88.9% (95% CI, 85.8-92.5%), respectively. Conclusions: Radiomic signature and radiomic nomogram may be low-cost, non-invasive means for successfully predicting risk for postoperative advanced HGSOC recurrence before or during the perioperative period. Radiomic signature is a potential prognostic marker that may allow for individualized evaluation of patients with advanced HGSOC.",2019,Frontiers in Oncology
A Spatial-Temporal QoS Prediction Approach for Time-aware Web Service Recommendation,"Due to the popularity of service-oriented architectures for various distributed systems, an increasing number of Web services have been deployed all over the world. Recently, Web service recommendation became a hot research topic, one that aims to accurately predict the quality of functional satisfactory services for each end user. Generally, the performance of Web service changes over time due to variations of service status and network conditions. Instead of employing the conventional temporal models, we propose a novel spatial-temporal QoS prediction approach for time-aware Web service recommendation, where a sparse representation is employed to model QoS variations. Specifically, we make a zero-mean Laplace prior distribution assumption on the residuals of the QoS prediction, which corresponds to a Lasso regression problem. To effectively select the nearest neighbor for the sparse representation of temporal QoS values, the geo-location of web service is employed to reduce searching range while improving prediction accuracy. The extensive experimental results demonstrate that the proposed approach outperforms state-of-art methods with more than 10% improvement on the accuracy of temporal QoS prediction for time-aware Web service recommendation.",2016,TWEB
Sparse Sliced Inverse Quantile Regression,"The current paper proposes the sliced inverse quantile regression method (SIQR). In addition to the latter this study proposes both the sparse sliced inverse quantile regression method with Lasso (LSIQR) and Adaptive Lasso (ALSIQR) penalties. This article introduces a comprehensive study of SIQR and sparse SIQR. The simulation and real data analysis have been employed to check the performance of the SIQR, LSIQR and ALSIQR. According to the results of median of mean squared error and the absolute correlation criteria, we can conclude that the SIQR, LSIQR and ALSIQR are the more advantageous approaches in practice.",2016,Journal of Mathematics and Statistics
Asymptotic properties of adaptive group Lasso for sparse reduced rank regression,"This paper studies the asymptotic properties of the penalized least squares estimator using an adaptive group Lasso penalty for the reduced rank regression. The group Lasso penalty is defined in the way that the regression coefficients corresponding to each predictor are treated as one group. It is shown that under certain regularity conditions, the estimator can achieve the minimax optimal rate of convergence. Moreover, the variable selection consistency can also be achieved, that is, the relevant predictors can be identified with probability approaching one. In the asymptotic theory, the number of response variables, the number of predictors, and the rank number are allowed to grow to infinity with the sample size.",2016,arXiv: Statistics Theory
A comparative analysis of methods for predicting clinical outcomes using high-dimensional genomic datasets.,"OBJECTIVE
The objective of this investigation is to evaluate binary prediction methods for predicting disease status using high-dimensional genomic data. The central hypothesis is that the Bayesian network (BN)-based method called efficient Bayesian multivariate classifier (EBMC) will do well at this task because EBMC builds on BN-based methods that have performed well at learning epistatic interactions.


METHOD
We evaluate how well eight methods perform binary prediction using high-dimensional discrete genomic datasets containing epistatic interactions. The methods are as follows: naive Bayes (NB), model averaging NB (MANB), feature selection NB (FSNB), EBMC, logistic regression (LR), support vector machines (SVM), Lasso, and extreme learning machines (ELM). We use a hundred 1000-single nucleotide polymorphism (SNP) simulated datasets, ten 10,000-SNP datasets, six semi-synthetic sets, and two real genome-wide association studies (GWAS) datasets in our evaluation.


RESULTS
In fivefold cross-validation studies, the SVM performed best on the 1000-SNP dataset, while the BN-based methods performed best on the other datasets, with EBMC exhibiting the best overall performance. In-sample testing indicates that LR, SVM, Lasso, ELM, and NB tend to overfit the data.


DISCUSSION
EBMC performed better than NB when there are several strong predictors, whereas NB performed better when there are many weak predictors. Furthermore, for all BN-based methods, prediction capability did not degrade as the dimension increased.


CONCLUSIONS
Our results support the hypothesis that EBMC performs well at binary outcome prediction using high-dimensional discrete datasets containing epistatic-like interactions. Future research using more GWAS datasets is needed to further investigate the potential of EBMC.",2014,Journal of the American Medical Informatics Association : JAMIA
Sparsity with sign-coherent groups of variables via the cooperative-Lasso,"We consider the problems of estimation and selection of parameters endowed with a known group structure, when the groups are assumed to be sign-coherent, that is, gathering either nonnegative, nonpositive or null parameters. To tackle this problem, we propose the cooperative-Lasso penalty. We derive the optimality conditions defining the cooperative-Lasso estimate for generalized linear models, and propose an efficient active set algorithm suited to high-dimensional problems. We study the asymptotic consistency of the estimator in the linear regression setup and derive its irrepresentable conditions, which are milder than the ones of the group-Lasso regarding the matching of groups with the sparsity pattern of the true parameters. We also address the problem of model selection in linear regression by deriving an approximation of the degrees of freedom of the cooperative-Lasso estimator. Simulations comparing the proposed estimator to the group and sparse group-Lasso comply with our theoretical results, showing consistent improvements in support recovery for sign-coherent groups. We finally propose two examples illustrating the wide applicability of the cooperative-Lasso: first to the processing of ordinal variables, where the penalty acts as a monotonicity prior; second to the processing of genomic data, where the set of differentially expressed probes is enriched by incorporating all the probes of the microarray that are related to the corresponding genes.",2012,The Annals of Applied Statistics
Machine learning of human plasma lipidomes for obesity estimation in a large population cohort,"Obesity is associated with changes in the plasma lipids. Although simple lipid quantification is routinely used, plasma lipids are rarely investigated at the level of individual molecules. We aimed at predicting different measures of obesity based on the plasma lipidome in a large population cohort using advanced machine learning modeling. A total of 1,061 participants of the FINRISK 2012 population cohort were randomly chosen, and the levels of 183 plasma lipid species were measured in a novel mass spectrometric shotgun approach. Multiple machine intelligence models were trained to predict obesity estimates, i.e., body mass index (BMI), waist circumference (WC), waist-hip ratio (WHR), and body fat percentage (BFP), and validated in 250 randomly chosen participants of the MalmÃ¶ Diet and Cancer Cardiovascular Cohort (MDC-CC). Comparison of the different models revealed that the lipidome predicted BFP the best (R2 = 0.73), based on a Lasso model. In this model, the strongest positive and the strongest negative predictor were sphingomyelin molecules, which differ by only 1 double bond, implying the involvement of an unknown desaturase in obesity-related aberrations of lipid metabolism. Moreover, we used this regression to probe the clinically relevant information contained in the plasma lipidome and found that the plasma lipidome also contains information about body fat distribution, because WHR (R2 = 0.65) was predicted more accurately than BMI (R2 = 0.47). These modeling results required full resolution of the lipidome to lipid species level, and the predicting set of biomarkers had to be sufficiently large. The power of the lipidomics association was demonstrated by the finding that the addition of routine clinical laboratory variables, e.g., high-density lipoprotein (HDL)- or low-density lipoprotein (LDL)- cholesterol did not improve the model further. Correlation analyses of the individual lipid species, controlled for age and separated by sex, underscores the multiparametric and lipid species-specific nature of the correlation with the BFP. Lipidomic measurements in combination with machine intelligence modeling contain rich information about body fat amount and distribution beyond traditional clinical assays.",2019,PLoS Biology
Hybrid deep learning for predicting hypertensive disorder onset using temporal and non-temporal data,"The aim of this proof-of-concept study is to demonstrate the extension of the PatientLevelPrediction R-Package with deep learning models using temporal and non-temporal data. We present a hybrid deep-learning based method to predict hypertensive disorder onset in patients with pharmaceutically treated depression using temporal measurement data fed into a deep convolutional neural network, and non-temporal data fed into a multi-layer perceptron. We compare the hybrid method with other algorithms and show a higher discriminative performance. The developed deep learning pipeline can easily be expanded to cover more advanced network structures and hybrids. We aim to evaluate the deep learning methods on more cohorts at risk and outcomes in multiple databases in the near future. Introduction The success of machine learning largely depends on the selection of an optimal feature representation. In the early days, the machine learning community mainly focused on algorithm development, while currently the field is shifting to more powerful feature engineering. Deep learning models are widely used to automatically learn highlevel features from both temporal and static raw data, and have achieved remarkable results in image processing and speech recognition [1,2]. Recently, interesting results have been shown in healthcare applications [3-5]. The applied model architectures are composed of multiple non-linear neural networks, e.g. convolutional neural network. However, these methods have not been assessed at large scale, i.e. many cohorts at risk and many outcomes on EHR data. This would require a systematic approach against a common data model. We believe, the Observational Health Sciences and Informatics (OHDSI) initiative is in the best position to expand the insights of the global machine learning community in the use of deep learning for patient-level predictive modelling. Methods and Results We implemented different several deep learning models in the PatientLevelPrediction Package using PyTorch (http://pytorch.org ), e.g., Convolutional Neural Network (CNN), Recurrent Neural Network (RNN), Multilayer Perceptron (MLP). The framework can easily be extended with more advanced network structures. As a proof-ofconcept, we demonstrate the use of the deep learning implementation for predicting hypertensive disorder in patients with pharmaceutically treated depression in the Integrate Primary Care Information (IPCI) GP database. The target population consists of n=133,349 patients with pharmaceutically treated depression (PTD) without a history of psychosis, mania, or dementia. In total 3,894 PTD patients developed a hypertensive disorder. We used a backward observation window of 365 days and a time at risk window of 365 days following the start of the cohort. In this study, we only considered measurement as temporal data and other patient information (procedures, drugs, conditions, observation and demographic data) are treated as non-temporal data. In total 224 types of measurements and 5648 types of other non-temporal data where used as candidate predictors. The temporal data, i.e. the measurements, are fed into the CNN, wherein we employ a similar CNN architecture as used by Razavian et al. [3], i.e. two convolutional layers and two fully connected layers, the dropout probability between each layer is 0.5. The non-temporal data, i.e. all except for the measurements, are fed into the a MLP with one hidden layer. The final prediction is averaging the output probabilities from the two individual models. We use a weighted loss function to overcome the class imbalance problem when training the model using Adam with regularization [6]. We compared the performance with Lasso as implemented in R (Cyclops) using only measurements and Lasso using the full feature set, CNN using only measurements, logistics regression (LR) and MLP as implemented in PyTorch on the full features set, and a hybrid of CNN and MLP/LR. Table 1. Discriminative performance of multiple algorithm for predicting hypertensive disorder in PTD patients. Algorithm Features Train AUC Test AUC Lasso (measurements) Measurements 0.71 0.68 CNN (measurements) Measurements 0.87 0.86",2017,
A Two-Stage Penalized Logistic Regression Approach to Case-Control Genome-Wide Association Studies,"We propose a two-stage penalized logistic regression approach to case-control genome-wide association studies. This approach consists of a screening stage and a selection stage. In the screening stage, main-effect and interaction-effect features are screened by using ð¿1-penalized logistic like-lihoods. In the selection stage, the retained features are ranked by the logistic likelihood with the smoothly clipped absolute deviation (SCAD) penalty (Fan and Li, 2001) and Jeffreyâ€™s Prior penalty (Firth, 1993), a sequence of nested candidate models are formed, and the models are assessed by a family of extended Bayesian information criteria (J. Chen and Z. Chen, 2008). The proposed approach is applied to the analysis of the prostate cancer data of the Cancer Genetic Markers of Susceptibility (CGEMS) project in the National Cancer Institute, USA. Simulation studies are carried out to compare the approach with the pair-wise multiple testing approach (Marchini et al. 2005) and the LASSO-patternsearch algorithm (Shi et al. 2007).",2012,Journal of Probability and Statistics
Exploring elastic net and multivariate regression,"When it comes to datasets with a tremendous amount of predictors, variable reduction techniques such as PCA or FA are often used. In this paper, the elastic net, which lies in between the LASSO method and ridge regression, is used as a variable reduction technique followed by further analysis with multivariate regression. Specifically, a messy only dataset is used to show how it can be 'tidied' up and broken down into sensible subsets using the aforementioned method.",2016,arXiv: Methodology
Stochastic Discrete First-order Algorithm for Feature Subset Selection,"This paper addresses the problem of selecting a significant subset of candidate features to use for multiple linear regression. Bertsimas et al. [5] recently proposed the discrete first-order (DFO) algorithm to efficiently find near-optimal solutions to this problem. However, this algorithm is unable to escape from locally optimal solutions. To resolve this, we propose a stochastic discrete first-order (SDFO) algorithm for feature subset selection. In this algorithm, random perturbations are added to a sequence of candidate solutions as a means to escape from locally optimal solutions, which broadens the range of discoverable solutions. Moreover, we derive the optimal step size in the gradient-descent direction to accelerate convergence of the algorithm. We also make effective use of the L2-regularization term to improve the predictive performance of a resultant subset regression model. The simulation results demonstrate that our algorithm substantially outperforms the original DFO algorithm. Our algorithm was superior in predictive performance to lasso and forward stepwise selection as well. key words: feature subset selection, optimization algorithm, linear regression, machine learning, statistics",2019,
"Cannings and Samworth introduce a very general method for high dimensional classification, based on careful combination of the results of applying an arbitrary base classifier to random projections of the feature vectors","High-dimensional statistics refers to statistical inference when the number of unknown parameters p is much larger than the sample size n. This includes regression and supervised classification models, when the number of covariates is of much larger order than n, and unsupervised settings, such as clustering, with more variables than observations. Image processing, information retrieval in text documents, food authentication studies are only a few examples of the applications in which such problems arise. In those contexts, standard statistical methods cannot be applied, as the involved matrices are in general not full rank and cannot be inverted. A solution to this problem, which has attracted large attention in the statistical literature, involves imposing a sparse structure on the estimated vector parameters through the introduction of an L1 penalty on their norm. Lasso (Tibshirani 1996) based approaches to regression, classification and dimension reduction methods have been populating the statistical literature since Tibshiraniâ€™s seminal paper. See Buhlmann, van de Geer (2011) and Hastie, Tibshirani, Wainwright (2015) for detailed references.",2017,
Optimism Bias Correction in Omics Studies with Big Data: Assessment of Penalized Methods on Simulated Data.,"Big Data generated by omics technologies require simultaneous analyses of large numbers of variables. This leads to complex model selection and parameter estimates that show optimism bias. This study on simulated data sets examined optimism-bias correction by penalty regression methods in case-control studies that involve clinical and omics variables. Least absolute shrinkage and selection operator (LASSO)-based methods (LASSO-penalized logistic regression, adaptive LASSO, and regularized LASSO for selection + ridge regression) were evaluated using power, the false positive rate (FPR), false discovery rate (FDR), and by estimated versus theoretical parameter comparisons. The ""ordinary"" LASSO overcorrects the optimism bias. The adaptive LASSO with LASSO estimation of the weights was unable to provide a sufficient correction. Importantly, the adaptive LASSO with ridge estimation of the weights showed the best parameter estimation. The regularized LASSO selection showed a slight optimism bias that decreased with the increase in the training set size. The optimism bias decreased with the increase of the number of variables selected among truly differentially expressed variables; however, power, FPR, and FDR were correlated. A compromise between model selection and estimation accuracy should be found. These results might prove useful because Big Data analyses are becoming commonplace in omics/multiomics studies in integrative biology, precision medicine, and planetary health.",2019,Omics : a journal of integrative biology
URTeC : 2897507 Statistical Controls on Induced Seismicity,"Seismicity in Oklahoma has shown a sharp increase since 2010 and is mostly attributed to wells used to dispose wastewater from hydraulically fractured production wells (McClure, Gibson, Chiu, & Ranganath, 2017). Many studies are conducted so far to include / eliminate various causes and solutions to this problem. The recent studies in this research area (Holland, 2011), show a general consensus on the main cause of the seismic events (earthquakes) as the high volume disposal in the Arbuckle formation causing the critically stressed faults in the basement rock to fail. Some of these studies include the modeling and simulation of the physical processes whereas some studies delve into the statistical analyses of the relationships between disposal wells and induced seismicity. However, most of the previous attempts on the statistical analysis of this dataset use a more qualitative view of the problem (Langenbruch & Zoback, 2016) instead of quantifying the impact of various parameters such as injection rate, volumes, pressures, etc. Other work like Gogri et al. (2017) uses geo-modeling and simulation approach, but this constraint the modeling to a smaller section due to limitations on the seismic data extent. In this work, we use various data analytics methods to quantify the impact of different injection well parameters and rock properties on the earthquake event magnitude and intensity. Our models show that hierarchical and K-means clustering are able to group the wells into clusters that conforms with the earthquake event density. Including more clusters in our analysis refined the results but in general, four clusters are enough to capture the trends in our dataset. Fuzzy clustering, which is a soft clustering yields good results only after number of clusters exceed five. For predictive modeling part, Gradient boost regression and random forest work better than least absolute shrinkage and selection operator (LASSO), elastic nets and linear models. Introduction One of the key challenges in the quantification of the seismic events is the spatial and temporal relationships between the rock and well parameters. As the injection parameters change from one year to the next, the inputs are nonstationary. The second challenge is the definition of the events that will be modeled. Adding to the complexity, there are other more involved facets of this problem like determining the exact location of the seismic events. In this work, we have assumed the location of the events given by Oklahoma Corporate Commission (OCC) is exact, and we formulate our problem-based on these. In this work we have attempted a suite of clustering and predictive modeling techniques. We present the results from the models that best fitted our dataset and are able to incorporate the variables interpreted as key indicators based on our expert knowledge. Our model is primarily based on the waste water disposal well information from OCC. The major challenge that we faced in our work is that the number of data points for the analysis is not optimal and the well operating conditions are not constant throughout the well injection cycle. The time window of analysis in our study is from year 2010 to year 2016. Some wells start in 2010 and stop injecting for a while. Some new wells are added and some wells are abandoned. Also in the later period the â€œtraffic lightâ€ (McNamara et al., 2015) approach adopted by OCC makes the analysis difficult. To resolve this, we take the median injection values and peak values of different input parameters.",2018,
Using Machine Learning to Model Claims Experience and Reporting Delays for Pricing and Reserving,"In this paper we review existing modelling approaches for analysing claims experience in the presence of reporting delays, reviewing the formulation of mortality incidence models such as GLMs. We then show how these approaches have traditionally been adjusted for late reporting of claims using either the IBNR approach or the more recent EBNER approach. We then go on to introduce a new model formulation that combines a model for late reported claims with a model for mortality incidence into a single model formulation. We then illustrate the use and performance of the traditional and the combined model formulations on data from a multinational reinsurer. We show how GLMs, lasso regression, gradient boosted trees and deep learning can be applied to the new formulation to produce results of superior accuracy compared to the traditional approaches.",2019,
Integrative approaches to reconstruct regulatory networks from multi-omics data : A review of state-ofthe-art methods,"Data generation using high throughput technologies has led to the accumulation of diverse types of molecular data.These data have different types (discrete,real,string etc.) and occur in various formats and sizes. Datasets including gene expression, miRNA expression, proteinDNA binding data (ChIP-Seq/ChIP-ChIP), mutation data(copy number variation, single nucleotide polymorphisms), GO annotations, proteinprotein interaction and disease-gene association data are some of the commonly used genomic datasets to study biological processes. Each of them provides a unique, complementary and partly independent view of the genome and hence embed essential information about their regulatory mechanisms. In order to understand the functions of genes, proteins and analyze mechanisms arising out of their interactions, information provided by each of these datasets individually may not be sufficient. Therefore integrating these multi-omic data and inferring regulatory interactions from the integrated dataset provides a system level biological insights in predicting gene functions and their phenotypic outcomes. To study genome functionality through interaction networks, different methods have been proposed for collective mining of information from an integrated dataset. We survey here data integration approaches using state-of-the-art techniques such as network integration, Bayesian networks, regularized regression (LASSO) and multiple kernel learning methods.",2018,
The Bayesian Lasso Trevor Park,"The Lasso estimate for linear regression parameters can be interpreted as a Bayesian posterior mode estimate when the regression parameters have independent Laplace (double-exponential) priors. Gibbs sampling from this posterior is possible using an expanded hierarchy with conjugate normal priors for the regression parameters and independent exponential priors on their variances. A connection with the inverse Gaussian distribution provides tractable full conditional distributions. The Bayesian Lasso provides interval estimates (Bayesian credible intervals) that can guide variable selection. Moreover, the structure of the hierarchical model provides both Bayesian and likelihood methods for selecting the Lasso parameter. Slight modifications lead to Bayesian versions of other Lasso-related estimation methods like bridge regression and a robust variant.",2005,
Hyperparameter estimation in maximum a posteriori regression using group sparsity with an application to brain imaging,"Hyperparameter estimation is a recurrent problem in the signal and statistics literature. Popular strategies are cross-validation or Bayesian inference, yet it remains an active topic of research in order to offer better or faster algorithms. The models considered here are sparse regression models with convex or non-convex group-Lasso-like penalties. Following the recent work of Pereyra et al. [1] we study the fixed point iteration algorithm they propose and show that, while it may be suitable for an analysis prior, it suffers from limitations when using high-dimensional sparse synthesis models. The first contribution of this paper is to show how to overcome this issue. Secondly, we demonstrate how one can extend the model to estimate a vector of regularization parameters. We illustrate this on models with group sparsity reporting improved support recovery and reduced amplitude bias on the estimated coefficients. This approach is compared with an alternative method that uses a single parameter but a non-convex penalty. Results are presented on simulations and an inverse problem relevant for neuroscience which is the localization of brain activations using magneto/electroencephalography.",2017,2017 25th European Signal Processing Conference (EUSIPCO)
The Heterogeneous Impacts of R&D on Innovation in Services Sector: A Firm-Level Study of Developing ASEAN,"Identifying the determinants of firmsâ€™ investment in knowledge, this study first explores the heterogeneous impacts of research and development (R&D) on product, process, organization, and marketing innovation. Second, it examines if there exists a complementary (substitute) relation in terms of firmsâ€™ preference between four types of innovation. Studying 1500 firms of seven developing economies of the Association of Southeast Asian Nations (ASEAN), we applied the least absolute shrinkage and selection operator (LASSO), a machine learning-based regression, to identify key predictors likely to influence firmsâ€™ R&D propensity and intensity. Estimating the knowledge function, we foundâ€”in line with LASSOâ€”that medium-sized firms, human capital (training) and credit facilities favorably affect firmsâ€™ decision to invest in R&D. Contrarily, the impact is adverse if the first or main product generates firmsâ€™ large share of revenue, a unique finding not captured by previous studies. The marginal effects of four univariate probit models indicate that firmsâ€™ investment in R&D translates into innovation. However, the application of the Gewekeâ€“Hajivassiliourâ€“Keane (GHK)-simulator based multivariate probit, which considers simultaneity of firmsâ€™ innovation decisions that univariate probit ignores, suggests that the relationship between different types of innovation is complementary. Firmsâ€™ strategy to adopt a particular type of innovation is influenced by other types. This led to the estimation of R&Dâ€™s impact on technological and nontechnological innovation, which shows that while firms innovate both types, there is a skewed link between nontechnological innovation and the services sector.",2020,Sustainability
Predicting human age using regional morphometry and inter-regional morphological similarity,"The goal of this study is predicting human age using neuro-metrics derived from structural MRI, as well as investigating the relationships between age and predictive neuro-metrics. To this end, a cohort of healthy subjects were recruited from 1000 Functional Connectomes Project. The ages of the participations were ranging from 7 to 83 (36.17Â±20.46). The structural MRI for each subject was preprocessed using FreeSurfer, resulting in regional cortical thickness, mean curvature, regional volume and regional surface area for 148 anatomical parcellations. The individual age was predicted from the combination of regional and inter-regional neuro-metrics. The prediction accuracy is r = 0.835, p < 0.00001, evaluated by Pearson correlation coefficient between predicted ages and actual ages. Moreover, the LASSO linear regression also found certain predictive features, most of which were inter-regional features. The turning-point of the developmental trajectories in human brain was around 40 years old based on regional cortical thickness. In conclusion, structural MRI could be potential biomarkers for the aging in human brain. The human age could be successfully predicted from the combination of regional morphometry and inter-regional morphological similarity. The inter-regional measures could be beneficial to investigating human brain connectome.",2016,
A Comparison of the Efficiency of Using a Deep CNN Approach with Other Common Regression Methods for the Prediction of EGFR Expression in Glioblastoma Patients,"To estimate epithermal growth factor receptor (EGFR) expression level in glioblastoma (GBM) patients using radiogenomic analysis of magnetic resonance images (MRI). A comparative study using a deep convolutional neural network (CNN)â€“based regression, deep neural network, least absolute shrinkage and selection operator (LASSO) regression, elastic net regression, and linear regression with no regularization was carried out to estimate EGFR expression of 166 GBM patients. Except for the deep CNN case, overfitting was prevented by using feature selection, and loss values for each method were compared. The loss values in the training phase for deep CNN, deep neural network, Elastic net, LASSO, and the linear regression with no regularization were 2.90, 8.69, 7.13, 14.63, and 21.76, respectively, while in the test phase, the loss values were 5.94, 10.28, 13.61, 17.32, and 24.19 respectively. These results illustrate that the efficiency of the deep CNN approach is better than that of the other methods, including Lasso regression, which is a regression method known for its advantage in high-dimension cases. A comparison between deep CNN, deep neural network, and three other common regression methods was carried out, and the efficiency of the CNN deep learning approach, in comparison with other regression models, was demonstrated.",2019,Journal of Digital Imaging
Error bounds for the convex loss Lasso in linear models,"In this paper we investigate error bounds for convex loss functions for the Lasso in linear models, by first establishing a gap in the theory with respect to the existing error bounds. Then, under the compatibility condition, we recover bounds for the absolute value estimation error and the squared prediction error under mild conditions, which appear to be far more appropriate than the existing bounds for the convex loss Lasso. Interestingly, asymptotically the only difference between the new bounds of the convex loss Lasso and the classical Lasso is a term solely depending on a well-known expression in the robust statistics literature appearing multiplicatively in the bounds. We show that this result holds whether or not the scale parameter needs to be estimated jointly with the regression coefficients. Finally, we use the ratio to optimize our bounds in terms of minimaxity. MSC 2010 subject classifications: Primary 62F35; secondary 62J07.",2017,Electronic Journal of Statistics
Preoperative Prediction of Infection Stones Using Radiomics Features From Computed Tomography,"Preoperative prediction of infection stones from CT images could provide additional information for treatment planning. We developed a radiomics algorithm that could apply data from non-contrast-enhanced CT images to distinguish infection stones from non-infection stones. This retrospective study included 98 patients with clinically confirmed infection kidney stones and 59 patients with non-infection kidney stones. Fifty-four radiomics features extracted from CT images were reduced to 27 key features by the LASSO algorithm, for which a radiomics signature was built with ensemble learning based on bagged trees. Multivariable logistic regression analysis was then used to develop a radiomics nomogram incorporating the radiomics signature and independent clinical factors. The radiomics signature, which consisted of morphological features and textural features, was significantly associated with infection kidney stones. Ensemble learning based on bagged trees could differentiate infection kidney stones from non-infection kidney stones with 90.7% accuracy, 85.81% sensitivity, 93.96% specificity, a 91% positive predictive value and a 91% negative predictive value. Predictors incorporated into the individualized prediction nomogram included the radiomics signature, white blood cell count and urine culture. Decision curve analysis demonstrated that the radiomics nomogram had potential clinical application for infection stone prediction.",2019,IEEE Access
High performance EEG feature extraction for fast epileptic seizure detection,"Epilepsy is a neurological disorder that affects around 70 million people worldwide. Early detection of epileptic seizures has the potential to help patients in improving their quality of life. Electroencephalogram (EEG) has been used to record the brain's electrical activities associated with seizures. This paper presents a fast method for selecting EEG features that are relevant to early detection of epileptic seizures. The feature extraction model is based on LASSO regression and is applied to the EEG spectrum to recognize the EEG spectral features pertinent to seizures. These features are then selected and fed into a random forest (RF) classifier for epileptic seizure recognition. Compared to the state-of-the-art methods, the proposed scheme achieves the highest detection performance of 100% sensitivity, 100% specificity, 100% classification accuracy, and 1.18 Sec detection delay. Furthermore, our model has proven to be robust in noisy and abnormal conditions.",2017,2017 IEEE Global Conference on Signal and Information Processing (GlobalSIP)
