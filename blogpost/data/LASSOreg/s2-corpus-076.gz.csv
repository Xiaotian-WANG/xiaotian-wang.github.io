title,abstract,year,journal
Construction and Validation of an Autophagy-Related Prognostic Risk Signature for Survival Predicting in Clear Cell Renal Cell Carcinoma Patients,"Background: Clear cell renal cell carcinoma (ccRCC) is a common type of malignant tumors in urinary system. Evaluating the prognostic outcome at the time of initial diagnosis is essential for patients. Autophagy is known to play a significant role in tumors. Here, we attempted to construct an autophagy-related prognostic risk signature based on the expression profile of autophagy-related genes (ARGs) for predicting the long-term outcome and effect of precise treatments for ccRCC patients. Methods: We obtained the expression profile of ccRCC from the cancer genome atlas (TCGA) database and extract the portion of ARGs. We conducted differentially expressed analysis on ARGs and then performed enrichment analyses to confirm the anomalous autophagy-related biological functions. Then, we performed univariate Cox regression to screen out overall survival (OS)-related ARGs. With these genes, we established an autophagy-related risk signature by least absolute shrinkage and selection operator (LASSO) Cox regression. We validated the reliability of the risk signature with receiver operating characteristic (ROC) analysis, survival analysis, clinic correlation analysis, and Cox regression. Then we analyzed the function of each gene in the signature by single-gene gene set enrichment analysis (GSEA). Finally, we analyzed the correlation between our risk score and expression level of several targets of immunotherapy and targeted therapy. Results: We established a seven-gene prognostic risk signature, according to which we could divide patients into high or low risk groups and predict their outcomes. ROC analysis and survival analysis validated the reliability of the signature. Clinic correlation analysis found that the risk group is significantly correlated with severity of ccRCC. Multivariate Cox regression revealed that the risk score could act as an independent predictor for the prognosis of ccRCC patients. Correlation analysis between risk score and targets of precise treatments showed that our risk signature could predict the effects of precise treatment powerfully. Conclusion: Our study provided a brand new autophagy-related seven-gene prognostic risk signature, which could perform as a prognostic indicator for ccRCC. Meanwhile, our study provides a novel sight to understand the role of autophagy and suggest therapeutic strategies in the category of precise treatment in ccRCC.",2020,
Identification of an Immune-Related Nine-lncRNA Signature Predictive of Overall Survival in Colon Cancer,"Growing evidence suggests that immune-related genes (IRGs) and long non-coding RNAs (lncRNAs) can serve as prognostic markers of overall survival (OS) in patients with colon cancer. This study aimed to identify an immune-related lncRNA signature for the prospective assessment of prognosis in these patients. Gene expression and clinical data of colon cancer patients were downloaded from The Cancer Genome Atlas (TCGA). Immune-related lncRNAs were identified by a correlation analysis between IRGs and lncRNAs. In total, 447 samples were divided into a training cohort (224 samples) and a testing cohort (223 samples). Univariate, lasso and multivariate Cox regression analyses identified an immune-related nine-lncRNA signature closely related to OS in colon cancer patients in the training dataset. A risk score formula involving nine immune-related lncRNAs was developed to evaluate the prognostic value of the lncRNA signature in the training dataset. Colon cancer patients with a high risk score had poorer OS than those with a low risk score. A multivariate Cox regression analysis confirmed that the immune-related nine-lncRNA signature could be an independent prognostic factor in colon cancer patients. The results were further confirmed in the testing cohort and the entire TCGA cohort. Furthermore, a gene set enrichment analysis revealed several pathways with significant enrichment in the high- and low-risk groups that may be helpful in formulating clinical strategies and understanding the underlying mechanisms. Finally, a quantitative real-time polymerase chain reaction assay found that the nine lncRNAs were significantly differentially expressed in colon cancer cell lines. The results of this study indicate that this signature has important clinical implications for improving predictive outcomes and guiding individualized treatment in colon cancer patients. These lncRNAs could be potential biomarkers affecting the prognosis of colon cancer.",2020,
Identifying subtype-specific associations between gene expression and DNA methylation profiles in breast cancer,"BackgroundBreast cancer is a complex disease in which different genomic patterns exists depending on different subtypes. Recent researches present that multiple subtypes of breast cancer occur at different rates, and play a crucial role in planning treatment. To better understand underlying biological mechanisms on breast cancer subtypes, investigating the specific gene regulatory system via different subtypes is desirable.MethodsGene expression, as an intermediate phenotype, is estimated based on methylation profiles to identify the impact of epigenomic features on transcriptomic changes in breast cancer. We propose a kernel weighted l1-regularized regression model to incorporate tumor subtype information and further reveal gene regulations affected by different breast cancer subtypes. For the proper control of subtype-specific estimation, samples from different breast cancer subtype are learned at different rate based on target estimates. Kolmogorov Smirnov test is conducted to determine learning rate of each sample from different subtype.ResultsIt is observed that genes that might be sensitive to breast cancer subtype show prediction improvement when estimated using our proposed method. Comparing to a standard method, overall performance is also enhanced by incorporating tumor subtypes. In addition, we identified subtype-specific network structures based on the associations between gene expression and DNA methylation.ConclusionsIn this study, kernel weighted lasso model is proposed for identifying subtype-specific associations between gene expressions and DNA methylation profiles. Identification of subtype-specific gene expression associated with epigenomic changes might be helpful for better planning treatment and developing new therapies.",2017,BMC Medical Genomics
Extension de la r\'egression lin\'eaire g\'en\'eralis\'ee sur composantes supervis\'ees (SCGLR) aux donn\'ees group\'ees,"We address component-based regularisation of a multivariate Generalized Linear Mixed Model. A set of random responses Y is modelled by a GLMM, using a set X of explanatory variables and a set T of additional covariates. Variables in X are assumed many and redundant: generalized linear mixed regression demands regularisation with respect to X. By contrast, variables in T are assumed few and selected so as to demand no regularisation. Regularisation is performed building an appropriate number of orthogonal components that both contribute to model Y and capture relevant structural information in X. We propose to optimize a SCGLR-specific criterion within a Schall's algorithm in order to estimate the model. This extension of SCGLR is tested on simulated and real data, and compared to Ridge-and Lasso-based regularisations.",2018,arXiv: Methodology
The Relationship Between Cardiac Troponin I and the Severe Degree in Patients With Chronic Heart Failure,"Objective:To investigate the relationship between cardiac troponin I(cTnI)and the classof cardiac function in patients with chronic heart failure(CHF).Methods:428 patients with CHF were selected in this study. All patients were divided into four groups according to NYHA functional class. Blood plasma cTnI were measured by immunofluorescence.Results:The plasma level of cTnI increased with the increasement of the class of NYHA heart function(P 0.01). However, there was no difference between the NYHAâ…  andâ…¡ group.There was significant positive correlation between the plasma level of cTnI and the NYHA functional classes(r= 0.7,P 0.01). Multiple linear regression indentified cTnI having correlation to CHF.Conclusion:The plasma level of cTnI are related with the class of NYHA heart function, regardless of the etiology. Plasma cTnI can be used as the index to evaluate the severity of CHF.",2013,Inner Mongolia Medical Journal
Channel selection for simultaneous and proportional myoelectric prosthesis control of multiple degrees-of-freedom.,"OBJECTIVE
Recent studies have shown the possibility of simultaneous and proportional control of electrically powered upper-limb prostheses, but there has been little investigation on optimal channel selection. The objective of this study is to find a robust channel selection method and the channel subsets most suitable for simultaneous and proportional myoelectric prosthesis control of multiple degrees-of-freedom (DoFs).


APPROACH
Ten able-bodied subjects and one person with congenital upper-limb deficiency took part in this study, and performed wrist movements with various combinations of two DoFs (flexion/extension and radial/ulnar deviation). During the experiment, high density electromyographic (EMG) signals and the actual wrist angles were recorded with an 8 Ã— 24 electrode array and a motion tracking system, respectively. The wrist angles were estimated from EMG features with ridge regression using the subsets of channels chosen by three different channel selection methods: (1) least absolute shrinkage and selection operator (LASSO), (2) sequential feature selection (SFS), and (3) uniform selection (UNI).


MAIN RESULTS
SFS generally showed higher estimation accuracy than LASSO and UNI, but LASSO always outperformed SFS in terms of robustness, such as noise addition, channel shift and training data reduction. It was also confirmed that about 95% of the original performance obtained using all channels can be retained with only 12 bipolar channels individually selected by LASSO and SFS.


SIGNIFICANCE
From the analysis results, it can be concluded that LASSO is a promising channel selection method for accurate simultaneous and proportional prosthesis control. We expect that our results will provide a useful guideline to select optimal channel subsets when developing clinical myoelectric prosthesis control systems based on continuous movements with multiple DoFs.",2014,Journal of neural engineering
LASSO tuning parameter selection,"The LASSO is a penalized regression method which simultaneously performs shrinkage and variable selection. The output produced by the LASSO consists of a piecewise linear solution path, starting with the null model and ending with the full least squares fit, as the value of a tuning parameter is decreased. The performance of the selected model therefore depends greatly on the choice of this parameter. This paper attempts to provide an overview of methods which are available to select the value of the tuning parameter for either prediction or variable selection purposes. A simulation study provides a comparison of these methods and assesses their performance.",2015,
Improved variable selection with Forward-Lasso adaptive shrinkage,"Recently, considerable interest has focused on variable selection methods in regression situations where the number of predictors, $p$, is large relative to the number of observations, $n$. Two commonly applied variable selection approaches are the Lasso, which computes highly shrunk regression coefficients, and Forward Selection, which uses no shrinkage. We propose a new approach, ""Forward-Lasso Adaptive SHrinkage"" (FLASH), which includes the Lasso and Forward Selection as special cases, and can be used in both the linear regression and the Generalized Linear Model domains. As with the Lasso and Forward Selection, FLASH iteratively adds one variable to the model in a hierarchical fashion but, unlike these methods, at each step adjusts the level of shrinkage so as to optimize the selection of the next variable. We first present FLASH in the linear regression setting and show that it can be fitted using a variant of the computationally efficient LARS algorithm. Then, we extend FLASH to the GLM domain and demonstrate, through numerous simulations and real world data sets, as well as some theoretical analysis, that FLASH generally outperforms many competing approaches.",2011,The Annals of Applied Statistics
"A multidimensional nomogram combining overall stage, dose volume histogram parameters and radiomics to predict progression-free survival in patients with locoregionally advanced nasopharyngeal carcinoma.","OBJECTIVES
To develop a multidimensional nomogram for predicting the progression-free survival (PFS) in patients with locoregionally advanced nasopharyngeal carcinoma (NPC) (stage III-IVa).


MATERIALS AND METHODS
A total of 224 patients with locoregionally advanced NPC (training cohort, nâ€¯=â€¯149; validation cohort, nâ€¯=â€¯75) were retrospectively included. We extracted 260 radiomic features from the primary tumor and lymph nodes on the axial contrast-enhanced T1 weighted and T2 weighted MRI. Radiomic signatures of the gross tumor volume (RSnx) and lymph node (RSnd), Dose Volume Histogram (DVH) signature reflecting planning score (PS), and clinical characteristics were included as potential predictors of PFS. The least absolute shrinkage and selection operator (LASSO) regression were applied for feature selection and data dimension reduction. A nomogram was developed by incorporating the selected predictors. The C-index and calibration curve were used to assess discrimination and calibration power of the nomogram, respectively.


RESULTS
RSnd, PS, and tumor-node-metastasis (TNM) stage were the independent predictors for PFS (all pâ€¯<â€¯0.05). The nomogram integrating the three factors achieved a C-index of 0.811 (95% CI: 0.74-0.882) in the validation cohort for predicting PFS, which outperformed than that of the TNM stage alone (C-index, 0.613, 95% CI: 0.532-0.694). Subgroup analysis showed Epstein-Barr virus (EBV) DNA status improved the predictive accuracy of the nomogram (C-index, 0.86, 95% CI: 0.787-0.933).


CONCLUSIONS
The multidimensional nomogram incorporating RSnd, PS, and TNM stage showed high performance for predicting PFS in patients with locoregionally advanced NPC.",2019,Oral oncology
PReMS: Parallel Regularised Regression Model Search for sparse bio-signature discovery,"There is increasing interest in developing point of care tests to diagnose disease and predict prognosis based upon biomarker signatures of RNA or protein expression levels. Technology to measure the required biomarkers accurately and in a time-frame useful to health care professionals will be easier to develop by minimising the number of biomarkers measured. In this paper we describe the Parallel Regularised Regression Model Search (PReMS) method which is designed to estimate parsimonious prediction models. Given a set of potential biomarkers PReMS searches over many logistic regression models constructed from optimal subsets of the biomarkers, iteratively increasing the model size. Zero centred Gaussian prior distributions are assigned to all regression coefficients to induce shrinkage. The method estimates the optimal shrinkage parameter, optimal model for each model size and the optimal model size. We apply PReMS to six freely available data sets and compare its performance with the LASSO and SCAD algorithms in terms of the number of covariates in the model, model accuracy, as measured by the area under the receiver operator curve (AUC) and root predicted mean square error, and model calibration. We show that PReMS typically selects models with fewer biomarkers than both the LASSO and SCAD algorithms but has comparable predictive accuracy. Availability: (PReMS) is freely available as an R package https://github.com/clivehoggart/PReMS",2018,bioRxiv
Comparison of Feature Selection Methods Based on Lasso,"The model and feature selection is one of the important subjects in statistics. Lasso is a feature selection method based on 1-norm. Compared with the existing features of selection methods, Lasso can not only accurately choose the important variables, but also has the stability of feature selection. This paper compares the Lasso algorithm of variable selection in linear regression model, and Lasso, Lars, the Adaptive Lasso, elastic net as well as other methods which are based on the linear model. The relationships between them are presented. A variable selection method is realized by the comparison tests of a few data selected from UCI.",2014,Journal of Anhui Vocational College of Electronics & Information Technology
When Does More Regularization Imply Fewer Degrees of Freedom? Sufficient Conditions and Counter Examples from Lasso and Ridge Regression,"Regularization aims to improve prediction performance of a given statistical modeling approach by moving to a second approach which achieves worse training error but is expected to have fewer degrees of freedom, i.e., better agreement between training and prediction error. We show here, however, that this expected behavior does not hold in general. In fact, counter examples are given that show regularization can increase the degrees of freedom in simple situations, including lasso and ridge regression, which are the most common regularization approaches in use. In such situations, the regularization increases both training error and degrees of freedom, and is thus inherently without merit. On the other hand, two important regularization scenarios are described where the expected reduction in degrees of freedom is indeed guaranteed: (a) all symmetric linear smoothers, and (b) linear regression versus convex constrained linear regression (as in the constrained variant of ridge regression and lasso).",2013,arXiv: Statistics Theory
Survival Prediction In High Dimensional Datasets â€“ Comparative Evaluation Of Lasso Regularization and Random Survival Forests,"Background High-dimensional data obtained using modern molecular technologies (e.g., gene expression, proteomics) and large clinical datasets is increasingly common. Risk stratification based on such high-dimensional data remains challenging. Traditional statistical models have a limited capability of handling large numbers of variables, non-linear effects, correlations and missing data. More importantly, as more variables are analyzed, models tend to over-fit (i.e., the model provides good predictions on the studied data but performs poorly on other data).

Recently two methods have been proposed for handling multivariate analysis of high-dimensional data. The Least Absolute Shrinkage and Selection Operator (LASSO) minimizes the number of Cox regression coefficients, favoring models that are parsimonious and less likely to over-fit. LASSO is computationally efficient and capable of handling correlated variables. Random Survival Forests (RSF) combines multiple decision trees built on randomly selected subsets of variables. This enables the evaluation of all variables, even in the presence of significant correlations or missing data, and reduces over-fitting. Few studies have evaluated these models on realistic datasets. This study compared the performance of LASSO and RSF to that of the traditional Cox Proportional Hazard (CoxPH) with respect to their ability to predict survival based on high dimensional reverse phase protein array (RPPA) data from Acute Myeloid Leukemia (AML) patients.

Methods Our data were derived from previous work to define the role of the pathway activation in AML using a custom made RPPA onto which were printed leukemia enriched cells from 511 newly diagnosed AML patients collected between 1992 and 2007. The RPPA was probed with 231 strictly validated antibodies. Data were normalized and analysis was performed for the 415 subjects that underwent therapy at MD Anderson. The dataset also included 38 clinical variables.

We removed cases with a documented survival of less than 4 weeks and imputed a small number of sporadically missing values by random sampling with replacement. We generated LASSO, RSF, and CoxPH models using R. We built RSF models using 1000 trees and 10 random split points. We then identified key features using the RSF max-subtree and the glmnet cross validation methods. We built an RSF and Cox models to these variables only. Models were trained on a bootstrap sample of the size of the dataset, randomly sampled with replacement, and tested on the un-sampled remaining cases. The process was repeated for 50 iterations and results were averaged. We report Harrellâ€™s concordance index (C-Index) and the Brier score for model performance. Harrellâ€™s C-Index is a pairwise comparison of all observations in the testset. It evaluates the probability of erroneously assigining longer survival time to one case over the other. Brier score calculates the difference between the predicted and observed probabilities. It evaluates how well the model fits the entire dataset.

Results Cox regression with LASSO regularization had the best performance based on both Brier score and C-Index ([Table 1][1] and figure 1). For the complete dataset Cox regression models did not converge even when using forward feature selection. Cox regression following feature selection based on RSF had inferior performance compared to other methods.

View this table:

Table 1 
Comparison of model performance



Conclusions LASSO and RSF allow for mutlivariate analysis of high-dimensional data that would have not been possible otherwise. LASSO regularization outperforms other methods in terms of accuracy and to select features for traditional Cox models. Using the latter approach has great appeal as traditional Cox models allow easy interpertation of the hazard associated with individual variables. Differences in Brier scores are more pronounced than C-Indexes, possibly indicating a tendency of LASSO regularization to overfit the data compared to RSF.

Disclosures: No relevant conflicts of interest to declare.

 [1]: #T1",2013,Blood
A New Sparsity-Based Band Selection Method for Target Detection of Hyperspectral Image,"Band selection (BS) plays an important role in the dimensionality reduction of hyperspectral data. However, as to the existing BS methods, few are specially designed for target detection. In this letter, we combine the target detection and BS process together and put forward a new BS method for target detection, named least absolute shrinkage and selection operator (LASSO)-based BS (LBS). Interestingly, by using a linear regression model with L1 regularization (LASSO model), LBS transforms the discrete BS problem into the continuous optimization problem, which cannot only avoid the complicated subset selection process but also evaluate the importance of all the bands simultaneously. The experiments on real hyperspectral data demonstrate that LBS is a very effective BS method for target detection.",2015,IEEE Geoscience and Remote Sensing Letters
Establishment of the Prognosis Predicting Signature for Endometrial Cancer Patient,"BACKGROUND Novel biomarkers provide clinicians more critical information on tumor genetic features and patients' prognosis. Here, we aimed to establish prognosis-predicting signatures for endometrial carcinoma (EC) patients based on the miRNA information. MATERIAL AND METHODS The Cancer Genome Atlas (TCGA) website was available for dataset extraction. Prognosis-associated miRNAs were generated by univariate Cox regression test. Online websites were used to predict the targeted genes of these enrolled miRNAs. The miRNA-mRNA network was described by Cytoscape software, while the relevant signaling pathways of these targeted genes were enriched by Gene ontology (GO) and Kyoto Encyclopedia of Genes and Genomes (KEGG) analyses. RESULTS The miRNA-based overall survival (OS) and recurrence-free survival (RFS) predicting signatures were constructed by LASSO Cox regression analyses, respectively, by which, the endometrial carcinoma patients were separated into high- and low-risk groups in both the discovery and validation sets. Univariate Cox regression analyses suggested that these high-risk patients had elevated death and recurrence risk compared to low-risk patients. In addition, multivariate Cox regression analysis confirmed that our signatures were independent prognosticate factors with or without clinicopathological features for endometrial carcinoma patients. Moreover, the miRNA-mRNA network was displayed by Cytoscape software, and the pathway enrichment analyses found that the targeted genes of these enrolled miRNAs were enriched in tumor progression and drug resistance-related pathways. CONCLUSIONS The OS and RFS predicting classifiers serve as independent prognosis-associated determiners for EC patients.",2019,Medical Science Monitor : International Medical Journal of Experimental and Clinical Research
A direct estimation of high dimensional stationary vector autoregressions,"The vector autoregressive (VAR) model is a powerful tool in learning complex time series and has been exploited in many elds. The VAR model poses some unique challenges to researchers: On one hand, the dimensionality, introduced by incorporating multiple numbers of time series and adding the order of the vector autoregression, is usually much higher than the time series length; On the other hand, the temporal dependence structure naturally present in the VAR model gives rise to extra diculties in data analysis. The regular way in cracking the VAR model is via \least squares"" and usually involves adding dierent penalty terms (e.g., ridge or lasso penalty) in handling high dimensionality. In this manuscript, we propose an alternative way in estimating the VAR model. The main idea is, via exploiting the temporal dependence structure, formulating the estimating problem to a linear program. There is instant advantage of the proposed approach over the lassotype estimators: The estimation equation can be decomposed to multiple sub-equations and accordingly can be solved eciently using parallel computing. Besides that, we also bring new theoretical insights into the VAR model analysis. So far the theoretical results developed in high dimensions (e.g., Song and Bickel, 2011 and Kock and Callot, 2015) are based on stringent assumptions that are not transparent. Our results, on the other hand, show that the spectral norms of the transition matrices play an important role in estimation accuracy and build estimation and prediction consistency accordingly. Moreover, we provide some experiments on both synthetic and real-world equity data. We show that there are empirical advantages of our method over the lasso-type estimators in parameter estimation and forecasting.",2015,J. Mach. Learn. Res.
Best Model for Swiss Banknote Data,"When we discriminate Swiss banknote data by IP-OLDF, we find that these data are linearly separable data (LSD). Because we examine all possible combination models, we can find that a two-variable model, such as (X4, X6), is the minimum linearly separable model. A total of 16 models, including these two variables, are linearly separable by the monotonic decrease of MNM (MNM p â‰¥ MNM(p+1)), and other 47 models are not linearly separable. Therefore, we compare eight LDFs by the best models with the minimum error rate mean in the validation sample (M2) and obtain good results. Although we could not explain the useful meaning of the 95 % CI of discriminant coefficient until now, the pass/fail determination using examination scores provide a clear understanding by normalizing the coefficient in Chap. 5. Seven LDFs become trivial LDFs. Only Fisherâ€™s LDF is not trivial. Seven LDFs are Revised IP-OLDF based on MNM, Revised LP-OLDF, Revised IPLP-OLDF, three SVMs, and logistic regression. We successfully explain the meaning of coefficient. Therefore, we discuss the relationship between the best model and coefficient more precisely by Swiss banknote data in Chap. 6. We study LSD discrimination by Swiss banknote data, Student linearly separable data in Chap. 4, six pass/fail determinations using examination scores in Chap. 5, and Japanese-automobile data in Chap. 7, precisely. When we discriminate six microarray datasets that are LSD in Chap. 8, only Revised IP-OLDF can naturally make feature-selection and reduce the high-dimensional gene space to the small gene space drastically. In gene analysis, we call all linearly separable models, â€œMatroska.â€ The full model is the largest Matroska that includes all smaller Matroskas in it. As we already knew, the smallest Matroska (BGS) can explain the Matroska structure completely through the monotonic decrease of MNM. We propose the Matroska feature-selection method for the microarray dataset (Method 2). Because LSD discrimination is no longer popular, we explain Method 2 through detailed examples of the Swiss banknote and Japanese-automobile data. On the other hand, LASSO attempts to make feature-selection. If it cannot find the small Matroska (SM) in the dataset, it cannot explain the Matroska structure. Swiss banknote data, Japanese-automobile data, and six microarray datasets are helpful for evaluating the usefulness of other feature-selection methods, including LASSO.",2016,
quantile regression processes under dependence and penalization,"We consider quantile regression processes from censored data under dependent data structures and derive a uniform Bahadur representation for those processes. We also consider cases where the dimension of the parameter in the quantile regression model is large. It is demonstrated that traditional penalization methods such as the adaptive lasso yield sub-optimal rates if the coefficients of the quantile regression cross zero. New penalization techniques are introduced which are able to deal with specific problems of censored data and yield estimates with an optimal rate. In contrast to most of the literature, the asymptotic analysis does not require the assumption of independent observations, but is based on rather weak assumptions, which are satisfied for many kinds of dependent data.",2012,
Monotone splines lasso,"The important problems of variable selection and estimation in nonparametric additive regression models for high-dimensional data are addressed. Several methods have been proposed to model nonlinear relationships when the number of covariates exceeds the number of observations by using spline basis functions and group penalties. Nonlinear monotone effects on the response play a central role in many situations, in particular in medicine and biology. The monotone splines lasso (MS-lasso) is constructed to select variables and estimate effects using monotone splines (I-splines). The additive components in the model are represented by their I-spline basis function expansion and the component selection becomes that of selecting the groups of coefficients in the I-spline basis function expansion. A recent procedure, called cooperative lasso, is used to select sign-coherent groups, i.e. selecting the groups with either exclusively non-negative or non-positive coefficients. This leads to the selection of important covariates that have nonlinear monotone increasing or decreasing effect on the response. An adaptive version of the MS-lasso reduces both the bias and the number of false positive selections considerably. The MS-lasso and the adaptive MS-lasso are compared with other existing methods for variable selection in high dimensions by simulation and the methods are applied to two relevant genomic data sets. Results indicate that the (adaptive) MS-lasso has excellent properties compared to the other methods both in terms of estimation and selection, and can be recommended for high-dimensional monotone regression.",2014,Comput. Stat. Data Anal.
First Steps in DATA MINING with SAS ENTERPRISE MINER,"SAS Enterprise Miner streamlines the data mining process to create highly accurate predictive and descriptive models based on analysis of vast amounts of data from across an enterprise. Data mining is applicable in a variety of industries and provides methodologies for such diverse business problems as fraud detection, householding, customer retention and attrition, database marketing, market segmentation, risk analysis, affinity analysis, customer satisfaction, bankruptcy prediction, and portfolio analysis. In SAS Enterprise Miner, the data mining process has the following (SEMMA) steps: Sample the data by creating one or more data sets. The sample should be large enough to contain significant information, yet small enough to process. This step includes the use of data preparation tools for data import, merge, append, and filter, as well as statistical sampling techniques. Explore the data by searching for relationships, trends, and anomalies in order to gain understanding and ideas. This step includes the use of tools for statistical reporting and graphical exploration, variable selection methods, and variable clustering. Modify the data by creating, selecting, and transforming the variables to focus the model selection process. This step includes the use of tools for defining transformations, missing value handling, value recoding, and interactive binning. Model the data by using the analytical tools to train a statistical or machine learning model to reliably predict a desired outcome. This step includes the use of techniques such as linear and logistic regression, decision trees, neural networks, partial least squares, LARS and LASSO, nearest neighbor, and importing models defined by other users or even outside SAS Enterprise Miner. Assess the data by evaluating the usefulness and reliability of the findings from the data mining process. This step includes the use of tools for comparing models and computing new fit statistics, cutoff analysis, decision support, report generation, and score code management. You might or might not include all of the SEMMA steps in an analysis, and it might be necessary to repeat one or more of the steps several times before you are satisfied with the results. After you have completed the SEMMA steps, you can apply a scoring formula from one or more champion models to new data that might or might not contain the target variable. Scoring new data that is not available at the time of model training is the goal of most data mining problems. Furthermore, advanced visualization tools enable you to quickly and easily examine large amounts of data in multidimensional histograms and to graphically compare modeling results.",2014,
A Statistical Note on Analyzing and Interpreting Individual-Level Epidemiological Data,"With the development of information technology and growing interest in collaboration with medical research, chances to analyze or interpret individual-level data have increased in the past decades, and such individual-level studies tend to be large-scale. It is important to know that even with large samples, analysis is often limited by few numbers of events and difficulty in interpreting P-values. In this article, we will argue several points that researchers should consider to correctly analyze and interpret individual-level data, and we will suggest some statistical methods for practitioners. 
 
The first issue is Cox regression modeling with rare events. Parameters of interest can be estimated by the maximum likelihood method. Unfortunately, however, it is well known that the maximum likelihood estimator (MLE) becomes unreliable under â€œmonotone likelihoodâ€ (ie, during the iterative calculation, the likelihood converges while some estimated parameters diverge to infinity).1 For a simple univariate case, such monotone likelihood occurs when a failed individual with the rare event has the highest or lowest value for a covariate in the risk set at each failure time, which also happens in the case of a linear combination of independent variables.1,2 The resultant estimates commonly produce large estimates and standard errors (SE). Although monotone likelihood is not rare and is likely to occur even with large samples, few authors tend to address this phenomenon.1 
 
The same problem occurs in logistic regression models. Although the logistic regression model can always have at least one global maximum because of concavity of log-likelihood,3 failure of convergence (ie, monotone likelihood, also known as â€œcomplete separationâ€ in logistic regression models) may occur when a linear combination of variables can perfectly predict the outcome.4,5 For example, in the simplest situation, if there is a zero cell in the 2 Ã— 2 table formulated by dichotomous independent and dependent variables, maximum likelihood fails to converge. As a more general example, let us suppose that a large dataset has dummy variables, including five age categories and four job categories (ie, 5 Ã— 4 = 20 categories). Under such conditions, it is reasonable to assume that everyone lacks the expected outcome for at least one category. This example produces large odds ratios and huge SEs, letting Wald-type chi-squared statistics approach zero. Although most statistical packages give an alert in such cases, researchers should pay attention when they encounter large odds ratios and confidence intervals (CIs). To diagnose whether the model is suffering from monotone likelihood, we suggest that researchers apply a simple and conventional â€œrule of 10 events per variableâ€, where each independent variable should contain at least 10 events in the use of logistic or Cox regression models.6 
 
A possible solution for handling rare events would be to drop the variables suspected to cause monotone likelihood. However, we do not recommend this method because the omitted variables may have strong power to predict the outcome. Instead, it is preferable to rearrange the categories or to revert to an originally continuous variable. Another solution is to apply an exact logistic regression model, Bayesian estimation, or a penalized maximum likelihood (PML) method, which we can be easily implemented with R and SAS.5,7â€“9 Since the exact logistic regression bases the inference on exact permutational distributions of the sufficient statistics for regression coefficients of interest, performing logistic regression sometimes becomes a computer-intensive task when working with a large dataset.10 The Bayesian method requires a prior distribution, and the estimates are sensitive to the choice of the prior distribution. The PML method imposes penalty terms for parameters with an ordinal likelihood term. Applying Jeffreyâ€™s prior penalty (log determinant of the Fisher information of the parameters11) is popular in the PML method. To handle the perfect separation problem in the case of high-dimensional data with rare events, ridge and least absolute shrinkage and selection operator (LASSO) methods are also recommended. The ridge and LASSO methods shrink regression coefficients toward zero to improve the predictive ability, but the shrinkage leads to bias in exchange for the variance (ie, bias-variance trade-off).7 Therefore, debiasing must be performed for shrinkage estimators, such as re-calculation of coefficients which are estimated to be non-zero, to get unbiased estimates.7 
 
The second issue is the interpretation of P-values for statistical tests with large samples. It is common to depend only on the P-values to detect important exposure variables. However, over-reliance on P-values may lead to accepting the hypothesis with no or little significance for medical practitioners. We frequently observe articles presenting many small P-values such as â€œP < 0.001â€. A P-value measures the distance between estimates (eg, odds ratios) and the null hypothesis using the unit of SE. Therefore, P-values derived from statistical tests, such as t-tests and chi-squared tests, can be formulated as a function of a sample size (ie, as the sample size increases, the CIs narrow) and the SE (ie, as SEs decrease, the CIs narrow). Large samples tend to yield smaller P-values (approaching zero) and larger likelihood of rejecting the null hypothesis with higher statistical power, even if the difference has no practical meaning. Statistical proof can be found elsewhere.12 Despite this property, many studies adhere to the conventional significance threshold of P < 0.05. One way to solve this problem is to stop using statistical tests and report only their point estimates and 95% CIs13 and let the readers interpret the significance of the findings. In using large-scale and individual-level data, researchers should interpret the results with caution by using these suggested techniques.",2015,Journal of Epidemiology
Efficient Regularized Piecewise-Linear Regression Trees,"We present a detailed analysis of the class of regression decision tree algorithms which employ a regulized piecewise-linear node-splitting criterion and have regularized linear models at the leaves. From a theoretic standpoint, based on Rademacher complexity framework, we present new high-probability upper bounds for the generalization error for the proposed classes of regularized regression decision tree algorithms, including LASSO-type, and $\ell_{2}$ regularization for linear models at the leaves. Theoretical result are further extended by considering a general type of variable selection procedure. Furthermore, in our work we demonstrate that the class of piecewise-linear regression trees is not only numerically stable but can be made tractable via an algorithmic implementation, presented herein, as well as with the help of modern GPU technology. Empirically, we present results on multiple datasets which highlight the strengths and potential pitfalls, of the proposed tree algorithms compared to baselines which grow trees based on piecewise constant models.",2019,ArXiv
GWAS identifies novel SLE susceptibility genes and explains the association of the HLA region,"In a genome-wide association study (GWAS) of individuals of European ancestry afflicted with systemic lupus erythematosus (SLE) the extensive utilization of imputation, step-wise multiple regression, lasso regularization and increasing study power by utilizing false discovery rate instead of a Bonferroni multiple test correction enabled us to identify 13 novel non-human leukocyte antigen (HLA) genes and confirmed the association of four genes previously reported to be associated. Novel genes associated with SLE susceptibility included two transcription factors (EHF and MED1), two components of the NF-ÎºB pathway (RASSF2 and RNF114), one gene involved in adhesion and endothelial migration (CNTN6) and two genes involved in antigen presentation (BIN1 and SEC61G). In addition, the strongly significant association of multiple single-nucleotide polymorphisms (SNPs) in the HLA region was assigned to HLA alleles and serotypes and deconvoluted into four primary signals. The novel SLE-associated genes point to new directions for both the diagnosis and treatment of this debilitating autoimmune disease.",2014,Genes and immunity
Understand driver awareness through brake behavior analysis: Reactive versus intended hard brake,"Driving is a highly dynamic activity where drivers' awareness to the traffic environment plays the essential role for successful performance. Easy, smooth driving depends on drivers' awareness to develop situation-specific expectations, where infrequent or unexpected situations are not taken into account. Understanding these unexpected situations provides important insight on driver situation awareness and accident prevention. This study takes advantage of recent development of wearable devices and uses driver physiological signals to identify such unexpected situations during driver hard brake. Based on a naturalistic driving dataset, we define two types of hard brake behavior: reactive and intended hard brake. The reactive hard brake relates to drivers reacting to unexpected situations that usually leads to deviated physiological signals due to stress. The intended hard brake relates to planned maneuver implementation that consists of stable physiological signals. By using the human evaluation, we identified the different situations in which these two types of hard brake occurs. Clear difference is observed between these two groups of situations. Our goal is to identify features that are representative of these two type of road environment, especially the situations where unexpected reactive hard brake happens. Following this direction, we extracted features from Lidar depth scanner to represent the road scene, and applied both lasso regression and logistic regression classifier for feature analysis. The regression model achieves high correlation of 0.77 between the prediction and the ground truth while the classification achieves F-score of 0.76. The selected Lidar features can serve as high level road scene representation that facilitate next generation advanced driver assistance systems (ADAS) to prevent accident in unexpected traffic scenarios.",2017,2017 IEEE Intelligent Vehicles Symposium (IV)
Factors associated with opioid cessation: a machine learning approach,"Background and Aims People with opioid use disorder (OUD) can stop using opioids on their own, with help from groups and with treatment, but there is limited research on the factors that influence opioid cessation. Methods We employed multiple machine learning prediction algorithms (LASSO, random forest, deep neural network, and support vector machine) to assess factors associated with ceasing opioid use in a sample comprised of African Americans (AAs) and European Americans (EAs) who met DSM-5 criteria for mild to severe OUD. Values for several thousand demographic, alcohol and other drug use, general health, and behavioral variables, as well as diagnoses for other psychiatric disorders, were obtained for each participant from a detailed semi-structured interview. Results Support vector machine models performed marginally better on average than those derived using other machine learning methods with maximum prediction accuracies of 75.4% in AAs and 79.4% in EAs. Subsequent stepwise regression analyses that considered the 83 most highly ranked variables across all methods and models identified less recent cocaine use (p<5Ã—10âˆ’8), a shorter duration of opioid use (p<5Ã—10âˆ’6), and older age (p<5Ã—10âˆ’9) as the strongest independent predictors of opioid cessation. Factors related to drug use comprised about half of the significant independent predictors, with other predictors related to non-drug use behaviors, psychiatric disorders, overall health, and demographics. Conclusions These proof-of-concept findings provide information that can help develop strategies for improving OUD management and the methods we applied provide a framework for personalizing OUD treatment.",2019,bioRxiv
Safe preselection in lasso-type problems by cross-validation freezing,"We propose a new approach to safe variable preselection in high-dimensional penalized regression, such as the lasso. Preselection - to start with a manageable set of covariates - has often been implemented without clear appreciation of its potential bias. Based on sequential implementation of the lasso with increasing lists of predictors, we find a new property of the set of corresponding cross-validation curves, a pattern that we call freezing. It allows to determine a subset of covariates with which we reach the same lasso solution as would be obtained using the full set of covariates. Freezing has not been characterized before and is different from recently discussed safe rules for discarding predictors. We demonstrate by simulation that ranking predictors by their univariate correlation with the outcome, leads in a majority of cases to early freezing, giving a safe and efficient way of focusing the lasso analysis on a smaller and manageable number of predictors. We illustrate the applicability of our strategy in the context of a GWAS analysis and on microarray genomic data. Freezing offers great potential for extending the applicability of penalized regressions to ultra highdimensional data sets. Its applicability is not limited to the standard lasso but is a generic property of many penalized approaches.",2012,arXiv: Methodology
Lung transcriptomic clock predicts premature aging in cigarette smoke-exposed mice,"Lung aging is characterized by a number of structural alterations including fibrosis, chronic inflammation and the alteration of inflammatory cell composition. Chronic exposure to cigarette smoke (CS) is known to induce similar alterations and may contribute to premature lung aging. Additionally, aging and CS exposure are associated with transcriptional alterations in the lung. The current work aims to explore the interaction between age- and CS- associated transcriptomic perturbations and develop a transcriptomic clock able to predict the biological age and the impact of external factors on lung aging. Our investigations revealed a substantial overlap between transcriptomic response to CS exposure and age-related transcriptomic alterations in the murine lung. Of particular interest is the strong upregulation of immunoglobulin genes with increased age and in response to CS exposure, indicating an important implication of B-cells in lung inflammation associated with aging and smoking. Furthermore, we used a machine learning approach based on Lasso regression to build a transcriptomic age model that can accurately predict chronological age in untreated mice and the deviations associated with certain exposures. Interestingly, CS-exposed-mice were predicted to be prematurely aged in contrast to mice exposed to fresh air or to heated tobacco products (HTPs). The accelerated aging rate associated with CS was reversed upon smoking cessation or switching to HTPs. Additionally, our model was able to predict premature aging associated with thoracic irradiation from an independent public dataset. Aging and CS exposure share common transcriptional alteration patterns in the murine lung. The massive upregulation of B-cell restricted genes during these processes shed light on the contribution of cell composition and particularly immune cells to the measured transcriptomic signal. Through machine learning approach, we show that gene expression changes can be used to accurately monitor the biological age and the modulations associated with certain exposures. Our findings also suggest that the premature lung aging is reversible upon the reduction of harmful exposures.",2020,BMC Genomics
Who Gets the Job and How are They Paid? Machine Learning Application on H-1B Case Data,"In this paper, we use machine learning techniques to explore the H-1B application dataset disclosed by the Department of Labor (DOL), from 2008 to 2018, in order to provide more stylized facts of the international workers in US labor market. We train a LASSO Regression model to analyze the impact of different features on the applicant's wage, and a Logistic Regression with L1-Penalty as a classifier to study the feature's impact on the likelihood of the case being certified. Our analysis shows that working in the healthcare industry, working in California, higher job level contribute to higher salaries. In the meantime, lower job level, working in the education services industry and nationality of Philippines are negatively correlated with the salaries. In terms of application status, a Ph.D. degree, working in retail or finance, majoring in computer science will give the applicants a better chance of being certified. Applicants with no or an associate degree, working in the education services industry, or majoring in education are more likely to be rejected.",2019,arXiv: Applications
L 1 penalty and shrinkage estimation in partially linear models with random coefficient autoregressive errors,"In partially linear models, we consider methodology for simultaneous model selection and parameter estimation with random coefficient autoregressive errors by using lasso and shrinkage strategies. We provide natural adaptive estimators that significantly improve upon the classical procedures in the situation where some of the predictors are nuisance variables that may or may not affect the association between the response and the main predictors. In the context of two competing partially linear regression models (full and submodels), we consider an adaptive shrinkage estimation strategy and propose the shrinkage estimator and the positive-rule shrinkage estimator. We develop the properties of these estimators by using the notion of asymptotic distributional risk. The shrinkage estimators are shown to have a higher efficiency than the classical estimators for a wide class of models. For the lasso-type estimation strategy, we devise efficient algorithms to obtain numerical results. We compare the relative performance of lasso with the shrinkage estimator and the other estimators. Monte Carlo simulation experiments are conducted for various combinations of the nuisance parameters and sample size, and the performance of each method is evaluated in terms of simulated mean squared error. The comparison reveals that lasso and shrinkage strategies outperform the classical procedure. The relative performance of lasso and shrinkage strategies is comparable. The shrinkage estimators perform better than the lasso strategy in the effective part of the parameter space when, and only when, there are many nuisance variables in the model. A data example is showcased to illustrate the usefulness of suggested methods. Copyright Â© 2011 John Wiley & Sons, Ltd.",2012,Applied Stochastic Models in Business and Industry
Sparse estimation for case-control studies with multiple subtypes of cases.,"The analysis of case-control studies with several subtypes of cases is increasingly common, e.g. in cancer epidemiology. For matched designs, we show that a natural strategy is based on a stratified conditional logistic regression model. Then, to account for the potential homogeneity among the subtypes of cases, we adapt the ideas of data shared lasso, which has been recently proposed for the estimation of regression models in a stratified setting. For unmatched designs, we compare two standard methods based on L1-norm penalized multinomial logistic regression. We describe formal connections between these two approaches, from which practical guidance can be derived. We show that one of these approaches, which is based on a symmetric formulation of the multinomial logistic regression model, actually reduces to a data shared lasso version of the other. Consequently, the relative performance of the two approaches critically depends on the level of homogeneity that exists among the subtypes of cases: more precisely, when homogeneity is moderate to high, the non-symmetric formulation with controls as the reference is not recommended. Empirical results obtained from synthetic data are presented, which confirm the benefit of properly accounting for potential homogeneity under both matched and unmatched designs. We also present preliminary results from the analysis a case-control study nested within the EPIC cohort, where the objective is to identify metabolites associated with the occurrence of subtypes of breast cancer.",2019,arXiv: Methodology
Nonconvex penalized regression using depth-based penalty functions: multitask learning and support union recovery in high dimensions,"We propose a new class of nonconvex penalty functions in the paradigm of penalized sparse regression that are based on data depth-based inverse ranking. Focusing on a one-step sparse estimator of the coefficient matrix that is based on local linear approximation of the penalty function, we derive its theoretical properties and provide the algorithm for its computation. For orthogonal design and independent responses, the resulting thresholding rule enjoys near-minimax optimal risk performance, similar to the adaptive lasso (Zou, 2006). A simulation study as well as real data analysis demonstrate its effectiveness compared to present methods that provide sparse solutions in multivariate regression.",2016,
A Brief Introduction to the Temporal Group LASSO and its Potential Applications in Healthcare,"The Temporal Group LASSO is an example of a multi-task, regularized regression approach for the prediction of response variables that vary over time. The aim of this work is to introduce the reader to the concepts behind the Temporal Group LASSO and its related methods, as well as to the type of potential applications in a healthcare setting that the method has. We argue that the method is attractive because of its ability to reduce overfitting, select predictors, learn smooth effect patterns over time, and finally, its simplicity",2017,arXiv: Machine Learning
