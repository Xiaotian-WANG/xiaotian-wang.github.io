title,abstract,year,journal
Exploring the areas of applicability of whole-genome prediction methods for Asian rice (Oryza sativa L.),"Key messageOur simulation results clarify the areas of applicability of nine prediction methods and suggest the factors that affect their accuracy at predicting empirical traits.AbstractWhole-genome prediction is used to predict genetic value from genome-wide markers. The choice of method is important for successful prediction. We compared nine methods using empirical data for eight phenological and morphological traits of Asian rice cultivars (Oryza sativa L.) and data simulated from real marker genotype data. The methods were genomic BLUP (GBLUP), reproducing kernel Hilbert spaces regression (RKHS), Lasso, elastic net, random forest (RForest), Bayesian lasso (Blasso), extended Bayesian lasso (EBlasso), weighted Bayesian shrinkage regression (wBSR), and the average of all methods (Ave). The objectives were to evaluate the predictive ability of these methods in a cultivar population, to characterize them by exploring the area of applicability of each method using simulation, and to investigate the causes of their different accuracies for empirical traits. GBLUP was the most accurate for one trait, RKHS and Ave for two, and RForest for three traits. In the simulation, Blasso, EBlasso, and Ave showed stable performance across the simulated scenarios, whereas the other methods, except wBSR, had specific areas of applicability; wBSR performed poorly in most scenarios. For each method, the accuracy ranking for the empirical traits was largely consistent with that in one of the simulated scenarios, suggesting that the simulation conditions reflected the factors that affected the method accuracy for the empirical results. This study will be useful for genomic prediction not only in Asian rice, but also in populations from other crops with relatively small training sets and strong linkage disequilibrium structures.",2014,Theoretical and Applied Genetics
A Robust Multivariate EWMA Control Chart for Detecting Sparse Mean Shifts,"In multivariate statistical process control (MSPC) applications, process mean shifts sometimes occur in only a few components. To solve this MSPC problem, many control charts were proposed in the literature. Most of these charts assumed that the multivariate quality characteristics are normally distributed. Among them, the control chart proposed by Zou and Qiu (2009), incorporating the least absolute shrinkage and selection operator (LASSO) method into the EWMA scheme, has the best overall performance. In this paper, we extend the classical multivariate LASSO control chart to a robust version that has an affine-invariance property and is distribution free under the family of elliptical direction distributions, indicating that the in-control run-length distribution is the same for any continuous distribution in this family and the control limit can be acquired from the multivariate standard normal distribution. Our simulation results show that the proposed method is very efficient in detecting various sparse shifts under heavy-tailed and skewed multivariate distributions. In addition, it is easy to implement with an iterative algorithm and the least angle regression (LARS) algorithm. White-wine data illustrates that the proposed control chart performs quite well in applications.",2016,Journal of Quality Technology
On quantifying quality of care,"In this thesis, we examine how the analysis of quality outcomes, such as 30-day mortality for patients with acute stroke, can help compare the quality of care between hospitals. As in the neighboring countries, the demand for quality control in hospitals is growing but also, for example for residential care centers and schools, both by government and patients as well as centers themselves. 
Given the potentially large impact of reported results, this requires a careful statistical analysis of the available data, as discussed in Chapter 1. To estimate the causal effect of the quality of care on the outcome of interest, we have to control for differences between patients on admission, such as age and initial disease severity. This is necessary because they may influence the outcome and they are possibly distributed differently across centers. Otherwise, hospitals treating mostly elderly patients may show higher mortality risks, even though the given care is excellent. The research questions in this thesis were mostly inspired by the analysis of the Swedish register for acute stroke care, Riksstroke (http://www.riksstroke.org/eng/), but the discussed methods are more generally applicable. To account for measured patient characteristics we will use, depending on the research question, directly or indirectly ized risks as performance measure. 
It has been proven that when standardized risks are estimated based on the popular normal mixed effects model, the estimated quality of care may be shrunken towards the average, often masking outlying performance of hospitals (Normand et al., 1997; Ash et al., 2012). In Chapter 2 we therefore investigated the use of a Firth corrected fixed effects model and found little shrinkage of the center effects towards the overall mean. This approach is thus particularly valuable when some centers have a small number of registered patients since the convergence of this estimation strategy is better than for fixed effects models and a better detection of outlying performance is obtained than for normal mixed effects models. Secondly, we investigate undue model extrapolation when estimating for example, directly standardized risks, especially if patient mix differs substantially between hospitals. Extrapolation in combination with the use of misspecified statistical models can yield biased results with an underestimated uncertainty. Therefore, we examined a method that weights observations by the inverse of the so-called propensity score, i.e. the probability to be treated in the observed center (Shahian and Normand, 2008). The investigated doubly robust method is protected against model misspecification (Robins et al., 2007) and, if the propensity score is very small, the user will be warned for extrapolation via inflated variance estimates. Although promising, the obtained results suggested to use the Firth corrected fixed effects method. 
Common adjustments for differences in patientmix generally assume that the effect of the given care level on the outcome is constant across patient groups (Ohlssen et al., 2007b; Shahian and Normand, 2008). In practice, however, this may be violated when some centers are for example specialized in care for the elderly (Nicholl et al., 2013;Mohammed et al., 2009). If then no center-patient interactions are included in the outcome regression model, we found in Chapter 3 that the directly and indirectly standardized risks will only be biased if the distribution of that patient characteristic differs substantially across centers, otherwise bias is negligible. Being able to justify common practice is especially important in settings where it is simply impossible to estimate these interactions in the model, because insufficient information is available in small hospitals, for example see Ash et al. (2012). 
In Chapter 4 we also examined how the number of (expensive, genetic) measurements - and thus the cost per patient - can be reduced when predicting individual patient outcomes or estimating standardized risks for hospital quality evaluation. Stochastic search algorithms allow for a relatively quick and costefficient variable selection and they can easily handle multiple imputed datasets when some measurements are missing. We have also illustrated how the search time can be further reduced by a priori performing a cost-efficient generalized LASSO search. 
Because we believe in the broad applicability of the statistical methods in this thesis, we havemade them available via the R-package RiskStandard (www.cvstat.ugent.be), as documented in Chapter 5.",2015,
Inference in High-Dimensional Linear Regression via Lattice Basis Reduction and Integer Relation Detection,"We focus on the high-dimensional linear regression problem, where the algorithmic goal is to efficiently infer an unknown feature vector $\beta^*\in\mathbb{R}^p$ from its linear measurements, using a small number $n$ of samples. Unlike most of the literature, we make no sparsity assumption on $\beta^*$, but instead adopt a different regularization: In the noiseless setting, we assume $\beta^*$ consists of entries, which are either rational numbers with a common denominator $Q\in\mathbb{Z}^+$ (referred to as $Q$-rationality); or irrational numbers supported on a rationally independent set of bounded cardinality, known to learner; collectively called as the mixed-support assumption. Using a novel combination of the PSLQ integer relation detection, and LLL lattice basis reduction algorithms, we propose a polynomial-time algorithm which provably recovers a $\beta^*\in\mathbb{R}^p$ enjoying the mixed-support assumption, from its linear measurements $Y=X\beta^*\in\mathbb{R}^n$ for a large class of distributions for the random entries of $X$, even with one measurement $(n=1)$. In the noisy setting, we propose a polynomial-time, lattice-based algorithm, which recovers a $\beta^*\in\mathbb{R}^p$ enjoying $Q$-rationality, from its noisy measurements $Y=X\beta^*+W\in\mathbb{R}^n$, even with a single sample $(n=1)$. We further establish for large $Q$, and normal noise, this algorithm tolerates information-theoretically optimal level of noise. We then apply these ideas to develop a polynomial-time, single-sample algorithm for the phase retrieval problem. Our methods address the single-sample $(n=1)$ regime, where the sparsity-based methods such as LASSO and Basis Pursuit are known to fail. Furthermore, our results also reveal an algorithmic connection between the high-dimensional linear regression problem, and the integer relation detection, randomized subset-sum, and shortest vector problems.",2019,arXiv: Statistics Theory
Iterative selection using orthogonal regression techniques,"High dimensional data are nowadays encountered in various branches of science. Variable selection techniques play a key role in analyzing high dimensional data. Generally two approaches for variable selection in the high dimensional data setting are consideredâ€”forward selection methods and penalization methods. In the former, variables are introduced in the model one at a time depending on their ability to explain variation and the procedure is terminated at some stage following some stopping rule. In penalization techniques such as the least absolute selection and shrinkage operator (LASSO), as optimization procedure is carried out with an added carefully chosen penalty function, so that the solutions have a sparse structure. Recently, the idea of penalized forward selection has been introduced. The motivation comes from the fact that the penalization techniques like the LASSO give rise to closed form expressions when used in one dimension, just like the least square estimator. Hence one can repeat such a procedure in a forward selection setting until it converges. The resulting procedure selects sparser models than comparable methods without compromising on predictive power. However, when the regressor is high dimensional, it is typical that many predictors are highly correlated. We show that in such situations, it is possible to improve stability and computational efficiency of the procedure further by introducing an orthogonalization step. At each selection step, variables potentially available to be selected in the model are screened on the basis of their correlation with variables already in the model, thus preventing unnecessary duplication. The new strategy, called the Selection Technique in Orthogonalized Regression Models (STORM), turns out to be extremely successful in reducing the model dimension further and also leads to improved predicting power. We also consider an aggressive version of the STORM, where a potential predictor will be permanently removed from further consideration if its regression coefficient is estimated as zero at any stage. We shall carry out a detailed simulation study to compare the newly proposed method with existing ones and analyze a real dataset. ï›™ 2013 Wiley Periodicals, Inc. Statistical Analysis and Data Mining 6: 557-564, 2013",2013,Statistical Analysis and Data Mining
Prognostic implications of autophagy-associated gene signatures in non-small cell lung cancer,"Autophagy, a highly conserved cellular proteolysis process, has been involved in non-small cell lung cancer (NSCLC). We tried to develop a prognostic prediction model for NSCLC patients based on the expression profiles of autophagy-associated genes. Univariate Cox regression analysis was used to determine autophagy-associated genes significantly correlated with overall survival (OS) of the TCGA lung cancer cohort. LASSO regression was performed to build multiple-gene prognostic signatures. We found that the 22-gene and 11-gene signatures could dichotomize patients with significantly different OS and independently predict the OS in TCGA lung adenocarcinoma (HR=2.801, 95% CI=2.252-3.486, P<0.001) and squamous cell carcinoma (HR=1.105, 95% CI=1.067-1.145, P<0.001), respectively. The prognostic performance of the 22-gene signature was validated in four GEO lung cancer cohorts. Moreover, GO, KEGG, and GSEA analyses unveiled several fundamental signaling pathways and cellular processes associated with the 22-gene signature in lung adenocarcinoma. We also constructed a clinical nomogram with a concordance index of 0.71 to predict the survival possibility of NSCLC patients by integrating clinical characteristics and the autophagy gene signature. The calibration curves substantiated fine concordance between nomogram prediction and actual observation. Overall, we constructed and verified a novel autophagy-associated gene signature that could improve the individualized outcome prediction in NSCLC.",2019,Aging (Albany NY)
Title: Contextual MDPs: A New Model and PAC Guarantees for Reinforcement Learning with Rich Observations,"s: Alekh Agarwal (Microsoft Research) Title: Contextual MDPs: A New Model and PAC Guarantees for Reinforcement Learning with Rich Observations We propose and study a new tractable model for reinforcement learning with highdimensional observation called Contextual-MDPs, generalizing contextual bandits to a sequential decision making setting. These models require an agent to take actions based on high-dimensional observations (features) with the goal of achieving long-term performance competitive with a large set of policies. Since the size of the observation space is a primary obstacle to sample-efficient learning, Contextual-MDPs are assumed to be summarizable by a small number of hidden states. In this setting, we design a new reinforcement learning algorithm that engages in global exploration while using a function class to approximate future performance. We also establish a sample complexity guarantee for this algorithm, proving that it learns near optimal behavior after a number of episodes that is polynomial in all relevant parameters, logarithmic in the number of policies, and independent of the size of the observation space. This represents an exponential improvement on the sample complexity of all existing alternative approaches and provides theoretical justification for reinforcement learning with function approximation. This is joint work with Akshay Krishnamurthy and John Langford. Available online at http://arxiv.org/abs/1602.02722. Jelena Bradic (UC San Diego) Title: Robust Machine Learning Recent advances in technologies for cheaper and faster data acquisition and storage have led to an explosive growth of data complexity in a variety of research areas such as high-throughput genomics, biomedical imaging, high-energy physics, astronomy and economics. As a result, noise accumulation, experimental variation and data inhomogeneity have become substantial. However, machine learning in such settings is known to pose many statistical challenges and hence calls for new methods and theories. Moreover, the impact of outliers or non-gaussianity is far from obvious. In this talk we provide two methods for machine learning that adapt to the unknown outliers in the data and we provide theoretical understanding of the impact of outliers on the learning. In particular, we propose a new boosting framework called Arch Boost. It is designed for augmenting the existing work such that its corresponding classification algorithms with non-convex losses are significantly more adaptable to unknown data contamination. Along with the Arch Boost framework, a family of non-convex losses are proposed which leads to new robust boosting algorithms, namedAdaptive Robust Boosting (ARB). Moreover, when the dimension of the feature space is high, we propose a novel, robust and sparse approximate message passing algorithm (RAMP), that is adaptive to the error distribution. Our algorithm includes many non-quadratic and non-differentiable loss functions. We derive its asymptotic mean squared error and show its convergence, while allowing p, n, s â†’ âˆž, with n/p âˆˆ (0, 1) and n/s âˆˆ (1, âˆž). Lastly, we present theoretical developments that showcase the difficulty of studying robustness in machine learning and lead to many open questions. Emmanuel CandÃ¨s (Stanford) Title: A knockoff filter for controlling the false discovery rate The big data era has created a new scientific paradigm: collect data first, ask questions later. Imagine that we observe a response variable together with a large number of potential explanatory variables, and would like to be able to discover which variables are truly associated with the response. At the same time, we need to know that the false discovery rate (FDR)---the expected fraction of false discoveries among all discoveries---is not too high, in order to assure the scientist that most of the discoveries are indeed true and replicable. We introduce the knockoff filter, a new variable selection procedure controlling the FDR in the statistical linear model whenever there are at least as many observations as variables. This method works by constructing fake variables, knockoffs, which can then be used as controls for the true variables; the method achieves exact FDR control in finite sample settings no matter the design or covariates, the number of variables in the model, and the amplitudes of the unknown regression coefficients, and does not require any knowledge of the noise level. This is joint work with Rina Foygel Barber. Chen-Nee Chuah (UC Davis) Title: Network Inference with Online Learning in Software Defined Networking Network measurement and inference play a key role in different networking applications including network design, traffic engineering, and security analytics. Obtaining fine-grained measurement data under hard resource constraints is a challenging task in todayâ€™s large-scale networks. This talk will demonstrate how flexibility of software-defined networking (SDN) can be leveraged to adapt measurement rules based on optimal online strategies to augment traditional network inference techniques to obtain better estimates of network characteristics, such as traffic matrix or per-hop delay/loss rates. We will present a few case studies that demonstrate how SDN-enabled network monitoring and inference problems can benefit from theories and results from compressive sensing, multi-arm bandit, and statistical online learning. Simulation results using mininet and prototyping effort using hardware OpenFlow will be discussed. Ian Davidson (UC Davis) Title: Variations of Spectral Clustering Formulations and Challenges Spectral clustering is a commonly used method for segmenting graphs due to its ease of implementation and underpinning in spectral graph theory such as connections to min-cut of a graph. Recent variations by our selves and others involve: i) adding in guidance and constraints, ii) segmenting multiple graphs and iii) adding a human to the process via active learning. I will cover all these three settings pointing out why they are useful, existing formulations and challenges, particularly core underlying questions that remain unanswered. Cho-Jui Hsieh (UC Davis) Title: Asynchronous Parallel Optimization in Machine Learning Asynchronous parallel optimization is important for solving large-scale machine learning problems. In this talk, I will present theoretical and practical challenges of asynchronous parallel optimization, and talk about our current approaches for addressing these challenges in multi-core or distributed system. Sham Kakade (University of Washington) Title: Discovering Hidden Structure in the Sparse Regime In many applications, we face the challenge of modeling the hidden interactions between multiple observations (e.g. discovering clusters of points in space or learning topics in documents). Furthermore, an added difficulty is that our datasets often have empirical distributions which are heavy tailed tailed (e.g. problems in natural language processing). In other words, even though we have large datasets, we are often in a sparse regime where there is a large fraction of our items that have only been observed a few times (e.g. Zipfâ€™s law which states that regardless of how big our corpus of text is, a large fraction of the words in our vocabulary will only be observed a few times). The question we consider is how to learn a model of our data when our dataset is large and yet is sparse. We provide an algorithm for learning certain natural latent variable models applicable to this sparse regime, making connections to a body of recent work in sparse random graph theory and community detection. We also discuss the implications to practice. Xiaodong Li (UC Davis) Title: Wirtinger flow and thresholded Wirtinger flow By dimension lifting techniques, phase retrieval can be also viewed as a special low-rank recovery problem. Although convex methods, such as PhaseLift, are provably effective and robust, the computational complexity and storage cost could be pretty high. To address these issues, we introduce a non-convex optimization algorithm, named Wirtinger flow, with theoretically guaranteed performances. It is much more efficient than convex methods in terms of computation and memory. Finally, I will introduce how to modify Wirtinger flow into an adaptive thresholded gradient descent method given the signal is known to be sparse, in order to achieve the minimax convergence rates under l_2 loss. Miles Lopes (UC Davis) Title: Unknown Sparsity in Compressed Sensing: Denoising and Inference The theory of Compressed Sensing (CS) asserts that an unknown signal x in R can be accurately recovered from an underdetermined set of n linear measurements with n<<p, provided that x is sufficiently sparse. However, in applications, the degree of sparsity ||x||0 is typically unknown, and the problem of directly estimating ||x||0 has been a longstanding gap between theory and practice. A closely related issue is that ||x||0 is a highly idealized measure of sparsity, and for real signals with entries not equal to 0, the value ||x||0=p is not a useful description of compressibility. In our previous work that examined these problems, we considered an alternative measure of â€œsoftâ€ sparsity, (||x||1/||x||2), and designed a procedure to estimate (||x||1/||x||2) that does not rely on sparsity assumptions. The present work offers a new deconvolution-based method for estimating unknown sparsity, which has wider applicability and sharper theoretical guarantees. In particular, we introduce a family of entropy-based sparsity measures sq(x) parameterized by qâ‰¥0, which includes (||x||1/||x||2) and ||x||0 as special cases. Also, we propose an estimator for sq(x) whose relative error converges at the dimension-free rate of n, even when p/n diverges. Our main results also describe the limiting distribution of the estimator, as well as some connections to Basis Pursuit Denosing, the Lasso, deterministic measure",2016,
Quantitative methods for metabolomic analyses evaluated in the Childrenâ€™s Health Exposure Analysis Resource (CHEAR),"With advances in technologies that facilitate metabolome-wide analyses, the incorporation of metabolomics in the pursuit of biomarkers of exposure and effect is rapidly evolving in population health studies. However, many analytic approaches are limited in their capacity to address high-dimensional metabolomics data within an epidemiologic framework, including the highly collinear nature of the metabolites and consideration of confounding variables. In this Childrenâ€™s Health Exposure Analysis Resource (CHEAR) network study, we showcase various analytic approaches that are established as well as novel in the field of metabolomics, including univariate single metabolite models, least absolute shrinkage and selection operator (LASSO), random forest, weighted quantile sum (WQSRS) regression, exploratory factor analysis (EFA), and latent class analysis (LCA). Here, in a Bangladeshi birth cohort (nâ€‰=â€‰199), we illustrate research questions that can be addressed by each analytic method in the assessment of associations between cord blood metabolites (1H NMR measurements) and birth anthropometric measurements (birth weight and head circumference).",2019,Journal of Exposure Science & Environmental Epidemiology
Genetic prediction of quantitative lipid traits: comparing shrinkage models to gene scores.,"Accurate genetic prediction of quantitative traits related to complex disease risk would have potential clinical impact, so investigation of statistical methodology to improve predictive performance is important. We compare a simple approach of polygenic scores using top ranking single nucleotide polymorphisms (SNPs) to a set of shrinkage models, namely Ridge Regression, Lasso and Hyper-Lasso. These penalised regression methods analyse all genotyped SNPs simultaneously, potentially including much larger sets of SNPs in the models, not only those with the smallest P values. We compare the accuracy of these models for predicting low-density lipoprotein (LDL) and high-density lipoprotein (HDL) cholesterol, two lipid traits of clinical relevance, in the Whitehall II and British Women's Health and Heart Study cohorts, using SNPs from the HumanCVD BeadChip. For gene scores, the most accurate predictions arise from multivariate weighted scores and include only a small number of SNPs, identified as top hits by the HumanCVD BeadChip. Furthermore, there was little benefit from including external results from published sets of SNPs. We found that shrinkage approaches rarely improved significantly on gene score results. Genetic predictive performance is trait specific, depending on the heritability and genetic architecture of the trait, and is limited by the training data sample size. Our results for lipid traits suggest no current benefit of more complex methods over existing gene score methods. Instead, the most important choice for the prediction model is the number of SNPs and selection of the most predictive SNPs to include. However further comparisons, in larger samples and for other phenotypes, would still be of interest.",2014,Genetic epidemiology
Radiomic Profiling of Head and Neck Cancer: 18F-FDG PET Texture Analysis as Predictor of Patient Survival,"Background and Purpose
The accurate prediction of prognosis and pattern of failure is crucial for optimizing treatment strategies for patients with cancer, and early evidence suggests that image texture analysis has great potential in predicting outcome both in terms of local control and treatment toxicity. The aim of this study was to assess the value of pretreatment 18F-FDG PET texture analysis for the prediction of treatment failure in primary head and neck squamous cell carcinoma (HNSCC) treated with concurrent chemoradiation therapy.


Methods
We performed a retrospective analysis of 90 patients diagnosed with primary HNSCC treated between January 2010 and June 2017 with concurrent chemo-radiotherapy. All patients underwent 18F-FDG PET/CT before treatment. 18F-FDG PET/CT texture features of the whole primary tumor were measured using an open-source texture analysis package. Least absolute shrinkage and selection operator (LASSO) was employed to select the features that are associated the most with clinical outcome, as progression-free survival and overall survival. We performed a univariate and multivariate analysis between all the relevant texture parameters and local failure, adjusting for age, sex, smoking, primary tumor site, and primary tumor stage. Harrell c-index was employed to score the predictive power of the multivariate cox regression models.


Results
Twenty patients (22.2%) developed local failure, whereas the remaining 70 (77.8%) achieved durable local control. Multivariate analysis revealed that one feature, defined as low-intensity long-run emphasis (LILRE), was a significant predictor of outcome regardless of clinical variables (hazard ratioâ€‰<â€‰0.001, P=0.001).The multivariate model based on imaging biomarkers resulted superior in predicting local failure with a c-index of 0.76 against 0.65 of the model based on clinical variables alone.


Conclusion
LILRE, evaluated on pretreatment 18F-FDG PET/CT, is associated with higher local failure in patients with HNSCC treated with chemoradiotherapy. Using texture analysis in addition to clinical variables may be useful in predicting local control.",2018,Contrast Media & Molecular Imaging
P117 Machine learning tool provides new insights into risk assessment in pulmonary endarterectomy,"Background Chronic thromboembolic pulmonary hypertension (CTEPH) is an uncommon disorder characterised by persistent obstruction of the pulmonary arteries by thromboembolic material, usually following an acute pulmonary embolus.1 Pulmonary endarterectomy (PEA) is the gold standard treatment for eligible patients and is potentially curative.1Whilst pre-operative parameters have been associated with post-operative mortality no sytematic method for predicting individualised PEA risk presently exists. Objectives To identify pre-operative risk factors of 90 day mortality (90DM), five year mortality (5YM) and improvement in self-reported functional status (DQ) following PEA for inclusion in a clinically-implementable risk prediction tool. Methods Consecutive patients undergoing PEA for CTEPH at Royal Papworth Hospital, UK between 2007 and 2017 were included. Potential pre-operative predictors including patient demographics, medical history and results of functional, physiological and patient self-reported measures were included in a hypothesis-free approach. Three stastical predictive models were considered (linear regression, lasso regression and random forest), each of which were calibrated, fitted and assessed using cross-validation ensuring internal consistency. Results 1336 individuals were included in risk modelling. 96 patients (6.4%) died within 90 days of hospital discharge and 154 (11.5%) within five years of PEA. Random forest based predictions were more accurate than linear or lasso based. All post-operative outcomes were predicted well from pre-operative variables (90DM: AUROC 0.82 (95% CI 0.78, 0.87); 5YM: C-Index 0.81 (0.76, 0.85); DQ (Spearmanâ€™s correlation 0.47 (0.43, 0.51)) using random forest modelling. The strongest individual pre-operative predictor of 90DM and 5YM was left atrial dilatation and of DQ, pulmonary vasodilator therapy. Post-hoc analysis confirmed not only excess mortaltiy following PEA in those with left atrial dilatation secondary to diastolic dysfunction but adverse functional, haemodynamic and patient-reported outcomes in this group. Conclusions Outcomes from PEA can be predicted from pre-operative observations to a clinically useful degree enabling individualised risk prediction. Post-hoc analysis highlights the under-recognised adverse outcomes in those with left atrial dilatation. We present an online application to facilitate use of these tools. Further work validating our model in other centres will be necessary and aided by the open availability of our methodology. Reference Galie N, Humbert M, Vachiery J-L, et al. 2015 ESC/ERS Guidelines for the diagnosis and treatment of pulmonary hypertension. Eur Heart J2016;37(1):67â€“119.",2019,Thorax
Machine learning approaches for supporting patient-specific cardiac rehabilitation programs,"Cardiac rehabilitation is a well-recognised non-pharmacological intervention that prevents the recurrence of cardiovascular events. Previous studies investigated the application of data mining techniques for the prediction of the rehabilitation outcome in terms of physical, but fewer reports are focused on using predictive models to support clinicians in the choice of a patient-specific rehabilitative treatment path. Aim of the work was to derive a prediction model for help clinicians in the prescription of the rehabilitation program. We enrolled 129 patients admitted for cardiac rehabilitation after a major cardiovascular event. Data on anthropometric measures, surgical procedure and complications, comorbidities and physical performance scales were collected at admission. The prediction outcome was the rehabilitation program divided in four different paths. Different algorithms were tested to find the best predictive model. Models performance were measured by prediction accuracy. Mean model accuracy was 0.790 (SD 0.118). Best model selected was Lasso regression showing an average classification accuracy on test set of0.935. Data mining techniques have shown to be a reliable tool for support clinicians in the decision of cardiac rehabilitation treatment path.",2016,2016 Computing in Cardiology Conference (CinC)
Development of predictive models to identify advanced-stage cancer patients in a US healthcare claims database.,"BACKGROUND
Although healthcare databases are a valuable source for real-world oncology data, cancer stage is often lacking. We developed predictive models using claims data to identify metastatic/advanced-stage patients with ovarian cancer, urothelial carcinoma, gastric adenocarcinoma, Merkel cell carcinoma (MCC), and non-small cell lung cancer (NSCLC).


METHODS
Patients with â‰¥1 diagnosis of a cancer of interest were identified in the HealthCore Integrated Research Database (HIRD), a United States (US) healthcare database (2010-2016). Data were linked to three US state cancer registries and the HealthCore Integrated Research Environment Oncology database to identify cancer stage. Predictive models were constructed to estimate the probability of metastatic/advanced stage. Predictors available in the HIRD were identified and coefficients estimated by Least Absolute Shrinkage and Selection Operator (LASSO) regression with cross-validation to control overfitting. Classification error rates and receiver operating characteristic curves were used to select probability thresholds for classifying patients as cases of metastatic/advanced cancer.


RESULTS
We used 2723 ovarian cancer, 6522 urothelial carcinoma, 1441 gastric adenocarcinoma, 109 MCC, and 12,373 NSCLC cases of early and metastatic/advanced cancer to develop predictive models. All models had high discrimination (Câ€¯>â€¯0.85). At thresholds selected for each model, PPVs were all >0.75: ovarian cancerâ€¯=â€¯0.95 (95% confidence interval [95% CI]: 0.94-0.96), urothelial carcinomaâ€¯=â€¯0.78 (95% CI: 0.70-0.86), gastric adenocarcinomaâ€¯=â€¯0.86 (95% CI: 0.83-0.88), MCCâ€¯=â€¯0.77 (95% CI 0.68-0.89), and NSCLCâ€¯=â€¯0.91 (95% CI 0.90 - 0.92).


CONCLUSION
Predictive modeling was used to identify five types of metastatic/advanced cancer in a healthcare claims database with greater accuracy than previous methods.",2019,Cancer epidemiology
Bud burst of birch in Finland and the United Kingdom - Logistic regression analysis and modeling,"The day of bud burst (DBB) of different tree species are known to be 
affected by factors such as growing degree days and temperature. In this 
paper a two state Markov chain is used to model DBB for birch. The model 
is fit using logistic regression and LASSO regularization is used to 
evaluate which of many potential factors best forecast DBB. Data of birch 
from both Finland and the United Kingdom is studied and differences 
between the models adapted to the two countries are investigated. For 
modeling purposes to capture the environment of forecasting, estimated 
interpolated gridded climate data was used and not directly measured 
climate data. 
It is found that the models give very accurate predictions on the DBB. For 
Finland it is little more than 2 days in mean absolute error (MAE). The 
model is also fairly compact having less than 10 explaining covariates. 
The covariate, accumulated growing degree days, was as expected part of 
the models as well as among others variation of precipitation.",2013,
The Sylvester Graphical Lasso (SyGlasso),"This paper introduces the Sylvester graphical lasso (SyGlasso) that captures multiway dependencies present in tensor-valued data. The model is based on the Sylvester equation that defines a generative model. The proposed model complements the tensor graphical lasso (Greenewald et al., 2019) that imposes a Kronecker sum model for the inverse covariance matrix by providing an alternative Kronecker sum model that is generative and interpretable. A nodewise regression approach is adopted for estimating the conditional independence relationships among variables. The statistical convergence of the method is established, and empirical studies are provided to demonstrate the recovery of meaningful conditional dependency graphs. We apply the SyGlasso to an electroencephalography (EEG) study to compare the brain connectivity of alcoholic and nonalcoholic subjects. We demonstrate that our model can simultaneously estimate both the brain connectivity and its temporal dependencies.",2020,ArXiv
Radiomics Features of 18F-fluorodeoxyglucose Positron-Emission Tomography as a Novel Prognostic Signature in Colorectal Cancer,"Purpose: The aim of this study was to investigate the prognostic value of radiomics signatures derived from 18F-fluorodeoxyglucose (18F-FDG) positron-emission tomography (PET) in patients with colorectal cancer (CRC). 
Methods: From April 2008 to Jan 2014, we identified CRC patients who underwent 18F-FDG-PET before starting any neoadjuvant treatments and surgery. Radiomics features were extracted from the primary lesions identified on 18F-FDG-PET. Patients were divided into a training and a validation set by random sampling. A least absolute shrinkage and selection operator (LASSO) Cox regression model was applied for prognostic signature building with progression-free survival (PFS) using the training set. Using the calculated radiomics score, a nomogram was developed, and the clinical utility of this nomogram was assessed in the validation set. 
Results: Three-hundred-and-eight-one patients with surgically resected CRC patients (training set 228 vs. validation set 153) were included. In the training set, a radiomics signature called a rad_score was generated using two PET-derived features such as Gray Level Run Length Matrix_Long-Run Emphasis (GLRLM_LRE) and Grey-Level Zone Length Matrix_Short-Zone Low Gray-level Emphasis (GLZLM_SZLGE). Patients with a high-rad_score in the training and validation set had shorter PFS. Multivariable analysis revealed that the rad_score was an independent prognostic factor in both training and validation sets. A radiomics nomogram, developed using rad_score, nodal stage, and lymphovascular invasion, showed good performance in the calibration curve and comparable predictive power with the staging system in the validation set. 
Conclusion: Textural features derived from 18F-FDG-PET images may enable more detailed stratification of prognosis in patients with CRC.",2019,medRxiv
"Role of differentially expressed genes and long non-coding RNAs in papillary thyroid carcinoma diagnosis, progression, and prognosis.","Currently, the combination of ultrasonography and fine-needle aspiration biopsy (FNAB) can not discriminate between benign and malignant tumor of thyroid in some cases. The main issue in assessing the patients with thyroid nodules is to distinguish thyroid cancer from benign nodules, and reduce diagnostic surgery. To identify potential molecular biomarkers for patients with indeterminate FNAB, we explored the differentially expressed genes (DEGs) and differentially expressed long non-coding RNAs (DElncRNAs) in TCGA database between 318 papillary thyroid carcinoma (PTC) tissues and 35 normal thyroid gland tissues by DESeq R. Furthermore, DEGs were verified by gene expression profile GSE33630. Ten top DEGs and DElncRNAs were identified as candidate biomarkers for diagnosis and Lasso (Least Absolute Shrinkage and Selection Operator) logistic regression analysis were performed to improve the diagnostic accuracy of them. Besides, partial molecular biomarkers of top DEGs and DElncRNAs were closely related to the tumor stage (T), lymph node metastasis (N), metastasis (M) and pathological stage of PTC, which could reflect behavior of tumor progression. According to multivariate Cox analysis, the combination of two DEGs (METTL7B and KCTD16) and two DElncRNAs (LINC02454 and LINC02471) could predict the outcome in a more exact way. In conclusion, top DEGs and DElncRNAs could raise diagnosis of PTC in indeterminate FNAB specimens, and some could function as molecule biomarkers for tumor progression and prognosis.",2018,Journal of cellular biochemistry
Predicting frequent ED use by people with epilepsy with health information exchange data,"Objectives:To describe (1) the predictability of frequent emergency department (ED) use (a marker of inadequate disease control and/or poor access to care), and (2) the demographics, comorbidities, and use of health services of frequent ED users, among people with epilepsy. Methods:We obtained demographics, comorbidities, and 2 years of encounter data for 8,041 people with epilepsy from a health information exchange in New York City. Using a retrospective cohort design, we explored bivariate relationships between baseline characteristics (year 1) and subsequent frequent ED use (year 2). We then built, evaluated, and compared predictive models to identify frequent ED users (â‰¥4 visits year 2), using multiple techniques (logistic regression, lasso, elastic net, CART [classification and regression trees], Random Forests, AdaBoost, support vector machines). We selected a final model based on performance and simplicity. Results:People with epilepsy who, in year 1, were adults (rather than children or seniors), male, Manhattan residents, frequent users of health services, users of multiple health systems, or had medical, neurologic, or psychiatric comorbidities, were more likely to frequently use the ED in year 2. Predictive techniques identified frequent ED visitors with good positive predictive value (approximately 70%) but poor sensitivity (approximately 20%). A simple strategy, selecting individuals with 11+ ED visits in year 1, performed as well as more sophisticated models. Conclusions:People with epilepsy with 11+ ED visits in a year are at highest risk of continued frequent ED use and may benefit from targeted intervention to avoid preventable ED visits. Future work should focus on improving the sensitivity of predictions.",2015,Neurology
"How to relax hypotheses on the LASSO and the Dantzig Selector, and application to the transductive context","We consider the linear regression problem, where the number $p$ of covariates is possibly larger than the number $n$ of observations $(x_{i},y_{i})_{i\leq i \leq n}$, under sparsity assumptions. On the one hand, several methods have been successfully proposed to perform this task, for example the LASSO in \cite{Tibshirani-LASSO} or the Dantzig Selector in \cite{Dantzig}. On the other hand, consider new values $(x_{i})_{n+1\leq i \leq m}$. If one wants to estimate the corresponding $y_{i}$'s, one should think of a specific estimator devoted to this task, referred by Vapnik as a ""transductive"" estimator. This estimator may differ from an estimator designed to the more general task ""estimate on the whole domain"". In this work, we propose a generalized version both of the LASSO and the Dantzig Selector, based on the geometrical remarks about the LASSO in previous works. The ""usual"" LASSO and Dantzig Selector, as well as new estimators interpreted as transductive versions of the LASSO, appear as special cases. These estimators are interesting at least from a theoretical point of view: we can give theoretical guarantees for these estimators under hypotheses that are relaxed versions of the hypotheses required in the papers about the ""usual"" LASSO. These estimators can also be efficiently computed, with results comparable to the ones of the LASSO.",2009,arXiv: Statistics Theory
Calibrated zero-norm regularized LS estimator for high-dimensional error-in-variables regression,"This paper is concerned with high-dimensional error-in-variables regression that aims at identifying a small number of important interpretable factors for corrupted data from many applications where measurement errors or missing data can not be ignored. Motivated by CoCoLasso due to Datta and Zou \cite{Datta16} and the advantage of the zero-norm regularized LS estimator over Lasso for clean data, we propose a calibrated zero-norm regularized LS (CaZnRLS) estimator by constructing a calibrated least squares loss with a positive definite projection of an unbiased surrogate for the covariance matrix of covariates, and use the multi-stage convex relaxation approach to compute the CaZnRLS estimator. Under a restricted eigenvalue condition on the true matrix of covariates, we derive the $\ell_2$-error bound of every iterate and establish the decreasing of the error bound sequence, and the sign consistency of the iterates after finite steps. The statistical guarantees are also provided for the CaZnRLS estimator under two types of measurement errors. Numerical comparisons with CoCoLasso and NCL (the nonconvex Lasso proposed by Poh and Wainwright \cite{Loh11}) demonstrate that CaZnRLS not only has the comparable or even better relative RSME but also has the least number of incorrect predictors identified.",2018,arXiv: Optimization and Control
Lasso Regularization Paths for NARMAX Models via Coordinate Descent,We propose a new algorithm for estimating NARMAX models with $L_{1}$ regularization for models represented as a linear combination of basis functions. Due to the $L_{1}$-norm penalty the Lasso estimation tends to produce some coefficients that are exactly zero and hence gives interpretable models. The novelty of the contribution is the inclusion of error regressors in the Lasso estimation (which yields a nonlinear regression problem). The proposed algorithm uses cyclical coordinate descent to compute the parameters of the NARMAX models for the entire regularization path. It deals with the error terms by updating the regressor matrix along with the parameter vector. In comparative timings we find that the modification does not reduce the computational efficiency of the original algorithm and can provide the most important regressors in very few inexpensive iterations. The method is illustrated for linear and polynomial models by means of two examples.,2018,2018 Annual American Control Conference (ACC)
Development and validation of a radiomics nomogram for identifying invasiveness of pulmonary adenocarcinomas appearing as subcentimeter ground-glass opacity nodules.,"The aim of the present study was to develop and validate a radiomics-based nomogram for differentiation of pre-invasive lesions from invasive lesions that appearing as ground-glass opacity nodules (GGNs) â‰¤10â€‰mm (sub-centimeter) in diameter at CT. A total of 542 consecutive patients with 626 pathologically confirmed pulmonary subcentimeter GGNs were retrospectively studied from October 2011 to September 2017. All the GGNs were divided into a training set (nâ€‰=â€‰334) and a validation set (nâ€‰=â€‰292). Researchers extracted 475 radiomics features from the plain CT images; a radiomics signature was constructed with the least absolute shrinkage and selection operator (LASSO) based on multivariable regression in the training set. Based on the multivariable logistic regression model, a radiomics nomogram was developed in the training set. The performance of the nomogram was evaluated with respect to its calibration, discrimination, and clinical-utility and this was assessed in the validation set. The constructed radiomics signature, which consisted of 15 radiomics features, was significantly associated with the invasiveness of subcentimeter GGNs (Pâ€‰<â€‰0.0001 for both training set and validation set). To build the nomogram model, radiomics signature and mean CT value were used. The nomogram model demonstrated good discrimination and calibration in both training set (C-index, 0.716 [95% CI, 0.632 to 0.801]) and validation set (C-index, 0.707 [95% CI, 0.625 to 0.788]). Decision curve analysis (DCA) indicated that radiomics-based nomogram was clinically useful. A radiomics-based nomogram that incorporates both radiomics signature and mean CT value is constructed in the study, which can be conveniently used to facilitate the preoperative individualized prediction of the invasiveness in patients with subcentimeter GGNs.",2019,European journal of radiology
A non-linear approximation to the distribution of total expenditure distribution of cruise tourists in Uruguay,"Abstract This study contributes to the literature on the determinants of tourism spending on cruises at a microeconomic level, through the application of innovative methodologies framed within the machine learning literature. The objective is to study the distribution of the total expenditure of cruise passengers in Uruguay, using data of the 2016â€“2017 cruise season survey (collected by the Ministry of Tourism of Uruguay). Due to the nature of this variable, we implement a two stages modeling strategy. In the first stage, we model the probability of spending, and in the second, the strictly positive spending. The paper analyze the distribution of conditional expenditure to a set of sociodemographic, travel, contextual and satisfaction variables applying non-linear regression techniques with Lasso penalty and nonparametric techniques such as Random Forest. The empirical results show that the key variables that determine the average spending of cruise tourists are their residence and the port of arrival of the cruise. The analysis of the predictive performance of the models (applied through a training sample and a test sample) shows that Random Forest method has the greater predictive capacity. Finally, the importance variable is analyzed by Random Forest.",2018,Tourism Management
"Joint support recovery under high-dimensional scaling: Benefits and perils of â„“ 1,âˆž -regularization","Given a collection of r â‰¥ 2 linear regression problems in p dimensions, suppose that the regression coefficients share partially common supports. This set-up suggests the use of l1/lâˆž-regularized regression for joint estimation of the p x r matrix of regression coefficients. We analyze the high-dimensional scaling of l1/lâˆž-regularized quadratic programming, considering both consistency rates in lâˆž-norm, and also how the minimal sample size n required for performing variable selection grows as a function of the model dimension, sparsity, and overlap between the supports. We begin by establishing bounds on the lâˆž-error as well sufficient conditions for exact variable selection for fixed design matrices, as well as designs drawn randomly from general Gaussian matrices. These results show that the high-dimensional scaling of l1/lâˆž-regularization is qualitatively similar to that of ordinary l1-regularization. Our second set of results applies to design matrices drawn from standard Gaussian ensembles, for which we provide a sharp set of necessary and sufficient conditions: the l1/lâˆž-regularized method undergoes a phase transition characterized by the rescaled sample size Î¸1,âˆž(n,p, s, Î±) = n/{(4 - 3Î±)s log(p - (2 - Î±) s)}. More precisely, for any Î´ > 0, the probability of successfully recovering both supports converges to 1 for scalings such that Î¸1,âˆž â‰¥ 1 + Î´, and converges to 0 for scalings for which Î¸1,âˆž â‰¤ 1-Î´. An implication of this threshold is that use of l1,âˆž-regularization yields improved statistical efficiency if the overlap parameter is large enough (Î± > 2/3), but performs worse than a naive Lasso-based approach for moderate to small overlap (Î± < 2/3). We illustrate the close agreement between these theoretical predictions, and the actual behavior in simulations.",2008,
Multi-source models for civil unrest forecasting,"Civil unrest events (protests, strikes, and â€œoccupyâ€ events) range from small, nonviolent protests that address specific issues to events that turn into large-scale riots. Detecting and forecasting these events is of key interest to social scientists and policy makers because they can lead to significant societal and cultural changes. We forecast civil unrest events in six countries in Latin America on a daily basis, from November 2012 through August 2014, using multiple data sources that capture social, political and economic contexts within which civil unrest occurs. The models contain predictors extracted from social media sites (Twitter and blogs) and news sources, in addition to volume of requests to Tor, a widely used anonymity network. Two political event databases and country-specific exchange rates are also used. Our forecasting models are evaluated using a Gold Standard Report, which is compiled by an independent group of social scientists and subject matter experts. We use logistic regression models with Lasso to select a sparse feature set from our diverse datasets. The experimental results, measured by F1-scores, are in the range 0.68â€“0.95, and demonstrate the efficacy of using a multi-source approach for predicting civil unrest. Case studies illustrate the insights into unrest events that are obtained with our method. The ablation study demonstrates the relative value of data sources for prediction. We find that social media and news are more informative than other data sources, including the political event databases, and enhance the prediction performance. However, social media increases the variation in the performance metrics.",2016,Social Network Analysis and Mining
A predictive model and socioeconomic and demographic determinants of under-five mortality in Sierra Leone,"Sierra Leone is among the countries that recorded high under-five child mortality rate in the world. To design and implement policies that can address this public health challenge, the present study developed a predictive model of factors that explained under-five mortality in Sierra Leone using the 2008 and 2013 Sierra Leone Demographic and Health Survey (SDHS) datasets. LASSO regression technique was used to select the predictors to build the under-five predictive single-level logit and multilevel logit models. Statistical analyses were performed in the R freeware version 3.6.1. About 588 (10.4%) and 1320 (11.1%) children under five were reported dead in 2008 and 2013, respectively. The significant predictors of under-five mortality in Sierra Leone were the total number of children ever born, number of children under five in the household, mother's birth in the last five years, mother's number of living children, and number of household members, household wealth, maternal contraceptive use and intention, number of eligible women in the household, type of toilet facility, sex of the child, and weight of the child at birth. The study identified certain predictors that deserve policy attention and interventions to strengthen the efforts of creating child welfare and survival atmosphere in Sierra Leone.",2020,Heliyon
Nonparametric Greedy Algorithms for the Sparse Learning Problem,"This paper studies the forward greedy strategy in sparse nonparametric regression. For additive models, we propose an algorithm called additive forward regression; for general multivariate models, we propose an algorithm called generalized forward regression. Both algorithms simultaneously conduct estimation and variable selection in nonparametric settings for the high dimensional sparse learning problem. Our main emphasis is empirical: on both simulated and real data, these two simple greedy methods can clearly outperform several state-of-the-art competitors, including LASSO, a nonparametric version of LASSO called the sparse additive model (SpAM) and a recently proposed adaptive parametric forward-backward algorithm called Foba. We also provide some theoretical justifications of specific versions of the additive forward regression.",2009,
Atrial Fibrillation Burden Signature and Near-Term Prediction of Stroke: A Machine Learning Analysis.,"BACKGROUND
Atrial fibrillation (AF) increases the risk of stroke 5-fold and there is rising interest to determine if AF severity or burden can further risk stratify these patients, particularly for near-term events. Using continuous remote monitoring data from cardiac implantable electronic devices, we sought to evaluate if machine learned signatures of AF burden could provide prognostic information on near-term risk of stroke when compared to conventional risk scores.


METHODS AND RESULTS
We retrospectively identified Veterans Health Administration serviced patients with cardiac implantable electronic device remote monitoring data and at least one day of device-registered AF. The first 30 days of remote monitoring in nonstroke controls were compared against the past 30 days of remote monitoring before stroke in cases. We trained 3 types of models on our data: (1) convolutional neural networks, (2) random forest, and (3) L1 regularized logistic regression (LASSO). We calculated the CHA2DS2-VASc score for each patient and compared its performance against machine learned indices based on AF burden in separate test cohorts. Finally, we investigated the effect of combining our AF burden models with CHA2DS2-VASc. We identified 3114 nonstroke controls and 71 stroke cases, with no significant differences in baseline characteristics. Random forest performed the best in the test data set (area under the curve [AUC]=0.662) and convolutional neural network in the validation dataset (AUC=0.702), whereas CHA2DS2-VASc had an AUC of 0.5 or less in both data sets. Combining CHA2DS2-VASc with random forest and convolutional neural network yielded a validation AUC of 0.696 and test AUC of 0.634, yielding the highest average AUC on nontraining data.


CONCLUSIONS
This proof-of-concept study found that machine learning and ensemble methods that incorporate daily AF burden signature provided incremental prognostic value for risk stratification beyond CHA2DS2-VASc for near-term risk of stroke.",2019,Circulation. Cardiovascular quality and outcomes
Predictive Models of Student Performance for Data-Driven Learning Analytics,"Analytic tools are useful for detecting patterns in education data and providing insights about student performance and learning. This study compared six supervised learning algorithms (linear regression, ridge regression, the lasso, regression trees, random forests regression, gradient boosted regression) and identified features important for predicting student performance. The dataset consisted of N=1044 observations from two secondary schools in Portugal (UCI-MLR, Cortez & Silva, 2008). Performance was assessed by final grades (range: 0-20) in two courses, mathematics and Portugese. The models were fit to training data with 27 independent variables and evaluated on a testing subset. Overall, performance was lower for students in mathematics than Portugese. The models selected a similar set of variables as important for predicting performance: mother's education level, student plans for higher education, and weekly study time were positively related to predicted performance, whereas course subject, school educational support, and romantic relationships were associated with decreased student performance. The models differed in the number, weighting, order and importance given to predictor variables. Linear regression provided a model with 13 predictors. Ridge regression shrank the coefficient estimates toward zero; the lasso performed variables selection for a model with 20 predictors. There was a tradeoff between model complexity and interpretability. The single pruned regression tree provided a simple, interpretable non-linear model with four features. Random forests regression and gradient boosting reduced overfitting, but were more difficult to interpret. Advantages and limitations of the different models are discussed. Applications for educational data mining (EDM) and learning analytics (LA) are considered.",2019,
Multivariate Regression with Small Samples: A Comparison of Estimation Methods,"High dimensional multivariate data, where the number of variables approaches or exceeds the sample size, is an increasingly common occurrence for social scientists. Several tools exist for dealing with such data in the context of univariate regression, including regularization methods (i.e., Lasso, Elastic net, Ridge Regression, as well as Bayesian models with spike and slab priors. These methods have not been widely studied in the context of multivariate regression modeling. Thus, the goal of this simulation study was to compare the performance of these methods for high dimensional data with multivariate regression, in which there exist more than one dependent variable. Simulation results revealed that the regularization methods, particularly Ridge Regression, were found to be particularly effective in terms of parameter estimation accuracy and control over the Type I error rate. Implications for practice are discussed. ocial scientists frequently work in contexts with multiple dependent variables of interest, where appropriate data analysis involves the use of multivariate linear models. In some situations, the number of independent variables (p) may approach, or even exceed the sample size (N), leading to what is commonly referred to as high dimensional data. When used with high dimensional data, standard regression estimators, including those associated with multivariate models, yield unstable coefficient estimates with inflated standard errors (BÃ¼hlmann & van de Geer, 2011), leading to reduced statistical power and erroneous conclusions regarding relationships between independent and dependent variables. Furthermore, when p exceeds N, it is simply not possible to obtain estimates for model parameters using standard estimation methods. The problems associated with high dimensional data in the univariate case could be further amplified when the data are multivariate in nature, given that the number of parameters to be estimated is the number of independent variables +1 multiplied by the number of dependent variables. Although prior research has been done focusing on methods for dealing with high dimensional univariate linear models, relatively little work has been done in the context of multivariate linear models. Therefore, the objective of this simulation study was to compare the performance of several methods for handling high dimensional multivariate data with one another, and with standard ordinary least squares (OLS) multivariate regression. First, a description of OLS regression is provided, followed by descriptions of models designed for use in the high dimensional case, including the lasso, elastic net, and ridge regression. Next, descriptions of two Bayesian alternatives for multivariate regression estimation are provided. The research goals and the methodology used to address those goals are then presented, followed by a discussion of the results of the simulation study, and an application of each method to an existing dataset. Finally, the implications of the simulation results, in light of existing research, are discussed.",2017,
Integrative approach for inference of gene regulatory networks using lasso-based random featuring and application to psychiatric disorders,"BackgroundInferring gene regulatory networks is one of the most interesting research areas in the systems biology. Many inference methods have been developed by using a variety of computational models and approaches. However, there are two issues to solve. First, depending on the structural or computational model of inference method, the results tend to be inconsistent due to innately different advantages and limitations of the methods. Therefore the combination of dissimilar approaches is demanded as an alternative way in order to overcome the limitations of standalone methods through complementary integration. Second, sparse linear regression that is penalized by the regularization parameter (lasso) and bootstrapping-based sparse linear regression methods were suggested in state of the art methods for network inference but they are not effective for a small sample size data and also a true regulator could be missed if the target gene is strongly affected by an indirect regulator with high correlation or another true regulator.ResultsWe present two novel network inference methods based on the integration of three different criteria, (i) z-score to measure the variation of gene expression from knockout data, (ii) mutual information for the dependency between two genes, and (iii) linear regression-based feature selection.Based on these criterion, we propose a lasso-based random feature selection algorithm (LARF) to achieve better performance overcoming the limitations of bootstrapping as mentioned above.ConclusionsIn this work, there are three main contributions. First, our z score-based method to measure gene expression variations from knockout data is more effective than similar criteria of related works. Second, we confirmed that the true regulator selection can be effectively improved by LARF. Lastly, we verified that an integrative approach can clearly outperform a single method when two different methods are effectively jointed. In the experiments, our methods were validated by outperforming the state of the art methods on DREAM challenge data, and then LARF was applied to inferences of gene regulatory network associated with psychiatric disorders.",2015,BMC Medical Genomics
Design and Analysis of Statistical Learning Algorithms which Control False Discoveries,"In this thesis, general theoretical tools are constructed which can be applied to develop ma- chine learning algorithms which are consistent, with fast convergence and which minimize the generalization error by asymptotically controlling the rate of false discoveries (FDR) of features, especially for high dimensional datasets. Even though the main inspiration of this work comes from biological applications, where the data is extremely high dimensional and often hard to obtain, the developed methods are applicable to any general statistical learning problem. 
In this work, the various machine learning tasks like hypothesis testing, classification, regression, etc are formulated as risk minimization algorithms. This allows such learning tasks to be viewed as optimization problems, which can be solved using first order optimization techniques in case of large data scenarios, while one could use faster converging second order techniques for small to moderately sized data sets. Further, such a formulation allows us to estimate the first order convergence rates of an empirical risk estimator for any arbitrary learning problem, using techniques from large deviation theory. 
In many scientific applications, robust discovery of factors affecting an outcome or a phe- notype, is more important than the accuracy of predictions. Hence, it is essential to find an appropriate approach to regularize an under-determined estimation problem and thereby control the generalization error. In this work, the use of local probability of false discovery is explored as such a regularization parameter, which forces the optimized solution towards functions with a lower probability to be a false discovery. Again, techniques from large devi- ation theory and the Gibbs principle allow the derivation of an appropriately regularized cost function. 
These two theoretical results are then used to develop concrete applications. First, the problem of multi-classification is analyzed, which classifies a sample from an arbitrary proba- bility measure into a finite number of categories, based on a given training data set. A general risk functional is derived, which can be used to learn Bayes optimal classifiers controlling the false discovery rate. 
Secondly, the problem of model selection in the regression context is considered, aiming to select a subset of given regressors which explains most of the observed variation i.e. perform ANOVA. Again, using techniques mentioned above, a risk function is derived which when optimized, controls the rate of false discoveries. This technique is shown to outperform the popular LASSO algorithm, which can be proven to not control the FDR, but only the FWER. 
Finally, the problem of inferring under-sampled and partially observed non-negative dis- crete random variables is addressed, which has applications to analyzing RNA sequencing data. By assuming infinite divisibility of the underlying random variable, its characterization as being a discrete Compound Poisson Measure (DCP), is derived. This allows construction of a non-parametric Bayesian model of DCPs with a Pitman-Yor Mixture process prior, which is shown to allow for consistent inference under Kullback-Liebler and Renyi divergences even in the under-sampled regime.",2018,
Spatio-temporal additive regression model selection for urban water demand,"Understanding the factors influencing urban water use is critical for meeting demand and conserving resources. To analyze the relationships between urban household-level water demand and potential drivers, we develop a method for Bayesian variable selection in partially linear additive regression models, particularly suited for high-dimensional spatio-temporally dependent data. Our approach combines a spike-and-slab prior distribution with a modified version of the Bayesian group lasso to simultaneously perform selection of null, linear, and nonlinear models and to penalize regression splines to prevent overfitting. We investigate the effectiveness of the proposed method through a simulation study and provide comparisons with existing methods. We illustrate the methodology on a case study to estimate and quantify uncertainty of the associations between several environmental and demographic predictors and spatio-temporally varying household-level urban water demand in Tampa, FL.",2019,Stochastic Environmental Research and Risk Assessment
Genome-Wide Association Studies and Comparison of Models and Cross-Validation Strategies for Genomic Prediction of Quality Traits in Advanced Winter Wheat Breeding Lines,"The aim of the this study was to identify SNP markers associated with five important wheat quality traits (grain protein content, Zeleny sedimentation, test weight, thousand-kernel weight, and falling number), and to investigate the predictive abilities of GBLUP and Bayesian Power Lasso models for genomic prediction of these traits. In total, 635 winter wheat lines from two breeding cycles in the Danish plant breeding company Nordic Seed A/S were phenotyped for the quality traits and genotyped for 10,802 SNPs. GWAS were performed using single marker regression and Bayesian Power Lasso models. SNPs with large effects on Zeleny sedimentation were found on chromosome 1B, 1D, and 5D. However, GWAS failed to identify single SNPs with significant effects on the other traits, indicating that these traits were controlled by many QTL with small effects. The predictive abilities of the models for genomic prediction were studied using different cross-validation strategies. Leave-One-Out cross-validations resulted in correlations between observed phenotypes corrected for fixed effects and genomic estimated breeding values of 0.50 for grain protein content, 0.66 for thousand-kernel weight, 0.70 for falling number, 0.71 for test weight, and 0.79 for Zeleny sedimentation. Alternative cross-validations showed that the genetic relationship between lines in training and validation sets had a bigger impact on predictive abilities than the number of lines included in the training set. Using Bayesian Power Lasso instead of GBLUP models, gave similar or slightly higher predictive abilities. Genomic prediction based on all SNPs was more effective than prediction based on few associated SNPs.",2018,Frontiers in Plant Science
StratÃ©gies d'analyses multi-marqueurs pour identifier des gÃ¨nes et des interactions gÃ¨ne-gÃ¨ne impliquÃ©s dans le mÃ©lanome cutanÃ©,"Le melanome cutane est un cancer des cellules de la peau (melanocytes) qui se situe, en France, au 11e rang des cancers les plus frequents. Sa mortalite reste elevee lorsquâ€™il est diagnostique a un stade tardif. Ce cancer resulte de nombreux facteurs genetiques, environnementaux et des interactions entre ces facteurs. La susceptibilite genetique a ce cancer recouvre un large spectre de variabilite genetique, depuis des mutations rares conferant un risque eleve jusquâ€™a des variants frequents conferant un risque modeste. Câ€™est dans le cadre de lâ€™identification de variants frequents lies a lâ€™apparition du melanome et a son pronostic que se situe mon travail de these. A ce jour, les etudes dâ€™associations pangenomiques du melanome ont identifie des variants frequents a effets relativement modestes qui expliquent seulement une part de la composante genetique. Les variants fonctionnels au sein des regions identifiees sont le plus souvent inconnus. Les etudes pangenomiques ont eu principalement recours a des analyses simple-marqueur qui peuvent manquer de puissance pour detecter des variants ayant un effet individuel faible ou interagissant avec dâ€™autres variants. Lâ€™objectif principal de ce travail de these a ete de proposer des strategies dâ€™analyse multi-marqueurs pour identifier de nouveaux genes impliques dans le melanome et pour caracteriser des variants potentiellement fonctionnels au sein des regions du genome associees au melanome.Pour identifier de nouveaux genes associes au risque de melanome et a un facteur pronostique de ce cancer (lâ€™indice de Breslow), nous avons propose une strategie dâ€™analyse multi-marqueurs qui integre une analyse de pathways biologiques basee sur la methode GSEA (Gene Set Enrichment Analysis) et une analyse dâ€™interactions entre genes au sein des pathways associes au melanome. Ces analyses ont ete menees dans deux etudes : lâ€™etude francaise MELARISK et lâ€™etude americaine du MD Anderson Cancer Center (MDACC), totalisant 2 980 cas et 3 823 temoins. Nous avons identifie une interaction entre les genes, TERF1 et AFAP1L2, pour le risque de melanome et une interaction entre les genes, CDC42 et SCIN, pour lâ€™indice de Breslow. Ces genes sont particulierement pertinents sur le plan biologique du fait de leur role dans la biologie des telomeres pour la premiere paire de genes et dans la dynamique des filaments dâ€™actine pour la seconde paire. Afin dâ€™identifier les variants potentiellement fonctionnels au sein des regions du genome mises en evidence par etudes pangenomiques, nous avons propose une strategie de cartographie fine qui repose principalement sur une methode de regression penalisee (methode HyperLasso) appliquee a tous les variants de la region etudiee. Par lâ€™analyse de la region 16q24 qui contient le gene MC1R dont les variants fonctionnels sont connus, nous avons montre que cette strategie etait capable dâ€™identifier ces variants parmi de nombreux variants associes au melanome dans cette region. Nous avons contribue a identifier cinq nouvelles regions du genome associees au melanome par meta-analyse dâ€™etudes pangenomiques realisees au niveau mondial (43 000 sujets) puis mene une etude de cartographie fine de toutes les regions associees au melanome, en se basant sur la strategie proposee et validee dans la region 16q24. Les strategies dâ€™analyses multi-marqueurs proposees dans le cadre de ce travail de these ont permis dâ€™identifier de nouveaux genes associes au risque de melanome et a un facteur pronostique de ce cancer et de caracteriser les variants genetiques potentiellement fonctionnels au sein des regions du genome identifiees par etudes pangenomiques.",2015,
