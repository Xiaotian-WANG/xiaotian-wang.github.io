title,abstract,year,journal
Sparsity oracle inequalities for the Lasso,"This paper studies oracle properties of l1-penalized least squares in nonparametric regression setting with random design. We show that the penalized least squares estimator satisfies sparsity oracle inequalities, i.e., bounds in terms of the number of non-zero components of the oracle vec- tor. The results are valid even when the dimension of the model is (much) larger than the sample size and the regression matrix is not positive definite. They can be applied to high-dimensional linear regression, to nonparamet- ric adaptive regression estimation and to the problem of aggregation of arbitrary estimators. AMS 2000 subject classifications: Primary 62G08; secondary 62C20, 62G05, 62G20. Keywords and phrases: sparsity, oracle inequalities, Lasso, penalized least squares, nonparametric regression, dimension reduction, aggregation, mutual coherence, adaptive estimation.",2007,Electronic Journal of Statistics
Predictive Correlation Screening: Application to Two-stage Predictor Design in High Dimension,"We introduce a new approach to variable selection, called Predictive Correlation Screening, for predictor design. Predictive Correlation Screening (PCS) implements false positive control on the selected variables, is well suited to small sample sizes, and is scalable to high dimensions. We establish asymptotic bounds for Familywise Error Rate (FWER), and resultant mean square error of a linear predictor on the selected variables. We apply Predictive Correlation Screening to the following two-stage predictor design problem. An experimenter wants to learn a multivariate predictor of gene expressions based on successive biological samples assayed on mRNA arrays. She assays the whole genome on a few samples and from these assays she selects a small number of variables using Predictive Correlation Screening. To reduce assay cost, she subsequently assays only the selected variables on the remaining samples, to learn the predictor coefficients. We show superiority of Predictive Correlation Screening relative to LASSO and correlation learning (sometimes popularly referred to in the literature as marginal regression or simple thresholding) in terms of performance and computational complexity.",2013,
Shrinkage Estimation in Partially Linear Models with Measurement Error,"In practice, measurement error in the covariates is often encountered. Measurement error has several effects when using ordinary least squares for the regression problems. In this thesis, we introduce the basic idea of correcting the bias caused by different types of measurement error. We then focus on the variable selection for partially linear models when some of the covariates are measured with additive errors. The bias caused by the measurement error is corrected by subtracting a bias correction term in the squared loss function. Adaptive LASSO is used for the variable selection procedure. The rate of convergence and the asymptotic normality of the estimators resulted by the proposed procedure are established. We also proved that, with the correct choice of the rate of the regularization parameter, the proposed procedure asymptotically performs as well as when the true model is known in advance. This is the so-called oracle properties.",2010,
Combined Analysis of Phase I and Phase II Data to Enhance the Power of Pharmacogenetic Tests,"We show through a simulation study how the joint analysis of data from phase I and phase II studies enhances the power of pharmacogenetic tests in pharmacokinetic (PK) studies. PK profiles were simulated under different designs along with 176 genetic markers. The null scenarios assumed no genetic effect, while under the alternative scenarios, drug clearance was associated with six genetic markers randomly sampled in each simulated dataset. We compared penalized regression Lasso and stepwise procedures to detect the associations between empirical Bayes estimates of clearance, estimated by nonlinear mixed effects models, and genetic variants. Combining data from phase I and phase II studies, even if sparse, increases the power to identify the associations between genetics and PK due to the larger sample size. Design optimization brings a further improvement, and we highlight a direct relationship between Î·-shrinkage and loss of genetic signal.",2016,CPT: Pharmacometrics & Systems Pharmacology
On the asymptotic properties of SLOPE,"Sorted L-One Penalized Estimator (SLOPE) is a relatively new convex optimization procedure for selecting predictors in large data bases. Contrary to LASSO, SLOPE has been proved to be asymptotically minimax in the context of sparse high-dimensional generalized linear models. Additionally, in case when the design matrix is orthogonal, SLOPE with the sequence of tuning parameters $\lambda^{BH}$, corresponding to the sequence of decaying thresholds for the Benjamini-Hochberg multiple testing correction, provably controls False Discovery Rate in the multiple regression model. In this article we provide new asymptotic results on the properties of SLOPE when the elements of the design matrix are iid random variables from the Gaussian distribution. Specifically, we provide the conditions, under which the asymptotic FDR of SLOPE based on the sequence $\lambda^{BH}$ converges to zero and the power converges to 1. We illustrate our theoretical asymptotic results with extensive simulation study. We also provide precise formulas describing FDR of SLOPE under different loss functions, which sets the stage for future results on the model selection properties of SLOPE and its extensions.",2019,arXiv: Statistics Theory
Scalable Electronic Phenotyping For Studying Patient Comorbidities,"Over 75 million Americans have multiple concurrent chronic conditions and medical decision making for these patients is mostly based on retrospective cohort studies. Current methods to generate cohorts of patients with comorbidities are neither scalable nor generalizable. We propose a supervised machine learning algorithm for learning comorbidity phenotypes without requiring manually created training sets. First, we generated myocardial infarction (MI) and type-2 diabetes (T2DM) patient cohorts using ICD9-based imperfectly labeled samples upon which LASSO logistic regression models were trained. Second, we assessed the effects of training sample size, inclusion of physician input, and inclusion of clinical text features on model performance. Using ICD9 codes as our labeling heuristic, we achieved comparable performance to models created using keywords as labeling heuristic. We found that expert input and higher training sample sizes could compensate for the lack of clinical text derived features. However, our best performing model included clinical text as features with a large training sample size.",2018,AMIA ... Annual Symposium proceedings. AMIA Symposium
Orientation Coding by Population of Neurons in Rats' Primary Visual Cortex,"Research on Primate visual cortex (V1 area) neurons orientation coding mechanism is the base of revealing the whole visual cortex information processing mechanism. Firstly, this paper adopted different orientation grating to stimulate visually on rats. Meanwhile, gather response signals of population neurons from V1 area using multi-electrode arrays. Then, screen effective response channels according to the orientation selection of different neurons in different channels. Besides, extract Spike average fire rate and LFPÎ³ band power feature in every effective channel signals within specific stimulus response time to construct population response joint features. Finally, taking Lasso regression model as coding model, use joint features to differentiate grating orientation, in order to research on V1 areas population neurons orientation coding. The consequences indicate that the results of population response joint features coding for six different orientation are superior to the results of any single feature of population response coding, and remarkably better than the results of single channel response feature coding.",2013,Applied Mechanics and Materials
Accounting for covariate distributions in slope-unit-based landslide susceptibility models. A case study in the alpine environment,"Abstract Thousands or even million of pixels can be contained in a single Slope Unit. Hence, each covariate used in spatial predictive models is characterized by a distribution of values for each Slope Unit. Here, we model the whole covariates' distribution within Slope Units for landslide susceptibility purposes. This is done by finely dissecting each covariate into quantiles and then modeling the susceptibility via a LASSO penalized Binary Logistic Regression. We choose a LASSO penalization because the common Stepwise procedure is not selective enough to shrink a large number of covariates to an interpretable subset (which we also demonstrate here). LASSO mostly selects 6 covariates out of 372 to explain the spatial distribution of shallow landslides in the Upper Badia valley, Italian Alps. This allows us to verify that the selection does not include any quantile close to the median hence, nor to the mean. The latter is the common representation of the covariates' distribution within Slope Units, which we also test and report in the supplements. Overall, we suggest to always investigate the whole distribution because the mean may not be the most informative nor the most performing way to generate Slope-Unit-based susceptibility models. In this general context, we generate our landslide inventory by combining semi-automated (OBIA) and manual mapping procedures. Our inventory, quantile covariates' representation and LASSO penalization produce excellent performances and interpretable relations between covariates and landslides.",2019,Engineering Geology
Distant Metastasis Risk Definition by Tumor Biomarkers Integrated Nomogram Approach for Locally Advanced Nasopharyngeal Carcinoma,"Identifying metastasis remains a challenge for death control and tailored therapy for nasopharyngeal carcinoma (NPC). Here, we addressed this by designing a nomogram-based Cox proportional regression model through integrating a panel of tumor biomarkers. A total of 147 locally patients with advanced NPC, derived from a randomized phase III clinical trial, were enrolled. We constructed the model by selecting the variables from 31 tumor biomarkers, including 6 pathological signaling pathway molecules and 3 Epstein-Barr virus-related serological variables. Through the least absolute shrinkage and selection operator (LASSO) Cox proportional regression analysis, a nomogram was designed to refine the metastasis risk of each NPC individuals. Using the LASSO Cox regression model, we constructed a 9 biomarkers-based prognostic nomogram: Beclin 1, Aurora-A, Cyclin D1, Ki-67, P27, Bcl-2, MMP-9, 14-3-3Ïƒ, and VCA-IgA. The time-dependence receiver operating characteristic analysis at 1, 3, and 5 years showed an appealing prognostic accuracy with the area under the curve of 0.830, 0.827, and 0.817, respectively. In the validation subset, the concordance index of this nomogram reached to 0.64 to identify the individual metastasis pattern. Supporting by this nomogram algorithm, the individual metastasis risk might be refined personally and potentially guiding the treatment decisions and target therapy against the related signaling pathways for patients with locally advanced NPC.",2019,Cancer Control : Journal of the Moffitt Cancer Center
Convergence analysis of accelerated proximal extra-gradient method with applications,"Abstract Proximal algorithms are popular class of methods for handling sparsity structure in the datasets due to their low iteration costs and faster convergence. In this paper, we consider the framework of the sum of two convex functions, one of which is a smooth function with a Lipschitz gradient, while the other may be a non-smooth function. The usages of such non-smooth functions for identifying complex sparsity-structures in datasets in form of non-smooth regularizers has been an active research direction in the recent past. In this paper, we present the convergence analysis for the extragradient-based fixed-point method with an inertial component, based on which recently a new accelerated proximal extragradient algorithm is designed. In addition, extending the application areas of this algorithm, we applied it to solve (i) the logistic regression problem with complex l1-based penalties, namely, overlapping group lasso and fused lasso frameworks, and (ii) a recently proposed structurally-regularized learning problem for representation selection where the objective function consists of a reconstruction error and structured regularizers as combination of group sparsity regularizer, diversity regularizer, and locality-sensitivity regularizer. With the help of extensive experiments on several publicly available real-world datasets, the efficacy of the inertial-based extragradient methods has been demonstrated for solving the extended lasso and representation selection problems of machine learning.",2020,Neurocomputing
MicroRNA Expression-Based Model Indicates Event-Free Survival in Pediatric Acute Myeloid Leukemia,"Purpose Children with acute myeloid leukemia (AML) whose disease is refractory to standard induction chemotherapy therapy or who experience relapse after initial response have dismal outcomes. We sought to comprehensively profile pediatric AML microRNA (miRNA) samples to identify dysregulated genes and assess the utility of miRNAs for improved outcome prediction. Patients and Methods To identify miRNA biomarkers that are associated with treatment failure, we performed a comprehensive sequence-based characterization of the pediatric AML miRNA landscape. miRNA sequencing was performed on 1,362 samples-1,303 primary, 22 refractory, and 37 relapse samples. One hundred sixty-four matched samples-127 primary and 37 relapse samples-were analyzed by using RNA sequencing. Results By using penalized lasso Cox proportional hazards regression, we identified 36 miRNAs the expression levels at diagnosis of which were highly associated with event-free survival. Combined expression of the 36 miRNAs was used to create a novel miRNA-based risk classification scheme (AMLmiR36). This new miRNA-based risk classifier identifies those patients who are at high risk (hazard ratio, 2.830; P â‰¤ .001) or low risk (hazard ratio, 0.323; P â‰¤ .001) of experiencing treatment failure, independent of conventional karyotype or mutation status. The performance of AMLmiR36 was independently assessed by using 878 patients from two different clinical trials (AAML0531 and AAML1031). Our analysis also revealed that miR-106a-363 was abundantly expressed in relapse and refractory samples, and several candidate targets of miR-106a-5p were involved in oxidative phosphorylation, a process that is suppressed in treatment-resistant leukemic cells. Conclusion To assess the utility of miRNAs for outcome prediction in patients with pediatric AML, we designed and validated a miRNA-based risk classification scheme. We also hypothesized that the abundant expression of miR-106a could increase treatment resistance via modulation of genes that are involved in oxidative phosphorylation.",2017,Journal of Clinical Oncology
ASFMR1 splice variant,"Objective To explore the association of a splice variant of the antisense fragile X mental retardation 1 (ASFMR1) gene, loss of fragile X mental retardation 1 (FMR1) AGG interspersions and FMR1 CGG repeat size with manifestation, and severity of clinical symptoms of fragile X-associated tremor/ataxia syndrome (FXTAS). Methods Premutation carriers (PMCs) with FXTAS, without FXTAS, and normal controls (NCs) had a neurologic evaluation and collection of skin and blood samples. Expression of ASFMR1 transcript/splice variant 2 (ASFMR1-TV2), nonspliced ASFMR1, total ASFMR1, and FMR1 messenger RNA were quantified and compared using analysis of variance. Least absolute shrinkage and selection operator (LASSO) logistic regression and receiver operating characteristic analyses were performed. Results Premutation men and women both with and without FXTAS had higher ASFMR1-TV2 levels compared with NC men and women (n = 135,135, p < 0.0001), and ASFMR1-TV2 had good discriminating power for FXTAS compared with NCs but not for FXTAS from PMC. After adjusting for age, loss of AGG, larger CGG repeat size (in men), and elevated ASFMR1-TV2 level (in women) were strongly associated with FXTAS compared with NC and PMC (combined). Conclusions This study found elevated levels of ASFMR1-TV2 and loss of AGG interruptions in both men and women with FXTAS. Future studies will be needed to determine whether these variables can provide useful diagnostic or predictive information.",2018,Neurology: Genetics
Adaptive Online Monitoring of Voltage Stability Margin via Local Regression,"An online voltage stability margin (VSM) monitoring approach based on local regression and adaptive database is proposed. Considering the increasing variability and uncertainty of power system operation, this approach utilizes the locality of underlying pattern between VSM and reactive power reserve (RPR), and can adapt to the changing condition of system. LASSO is tailored to solve the local regression problem so as to mitigate the curse of dimensionality for large scale system. Along with the VSM prediction, its confidence interval is also estimated simultaneously in a simple but effective way, and utilized as an evidence to trigger the database updating. IEEE 30-bus system and a 60,000-bus large system are used to test and demonstrate the proposed approach. The results show that the proposed approach can be successfully employed in online voltage stability monitoring for real size systems, and the adaptivity of model and data endows the proposed approach with the advantage in the circumstances where large and unforeseen changes of system condition are inevitable.",2018,IEEE Transactions on Power Systems
Discussion,"To these points we would add the following. A plethora of applicable model-selection techniques has been developed since Daniel first introduced the normal and half-normal plots of eâ†µects in the 1950s. Some popular alternatives include stepwise regression, lars-lasso, and the Dantzig selector. See, for example, DraguljÃ­c et al. (2014) for a recent assessment of these techniques for the analysis of interactions. Can these methods contribute eâ†µectively to the analysis of saturated experiments? Providing a general purpose (OK, cookbook) approach to model selection would make the analysis a bit less daunting for the nonstatistician. Given that weâ€™ve chosen to abandon the Daniel plot, what alternatives are suggested? Simple dot plots and/or Pareto plots of estimated eâ†µects oâ†µer advantages, but judgment as to statistical significance is still subjective. Lenthâ€™s (1989) method, with eâ†µects displayed in a Pareto plot with a superimposed reference line and simulated P -values (as in the JMP implementation) is a plausible alternative. But Lenthâ€™s method has limitations as well. The breakdown point occurs whenever more than half of the eâ†µects are active and the method relies on normality, eâ†µect-sparsity, and standard errors. Randomization analysis is an alternative that seems to get little attention, and which oâ†µers the potential for robustness to the level of sparsity and normality. Loughlin and Noble (1997) provided methodology for implementing a randomization analysis for unreplicated two-level factorial and fractional factorial designs.",2015,Journal of Quality Technology
Variable-width confidence intervals in Gaussian regression and penalized maximum likelihood estimators,"Hard thresholding, LASSO , adaptive LASSO and SCAD point estimators have been suggested for use in the linear regression context when most of the components of the regression parameter vector are believed to be zero, a sparsity type of assumption. Potscher and Schneider, 2010, Electronic Journal of Statistics, have considered the properties of fixed-width confidence intervals that include one of these point estimators (for all possible data values). They consider a normal linear regression model with orthogonal regressors and show that these confidence intervals are longer than the standard confidence interval (based on the maximum likelihood estimator) when the tuning parameter for these point estimators is chosen to lead to either conservative or consistent model selection. We extend this analysis to the case of variable-width confidence intervals that include one of these point estimators (for all possible data values). In consonance with these findings of Potscher and Schneider, we find that these confidence intervals perform poorly by comparison with the standard confidence interval, when the tuning parameter for these point estimators is chosen to lead to consistent model selection. However, when the tuning parameter for these point estimators is chosen to lead to conservative model selection, our conclusions differ from those of Potscher and Schneider. We consider the variable-width confidence intervals of Farchione and Kabaila, 2008, Statistics & Probability Letters, which have advantages over the standard confidence interval in the context that there is a belief in a sparsity type of assumption. These variable-width confidence intervals are shown to include the hard thresholding, LASSO, adaptive LASSO and SCAD estimators (for all possible data values) provided that the tuning parameters for these estimators are chosen to belong to an appropriate interval.",2010,arXiv: Methodology
Using Double-Lasso Regression for Principled Variable Selection,"The decision of whether to control for covariates, and how to select which covariates to include, is ubiquitous in psychological research. Failing to control for valid covariates can yield biased parameter estimates in correlational analyses or in imperfectly randomized experiments and contributes to underpowered analyses even in effectively randomized experiments. We introduce double-lasso regression as a principle method for variable selection. The double lasso method is calibrated to not over-select potentially spurious covariates, and simulations demonstrate that using this method reduces error and increases statistical power. This method can be used to identify which covariates have sufficient empirical support for inclusion in analyses of correlations, moderation, mediation and experimental interventions, as well as to test for the effectiveness of randomization. We illustrate both the methodâ€™s usefulness and how to implement it in practice by applying it to four analyses from the prior literature, using both correlational and experimental data.",2016,
A Dual Certificates Analysis of Compressive Off-the-Grid Recovery,"Many problems in machine learning and imaging can be framed as an infinite dimensional Lasso problem to estimate a sparse measure. This includes for instance regression using a continuously parameterized dictionary, mixture model estimation and super-resolution of images. To make the problem tractable, one typically sketches the observations (often called compressive-sensing in imaging) using randomized projections. In this work, we provide a comprehensive treatment of the recovery performances of this class of approaches, proving that (up to log factors) a number of sketches proportional to the sparsity is enough to identify the sought after measure with robustness to noise. We prove both exact support stability (the number of recovered atoms matches that of the measure of interest) and approximate stability (localization of the atoms) by extending two classical proof techniques (minimal norm dual certificate and golfing scheme certificate).",2018,ArXiv
Variable Selection with Prior Information for Generalized Linear Models via the Prior LASSO Method.,"LASSO is a popular statistical tool often used in conjunction with generalized linear models that can simultaneously select variables and estimate parameters. When there are many variables of interest, as in current biological and biomedical studies, the power of LASSO can be limited. Fortunately, so much biological and biomedical data have been collected and they may contain useful information about the importance of certain variables. This paper proposes an extension of LASSO, namely, prior LASSO (pLASSO), to incorporate that prior information into penalized generalized linear models. The goal is achieved by adding in the LASSO criterion function an additional measure of the discrepancy between the prior information and the model. For linear regression, the whole solution path of the pLASSO estimator can be found with a procedure similar to the Least Angle Regression (LARS). Asymptotic theories and simulation results show that pLASSO provides significant improvement over LASSO when the prior information is relatively accurate. When the prior information is less reliable, pLASSO shows great robustness to the misspecification. We illustrate the application of pLASSO using a real data set from a genome-wide association study.",2016,Journal of the American Statistical Association
Subgroup identification in clinical trials: an overview of available methods and their implementations with R.,"Randomized controlled trials (RCTs) usually enroll heterogeneous study population, and thus it is interesting to identify subgroups of patients for whom the treatment may be beneficial or harmful. A variety of methods have been developed to do such kind of post hoc analyses. Conventional generalized linear model is able to include prognostic variables as a main effect and predictive variables in an interaction with treatment variable. A statistically significant and large interaction effect usually indicates potential subgroups that may have different responses to the treatment. However, the conventional regression method requires to specify the interaction term, which requires knowledge of predictive variables or becomes infeasible when there is a large number of feature variables. The Least Absolute Shrinkage and Selection Operator (LASSO) method does variable selection by shrinking less clear effects (including interaction effects) to zero and in this way selects only certain variables and interactions for the model. There are many tree-based methods for subgroup identification. For example, model-based recursive partitioning incorporates parametric models such as generalized linear models into trees. The model incorporated is usually a simple model with only the treatment as covariate. Predictive and prognostic variables are found and incorporated automatically via the tree. The present article gives an overview of these methods and explains how to perform them using the free software environment for statistical computing R (version 3.3.2). A simulated dataset is employed for illustrating the performance of these methods.",2018,Annals of translational medicine
A Highly Efficient Semismooth Newton Augmented Lagrangian Method for Solving Lasso Problems,"We develop a fast and robust algorithm for solving large-scale convex composite optimization models with an emphasis on the $\ell_1$-regularized least squares regression (lasso) problems. Despite the fact that there exist a large number of solvers in the literature for the lasso problems, we found that no solver can efficiently handle difficult large-scale regression problems with real data. By leveraging on available error bound results to realize the asymptotic superlinear convergence property of the augmented Lagrangian algorithm, and by exploiting the second order sparsity of the problem through the semismooth Newton method, we are able to propose an algorithm, called Ssnal, to efficiently solve the aforementioned difficult problems. Under very mild conditions, which hold automatically for lasso problems, both the primal and the dual iteration sequences generated by Ssnal possess a fast linear convergence rate, which can even be superlinear asymptotically. Numerical comparisons between our approach an...",2018,SIAM Journal on Optimization
InfÃ©rence de rÃ©seaux de rÃ©gulation orientÃ©s pour les facteurs de transcription d'Arabidopsis thaliana et crÃ©ation de groupes de co-rÃ©gulation,"Dans cette these, nous cherchons a caracteriser les facteurs de transcription de la plante Arabidopsis thaliana, genes importants pour la regulation de l'expression du genome. A l'aide de donnees d'expression, notre objectif biologique est de classer ces facteurs de transcription en groupes de genes co-regulateurs et en groupes de genes co-regules. Nous procedons en deux phases pour y parvenir. La premiere phase consiste a construire un reseau de regulation entre les facteurs de transcription. La seconde phase consiste en la classification des facteurs de transcription selon les liens de regulation etablis par ce reseau. D'un point de vue statistique, les facteurs de transcription sont les variables et les donnees d'expression sont les observations. Nous representons le reseau a inferer par un graphe oriente dont les nÅ“uds sont les variables. L'estimation de ses aretes est vue comme un probleme de selection de variables en grande dimension avec un faible nombre d'unites statistiques. Nous traitons ce probleme a l'aide de regressions lineaires penalisees de type LASSO. Une approche preliminaire qui consiste a selectionner un ensemble de variables du chemin de regularisation par le biais de criteres de vraisemblance penalisee s'avere etre instable et fournit trop de variables explicatives. Pour contrecarrer cela, nous proposons et mettons en competition deux procedures de selection, adaptees au probleme de la haute dimension et melant regression lineaire penalisee et reechantillonnage. L'estimation des differents parametres de ces procedures a ete effectuee dans le but d'obtenir des ensembles de variables stables. Nous evaluons la stabilite des resultats a l'aide de jeux de donnees simules selon notre modele graphique. Nous faisons appel ensuite a une methode de classification non supervisee sur chacun des graphes orientes obtenus pour former des groupes de nÅ“uds vus comme controleurs et des groupes de nÅ“uds vus comme controles. Pour evaluer la proximite entre les classifications doubles des nÅ“uds obtenus sur differents graphes, nous avons developpe un indice de comparaison de couples de partition dont nous eprouvons et promouvons la pertinence. D'un point de vue pratique, nous proposons une methode de simulation en cascade, exigee par la complexite de notre modele et inspiree du bootstrap parametrique, pour simuler des jeux de donnees en accord avec notre modele. Nous avons valide notre modele en evaluant la proximite des classifications obtenues par application de la procedure statistique sur les donnees reelles et sur ces donnees simulees.",2017,
A System-Wide Approach to Measure Connectivity in the Financial Sector,"We develop and estimate a measure of network connectivity for a sample of very large financial institutions of the U.S. Our model takes a system-wide approach that explicitly recognizes the possibility of connectivity of all the firms in the network. This is in sharp contrast with extant measures of systemic risk that, either explicitly or implicitly, estimate such connections using pair-wise relationships between institutions. We show that such a pair-wise approach may not be able to identify the correct topology of the network, which in turn can result in improper classification of banks as systemically important institutions. Our system-wide approach, based on a recently developed Lasso penalized Vector Auto-Regression (LVAR) model, allows us to detect important systemic events and identify systemically important institutions in a statistically principled manner.",2017,
A network-based variable selection approach for identification of modules and biomarker genes associated with end-stage kidney disease.,"AIMS
Intervention for end-stage kidney disease (ESKD), which is associated with adverse prognoses and major economic burdens, is challenging due to its complex pathogenesis. The study was performed to identify biomarker genes and molecular mechanisms for ESKD by bioinformatics approach.


METHODS
Using the Gene Expression Omnibus (GEO) dataset GSE37171, this study identified pathways and genomic biomarkers associated with ESKD via a multi-stage knowledge discovery process, including identification of modules of genes by weighted gene co-expression network analysis (WGCNA), discovery of important involved pathways by Gene Ontology (GO) and Kyoto Encyclopedia of Genes and Genomes (KEGG) enrichment analyses, selection of differentially expressed genes (DEGs) by the empirical Bayes method, and screening biomarker genes by the least absolute shrinkage and selection operator (Lasso) logistic regression. The results were validated using GSE70528, an independent testing dataset.


RESULTS
Three clinically important gene modules associated with ESKD, were identified by WGCNA. Within these modules, GO and KEGG enrichment analyses revealed important biological pathways involved in ESKD, including TGF-Î² and Wnt signaling, RNA-splicing, autophagy, and chromatin and histone modification. Furthermore, Lasso logistic regression was conducted to identify five final genes, namely, CNOT8, MST4, PPP2CB, PCSK7 and RBBP4, that are differentially expressed and associated with ESKD. The accuracy of the final model in distinguishing the ESKD cases and controls was 96.8% and 91.7% in the training and validation datasets, respectively.


CONCLUSIONS
Network-based variable selection approaches can identify biological pathways and biomarker genes associated with ESKD. The findings may inform more in-depth follow-up research and effective therapy. This article is protected by copyright. All rights reserved.",2019,Nephrology
Pressure Ulcer Risk Factors in Persons with Mobility-Related Disabilities,"ABSTRACT OBJECTIVE To assess pressure ulcer (PU) risk in persons with mobility impairments using a large data set to identify demographic, laboratory, hemodynamic, and pharmacologic risk factors. METHODS The cohort of interest was persons with disabilities who have mobility impairments and are diagnostically at risk of PUs. To define this cohort, diagnoses that qualify patients for skin protection wheelchair cushions were used. Data were obtained from the Cerner Health Facts data warehouse. Two cohorts were defined: persons with and without a history of PUs. Analysis included descriptive statistics and multivariate logistic regression modeling. Variables retained in the model were identified using LASSO, gradient boosting, and Bayesian model averaging. MAIN RESULTS The resulting cohorts included more than 87,000 persons with a history of PUs and more than 1.1 million persons who did not have a PU. The data revealed seven disability groups with the greatest prevalence of PUs: those with Alzheimer disease, cerebral palsy, hemiplegia, multiple sclerosis, paraplegia/quadriplegia, Parkinson disease, and spina bifida. Ulcers in the pelvic region accounted for 82% of PUs. Persons with disabilities who were male or black had a greater prevalence of PUs. Physiologic risk factors included the presence of kidney or renal disease, decreased serum albumin, and increased serum C-reactive protein. CONCLUSIONS The results indicate that, although persons with disabilities can exhibit a wide functional range, they remain at risk of PUs and should be evaluated for proper preventive measures, including support surfaces and wheelchair cushions.",2020,Advances in Skin & Wound Care
Strategies and Software for Machine Learning Accelerated Discovery in Transition Metal Chemistry,"Machine learning the electronic structure of open shell transition metal complexes presents unique challenges, including robust and automated data set generation. Here, we introduce tools that simplify data acquisition from density functional theory (DFT) and validation of trained machine learning models using the molSimplify automatic design (mAD) workflow. We demonstrate this workflow by training and comparing the performance of LASSO, kernel ridge regression (KRR), and artificial neural network (ANN) models using heuristic, topological revised autocorrelation (RAC) descriptors we have recently introduced for machine learning inorganic chemistry. On a series of open shell transition metal complexes, we evaluate set aside test errors of these models for predicting the HOMO level and HOMOâ€“LUMO gap. The best performing models are ANNs, which show 0.15 and 0.25 eV test set mean absolute errors on the HOMO level and HOMOâ€“LUMO gap, respectively. Poor performing KRR models using the full 153-feature RAC set ar...",2018,Industrial & Engineering Chemistry Research
Untersuchung echokardiographischer Parameter zur prÃ¤diktiven EinschÃ¤tzung des Therapieerfolges einer Radiofrequenzablation bei Patienten mit paroxysmalem Vorhofflimmern,"Als kausale Therapie des paroxysmalen Vorhofflimmern wird die Radiofrequenzablation (RF) der Pulmonalvenen durchgefuhrt. Es wurden retrospektiv bei 100 Patienten anamnestische und praprozedurale echokardiographische Parameter untersucht, die eine Abschatzung des Therapieerfolges einer RF uberhaupt ermoglichen, bzw. welches Regim (Lasso/ Pappone) bei definiertem Parameter mit einem Therapieerfolg assoziiert sein kann. Statistisch signifikant fur einen Ablationserfolg waren ohne Berucksichtigung der Methode der RF die Grose des linke Atrium (LA) (p=0.028), bei â€žPapponeâ€œ das LA longitudinal (p=0.014) und LA (p=0.020), bei â€žLassoâ€œ LA (p=0.006). Die Regressionsanalyse ergab bei zunehmendem LA einen negativen Therapieerfolg (p=0.023) lediglich bei â€žPapponeâ€œ.",2011,
LASSO Method for Additive Risk Models with High Dimensional Covariates,"1 Summary. The additive risk model is a useful alternative to the proportional hazards model. It postulates that the hazard function is the sum of the baseline hazard function and the regression function of covariates. In this article, we investigate estimation in the additive risk model with right censored survival data and high dimensional covariates. A LASSO (least absolute shrinkage and selection operator) approach is proposed for estimating the regression parameters. We propose using the L 1 boosting algorithm, which is computationally affordable and relatively easy to implement, to compute the LASSO estimates. The V-fold cross validation is applied to select the tuning parameter and the weighted bootstrap is used to estimate the variances of the LASSO estimators. The proposed approach is illustrated with analysis of the PBC clinical data and the DLBCL genomic data. It is shown that this approach can provide interpretable and sparse predictive models with satisfactory predication and classification properties.",2005,
Nonparametric Dissection of the Cross Section of Expected Stock Returnsâˆ—,"We propose a nonparametric method to test which characteristics provide independent information for the cross section of expected returns. We use the adaptive group LASSO to select characteristics and to estimate how they affect expected returns nonparametrically. Our method can handle a large number of characteristics, allows for a flexible functional form, and is insensitive to outliers. Many of the previously identified return predictors do not provide incremental information for expected returns, and nonlinearities are important. Our proposed method has higher out-of-sample explanatory power compared to linear panel regressions, and increases Sharpe ratios by 50%.",2017,
Feature selection method using WF-LASSO for gene expression data analysis,"There has been a lot of research that demonstrates the phenomenon of life or the origin of the disease, and classifies or diagnoses the state of the cell. These are usually achieved by the strength of the gene expression under certain circumstances using microarrays, which can observe tens and thousands of gene expression profiles. It is not feasible to use all the attributes because of the huge amount of gene expression data that are involved in microarray experiments. It is not feasible to use all the attributes because a lots of gene expression data are involved in microarray experiments. That is, because microarray data have a small number of samples compared to the number of the attributes, in the analyzing of the data there will be overfitting which requires a high cost due to the high dimensionality of the data. We propose a feature selection method using a technique which combines filter method with wavelet transform, and LASSO regression method based on a statistical regression analysis. We obtain the best classification results by applying, in order, the DWT, the filter method, and then finally LASSO. That is, the feature selection method with the best classification performance was WF-LASSO method. The contribution of this paper is in that it is possible to solve problems by reducing the dimensionality of a high volume of data by using the proposed method, so that the performance of the classification can be improved and a more stable classification model can be constructed.",2011,
Generalized Sparse Learning of Linear Models Over the Complete Subgraph Feature Set,"Supervised learning over graphs is an intrinsically difficult problem: simultaneous learning of relevant features from the complete subgraph feature set, in which enumerating all subgraph features occurring in given graphs is practically intractable due to combinatorial explosion. We show that 1) existing graph supervised learning studies, such as Adaboost, LPBoost, and LARS/LASSO, can be viewed as variations of a branch-and-bound algorithm with simple bounds, which we call Morishita-Kudo bounds; 2) We present a direct sparse optimization algorithm for generalized problems with arbitrary twice-differentiable loss functions, to which Morishita-Kudo bounds cannot be directly applied; 3) We experimentally showed that i) our direct optimization method improves the convergence rate and stability, and ii) L1-penalized logistic regression (L1-LogReg) by our method identifies a smaller subgraph set, keeping the competitive performance, iii) the learned subgraphs by L1-LogReg are more size-balanced than competing methods, which are biased to small-sized subgraphs.",2017,IEEE Transactions on Pattern Analysis and Machine Intelligence
Robust sparse regression and tuning parameter selection via the efficient bootstrap information criteria,"There is currently much discussion about lasso-type regularized regression which is a useful tool for simultaneous estimation and variable selection. Although the lasso-type regularization has several advantages in regression modelling, owing to its sparsity, it suffers from outliers because of using penalized least-squares methods. To overcome this issue, we propose a robust lasso-type estimation procedure that uses the robust criteria as the loss function, imposing L1-type penalty called the elastic net. We also introduce to use the efficient bootstrap information criteria for choosing optimal regularization parameters and a constant in outlier detection. Simulation studies and real data analysis are given to examine the efficiency of the proposed robust sparse regression modelling. We observe that our modelling strategy performs well in the presence of outliers.",2014,Journal of Statistical Computation and Simulation
"Longitudinal deformation models, spatial regularizations and learning strategies to quantify Alzheimer's disease progression","In the context of Alzheimer's disease, two challenging issues are (1) the characterization of local hippocampal shape changes specific to disease progression and (2) the identification of mild-cognitive impairment patients likely to convert. In the literature, (1) is usually solved first to detect areas potentially related to the disease. These areas are then considered as an input to solve (2). As an alternative to this sequential strategy, we investigate the use of a classification model using logistic regression to address both issues (1) and (2) simultaneously. The classification of the patients therefore does not require any a priori definition of the most representative hippocampal areas potentially related to the disease, as they are automatically detected. We first quantify deformations of patients' hippocampi between two time points using the large deformations by diffeomorphisms framework and transport these deformations to a common template. Since the deformations are expected to be spatially structured, we perform classification combining logistic loss and spatial regularization techniques, which have not been explored so far in this context, as far as we know. The main contribution of this paper is the comparison of regularization techniques enforcing the coefficient maps to be spatially smooth (Sobolev), piecewise constant (total variation) or sparse (fused LASSO) with standard regularization techniques which do not take into account the spatial structure (LASSO, ridge and ElasticNet). On a dataset of 103 patients out of ADNI, the techniques using spatial regularizations lead to the best classification rates. They also find coherent areas related to the disease progression.",2014,NeuroImage : Clinical
Robust prediction of later asthma in symptomatic toddlers: A novel approach,"Aim: Many children have asthma-like symptoms in early life, but few develop asthma. Several models for predicting later asthma in symptomatic toddlers have been built, but some included factors that are difficult to assess, and methods used were prone to overfitting, leading to selection or exaggeration of irrelevant factors. We aimed to identify predictors for later asthma avoiding previous limitations. Methods: In a population-based cohort, we selected 1-3 year-olds with respiratory symptoms (current wheeze or recurrent cough) and related healthcare visits. Asthma (current wheeze and treatment) was assessed 5 years (N=1226) and 8 years (N=866) later. The included factors are easy to assess in clinical practice: family history, symptoms at baseline, demographic and perinatal data. We used lasso penalized logistic regression to select predictors. This minimizes the number of included predictors while maximizing area under ROC curve (AUC). Results: Main predictors selected in the model for asthma 5 yrs later (AUC=0.76) were â‰¥4 wheezing attacks in the past 12mo (OR=1.65), wheeze causing breathlessness (3.1) and activity disturbance (2.4), eczema (1.5) and male sex (1.5). Other predictors (OR Conclusion: Among factors easy to assess in symptomatic toddlers, wheeze severity, eczema and male sex are main predictors of asthma in mid-childhood. Because our approach for variable selection avoids overfitting, the resulting prediction models should perform well with new data. However, external validation is needed. Funding: SNF PDFMP3-123162.",2011,European Respiratory Journal
Improved subspace clustering via exploitation of spatial constraints,"We present a novel approach to improving subspace clustering by exploiting the spatial constraints. The new method encourages the sparse solution to be consistent with the spatial geometry of the tracked points, by embedding weights into the sparse formulation. By doing so, we are able to correct sparse representations in a principled manner without introducing much additional computational cost. We discuss alternative ways to treat the missing and corrupted data using the latest theory in robust lasso regression and suggest numerical algorithms so solve the proposed formulation. The experiments on the benchmark Johns Hopkins 155 dataset demonstrate that exploiting spatial constraints significantly improves motion segmentation.",2012,2012 IEEE Conference on Computer Vision and Pattern Recognition
"Targeted Methods for Biomarker Discovery, the Search for a Standard","More often than not biomarker studies analyze large quantities of variables with complicated and generally unknown correlation structure. There are numerous statistical methods which attempt to unravel these variables and determine the underlying mechanism through identification of causally related biomarkers. Results from these methods are generally difficult to interpret and nearly impossible to compare across studies. The FDA has currently called for a standardization of methods and protocol for biomarker detection. In response, we propose targeted variable importance (tVIM) as a standardized method for biomarker discovery. Through the use of targeted Maximum Likelihood, tVIM provides double robust estimates of variable importance along with formal inference. These measures are biologically interpretable as a causal effect under specified conditions, allowing for reproducibility across populations. In this analysis we compare tVIM to four different measures of importance provided by three different statistical methods: univariate linear regression (LM), LASSO penalized multiple regression (Q), and two importance measures from randomForest (RF1 and RF2). Their performance is compared in simulation under conditions of increasing correlation. We are interested in their ability to distinguish â€œtrueâ€ relevant biomarkers from correlated decoy biomarkers. The comparisons are based on the resulting ranked variable list for each method using the importance measures and p-values when available. In simulation, tVIM coupled with a data-adaptive model selection method outperforms linear regression, LASSO, and randomForest and is more resilient to increases in correlation. In application we apply all methods to the Golub et al 1999 Leukemia data and compare the resulting gene lists based on biological relevance. Both LM and tVIM are also applied to the vanâ€™t Veer breast cancer data. We compare them based on the top 10 most important genes. From these results, tVIM appears to rank more biologically relevant genes at the top its list than the other methods. Given extreme correlations, methods to reduce bias and provide realistic gene lists are also discussed.",2008,
