title,abstract,year,journal
Bayesian Analysis with Stata,List of figures List of tables Preface Acknowledgments The problem of priors Case study 1: An early phase vaccine trial Bayesian calculations Benefits of a Bayesian analysis Selecting a good prior Starting points Exercises Evaluating the posterior Introduction Case study 1: The vaccine trial revisited Marginal and conditional distributions Case study 2: Blood pressure and age Case study 2: BP and age continued General log posteriors Adding distributions to logdensity Changing parameterization Starting points Exercises Metropolis-Hastings Introduction The MH algorithm in Stata The mhs commands Case study 3: Polyp counts Scaling the proposal distribution The mcmcrun command Multiparameter models Case study 3: Polyp counts continued Highly correlated parameters Case study 3: Polyp counts yet again Starting points Exercises Gibbs sampling Introduction Case study 4: A regression model for pain scores Conjugate priors Gibbs sampling with nonstandard distributions The gbs commands Case study 4 continued: Laplace regression Starting points Exercises Assessing convergence Introduction Detecting early drift Detecting too short a run Running multiple chains Convergence of functions of the parameters Case study 5: Beta-blocker trials Further reading Exercises Validating the Stata code and summarizing the results Introduction Case study 6: Ordinal regression Validating the software Numerical summaries Graphical summaries Further reading Exercises Bayesian analysis with Mata Introduction The basics of Mata Case study 6: Revisited Case study 7: Germination of broomrape Further reading Exercises Using WinBUGS for model fitting Introduction Installing the software Preparing a WinBUGS analysis Case study 8: Growth of sea cows Case study 9: Jawbone size Advanced features of WinBUGS GeoBUGS Programming a series of Bayesian analyses OpenBUGS under Linux Debugging WinBUGS Starting points Exercises Model checking Introduction Bayesian residual analysis The mcmccheck command Case study 10: Models for Salmonella assays Residual checking with Stata Residual checking with Mata Further reading Exercises Model selection Introduction Case study 11: Choosing a genetic model Calculating a BF Calculating the BFs for the NTD case study Robustness of the BF Model averaging Information criteria DIC for the genetic models Starting points Exercises Further case studies Introduction Case study 12: Modeling cancer incidence Case study 13: Creatinine clearance Case study 14: Microarray experiment Case study 15: Recurrent asthma attacks Exercises Writing Stata programs for specific Bayesian analysis Introduction The Bayesian lasso The Gibbs sampler The Mata code A Stata ado-file Testing the code Case study 16: Diabetes data Extensions to the Bayesian lasso program Exercises A Standard distributions References Author index Subject index,2014,
Paradigm free mapping : detection and characterization of single trial fMRI BOLD responses without prior stimulus information,"The increased contrast to noise ratio available at Ultrahigh (7T) Magnetic Resonance Imaging (MRI) allows mapping in space and time the brain's response to single trial events with functional MRI (fMRI) based on the Blood Oxygenation Level Dependent (BOLD) contrast. This thesis primarily concerns with the development of techniques to detect and characterize single trial event-related BOLD responses without prior paradigm information, Paradigm Free Mapping, and assess variations in BOLD sensitivity across brain regions at high field fMRI.

Based on a linear haemodynamic response model, Paradigm Free Mapping (PFM) techniques rely on the deconvolution of the neuronal-related signal driving the BOLD effect using regularized least squares estimators. The first approach, named PFM, builds on the ridge regression estimator and spatio-temporal t-statistics to detect statistically significant changes in the deconvolved fMRI signal. The second method, Sparse PFM, benefits from subset selection features of the LASSO and Dantzig Selector estimators that automatically detect the single trial BOLD responses by promoting a sparse deconvolution of the signal. The third technique, Multicomponent PFM, exploits further the benefits of sparse estimation to decompose the fMRI signal into a haemodynamical component and a baseline component using the morphological component analysis algorithm.

These techniques were evaluated in simulations and experimental fMRI datasets, and the results were compared with well-established fMRI analysis methods. In particular, the methods developed here enabled the detection of single trial BOLD responses to visually-cued and self-paced finger tapping responses without prior information of the events. The potential application of Sparse PFM to identify interictal discharges in idiopathic generalized epilepsy was also investigated. Furthermore, Multicomponent PFM allowed us to extract cardiac and respiratory fluctuations of the signal without the need of physiological monitoring. 

To sum up, this work demonstrates the feasibility to do single trial fMRI analysis without prior stimulus or physiological information using PFM techniques.",2010,
Variable Selection and Coefficient Estimation via Regularized Rank Regression,"The penalized least squares method with some appropriately defined penalty is widely used for simultaneous variable selection and coefficient estimation in linear regression. However, the efficiency of least squares (LS) based methods is adversely affected by outlying observations and heavy tailed distributions. On the other hand, the least absolute deviation (LAD) estimator is more robust, but may be inefficient for many distributions of interest. To overcome these issues, we propose a novel method termed the regularized rank regression (R 3 ) estimator. It is shown that the proposed estimator is highly efficient across a wide spectrum of error distributions. We show further that when the adaptive LASSO penalty is used, the estimator can be made consistent in variable selection. We propose using a score statistic-based information criterion for choosing the tuning parameters, which bypasses density estimation. Simulations and data analysis both show that the proposed method performs well in finite sample cases.",2010,
Discovery and Replication of Gene Influences on Brain Structure Using LASSO Regression,"We implemented least absolute shrinkage and selection operator (LASSO) regression to evaluate gene effects in genome-wide association studies (GWAS) of brain images, using an MRI-derived temporal lobe volume measure from 729 subjects scanned as part of the Alzheimer's Disease Neuroimaging Initiative (ADNI). Sparse groups of SNPs in individual genes were selected by LASSO, which identifies efficient sets of variants influencing the data. These SNPs were considered jointly when assessing their association with neuroimaging measures. We discovered 22 genes that passed genome-wide significance for influencing temporal lobe volume. This was a substantially greater number of significant genes compared to those found with standard, univariate GWAS. These top genes are all expressed in the brain and include genes previously related to brain function or neuropsychiatric disorders such as MACROD2, SORCS2, GRIN2B, MAGI2, NPAS3, CLSTN2, GABRG3, NRXN3, PRKAG2, GAS7, RBFOX1, ADARB2, CHD4, and CDH13. The top genes we identified with this method also displayed significant and widespread post hoc effects on voxelwise, tensor-based morphometry (TBM) maps of the temporal lobes. The most significantly associated gene was an autism susceptibility gene known as MACROD2. We were able to successfully replicate the effect of the MACROD2 gene in an independent cohort of 564 young, Australian healthy adult twins and siblings scanned with MRI (mean age: 23.8â€‰Â±â€‰2.2 SD years). Our approach powerfully complements univariate techniques in detecting influences of genes on the living brain.",2012,Frontiers in Neuroscience
Genomic Prediction of Growth and Stem Quality Traits in Eucalyptus globulus Labill. at Its Southernmost Distribution Limit in Chile,"The present study was undertaken to examine the ability of different genomic selection (GS) models to predict growth traits (diameter at breast height, tree height and wood volume), stem straightness and branching quality of Eucalyptus globulus Labill. trees using a genome-wide Single Nucleotide Polymorphism (SNP) chip (60 K), in one of the southernmost progeny trials of the species, close to its southern distribution limit in Chile. The GS methods examined were Ridge Regression-BLUP (RRBLUP), Bayes-A, Bayes-B, Bayesian least absolute shrinkage and selection operator (BLASSO), principal component regression (PCR), supervised PCR and a variant of the RRBLUP method that involves the previous selection of predictor variables (RRBLUP-B). RRBLUP-B and supervised PCR models presented the greatest predictive ability (PA), followed by the PCR method, for most of the traits studied. The highest PA was obtained for the branching quality (~0.7). For the growth traits, the maximum values of PA varied from 0.43 to 0.54, while for stem straightness, the maximum value of PA reached 0.62 (supervised PCR). The study population presented a more extended linkage disequilibrium (LD) than other populations of E. globulus previously studied. The genome-wide LD decayed rapidly within 0.76 Mbp (threshold value of r2 = 0.1). The average LD on all chromosomes was r2 = 0.09. In addition, the 0.15% of total pairs of linked SNPs were in a complete LD (r2 = 1), and the 3% had an r2 value >0.5. Genomic prediction, which is based on the reduction in dimensionality and variable selection may be a promising method, considering the early growth of the trees and the low-to-moderate values of heritability found in the traits evaluated. These findings provide new understanding of how develop novel breeding strategies for tree improvement of E. globulus at its southernmost range limit in Chile, which could represent new opportunities for forest planting that can benefit the local economy.",2018,Forests
Risk factors for preterm delivery in Burkina Faso (west Africa).,"The environmental and socioeconomic risk factors for preterm delivery were assessed in a West African urban population (Bobo-Dioulasso, Burkina Faso). The study population were 102 cases of preterm delivery matched with 102 controls obtained from 4124 sequential deliveries which occurred between May and October 1989 in the three maternity centres in the city. The univariate analysis identified the risk factors as age (< 20 years), primiparity, marital status (single), low frequency of antenatal visits, death of a previous child and level of education of the mother. The following risk factors identified by multivariate analysis (logistic regression) are consistent with those identified in previous studies: youth of the mother, primiparity (P = 0.01) and death of a previous child (P < 0.05). On the other hand, in this study, the level of education of the parent was identified as an independent risk factor (P < 0.001). This finding could be used to determine a target population for prevention programmes.",1993,International journal of epidemiology
Exploring Machine Learning to Correct Satellite-Derived Sea Surface Temperatures,"Machine learning techniques are attractive tools to establish statistical models with a high degree of non linearity. They require a large amount of data to be trained and are therefore particularly suited to analysing remote sensing data. This work is an attempt at using advanced statistical methods of machine learning to predict the bias between Sea Surface Temperature (SST) derived from infrared remote sensing and ground â€œtruthâ€ from drifting buoy measurements. A large dataset of collocation between satellite SST and in situ SST is explored. Four regression models are used: Simple multi-linear regression, Least Square Shrinkage and Selection Operator (LASSO), Generalised Additive Model (GAM) and random forest. In the case of geostationary satellites for which a large number of collocations is available, results show that the random forest model is the best model to predict the systematic errors and it is computationally fast, making it a good candidate for operational processing. It is able to explain nearly 31% of the total variance of the bias (in comparison to about 24% for the multi-linear regression model).",2018,Remote Sensing
Heterogeneous Endogenous Effects in Networks,"This paper proposes a new method to identify leaders and followers in a network. Prior works use spatial autoregression models (SARs) which implicitly assume that each individual in the network has the same peer effects on others. Mechanically, they conclude the key player in the network to be the one with the highest centrality. However, when some individuals are more influential than others, centrality may fail to be a good measure. I develop a model that allows for individual-specific endogenous effects and propose a two-stage LASSO procedure to identify influential individuals in a network. Under an assumption of sparsity: only a subset of individuals (which can increase with sample size n) is influential, I show that my 2SLSS estimator for individual-specific endogenous effects is consistent and achieves asymptotic normality. I also develop robust inference including uniformly valid confidence intervals. These results also carry through to scenarios where the influential individuals are not sparse. I extend the analysis to allow for multiple types of connections (multiple networks), and I show how to use the sparse group LASSO to detect which of the multiple connection types is more influential. Simulation evidence shows that my estimator has good finite sample performance. I further apply my method to the data in Banerjee et al. (2013) and my proposed procedure is able to identify leaders and effective networks.",2019,arXiv: Econometrics
"Statistics for Microarrays: Design, Analysis and Inference","Preface.1 Preliminaries.1.1 Using the R Computing Environment.1.1.1 Installing smida.1.1.2 Loading smida.1.2 Data Sets from Biological Experiments.1.2.1 Arabidopsis experiment: Anna Amtmann.1.2.2 Skin cancer experiment: Nighean Barr.1.2.3 Breast cancer experiment: John Bartlett.1.2.4 Mammary gland experiment: Gusterson group.1.2.5 Tuberculosis experiment: B G@S group.I Getting Good Data.2 Set-up of a Microarray Experiment.2.1 Nucleic Acids: DNA and RNA.2.2 Simple cDNA Spotted Microarray Experiment.2.2.1 Growing experimental material.2.2.2 Obtaining RNA.2.2.3 Adding spiking RNA and poly-T primer.2.2.4 Preparing the enzyme environment.2.2.5 Obtaining labelled cDNA.2.2.6 Preparing cDNA mixture for hybridization.2.2.7 Slide hybridization.3 Statistical Design of Microarrays.3.1 Sources of Variation.3.2 Replication.3.2.1 Biological and technical replication.3.2.2 How many replicates?3.2.3 Pooling samples.3.3 Design Principles.3.3.1 Blocking, crossing and randomization.3.3.2 Design and normalization.3.4 Single-channelMicroarray Design.3.4.1 Design issues.3.4.2 Design layout.3.4.3 Dealing with technical replicates.3.5 Two-channelMicroarray Designs.3.5.1 Optimal design of dual-channel arrays.3.5.2 Several practical two-channel designs.4 Normalization.4.1 Image Analysis.4.1.1 Filtering.4.1.2 Gridding.4.1.3 Segmentation.4.1.4 Quantification.4.2 Introduction to Normalization.4.2.1 Scale of gene expression data.4.2.2 Using control spots for normalization.4.2.3 Missing data.4.3 Normalization for Dual-channel Arrays.4.3.1 Order for the normalizations.4.3.2 Spatial correction.4.3.3 Background correction.4.3.4 Dye effect normalization.4.3.5 Normalization within and across conditions.4.4 Normalization of Single-channel Arrays.4.4.1 Affymetrix data structure.4.4.2 Normalization of Affymetrix data.5 Quality Assessment.5.1 Using MIAME in Quality Assessment.5.1.1 Components of MIAME.5.2 Comparing Multivariate Data.5.2.1 Measurement scale.5.2.2 Dissimilarity and distance measures.5.2.3 Representing multivariate data.5.3 Detecting Data Problems.5.3.1 Clerical errors.5.3.2 Normalization problems.5.3.3 Hybridization problems.5.3.4 Array mishandling.5.4 Consequences of Quality Assessment Checks.6 Microarray Myths: Data.6.1 Design.6.1.1 Single-versus dual-channel designs?6.1.2 Dye-swap experiments.6.2 Normalization.6.2.1 Myth: 'microarray data is Gaussian'.6.2.2 Myth: 'microarray data is not Gaussian'.6.2.3 Confounding spatial and dye effect.6.2.4 Myth: 'non-negative background subtraction'.II Getting Good Answers.7 Microarray Discoveries.7.1 Discovering Sample Classes.7.1.1 Why cluster samples?7.1.2 Sample dissimilarity measures.7.1.3 Clustering methods for samples.7.2 Exploratory Supervised Learning.7.2.1 Labelled dendrograms.7.2.2 Labelled PAM-type clusterings.7.3 Discovering Gene Clusters.7.3.1 Similarity measures for expression profiles.7.3.2 Gene clustering methods.8 Differential Expression.8.1 Introduction.8.1.1 Classical versus Bayesian hypothesis testing.8.1.2 Multiple testing 'problem'.8.2 Classical Hypothesis Testing.8.2.1 What is a hypothesis test?8.2.2 Hypothesis tests for two conditions.8.2.3 Decision rules.8.2.4 Results from skin cancer experiment.8.3 Bayesian Hypothesis Testing.8.3.1 A general testing procedure.8.3.2 Bayesian t-test.9 Predicting Outcomes with Gene Expression Profiles.9.1 Introduction.9.1.1 Probabilistic classification theory.9.1.2 Modelling and predicting continuous variables.9.2 Curse of Dimensionality: Gene Filtering.9.2.1 Use only significantly expressed genes.9.2.2 PCA and gene clustering.9.2.3 Penalized methods.9.2.4 Biological selection.9.3 Predicting ClassMemberships.9.3.1 Variance-bias trade-off in prediction.9.3.2 Linear discriminant analysis.9.3.3 k-nearest neighbour classification.9.4 Predicting Continuous Responses.9.4.1 Penalized regression: LASSO.9.4.2 k-nearest neighbour regression.10 Microarray Myths: Inference.10.1 Differential Expression.10.1.1 Myth: 'Bonferroni is too conservative'.10.1.2 FPR and collective multiple testing.10.1.3 Misinterpreting FDR.10.2 Prediction and Learning.10.2.1 Cross-validation.Bibliography.Index.",2004,
Predicting individual differences in placebo analgesia: contributions of brain activity during anticipation and pain experience.,"Recent studies have identified brain correlates of placebo analgesia, but none have assessed how accurately patterns of brain activity can predict individual differences in placebo responses. We reanalyzed data from two fMRI studies of placebo analgesia (N = 47), using patterns of fMRI activity during the anticipation and experience of pain to predict new subjects' scores on placebo analgesia and placebo-induced changes in pain processing. We used a cross-validated regression procedure, LASSO-PCR, which provided both unbiased estimates of predictive accuracy and interpretable maps of which regions are most important for prediction. Increased anticipatory activity in a frontoparietal network and decreases in a posterior insular/temporal network predicted placebo analgesia. Patterns of anticipatory activity across the cortex predicted a moderate amount of variance in the placebo response (âˆ¼12% overall, âˆ¼40% for study 2 alone), which is substantial considering the multiple likely contributing factors. The most predictive regions were those associated with emotional appraisal, rather than cognitive control or pain processing. During pain, decreases in limbic and paralimbic regions most strongly predicted placebo analgesia. Responses within canonical pain-processing regions explained significant variance in placebo analgesia, but the pattern of effects was inconsistent with widespread decreases in nociceptive processing. Together, the findings suggest that engagement of emotional appraisal circuits drives individual variation in placebo analgesia, rather than early suppression of nociceptive processing. This approach provides a framework that will allow prediction accuracy to increase as new studies provide more precise information for future predictive models.",2011,The Journal of neuroscience : the official journal of the Society for Neuroscience
Development of a radiomics nomogram based on the 2D and 3D CT features to predict the survival of non-small cell lung cancer patients,"ObjectivesThe aim of this study was to develop a radiomics nomogram by combining the optimized radiomics signatures extracted from 2D and/or 3D CT images and clinical predictors to assess the overall survival of patients with non-small cell lung cancer (NSCLC).MethodsOne training cohort of 239 and two validation datasets of 80 and 52 NSCLC patients were enrolled in this study. Nine hundred seventy-five radiomics features were extracted from each patientâ€™s 2D and 3D CT images. Least absolute shrinkage and selection operator (LASSO) regression was used to select features and generate a radiomics signature. Cox hazard survival analysis and Kaplan-Meier were performed in both cohorts. The radiomics nomogram was developed by integrating the optimized radiomics signature and clinical predictors, its calibration and discrimination were evaluated.ResultsThe radiomics signatures were significantly associated with NSCLC patientsâ€™ survival time. The signature derived from the combined 2D and 3D features showed a better prognostic performance than those from 2D or 3D alone. Our radiomics nomogram integrated the optimal radiomics signature with clinical predictors showed a significant improvement in the prediction of patientsâ€™ survival compared with clinical predictors alone in the validation cohort. The calibration curve showed predicted survival time was very close to the actual one.ConclusionsThe radiomics signature from the combined 2D and 3D features further improved the predicted accuracy of survival prognosis for the patients with NSCLC. Combination of the optimal radiomics signature and clinical predictors performed better for individualied survival prognosis estimation in patients with NSCLC. These findings might affect trearment strategies and enable a step forward for precise medicine.Key Pointsâ€¢ We found both 2D and 3D radiomics signature have favorable prognosis, but 3D signature had a better performance.â€¢ The radiomics signature generated from the combined 2D and 3D features had a better predictive performance than those from 2D or 3D features.â€¢ Integrating the optimal radiomics signature with clinical predictors significantly improved the predictive power in patientsâ€™ survival compared with clinical TNM staging alone.",2018,European Radiology
Model-based clustering for high-dimension data. Application to functional data.,"Finite mixture regression models are useful for modeling the relationship between response and predictors, arising from different subpopulations. In this article, we study high-dimensional predic- tors and high-dimensional response, and propose two procedures to deal with this issue. We propose to use the Lasso estimator to take into account the sparsity, and a penalty on the rank, to take into account the matrix structure. Then, we extend these procedures to the functional case, where predictors and responses are functions. For this purpose, we use a wavelet-based approach. Finally, for each situation, we provide algorithms, and apply and evaluate our methods both on simulations and real datasets.",2014,
Log-Contrast Regression with Functional Compositional Predictors: Linking Preterm Infant's Gut Microbiome Trajectories in Early Postnatal Period to Neurobehavioral Outcome,"When compositional data serve as predictors in regression, the log-contrast model is commonly applied. A prominent feature of the model is that it complies with the simplex geometry and enables the regression analysis to have various desirable invariance properties. Motivated by the needs in understanding how the trajectories of gut microbiome compositions during early postnatal stage impact later neurobehavioral outcomes among preterm infants, we develop a sparse log-contrast regression with functional compositional predictors. The functional simplex structure is preserved by a set of zero-sum constraints on the parameters, and the compositional predictors are allowed to have sparse, smoothly varying, and accumulating effects on the outcome through time. Through basis expansion, the problem boils down to a linearly constrained group lasso regression, for which we develop an efficient augmented Lagrangian algorithm and obtain theoretical performance guarantees. The proposed approach yields interesting results in the preterm infant study. The identified microbiome markers and the estimated time dynamics of their impact on the neurobehavioral outcome shed lights on the functional linkage between stress accumulation in early postnatal stage and neurodevelpomental process of infants.",2018,arXiv: Methodology
Joint Estimation of Multiple Conditional Gaussian Graphical Models,"In this paper, we propose a joint conditional graphical Lasso to learn multiple conditional Gaussian graphical models, also known as Gaussian conditional random fields, with some similar structures. Our model builds on the maximum likelihood method with the convex sparse group Lasso penalty. Moreover, our model is able to model multiple multivariate linear regressions with unknown noise covariances via a convex formulation. In addition, we develop an efficient approximated Newtonâ€™s method for optimizing our model. Theoretically, we establish the asymptotic properties of our model on consistency and sparsistency under the high-dimensional settings. Finally, extensive numerical results on simulations and real data sets demonstrate that our method outperforms the compared methods on structure recovery and structured output prediction. To the best of our knowledge, the joint learning of multiple multivariate regressions with unknown covariance is first studied.",2018,IEEE Transactions on Neural Networks and Learning Systems
Adaptive posterior mode estimation of a sparse sequence for model selection,"For the problem of estimating a sparse sequence of coefficients of a parametric or non-parametric generalized linear model, posterior mode estimation with a Subbotin(""Î»"",""Î½"") prior achieves thresholding and therefore model selection when ""Î½"" is an element of [0,1] for a class of likelihood functions. The proposed estimator also offers a continuum between the (forward/backward) best subset estimator (""Î½"" e 0 ), its approximate convexification called lasso (""Î½"" e 1 ) and ridge regression (""Î½"" e 2 ). Rather than fixing ""Î½"", selecting the two hyperparameters ""Î»"" and ""Î½"" adds flexibility for a better fit, provided both are well selected from the data. Considering first the canonical Gaussian model, we generalize the Stein unbiased risk estimate, SURE(""Î»"",""Î½""), to the situation where the thresholding function is not almost differentiable (i.e. ""Î½"" l 1 ). We then propose a more general selection of ""Î»"" and ""Î½"" by deriving an information criterion that can be employed for instance for the lasso or wavelet smoothing. We investigate some asymptotic properties in parametric and non-parametric settings. Simulations and applications to real data show excellent performance. Copyright (c) 2009 Board of the Foundation of the Scandinavian Journal of Statistics.",2009,Scandinavian Journal of Statistics
Combining micro-RNA and protein sequencing to detect robust biomarkers for Gravesâ€™ disease and orbitopathy,"Gravesâ€™ Disease (GD) is an autoimmune condition in which thyroid-stimulating antibodies (TRAB) mimic thyroid-stimulating hormone function causing hyperthyroidism. 5% of GD patients develop inflammatory Gravesâ€™ orbitopathy (GO) characterized by proptosis and attendant sight problems. A major challenge is to identify which GD patients are most likely to develop GO and has relied on TRAB measurement. We screened sera/plasma from 14 GD, 19 GO and 13 healthy controls using high-throughput proteomics and miRNA sequencing (Illuminaâ€™s HiSeq2000 and Agilent-6550 Funnel quadrupole-time-of-flight mass spectrometry) to identify potential biomarkers for diagnosis or prognosis evaluation. Euclidean distances and differential expression (DE) based on miRNA and protein quantification were analysed by multidimensional scaling (MDS) and multinomial regression respectively. We detected 3025 miRNAs and 1886 proteins and MDS revealed good separation of the 3 groups. Biomarkers were identified by combined DE and Lasso-penalized predictive models; accuracy of predictions was 0.86 (Â±0:18), and 5 miRNA and 20 proteins were found including Zonulin, Alpha-2 macroglobulin, Beta-2 glycoprotein 1 and Fibronectin. Functional analysis identified relevant metabolic pathways, including hippo signaling, bacterial invasion of epithelial cells and mRNA surveillance. Proteomic and miRNA analyses, combined with robust bioinformatics, identified circulating biomarkers applicable to diagnose GD, predict GO disease status and optimize patient management.",2018,Scientific Reports
Extension de la rÃ©gression linÃ©aire gÃ©nÃ©ralisÃ©e sur composantes supervisÃ©es (SCGLR) aux donnÃ©es groupÃ©es,"Nous proposons de construire des composantes permettant de regulariser un Modele Lineaire Generalise Mixte (GL2M) multivarie. Un ensemble de reponses aleatoires Y est modelise par un GL2M, au moyen d'un ensemble X de variables explicatives, et d'un ensemble T de variables additionnelles. Les variables explicatives dans X sont supposees nombreuses et redondantes : il est donc necessaire de regulariser la regression lineaire generalisee mixte. A l'inverse, les variables de T sont supposees peu nombreuses et selectionnees de sorte a n'exiger aucune regularisation. La regularisation consiste ici a construire un nombre approprie de composantes orthogonales permettant tout a la fois une bonne modelisation de Y et l'extraction d'informations structurelles dans X. Pour cela, nous proposons d'inserer a chaque etape de l'algorithme de Schall permettant l'estimation d'un GL2M, l'optimisation d'un critere propre a SCGLR. Cette extension de la methode SCGLR est testee et comparee a d'autres methodes de regularisation de type Ridge et Lasso, sur donnees simulees et reelles.",2016,
Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers,"Many problems of recent interest in statistics and machine learning can be posed in the framework of convex optimization. Due to the explosion in size and complexity of modern datasets, it is increasingly important to be able to solve problems with a very large number of features or training examples. As a result, both the decentralized collection or storage of these datasets as well as accompanying distributed solution methods are either necessary or at least highly desirable. In this review, we argue that the alternating direction method of multipliers is well suited to distributed convex optimization, and in particular to large-scale problems arising in statistics, machine learning, and related areas. The method was developed in the 1970s, with roots in the 1950s, and is equivalent or closely related to many other algorithms, such as dual decomposition, the method of multipliers, Douglasâ€“Rachford splitting, Spingarn's method of partial inverses, Dykstra's alternating projections, Bregman iterative algorithms for l1 problems, proximal methods, and others. After briefly surveying the theory and history of the algorithm, we discuss applications to a wide variety of statistical and machine learning problems of recent interest, including the lasso, sparse logistic regression, basis pursuit, covariance selection, support vector machines, and many others. We also discuss general distributed optimization, extensions to the nonconvex setting, and efficient implementation, including some details on distributed MPI and Hadoop MapReduce implementations.",2011,Foundations and Trends in Machine Learning
Gamma distribution linear modeling with statistical downscaling to predict extreme monthly rainfall in Indramayu,"Rainfall is an important factor in the agricultural process. Several methods to predict the rainfall have been carried out in Indonesia, such as the modeling of Statistical Downscaling (SDS). SDS models might involve ill-conditioned covariates (large dimension and high correlation/multi collinear). This problem could be solved by a variable selection technique such as L1 regularization/LASSO or a dimension reduction approach such as principal component analysis (PCA). In this paper, both methods were applied to generalized linear modeling with gamma distribution and compared in order to predict extreme monthly rainfall at 11 rain posts in Indramayu. Simulations were conducted to compare L1 regularization technique and principal component analysis in the prediction of responses. Two scenarios were based on the coefficient of beta and the distribution of response scenarios. The covariates used in this study were in observational data of GPCP version 2.2. The coefficient of beta scenarios were the combination of beta less than 1, equal 0, and greater than 1 vs all betas less than 1. Gamma distributions were used for distribution of response scenario with three different shape parameters. The simulation showed that L1 regularization technique resulted in almost better prediction than principal component analysis as the shape parameter was larger. The Root Mean Square Error (RMSE) of generalized linear model with Gamma distribution was less than that of principal component regression. However, all generalized linear models with Gamma distribution gave the smaller RMSE values for extreme value prediction above outliers. In this case, the quantiles, Q(0.90) and Q(0.95), were better prediction of extreme monthly rainfall.",2016,"2016 12th International Conference on Mathematics, Statistics, and Their Applications (ICMSA)"
Minimax Semiparametric Learning With Approximate Sparsity,"Many objects of interest can be expressed as a linear, mean square continuous functional of a least squares projection (regression). Often the regression may be high dimensional, depending on many variables. This paper gives minimal conditions for root-n consistent and efficient estimation of such objects when the regression and the Riesz representer of the functional are approximately sparse and the sum of the absolute value of the coefficients is bounded. The approximately sparse functions we consider are those where an approximation by some $t$ regressors has root mean square error less than or equal to $Ct^{-\xi}$ for $C,$ $\xi>0.$ We show that a necessary condition for efficient estimation is that the sparse approximation rate $\xi_{1}$ for the regression and the rate $\xi_{2}$ for the Riesz representer satisfy $\max\{\xi_{1} ,\xi_{2}\}>1/2.$ This condition is stronger than the corresponding condition $\xi_{1}+\xi_{2}>1/2$ for Holder classes of functions. We also show that Lasso based, cross-fit, debiased machine learning estimators are asymptotically efficient under these conditions. In addition we show efficiency of an estimator without cross-fitting when the functional depends on the regressors and the regression sparse approximation rate satisfies $\xi_{1}>1/2$.",2019,arXiv: Statistics Theory
Sparse Networks Through Regularised Regressions,"We propose a Bayesian approach to the problem of variable selection and shrinkage in high dimensional sparse regression models where the regularisation method is an extension of a previous LASSO. The model allows us to include a large number of institutions which improves the identification of the relationship and maintains at the same time the flexibility of the univariate framework. Furthermore, we obtain a weighted directed network since the adjacency matrix is built â€œrow by rowâ€ using for each institutions the posterior inclusion probabilities of the other institutions in the system.",2018,
Bootstrap group penalty for highdimensional regression models,The paper presents a new penalization procedure for variable selection in regression models.We propose the Bootstrap Group Penalty (BGP) that extends the bootstrap version of the LASSO method by taking into account the grouping structure which may be present or introduced in a model. Based on a simulation study we demonstrate that the new procedure outperforms some existing group penalization methods in terms of both prediction accuracy and variable selection quality.,2017,
