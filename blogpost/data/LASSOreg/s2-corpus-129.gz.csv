title,abstract,year,journal
A Network-Based Approach to Enhance Electricity Load Forecasting,"In the field of energy analysis, time series forecasting techniques are widely used to predict customer electricity consumptions. To enhance the electricity forecasting accuracy, in current approaches, clustering techniques are first applied to identify groups of customers exhibiting the same electricity load profile, from which a representative consumption pattern can be extracted. This pattern is later used to predict customers' subsequent electricity consumption. In the vast majority of clustering approaches, authors use the entire data set as input to identify customer consumption groups. However, electricity load data vary extremely rapidly and can thus be dominated by outdated historical information which may influence the effective cluster status at a given time-stamp. To overcome this constraint, instead of using the entire data set, we propose an adaptive process which involves tracking the evolution of identified customer consumption groups at different time-stamps. A network structure is used to model the interrelation between customer electricity load profiles. The network is then split into subnetworks that are treated as customer electricity consumption clusters. Representative subseries, called master subseries, are extracted to track the evolution of clusters over time. Finally, the master subseries are used as a knowledge base for forecasting customers' electricity consumption at later time-stamps and automatically predicting future cluster status. The load forecasting is done using a seasonal autoregressive integrated moving average model, which is compared to a multi-layer perceptron, support vector regression, lasso regression, bayesian ridge regression and K-nearest neighbor regression models.",2018,2018 IEEE International Conference on Data Mining Workshops (ICDMW)
Appearance-Based Outdoor Localization Using Group LASSO Regression,"This paper presents appearance-based localization for an omni-directional camera that builds on a combination of the group Least Absolute Shrinkage and Selection Operator (LASSO) and the extended Kalman filter (EKF). A histogram that represents the population of the Speeded-Up Robust Features (SURF points) is computed for each image, the features of which are selected via the group LASSO regression. The EKF takes the output of the LASSO regression-based first localization as observations for the final localization. The experimental results demonstrate the effectiveness of our approach.Copyright Â© 2015 by ASME",2015,
Logistic Regression with Structured Sparsity,"Binary logistic regression with a sparsity constraint on the solution plays a vital role in many high dimensional machine learning applications. In some cases, the features can be grouped together, so that entire subsets of features can be selected or zeroed out. In many applications, however, this can be very restrictive. In this paper, we are interested in a less restrictive form of structured sparse feature selection: we assume that while features can be grouped according to some notion of similarity, not all features in a group need be selected for the task at hand. This is sometimes referred to as a â€œsparse groupâ€ lasso procedure, and it allows for more flexibility than traditional group lasso methods. Our framework generalizes conventional sparse group lasso further by allowing for overlapping groups, an additional flexiblity that presents further challenges. The main contribution of this paper is a new procedure called Sparse Overlapping Sets (SOS) lasso, a convex optimization program that automatically selects similar features for learning in high dimensions. We establish consistency results for the SOSlasso for classification problems using the logistic regression setting, which specializes to results for the lasso and the group lasso, some known and some new. In particular, SOSlasso is motivated by multi-subject fMRI studies in which functional activity is classified using brain voxels as features, source localization problems in Magnetoencephalography (MEG), and analyzing gene activation patterns in microarray data analysis. Experiments with real and synthetic data demonstrate the advantages of SOSlasso compared to the lasso and group lasso.",2014,ArXiv
"Solar Panel Tilt Angle Optimization Using Machine Learning Model: A Case Study of Daegu City, South Korea","Finding optimal panel tilt angle of photovoltaic system is an important matter as it would convert the amount of sunlight received into energy efficiently. Numbers of studies used various research methods to find tilt angle that maximizes the amount of radiation received by the solar panel. However, recent studies have found that conversion efficiency is not solely dependent on the amount of radiation received. In this study, we propose a solar panel tilt angle optimization model using machine learning algorithms. Rather than trying to maximize the received radiation, the objective is to find tilt angle that maximizes the converted energy of photovoltaic (PV) systems. Considering various factors such as weather, dust level, and aerosol level, five forecasting models were constructed using linear regression (LR), least absolute shrinkage and selection operator (LASSO), random forest (RF), support vector machine (SVM), and gradient boosting (GB). Using the best forecasting model, our model showed increase in PV output compared with optimal angle models.",2020,Energies
Regression Approximations to Estimate Sensitivities,"Chapter 5 explores the idea of using regression problems to estimate sensitivities. Section 5.1 explains how one might approximate the gradient of the QoI at a nominal point using a least-squares (regression) formulation. This naive approach requires more QoI evaluations than one-sided finite differences as described in the previous chapter. Section 5.2 introduces a regularization term into the least-squares minimization problem, allowing for useful solutions also for the case where fewer QoI evaluations than parameters are available; sparsity-promoting regularization (1-norm, LASSO) and a combination of 1-norm and 2-norm (elastic net) are considered. Section 5.3 adds cross-validation techniques for selecting the regularization parameters.",2018,
Gradient Directed Regularization,"Regularization in linear regression and classiÂ…cation is viewed as a twoÂ–stage process. First a set of candidate models is deÂ…ned by a path through the space of joint parameter values, and then a point on this path is chosen to be the Â…nal model. Various pathÂ…nding strategies for the Â…rst stage of this process are examined, based on the notion of generalized gradient descent. Several of these strategies are seen to produce paths that closely correspond to those induced by commonly used penalization methods. Others give rise to new regularization techniques that are shown to be advantageous in some situations. In all cases, the gradient descent pathÂ…nding paradigm can be readily generalized to include the use of a wide variety of loss criteria, leading to robust methods for regression and classiÂ…cation, as well as to apply user deÂ…ned constraints on the parameter values. Key words and phrases : linear models, regularization, regression, classiÂ…cation, gradient descent, robustness, constrained estimation, lasso, ridgeÂ–regression, leastÂ–angle regression LARS, partial least squares PLS, linear support vector machines SVM.",2004,
Integrative Analysis for Lung Adenocarcinoma Predicts Morphological Features Associated with Genetic Variations,"Lung cancer is one of the most deadly cancers and lung adenocarcinoma (LUAD) is the most common histological type of lung cancer. However, LUAD is highly heterogeneous due to genetic difference as well as phenotypic differences such as cellular and tissue morphology. In this paper, we systematically examine the relationships between histological features and gene transcription. Specifically, we calculated 283 morphological features from histology images for 201 LUAD patients from TCGA project and identified the morphological feature with strong correlation with patient outcome. We then modeled the morphology feature using multiple co-expressed gene clusters using Lasso-regression. Many of the gene clusters are highly associated with genetic variations, specifically DNA copy number variations, implying that genetic variations play important roles in the development cancer morphology. As far as we know, our finding is the first to directly link the genetic variations and functional genomics to LUAD histology. These observations will lead to new insight on lung cancer development and potential new integrative biomarkers for prediction patient prognosis and response to treatments.",2017,Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing
Group MCP for Cox Models with Time-Varying Coefficients,"Abstract Coxâ€™s proportional hazard models with time-varying coefficients have much flexibility for modeling the dynamic of covariate effects. Although many variable selection procedures have been developed for Coxs proportional hazard model, the study of such models with time-varying coefficients appears to be limited. The variable selection methods involving nonconvex penalty function, such as the minimax concave penalty (MCP), introduces numerical challenge, but they still have attractive theoretical properties and were indicated that they are worth to be alternatives of other competitive methods. We propose a group MCP method that uses B-spline basis to expand coefficients and maximizes the log partial likelihood with nonconvex penalties on regression coefficients in groups. A fast, iterative group shooting algorithm is carried out for model selection and estimation. Under some appropriate conditions, the simulated example shows that our method performs competitively with the group lasso method. By comparison, the group MCP method and group lasso select the same amount of important covariates, but group MCP method tends to outperform the group lasso method in selection of unimportant covariates.",2016,Journal of Systems Science and Information
Whole genome analysis identifies the association of TP53 genomic deletions with lower survival in Stage III colorectal cancer,"DNA copy number aberrations (CNA) were frequently observed in colorectal cancers (CRC). There is an urgent call for CNA-based biomarkers in clinics, in particular for Stage III CRC, if combined with imaging or pathologic evidence, promise more precise care at the timing. We conducted this Stage III specific biomarker discovery with a cohort of 134 CRCs, and with a newly developed high-efficiency CNA profiling protocol. Specifically, we developed the profiling protocol for tumor-normal matched tissue samples based on low-coverage clinical whole-genome sequencing (WGS). We demonstrated the protocolâ€™s accuracy and robustness by a systematic benchmark with microarray, high-coverage whole-exome and -genome approaches, where the low-coverage WGS-derived CNA segments were highly accordant (PCC>0.95) with those derived from microarray, and they were substantially less variable if compared to exome-derived segments. A lasso-based model and multivariate cox regression analysis identified a chromosome 17p loss, containing the TP53 tumor suppressor gene, that was significantly associated with reduced survival (P=0.0139, HR=1.688, 95% CI = [1.112-2.562]), which was validated by an independent cohort of 187 Stage III CRCs. In summary, the new low-coverage WGS protocol has high sensitivity, high resolution and low cost and the identified 17p-loss is an effective poor prognosis marker for Stage III patients.",2019,bioRxiv
The potential role of pain-related SSEPs in the early prognostication of long-term functional outcome in post-anoxic coma.,"BACKGROUND
Cardiac arrest (CA) is a common cause of disability. Multimodal evaluation has improved prognosis but precocious biomarkers are not appropriate in determining long-term functional outcome.


AIM
To identify early prognostication markers of long-term functional outcome in post-anoxic coma.


DESIGN
Retrospective assessment of outcomes.


POPULATION
Individuals older than 18 years with post-anoxic coma hospitalized in intensive care units after cardiac arrest (CA) regardless of cause (cardiac or non-cardiac) and location of event (in or out-of-hospital).


METHODS
Clinical, biological and neurophysiological data were collected within 48 hours from CA. Clinical data included time of no and low flow, CA rhythm, pupillary reflex, Glasgow motor score at admission and hyperthermia. Biological marker was the highest creatinine level. Neurophysiological parameters included EEG pattern and reactivity, Somatosensory Evoked Potential (SSEP), and Middle-Latency (ML) SSEP evoked at low (10 mA) and high (50 mA) intensity stimulation. Level of Cognitive Functioning Scale (LCFS), Disability Rating Scale and recovery from coma (Revised coma Recovery Scale [CRS-R]) were collected at 12 months. A LASSO multiple regression analysis was fitted to data to investigate the best predictors of LCF, DRS and CRS-R. In-sample prediction was obtained to verify the quality of fitting, and accuracy indices (i.e., total error rate) produced.


RESULTS
Presence of short and medium latency SSEPs with low and high stimulation intensity were identified as prognostic predictors of outcome for all the scales. Error rate was 4.5% for CRS and LCF, and 9.1% for DRS.


CONCLUSIONS
Middle latency somatosensory evoked potentials associated with short latency somatosensory evoked potentials during the first 48 hours after a cardiac arrest are strong predictors of functional outcome at 12 months from the event. Replication on larger cohorts is needed to support their routine use as prognostic markers.


CLINICAL REHABILITATION IMPACT
These markers could inform more appropriate allocation of resources, provide a basis for realistic goal-setting, and help the family to adjust its expectations.",2017,European journal of physical and rehabilitation medicine
Penalized Estimation of Sparse Directed Acyclic Graphs From Categorical Data Under Intervention,"We develop in this article a penalized likelihood method to estimate sparse causal Bayesian networks from categorical data under experimental intervention. The structure of a Bayesian network is represented by a directed acyclic graph (DAG). We model causal interactions in a discrete network by the multi-logit regression and achieve structure estimation of a DAG via maximizing a regularized likelihood. The adaptive group lasso penalty is employed to encourage sparsity by selecting grouped dummy variables encoding the level of a factor together. We develop a blockwise coordinate descent algorithm to solve the penalized likelihood problem subject to the acyclicity constraint of a DAG. We apply our method to three simulated networks and a real biological network, and demonstrate that our method shows very competitive performance compared to existing methods.",2014,
A multivariable approach for risk markers from pooled molecular data with only partial overlap,"BackgroundIncreasingly, molecular measurements from multiple studies are pooled to identify risk scores, with only partial overlap of measurements available from different studies. Univariate analyses of such markers have routinely been performed in such settings using meta-analysis techniques in genome-wide association studies for identifying genetic risk scores. In contrast, multivariable techniques such as regularized regression, which might potentially be more powerful, are hampered by only partial overlap of available markers even when the pooling of individual level data is feasible for analysis. This cannot easily be addressed at a preprocessing level, as quality criteria in the different studies may result in differential availability of markers â€“ even after imputation.MethodsMotivated by data from the InterLymph Consortium on risk factors for non-Hodgkin lymphoma, which exhibits these challenges, we adapted a regularized regression approach, componentwise boosting, for dealing with partial overlap in SNPs. This synthesis regression approach is combined with resampling to determine stable sets of single nucleotide polymorphisms, which could feed into a genetic risk score. The proposed approach is contrasted with univariate analyses, an application of the lasso, and with an analysis that discards studies causing the partial overlap. The question of statistical significance is faced with an approach called stability selection.ResultsUsing an excerpt of the data from the InterLymph Consortium on two specific subtypes of non-Hodgkin lymphoma, it is shown that componentwise boosting can take into account all applicable information from different SNPs, irrespective of whether they are covered by all investigated studies and for all individuals in the single studies. The results indicate increased power, even when studies that would be discarded in a complete case analysis only comprise a small proportion of individuals.ConclusionsGiven the observed gains in power, the proposed approach can be recommended more generally whenever there is only partial overlap of molecular measurements obtained from pooled studies and/or missing data in single studies. A corresponding software implementation is available upon request.Trial registrationAll involved studies have provided signed GWAS data submission certifications to the U.S. National Institute of Health and have been retrospectively registered.",2019,BMC Medical Genetics
A relative error-based approach for variable selection,"The accelerated failure time model or the multiplicative regression model is well-suited to analyze data with positive responses. For the multiplicative regression model, the authors investigate an adaptive variable selection method via a relative error-based criterion and Lasso-type penalty with desired theoretical properties and computational convenience. With fixed or diverging number of variables in regression model, the resultant estimator achieves the oracle property. An alternating direction method of multipliers algorithm is proposed for computing the regularization paths effectively. A data-driven procedure based on the Bayesian information criterion is used to choose the tuning parameter. The finite-sample performance of the proposed method is examined via simulation studies. An application is illustrated with an analysis of one period of stock returns in Hong Kong Stock Exchange.",2016,Comput. Stat. Data Anal.
Robust sparse accelerated failure time model for survival analysis,"To identify the bio-mark genes related to disease with high dimension and low sample size gene expression data, various regression approaches with different regularization methods have been proposed to solve this problem. Nevertheless, high-noises in biological data significantly reduce the performances of methods. The accelerated failure time (AFT) modelwas designed for gene selection and survival time estimation in cancer survival analysis. In this article, we proposed a novel robust sparse accelerated failure time model (RS-AFT) through combining the least absolute deviation (LAD) and Lq regularization. An iterative weighted linear programming algorithm without regularization parameter tuning was proposed to solve this RS-AFT model. The results of the experiments show our method has better performancebothin gene selection and survival time estimationthan some widely used regularization methods such as lasso, elastic net and SCAD. Hence we thought the RS-AFT model may be a competitive regularization method in cancer survival analysis.",2018,Technology and Health Care
Crime risk analysis through big data algorithm with urban metrics,"Abstract Crime is pervasive all around the world. Understanding the influence of social features on crime occurrences of a city is a hot topic among researchers. Correlations between crime and other social characteristics have been studied by large amounts of statistical models, including Ordinary Least Square (OLS) linear regression model, Random Forest (RF) regression model, Artificial Neural Network (ANN) model and so on. However, results of these studies, such as the prediction accuracy, are not satisfying and many contradictory conclusions are achieved in previous research works. These controversies are triggered by several factors, including the non-Gaussian distributions and multicollinearity of urban social data, inaccuracy and inadequacy of the processed data, etc. To fill these gaps, we analyzed the influence of 18 urban indicators within 6 categories including geography, economy, education, housing, urbanization and population structure on crime risk in Chinaâ€™s major prefecture-level cities by year. We used the big data algorithm, Least Absolute Shrinkage and Selection Operator (LASSO) and Extremely-randomized Trees (Extra-Trees), to predict the crime risk and quantify the influence of urban parameters on crime. 83% of accuracy on crime risk prediction can be obtained from our fitted model and the importance of urban indicators is ranked. Results show that area of land used for living, number of subscribers of mobile telephone, employed population are the three main factors on the crime occurrences in China. Our research makes contributions to better understanding of the effects of urban indicators on crime in a socialist nation, and providing instructions and strategies for crime prediction and crime rate control with governments, in this big-data era.",2020,Physica A-statistical Mechanics and Its Applications
Measuring productivity and its relationship to community health worker performance in Uganda: a cross-sectional study,"BackgroundTo explore the nature of the relationship between and factors associated with productivity and performance among the community health volunteer (CHV) cadre (Village Health Teams, VHT) in Busia District, Eastern Uganda. The study was carried out to contribute to the global evidence on strategies to improve CHV productivity and performance.MethodsThis cross-sectional study was conducted with 140 VHT members as subjects and respondents. Data were collected between March and May 2013 on the performance and productivity of VHT members related to village visits and activities for saving maternal and child lives, as well as on independent factors that may be associated with these measures. Data were collected through direct observation of VHT activities, structured interviews with VHTs, and review of available records. The correlation between performance and productivity scores was estimated, and LASSO regression analyses were conducted to identify factors associated with these two scores independently.ResultsVHTs demonstrated wide variation in productivity measures, conducting a median of 13.2 service units in a three-month span (range: 2.0-114.9). Performance of the studied VHTs was generally high, with a median performance score (out of 100) of 96.4 (range: 50.9-100.0). We observed a weak correlation coefficient of 0.05 (pâ€‰=â€‰0.57) between productivity and performance scores. Older VHT age (â‰¥50Â years old, reference: <50Â years old) (11.14, 95% CI: 3.26-19.01) and knowledge of danger signs (in units of ten-percentage points, 1.92, 95% CI: 0.01-3.83) were positively associated with productivity scores. Job satisfaction (1.46, 95% CI: 0.13-2.80) and knowledge of danger signs (in units of ten-percentage points, 1.02, 95% CI: 0.05-1.98) were positively associated with performance scores.ConclusionsOlder VHT age and knowledge of danger signs were positively associated with productivity, and job satisfaction and knowledge of danger signs were positively associated with performance. No correlation was observed between productivity and performance scores. This lack of correlation suggests that interventions to improve CHV effectiveness may affect the two dimensions of effectiveness differently. We recommend that productivity and performance both be monitored to evaluate the overall impact of interventions to increase CHV effectiveness.",2018,BMC Health Services Research
Prediction of Early Breast Cancer Metastasis from DNA Microarray Data Using High-Dimensional Cox Regression Models,"Background DNA microarray studies identified gene expression signatures predictive of metastatic relapse in early breast cancer. Standard feature selection procedures applied to reduce the set of predictive genes did not take into account the correlation between genes. In this paper, we studied the performances of three high-dimensional regression methods â€“ CoxBoost, LASSO (Least Absolute Shrinkage and Selection Operator), and Elastic net â€“ to identify prognostic signatures in patients with early breast cancer. Methods We analyzed three public retrospective datasets, including a total of 384 patients with axillary lymph node-negative breast cancer. The Amsterdam van't Veer's training set of 78 patients was used to determine the optimal gene sets and classifiers using sensitivity thresholds resulting in misclassification of no more than 10% of the poor-prognosis group. To ensure the comparability between different methods, an automatic selection procedure was used to determine the number of genes included in each model. The van de Vijver's and Desmedt's datasets were used as validation sets to evaluate separately the prognostic performances of our classifiers. The results were compared to the original Amsterdam 70-gene classifier. Results The automatic selection procedure reduced the number of predictive genes up to a minimum of six genes. In the two validation sets, the three models (Elastic net, LASSO, and CoxBoost) led to the definition of genomic classifiers predicting the 5-year metastatic status with similar performances, with respective 59, 56, and 54% accuracy, 83, 75, and 83% sensitivity, and 53, 52, and 48% specificity in the Desmedt's dataset. In comparison, the Amsterdam 70-gene signature showed 45% accuracy, 97% sensitivity, and 34% specificity. The gene overlap and the classification concordance between the three classifiers were high. All the classifiers added significant prognostic information to that provided by the traditional prognostic factors and showed a very high overlap with respect to gene ontologies (GOs) associated with genes overexpressed in the predicted poor-prognosis vs. good-prognosis classes and centred on cell proliferation. Interestingly, all classifiers reported high sensitivity to predict the 4-year status of metastatic disease. Conclusions High-dimensional regression methods are attractive in prognostic studies because finding a small subset of genes may facilitate the transfer to the clinic, and also because they strengthen the robustness of the model by limiting the selection of false-positive predictive genes. With only six genes, the CoxBoost classifier predicted the 4-year status of metastatic disease with 93% sensitivity. Selecting a few genes related to ontologies other than cell proliferation might further improve the overall sensitivity performance.",2015,Cancer Informatics
Block sparse linear models for learning structured dynamical systems in aeronautics,"This paper addresses an aircraft dynamical system identification problem, with the goal of using the learned models for trajectory optimization purposes. Our approach is based on multi-task regression. We present in this setting a new class of estimators that we call Block sparse Lasso, which conserves a certain structure between the tasks and some groups of variables, while promoting sparsity within these groups. An implementation leading to consistent feature selection is suggested, allowing to obtain accurate models, which are suitable for trajectory optimization. An additional regularizer is also proposed to help in recovering hidden representations of the initial dynamical system. We illustrate our method with numerical results based on real flight data from 25 medium haul aircraft, totaling 8 million observations.",2018,
OP III â€“ 4 Exposure assessment models for no2 and pm2.5 in the elapse study: a comparison of supervised linear regression and machine learning approaches,"Background/aim Recent studies suggested machine learning as an alternative for supervised linear regression (SLR) in developing Land Use Regression models for air pollution exposure assessment. However, few studies have made direct comparisons. This study aimed to develop novel models using machine learning approaches, and compare the model performance to SLR models using an external dataset for validation. Methods A set of novel European-wide models were developed to estimate 2010 annual means for NO2 and PM2.5, based on AIRBASE routine monitoring data. Satellite observations, chemical transport model estimates, land use and traffic data were used as predictor variables. The alternative algorithms we used included shrinkage techniques (lasso, elastic net, ridge), ensemble learning (bagging, boosting, random forest), support vector machine and a super-learner algorithm. Besides 5-fold cross-validation, we also performed external validation using data from the ESCAPE study to evaluate the model performance. The novel models were compared to the previously developed models (SLR for both NO2 and PM2.5, with additional kriging on residuals in PM2.5 models). Results Random forest suggested a moderate improvement in cross-validation with R2 of 0.66 for NO2 models compared to the conventional supervised linear regression model (R2=0.58), while the external validation R2 was lower (0.46 compared to 0.50). The super-learner algorithm had the highest external validation R2 of 0.51, which was less than 0.01 higher than the original supervised linear regression model. For PM2.5, most of the machine learning methods showed similar or worse performance compared to the original supervised linear regression model. The super-learner algorithm had the highest cross-validation R2 of 0.72, which was 0.02 higher than the supervised linear regression model. However, no machine learning algorithm showed better performance in external validation. Conclusion Machine learning algorithms did not perform better than supervised linear regression in our Europe-wide datasets.",2018,Occupational and Environmental Medicine
Monte Carlo Simulation for Lasso-Type Problems by Estimator Augmentation,"Regularized linear regression under the l1 penalty, such as the Lasso, has been shown to be effective in variable selection and sparse modeling. The sampling distribution of an l1-penalized estimator is hard to determine as the estimator is defined by an optimization problem that in general can only be solved numerically and many of its components may be exactly zero. Let S be the subgradient of the l1 norm of the coefficient vector Î² evaluated at . We find that the joint sampling distribution of and S, together called an augmented estimator, is much more tractable and has a closed-form density under a normal error distribution in both low-dimensional (p â©½ n) and high-dimensional (p > n) settings. Given Î² and the error variance Ïƒ2, one may employ standard Monte Carlo methods, such as Markov chain Monte Carlo (MCMC) and importance sampling (IS), to draw samples from the distribution of the augmented estimator and calculate expectations with respect to the sampling distribution of . We develop a few concret...",2014,Journal of the American Statistical Association
"Three essays on time series : spatio-temporal modelling, dimension reduction and change-point detection","Modelling high dimensional time series and non-stationary time series are two import aspects in time series analysis nowadays. The main objective of this thesis is to deal with these two problems. The first two parts deal with high dimensionality and the third part considers a change point detection problem. In the first part, we consider a class of spatio-temporal models which extend popular econometric spatial autoregressive panel data models by allowing the scalar coefficients for each location (or panel) different from each other. The model is of the following form: yt = D(Î»0)Wyt + D(Î»1)ytâˆ’1 + D(Î»2)Wytâˆ’1 + et, (1) where yt = (y1,t, . . . , yp,t) T represents the observations from p locations at time t, D(Î»k) = diag(Î»k1, . . . , Î»kp) and Î»kj is the unknown coefficient parameter for the j-th location, and W is the pÃ—p spatial weight matrix which measures the dependence among different locations. All the elements on the main diagonal of W are zero. It is a common practice in spatial econometrics to assume W known. For example, we may let wij = 1/(1 + dij ), for i = j, where dij â‰¥ 0 is an appropriate distance between the i-th and the j-th location. It can simply be the geographical distance between the two locations or the distance reflecting the correlation or association between the variables at the two locations. In the above model, D(Î»0) captures the pure spatial effect, D(Î»1) captures the pure dynamic effect, and D(Î»2) captures the time-lagged spatial effect. We also assume that the error term et = (e1,t, e2,t, . . . , ep,t) T in (1) satisfies the condition Cov (ytâˆ’1, et) = 0. When Î»k1 = Â· Â· Â· = Î»kp for all k = 1, 2, 3, (1) reduces to the model of Yu et al. (2008), in which there are only 3 unknown regressive coefficient parameters. In general the regression function in (1) contains 3p unknown parameters. To overcome the innate endogeneity, we propose a generalized Yule-Walker estimation method which applies the least squares estimation to a Yule-Walker equation. The asymptotic theory is developed under the setting that both the sample size and the number of locations (or panels) tend to infinity under a general setting for stationary and Î±-mixing processes, which includes spatial autoregressive panel data models driven by i.i.d. innovations as special cases. The proposed methods are illustrated using both simulated and real data. 
In part 2, we consider a multivariate time series model which decomposes a vector process into a latent factor process and a white noise process. Let yt = (y1,t, Â· Â· Â· , yp,t) T be an observable p Ã— 1 vector time series process. The factor model decomposes yt in the following form: yt = Axt + et , (2) where xt = (x1,t, Â· Â· Â· , xr,t) T is a r Ã— 1 latent factor time series with unknown r â‰¤ p and A = (a1, a2, Â· Â· Â· , ar) is a p Ã— r unknown constant matrix. et is a white noise process with mean 0 and covariance matrix Î£e. The first part of (2) is a dynamic part and the serial dependence of yt is driven by xt. We will achieve dimension reduction once r â‰ª p in the sense that the dynamics of yt is driven by a much lower dimensional process xt. Motivated by practical needs and the characteristic of high dimensional data, the sparsity assumption on factor loading matrix is imposed. Different from Lam, Yao and Bathia (2011)â€™s method, which is equivalent to an eigenanalysis of a non negative definite matrix, we add a constraint to control the number of nonzero elements in each column of the factor loading matrix. Our proposed sparse estimator is then the solution of a constrained optimization problem. The asymptotic theory is developed under the setting that both the sample size and the dimensionality tend to infinity. When the common factor is weak in the sense that Î´ > 1/2 in Lam, Yao and Bathia (2011)â€™s paper, the new sparse estimator may have a faster convergence rate. Numerically, we employ the generalized deflation method (Mackey (2009)) and the GSLDA method (Moghaddam et al. (2006)) to approximate the estimator. The tuning parameter is chosen by cross validation. The proposed method is illustrated with both simulated and real data examples. The third part is a change point detection problem. we consider the following covariance structural break detection problem: Cov(yt)I(tjâˆ’1 â‰¤ t < tj ) = Î£tjâˆ’1, j = 1, Â· Â· Â· , m + 1, where yt is a p Ã— 1 vector time series, Î£tjâˆ’1 = Î£tj and {t1, . . ., tm} are change points, 1 = t0 < t1 < Â· Â· Â· < tm+1 = n. In the literature, the number of change points m is usually assumed to be known and small, because a large m would involve a huge amount of computational burden for parameters estimation. By reformulating the problem in a variable selection context, the group least absolute shrinkage and selection operator (LASSO) is proposed to estimate m and the locations of the change points {t1, . . ., tm}. Our method is model free, it can be extensively applied to multivariate time series, such as GARCH and stochastic volatility models. It is shown that both m and the locations of the change points {t1, . . . , tm} can be consistently estimated from the data, and the computation can be efficiently performed. An improved practical version that incorporates group LASSO and the stepwise regression variable selection technique are discussed. Simulation studies are conducted to assess the finite sample performance.",2015,
Backward Elimination Algorithm for High Dimensional Variable Screening,"In recent times, variable selection in high-dimensional data has become a challenging problem. We investigate here a popular but classical variable screening method, the Backward Elimination (BE) in a high dimensional setup (small-n-large P). The BE method as a variable screening method reduces the dimension of small-n-large P data into a lower dimensional data and then established shrinkage methods such as: LASSO, SCAD and MCP can be applied directly. To overcome the problems in high dimensional data, Chen and Chen (2008) recently developed a family of Extended Bayesian Information Criterion (EBIC) which is consistent with finite sample properties (Chen and Chen, 2008) which we used in this study to select the best candidate model from the models generated by the proposed BE method. We compare the BE with other screening methods such as: Sure Independence Screening(SIS), Iterative Sure Independence Screening and Forward Regression (FR) in simulation studies and real-data analysis to illustrate the selection consistency of our proposed BE method. Our numerical analysis reveals that the BE with EBIC can identify all important variables with high coverage probability, low false discovery rate and a very good model size with high signal-to-noise.",2018,
Macroeconomic Forecasting Using Penalized Regression Methods,"We study the suitability of lasso-type penalized regression techniques when applied to macroeconomic forecasting with high-dimensional datasets. We consider performance of the lasso-type methods when the true DGP is a factor model, contradicting the sparsity assumption underlying penalized regression methods. We also investigate how the methods handle unit roots and cointegration in the data. In an extensive simulation study we find that penalized regression methods are morerobust to mis-specification than factor models estimated by principal components, even if the underlying DGP is a factor model. Furthermore, the penalized regression methods are demonstrated to deliver forecast improvements over traditional approaches when applied to non-stationary data containing cointegrated variables, despite a deterioration of the selective capabilities. Finally, we also consider an empirical application to a large macroeconomic U.S. dataset and demonstrate that, in line with our simulations, penalized regression methods attain the best forecast accuracy most frequently.",2016,research memorandum
Development and validation of a novel immune-related prognostic model in hepatocellular carcinoma,"Growing evidence has suggested that immune-related genes play crucial roles in the development and progression of hepatocellular carcinoma (HCC). Nevertheless, the utility of immune-related genes for evaluating the prognosis of HCC patients are still lacking. The study aimed to explore gene signatures and prognostic values of immune-related genes in HCC. We comprehensively integrated gene expression data acquired from 374 HCC and 50 normal tissues in The Cancer Genome Atlas (TCGA). Differentially expressed genes (DEGs) analysis and univariate Cox regression analysis were performed to identify DEGs that related to overall survival. An immune prognostic model was constructed using the Lasso and multivariate Cox regression analyses. Furthermore, Cox regression analysis was applied to identify independent prognostic factors in HCC. The correlation analysis between immune-related signature and immune cells infiltration were also investigated. Finally, the signature was validated in an external independent dataset. A total of 329 differentially expressed immuneâ€related genes were detected. 64 immuneâ€related genes were identified to be markedly related to overall survival in HCC patients using univariate Cox regression analysis. Then we established a TF-mediated network for exploring the regulatory mechanisms of these genes. Lasso and multivariate Cox regression analyses were applied to construct the immune-based prognostic model, which consisted of nine immuneâ€related genes. Further analysis indicated that this immune-related prognostic model could be an independent prognostic indicator after adjusting to other clinical factors. The relationships between the risk score model and immune cell infiltration suggested that the nine-gene signature could reflect the status of tumor immune microenvironment. The prognostic value of this nine-gene prognostic model was further successfully validated in an independent database. Together, our study screened potential prognostic immune-related genes and established a novel immune-based prognostic model of HCC, which not only provides new potential prognostic biomarkers and therapeutic targets, but also deepens our understanding of tumor immune microenvironment status and lays a theoretical foundation for immunotherapy.",2020,Journal of Translational Medicine
Debiasing Linear Prediction,"Standard methods in supervised learning separate training and prediction: the model is fit independently of any test points it may encounter. However, can knowledge of the next test point $\mathbf{x}_{\star}$ be exploited to improve prediction accuracy? We address this question in the context of linear prediction, showing how debiasing techniques can be used transductively to combat regularization bias. We first lower bound the $\mathbf{x}_{\star}$ prediction error of ridge regression and the Lasso, showing that they must incur significant bias in certain test directions. Then, building on techniques from semi-parametric inference, we provide non-asymptotic upper bounds on the $\mathbf{x}_{\star}$ prediction error of two transductive, debiased prediction rules. We conclude by showing the efficacy of our methods on both synthetic and real data, highlighting the improvements test-point-tailored debiasing can provide in settings with distribution shift.",2019,ArXiv
Differentially Private Model Selection via Stability Arguments and the Robustness of the Lasso,"We design differentially private algorithms for statistical model selection. Given a data set and a large, discrete collection of â€œmodelsâ€, each of which is a family of probability distributions, the goal is to determine the model that best â€œfitsâ€ the data. This is a basic problem in many areas of statistics and machine learning. We consider settings in which there is a well-defined answer, in the following sense: Suppose that there is a nonprivate model selection procedure f which is the reference to which we compare our performance. Our differentially private algorithms output the correct value f(D) whenever f is stable on the input data set D. We work with two notions, perturbation stability and subsampling stability. We give two classes of results: generic ones, that apply to any function with discrete output set; and specific algorithms for the problem of sparse linear regression. The algorithms we describe are efficient and in some cases match the optimal nonprivate asymptotic sample complexity. Our algorithms for sparse linear regression require analyzing the stability properties of the popular LASSO estimator. We give sufficient conditions for the LASSO estimator to be robust to small changes in the data set, and show that these conditions hold with high probability under essentially the same stochastic assumptions that are used in the literature to analyze convergence of the LASSO.",2013,
"Centennial Changes in the Nearâ€Shore Mysid Fauna of the Gulf of Naples (Mediterranean Sea), with Description of Heteromysis riedli sp. n. (Crustacea, Mysidacea)",". The marine mysid fauna of the Gulf of Naples is the best known in the Mediterranean, dating back to faunal lists and revisions given by founder authors in 1877â€Šâ€“â€Š1929. Up to 1930, a total of 21 (currently valid) benthopelagic and benthic coastal species were recorded. The new census in 1975â€Šâ€“â€Š2000 yielded no species in brackish and freshwaters (salinity range 0â€Šâ€“â€Š30), only one species in mixoeuhaline waters (30â€Šâ€“â€Š39), and 39 species in fully marine near-shore waters (36â€Šâ€“â€Š38). Most species were restricted to islands and submarine banks as hotspots of biodiversity, while only four species were also found along the more intensively urbanized continental coasts of the gulf. Compared with the situation in the 19th century, two marine species, Acanthomysis longicornis and Mysidopsis angusta, have disappeared from the Gulf of Naples, while still present in the less urbanized and largely oligotrophic Gulf of Salerno. The numbers of known euthalassobiontic species decreased in the â€˜continentalâ€™ Gulf of Naples, but increased in the â€˜insularâ€™ gulf. 
 
 
 
Local population extinctions are related mainly to human impact on freshwater input into shallow coastal waters. Urbanization processes favoured the disappearance of brackish and freshwaters from the surface (through drainage and canalization) and the deterioration (through urban and industrial pollution) of the remaining ones, resulting in eutrophication of coastal waters and in impoverishment of the benthos, such as the visible regression of seagrasses. Increased species numbers in the â€˜insularâ€™ gulf reflect the lower degree of urbanization in the less polluted peripheral zones and the increased use of more specific sampling methods in combination with improved knowledge of mysid biology. 
 
 
 
With the use of epibenthic nets during the day and at night, Heteromysis (Heteromysis) riedli sp.Â n. was sampled from Posidonia oceanica meadows on the Island of Â­Ischia. The males of the new species are exceptional in having a pair of modified, backwards-oriented, flagellate setae terminally on the antennular trunc, while the females show only the usual forwards-oriented, smooth setae in this position.",2001,Marine Ecology
clogitLasso: an R package for highâ€“dimensional analysis of matched caseâ€“control and caseâ€“crossover data,"The conditional logistic regression model is the standard tool for the analysis of epidemiological studies in which one or more cases (the event of interest), are individually matched with one or more controls (not showing the event). These situations arise, for example, in matched case-control and case-crossover studies. 
Usually, regression coefficients are estimated by maximizing the conditional log-likelihood function and variable selection is performed by conventional manual or automatic selection procedures, such as stepwise. These techniques are, however, unsatisfactory in sparse, high-dimensional settings in which penalized methods, such as the lasso (least absolute shrinkage and selection operator) [Tibshirani, 1996], have emerged as an alternative. In particular, the lasso and related methods have recently been adapted to conditional logistic regression [Avalos et al., 2012b]. 
The R package clogitLasso brings together algorithms to estimate parameters of conditional logistic regression models using lasso, other sparse methods (elastic net, adaptive lasso and bootstrapped versions of lasso). Different criteria are implemented for choosing the regularization term accounting for the dependent nature of data. Resampling methods for evaluating the stability of the selected model are proposed. The most common individually matched study designs are available.",2013,
An â„“1-oracle inequality for the Lasso in multivariate finite mixture of multivariate Gaussian regression models,"We consider a multivariate finite mixture of Gaussian regression models for high-dimensional data, where the number of covariates and the size of the response may be much larger than the sample size. We provide an l 1 -oracle inequality satisfied by the Lasso estimator according to the Kullbackâˆ’Leibler loss. This result is an extension of the l 1 -oracle inequality established by Meynet in [ ESAIM: PS 17 (2013) 650â€“671]. in the multivariate case. We focus on the Lasso for its l 1 -regularization properties rather than for the variable selection procedure.",2015,Esaim: Probability and Statistics
Statistical consistency and asymptotic normality for high-dimensional robust M-estimators,"We study theoretical properties of regularized robust M-estimators, applicable when data are drawn from a sparse high-dimensional linear model and contaminated by heavy-tailed distributions and/or outliers in the additive errors and covariates. We first establish a form of local statistical consistency for the penalized regression estimators under fairly mild conditions on the error distribution: When the derivative of the loss function is bounded and satisfies a local restricted curvature condition, all stationary points within a constant radius of the true regression vector converge at the minimax rate enjoyed by the Lasso with sub-Gaussian errors. When an appropriate nonconvex regularizer is used in place of an l_1-penalty, we show that such stationary points are in fact unique and equal to the local oracle solution with the correct support---hence, results on asymptotic normality in the low-dimensional case carry over immediately to the high-dimensional setting. This has important implications for the efficiency of regularized nonconvex M-estimators when the errors are heavy-tailed. Our analysis of the local curvature of the loss function also has useful consequences for optimization when the robust regression function and/or regularizer is nonconvex and the objective function possesses stationary points outside the local region. We show that as long as a composite gradient descent algorithm is initialized within a constant radius of the true regression vector, successive iterates will converge at a linear rate to a stationary point within the local region. Furthermore, the global optimum of a convex regularized robust regression function may be used to obtain a suitable initialization. The result is a novel two-step procedure that uses a convex M-estimator to achieve consistency and a nonconvex M-estimator to increase efficiency.",2015,ArXiv
Title : Accuracy of Genomic Selection Methods in a Standard Dataset of Loblolly Pine ( Pinus taeda L . ),"Genomic selection can increase genetic gain per generation through early selection. Genomic selection is expected to be particularly valuable for traits that are costly to phenotype, and expressed late in the life-cycle of long-lived species. Alternative approaches to genomic selection prediction models may perform differently for traits with distinct genetic properties. Here the performance of four different original methods of genomic selection that differ with respect to assumptions regarding distribution of marker effects, including (i) Ridge Regression â€“ Best Linear Unbiased Prediction (RRBLUP), (ii) Bayes A, (iii) Bayes CÏ€, and (iv) Bayesian LASSO are presented. In addition, a modified RR-BLUP (RR-BLUP B) that utilizes a selected subset of markers was evaluated. The accuracy of these methods was compared across 17 traits with distinct heritabilities and genetic architectures, including growth, development and diseaseresistance properties, measured in a Pinus taeda (loblolly pine) training population of 951 individuals genotyped with 4,853 SNPs. The predictive ability of the methods was evaluated using a 10-fold, cross-validation approach, and differed only marginally for most method/trait combinations. Interestingly, for fusiform rust disease-resistance traits Bayes CÏ€, Bayesian LASSO and RR-BLUB B had higher predictive ability than RRBLUP and Bayes A. Fusiform rust is controlled by few genes of large effect. A limitation of RR-BLUP is the assumption of equal contribution of all markers to the observed variation. The genotypic and phenotypic data used in this study is publically available for comparative analysis of genomic selection prediction models.",2012,
