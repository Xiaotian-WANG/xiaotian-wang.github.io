title,abstract,year,journal
Research on regional differences and influencing factors of green technology innovation efficiency of China's high-tech industry,"Abstract Through the K-means clustering analysis, it divides the regions of China into four clusters according to the differences in high-tech industry development level between 2008 and 2016. Considering â€environmental pollutionâ€ and â€innovation failureâ€, an improved SBM-DEA efficiency measurement model was constructed to measure the green technology innovation efficiency of Chinaâ€™s high-tech industry clusters. Lasso regression was used to screen out the factors affecting the green technology innovation efficiency of high-tech industry in each cluster area. On this basis, quantile regression method is used to study the influence degree and regional differences of various influencing factors on green innovation efficiency of high-tech industry at different quantile. Meanwhile, DEA-tobit model is used for robustness test. The research shows that in each cluster area, the factors that significantly affect the green innovation efficiency of high-tech industry are different, and the degree of influence of each factor on the innovation efficiency at different quantile is also different. Combining the empirical results with the reality of high-tech industries in various regions, the corresponding policy recommendations are put forward.",2020,J. Comput. Appl. Math.
Explaining the Stars: Weighted Multiple-Instance Learning for Aspect-Based Sentiment Analysis,"This paper introduces a model of multiple-instance learning applied to the prediction of aspect ratings or judgments of specific properties of an item from user-contributed texts such as product reviews. Each variable-length text is represented by several independent feature vectors; one word vector per sentence or paragraph. For learning from texts with known aspect ratings, the model performs multiple-instance regression (MIR) and assigns importance weights to each of the sentences or paragraphs of a text, uncovering their contribution to the aspect ratings. Next, the model is used to predict aspect ratings in previously unseen texts, demonstrating interpretability and explanatory power for its predictions. We evaluate the model on seven multi-aspect sentiment analysis data sets, improving over four MIR baselines and two strong bag-of-words linear models, namely SVR and Lasso, by more than 10% relative in terms of MSE.",2014,
Integrative multi-view regression: Bridging group-sparse and low-rank models.,"Multi-view data have been routinely collected in various fields of science and engineering. A general problem is to study the predictive association between multivariate responses and multi-view predictor sets, all of which can be of high dimensionality. It is likely that only a few views are relevant to prediction, and the predictors within each relevant view contribute to the prediction collectively rather than sparsely. We cast this new problem under the familiar multivariate regression framework and propose an integrative reduced-rank regression (iRRR), where each view has its own low-rank coefficient matrix. As such, latent features are extracted from each view in a supervised fashion. For model estimation, we develop a convex composite nuclear norm penalization approach, which admits an efficient algorithm via alternating direction method of multipliers. Extensions to non-Gaussian and incomplete data are discussed. Theoretically, we derive non-asymptotic oracle bounds of iRRR under a restricted eigenvalue condition. Our results recover oracle bounds of several special cases of iRRR including Lasso, group Lasso, and nuclear norm penalized regression. Therefore, iRRR seamlessly bridges group-sparse and low-rank methods and can achieve substantially faster convergence rate under realistic settings of multi-view learning. Simulation studies and an application in the Longitudinal Studies of Aging further showcase the efficacy of the proposed methods.",2018,Biometrics
Association of MRI-derived radiomic biomarker with disease-free survival in patients with early-stage cervical cancer,"Pre-treatment survival prediction plays a key role in many diseases. We aimed to determine the prognostic value of pre-treatment Magnetic Resonance Imaging (MRI) based radiomic score for disease-free survival (DFS) in patients with early-stage (IB-IIA) cervical cancer. Methods: A total of 248 patients with early-stage cervical cancer underwent radical hysterectomy were included from two institutions between January 1, 2011 and December 31, 2017, whose MR imaging data, clinicopathological data and DFS data were collected. Patients data were randomly divided into the training cohort (n = 166) and the validation cohort (n=82). Radiomic features were extracted from the pre-treatment T2-weighted (T2w) and contrast-enhanced T1-weighted (CET1w) MR imagings for each patient. Least absolute shrinkage and selection operator (LASSO) regression and Cox proportional hazard model were applied to construct radiomic score (Rad-score). According to the cutoff of Rad-score, patients were divided into low- and high- risk groups. Pearson's correlation and Kaplan-Meier analysis were used to evaluate the association of Rad-score with DFS. A combined model incorporating Rad-score, lymph node metastasis (LNM) and lymphovascular space invasion (LVI) by multivariate Cox proportional hazard model was constructed to estimate DFS individually. Results: Higher Rad-scores were significantly associated with worse DFS in the training and validation cohorts (P<0.001 and P=0.011, respectively). The Rad-score demonstrated better prognostic performance in estimating DFS (C-index, 0.753; 95% CI: 0.696-0.805) than the clinicopathological features (C-index, 0.632; 95% CI: 0.567-0.700). However, the combined model showed no significant improvement (C-index, 0.714; 95%CI: 0.642-0.784). Conclusion: The results demonstrated that MRI-derived Rad-score can be used as a prognostic biomarker for patients with early-stage (IB-IIA) cervical cancer, which can facilitate clinical decision-making.",2020,Theranostics
Preface for the Special Issue on Computational Techniques in Theoretical and Applied Statistics,"Recent technological developments and the exponential increase in computational power provide researchers in statistics with endless possibilities for developing methodologies and applying new techniques. This special issue, entitled â€œComputational Techniques in Theoretical and Applied Statisticsâ€, highlights some topics and specific problems, combining theoretical results, applications and computational techniques. The works presented in this special issue cover different topics of research, as for example: Distribution Theory, Nonparametric Inference, Shrinkage/Penalized Estimation Strategies, Actuarial Sciences, Survival Analysis, and etc. In what follows we present a brief review of the various works published in this special issue. In Survival Analysis, Balakrishnan and Feng introduced a ConwayMaxwellPoisson Cure rate model under a proportional odds assumption for lifetime of susceptibles, with the baseline either the Weibull or the logistic distribution. A computationally efficient EM algorithm is presented, followed by an extensive Monte Carlo simulation study to illustrate the performance of this flexible model. In the topic of Distribution Theory, Marques developed near-exact approximations for the linear combination of independent Logistic random variables which are based on the difference of two independent Generalized Integer Gamma distributions. The practical implementation of these distributions is only possible using software with high precision. In Bekker and Ferreira, the paper starts with a description of the setting. Bivariate gamma type distributions, within the elliptical framework are proposed, with the bivariate Nakagami distribution that is well known in the communications systems domain to describe the signal behaviour of fading channel, as a special case. Interesting observations follow from the calculated values of the outage probability and Laplace expressions. This work has a significant impact on the assumption of the underlying normality that is pervasive in the current literature in the communications systems field. In Nunes et al., theoretical results about limiting distributions of asymptotically linear statistics are demonstrated and applied through the use of computational simulations. In Wu et al., the authors considered the problem in Actuarial Sciences of modelling aggregate claims with a dependence structure. The authors combine theoretical results for the aggregate claim distributions with computational algorithms for its implementation and present numerical studies for a specific case. In the topic of Shrinkage/Penalized Estimation Strategies, Saleh and Norouzirad, proposed a closed form modified LASSO in multiple regression and compared its performance with shrinkage estimators. More precisely, using the principle of marginal distribution theory, shrinkage and penalized estimation strategies are developed for a non-orthogonal design matrix. Hence, a closed form modified LASSO estimator is proposed for the non-orthogonal design. In an extensive simulation study, the performance of shrinkage estimators is compared with the ridge regression and modified LASSO estimators. The computational aspect of this study is on finding the upper bound of risk functions and theoretical comparisons. In Emami and Kiani, shrinkage differenced based linear unified (Liu) estimators are proposed in semiparametric linear models, when multicollinearity is present. Exact characteristics of the estimators are derived and the region of optimality of each estimator is explored. These attempts formed the computational framework of this study, in the field of shrinkage estimation with multicollinearity problem. In Kazemi et al., a procedure is developed for ultra-high dimensional additive models, to identify nonzero and linear components. Hence a sure independence screening procedure based on the distance correlation between predictors and marginal distribution function of the response variable are proposed. Afterwards, penalized estimation techniques are used to identify nonzero and linear components, simultaneously. In this piece of work, complex computational aspects are revealed by proposing the procedures in ultra-high dimensions and algorithms used for",2018,"Statistics, Optimization and Information Computing"
Lasso-Based Tag Expansion and Tag-Boosted Collaborative Filtering,"With the increasing popularity of the social tagging systems, tags can be effectively utilized to enhance Collaborative Filtering (CF) algorithms. Tags not only reflect users' preference, but also are a cue to describe the semantics of items. This paper formulates the problem of collaborative filtering as random walks over the user-item-tag tripartite graph. In order to alleviate the sparsity of tags, a lasso logistic regression model is conducted to accomplish tag expansion, i.e., adding relevant tags and removing irrelevant tags for each item. Experimental results on MovieLens dataset demonstrate the superiority of the proposed algorithms over several existing CF algorithms in terms of ranking performance measure F1 and Macro DOA.",2010,
"Discussion of ""Least angle regression"" by Efron et al","Algorithms for simultaneous shrinkage and selection in regression and classification provide attractive solutions to knotty old statistical challenges. Nevertheless, as far as we can tell, Tibshirani's Lasso algorithm has had little impact on statistical practice. Two particular reasons for this may be the relative inefficiency of the original Lasso algorithm and the relative complexity of more recent Lasso algorithms [e.g., Osborne, Presnell and Turlach (2000)]. Efron, Hastie, Johnstone and Tibshirani have provided an efficient, simple algorithm for the Lasso as well as algorithms for stagewise regression and the new least angle regression. As such this paper is an important contribution to statistical computing. 1. Predictive performance. The authors say little about predictive performance issues. In our work, however, the relative out-of-sample predictive performance of LARS, Lasso and Forward Stagewise (and variants thereof) takes center stage. Interesting connections exist between boosting and stage-wise algorithms so predictive comparisons with boosting are also of interest. The authors present a simple C p statistic for LARS. In practice, a cross-validation (CV) type approach for selecting the degree of shrinkage, while computationally more expensive, may lead to better predictions. We considered this using the LARS software. Here we report results for the authors' diabetes data, the Boston housing data and the Servo data from the UCI Machine Learning Repository. Specifically, we held out 10% of the data and chose the shrinkage level using either C p or nine-fold CV using 90% of the data. Then we estimated mean square error (MSE) on the 10% hold-out sample. Table 1 shows the results for main-effects models. Table 1 exhibits two particular characteristics. First, as expected, Stage-wise, LARS and Lasso perform similarly. Second, C p performs as well as cross-validation; if this holds up more generally, larger-scale applications will want to use C p to select the degree of shrinkage.",2004,arXiv: Statistics Theory
Leave-one-out prediction intervals in linear regression models with many variables,"We study prediction intervals based on leave-one-out residuals in a linear regression model where the number of explanatory variables can be large compared to sample size. We establish uniform asymptotic validity (conditional on the training sample) of the proposed interval under minimal assumptions on the unknown error distribution and the high dimensional design. Our intervals are generic in the sense that they are valid for a large class of linear predictors used to obtain a point forecast, such as robust M-estimators, James-Stein type estimators and penalized estimators like the LASSO. These results show that despite the serious problems of resampling procedures for inference on the unknown parameters, leave-one-out methods can be successfully applied to obtain reliable predictive inference even in high dimensions.",2016,arXiv: Statistics Theory
Regularized Extreme Learning Machine Ensemble Using Bagging for Tropical Cyclone Tracks Prediction,"This paper aims to improve the prediction accuracy of Tropical Cyclone Tracks (TCTs) over the South China Sea (SCS) and its coastal regions with 24 h lead time. The model proposed in this paper is a regularized extreme learning machine (ELM) ensemble using bagging. A new method is proposed in this paper to solve lasso and elastic net problem in ELM, which turns the original problem into familiar quadratic programming (QP) problem. The forecast error of TCTs data set is the distance between real position and forecast position. Compared with the stepwise regression method widely used in TCTs, 16.49 km accuracy improvement is obtained by our model. Results show that the regularized ELM ensemble using bagging has a better generalization capactity on TCTs data set.",2018,
Regularized Linear Regression: A Precise Analysis of the Estimation Error,"Non-smooth regularized convex optimization procedures have emerged as a powerful tool to recover structured signals (sparse, low-rank, etc.) from (possibly compressed) noisy linear measurements. We focus on the problem of linear regression and consider a general class of optimization methods that minimize a loss function measuring the misfit of the model to the observations with an added structured-inducing regularization term. Celebrated instances include the LASSO, GroupLASSO, Least-Absolute Deviations method, etc.. We develop a quite general framework for how to determine precise prediction performance guaranties (e.g. mean-square-error) of such methods for the case of Gaussian measurement ensemble. The machinery builds upon Gordonâ€™s Gaussian min-max theorem under additional convexity assumptions that arise in many practical applications. This theorem associates with a primary optimization (PO) problem a simplified auxiliary optimization (AO) problem from which we can tightly infer properties of the original (PO), such as the optimal cost, the norm of the optimal solution, etc. Our theory applies to general loss functions and regularization and provides guidelines on how to optimally tune the regularizer coefficient when certain structural properties (such as sparsity level, rank, etc.) are known.",2015,
Machine learning: how much does it improve the prediction of unplanned hospital admissions?,"IntroductionRisk prediction models can be used to inform decision-making in clinical settings. With large and detailed electronic medical record data, machine learning may improve predictions. The objective of this work is to determine the feasibility and accuracy of machine learning versus logistic regression to predict unplanned hospital admissions. 
Objectives and ApproachData from primary care electronic medical records for community-dwelling adults in Alberta, Canada available from the Canadian Primary Care Sentinel Surveillance Network will be linked to acute care administrative health data held by Alberta Health Services. Two regression methods (forward stepwise logistic, LASSO logistic) will be compared with three machine learning methods (classification tree, random forest, gradient boosted trees). Prior primary and acute care use will be used to predict three outcomes: â‰¥1 unplanned admission within 1 year, â‰¥1 unplanned admission within 90 days, and â‰¥1 unplanned admission within 1 year due to an ambulatory care sensitive condition. 
ResultsThe results of this work in progress will be presented at the conference. 41,142 patients will have their primary and acute care data linked. We anticipate that the machine learning methods will improve predictive performance but will be more challenging for clinicians and patients to understand, including why a given patient is predicted to be at higher risk. The primary comparison of machine learning and regression methods will be based on positive predictive values corresponding to the top 5% predicted risk threshold, and estimated via 10-fold cross-validation. 
Conclusion/ImplicationsThis project aims to help researchers decide which statistical methods to use for risk prediction models. When considering machine learning methods the best approach may be to try multiple methods, compare their predictive accuracy and interpretability, and then choose a final method.",2018,International Journal for Population Data Science
Oracle Inequalities and Optimal Inference under Group Sparsity,"We consider the problem of estimating a sparse linear regression vector s* under a gaussian noise model, for the purpose of both prediction and model selection. We assume that prior knowledge is available on the sparsity pattern, namely the set of variables is partitioned into prescribed groups, only few of which are relevant in the estimation process. This group sparsity assumption suggests us to consider the Group Lasso method as a means to estimate s*. We establish oracle inequalities for the prediction and l2 estimation errors of this estimator. These bounds hold under a restricted eigenvalue condition on the design matrix. Under a stronger coherence condition, we derive bounds for the estimation error for mixed (2,p)-norms with 1=p=8. When p=8, this result implies that a threshold version of the Group Lasso estimator selects the sparsity pattern of s* with high probability. Next, we prove that the rate of convergence of our upper bounds is optimal in a minimax sense, up to a logarithmic factor, for all estimators over a class of group sparse vectors. Furthermore, we establish lower bounds for the prediction and l2 estimation errors of the usual Lasso estimator. Using this result, we demonstrate that the Group Lasso can achieve an improvement in the prediction and estimation properties as compared to the Lasso.",2010,Annals of Statistics
Network-based analysis of prostate cancer cell lines reveals novel marker gene candidates associated with radioresistance and patient relapse,"Radiation therapy is an important and effective treatment option for prostate cancer, but high-risk patients are prone to relapse due to radioresistance of cancer cells. Molecular mechanisms that contribute to radioresistance are not fully understood. Novel computational strategies are needed to identify radioresistance driver genes from hundreds of gene copy number alterations. We developed a network-based approach based on lasso regression in combination with network propagation for the analysis of prostate cancer cell lines with acquired radioresistance to identify clinically relevant marker genes associated with radioresistance in prostate cancer patients. We analyzed established radioresistant cell lines of the prostate cancer cell lines DU145 and LNCaP and compared their gene copy number and expression profiles to their radiosensitive parental cells. We found that radioresistant DU145 showed much more gene copy number alterations than LNCaP and their gene expression profiles were highly cell line specific. We learned a genome-wide prostate cancer-specific gene regulatory network and quantified impacts of differentially expressed genes with directly underlying copy number alterations on known radioresistance marker genes. This revealed several potential driver candidates involved in the regulation of cancer-relevant processes. Importantly, we found that ten driver candidates from DU145 (ADAMTS9, AKR1B10, CXXC5, FST, FOXL1, GRPR, ITGA2, SOX17, STARD4, VGF) and four from LNCaP (FHL5, LYPLAL1, PAK7, TDRD6) were able to distinguish irradiated prostate cancer patients into early and late relapse groups. Moreover, in-depth in vitro validations for VGF (Neurosecretory protein VGF) showed that siRNA-mediated gene silencing increased the radiosensitivity of DU145 and LNCaP cells. Our computational approach enabled to predict novel radioresistance driver gene candidates. Additional preclinical and clinical studies are required to further validate the role of VGF and other candidate genes as potential biomarkers for the prediction of radiotherapy responses and as potential targets for radiosensitization of prostate cancer.",2019,PLoS Computational Biology
A data-driven approach to modeling physical fatigue in the workplace using wearable sensors.,"Wearable sensors are currently being used to manage fatigue in professional athletics, transportation and mining industries. In manufacturing, physical fatigue is a challenging ergonomic/safety ""issue"" since it lowers productivity and increases the incidence of accidents. Therefore, physical fatigue must be managed. There are two main goals for this study. First, we examine the use of wearable sensors to detect physical fatigue occurrence in simulated manufacturing tasks. The second goal is to estimate the physical fatigue level over time. In order to achieve these goals, sensory data were recorded for eight healthy participants. Penalized logistic and multiple linear regression models were used for physical fatigue detection and level estimation, respectively. Important features from the five sensors locations were selected using Least Absolute Shrinkage and Selection Operator (LASSO), a popular variable selection methodology. The results show that the LASSO model performed well for both physical fatigue detection and modeling. The modeling approach is not participant and/or workload regime specific and thus can be adopted for other applications.",2017,Applied ergonomics
Environmental factors analysis and comparison affecting software reliability in development of multi-release software,"Abstract As the application of the principles of agile and lean software development, software multiple release becomes very common in the modern society. Short iteration and short release cycle have driven the significant changes of the development process of multi-release software product, compared with single release software product. Thus, it is time to conduct a new study investigating the impact level of environmental factors on affecting software reliability in the development of multi-release software to provide a sound and concise guidance to software practitioners and researchers. Statistical learning methods, like principle component analysis, stepwise backward elimination, lasso regression, multiple linear regression, and Tukey method, are applied in this study. Comparisons regarding significant environmental factors during the whole development process, principle components, significant environmental factors in each development phase and significance level of each development phase between the development of single release software and multi-release software are also discussed.",2017,J. Syst. Softw.
Machine Learning Techniques Applied to US Army and Navy Data,"We apply machine learning techniques to the synthetic data (Stevens and Anderson-Cook, 2017a), which is univariate data with a binary response of passing or failing for complex munitions generated to match age and usage rate, found in United States Department of Defense complex systems (the Army and Navy). Instead of the generalized linear model (GLM) with probit link function used in Stevens and Anderson-Cook (2017a), we propose applying machine learning techniques to predict the binary response of passing or failing for the Army and Navy data. We propose two methods of machine learning techniques to find the best models for the Army and Navy. The first method is the artificial neural networks (ANN), which simulates biological neural networks to make predictions through transforming artificial neurons (variables) by particular learning rule(s). The second method is the penalized logistic regression. It adds penalty to the coefficients of the logistic general linear regression. We use three penalized methods: lasso, ridge, and elastic net. We compare the predictive accuracy among the ANN, three types of penalized regression, GLM with logit link, and GLM with probit link. We use two criteria to measure the predictive accuracy: Root-meansquare error (RMSE) and the area under Receiver Operating Characteristic (ROC) curve (AUC). Both show that machine learning techniques give better predictions compared to the standard GLMs.",2019,International Journal of Productivity and Quality Management
Regularized Regression Versus the High-Dimensional Propensity Score for Confounding Adjustment in Secondary Database Analyses.,"Selection and measurement of confounders is critical for successful adjustment in nonrandomized studies. Although the principles behind confounder selection are now well established, variable selection for confounder adjustment remains a difficult problem in practice, particularly in secondary analyses of databases. We present a simulation study that compares the high-dimensional propensity score algorithm for variable selection with approaches that utilize direct adjustment for all potential confounders via regularized regression, including ridge regression and lasso regression. Simulations were based on 2 previously published pharmacoepidemiologic cohorts and used the plasmode simulation framework to create realistic simulated data sets with thousands of potential confounders. Performance of methods was evaluated with respect to bias and mean squared error of the estimated effects of a binary treatment. Simulation scenarios varied the true underlying outcome model, treatment effect, prevalence of exposure and outcome, and presence of unmeasured confounding. Across scenarios, high-dimensional propensity score approaches generally performed better than regularized regression approaches. However, including the variables selected by lasso regression in a regular propensity score model also performed well and may provide a promising alternative variable selection method.",2015,American journal of epidemiology
Principled estimation of regression discontinuity designs with covariates: a machine learning approach,"The regression discontinuity design (RDD) has become the ""gold standard"" for causal inference with observational data. Local average treatment effects (LATE) for RDDs are often estimated using local linear regressions with pre-treatment covariates typically added to increase the efficiency of treatment effect estimates, but their inclusion can have large impacts on LATE point estimates and standard errors, particularly in small samples. In this paper, I propose a principled, efficiency-maximizing approach for covariate adjustment of LATE in RDDs. This approach allows researchers to combine context-specific, substantive insights with automated model selection via a novel adaptive lasso algorithm. When combined with currently existing robust estimation methods, this approach improves the efficiency of LATE RDD with pre-treatment covariates. The approach will be implemented in a forthcoming R package, AdaptiveRDD which can be used to estimate and compare treatment effects generated by this approach with extant approaches.",2019,arXiv: Applications
Prediction of genomic breeding values for reproductive traits in Nellore heifers.,"The objective of this study was to assess the accuracy of genomic predictions for female reproductive traits in Nellore cattle. A total of 1853 genotyped cows and 305,348 SNPs were used for genomic selection analyses. GBLUP, BAYESCÏ€, and IBLASSO were applied to estimate SNP effects. The pseudo-phenotypes used as dependent variables were: observed phenotype (PHEN), adjusted phenotype (CPHEN), estimated breeding value (EBV), and deregressed estimated breeding value (DEBV). Predictive abilities were assessed by the average correlation between CPHEN and genomic estimated breeding value (GEBV) and by the average correlation between DEBV and GEBV in the validation population. Regression coefficients of pseudo-phenotypes on GEBV in the validation population were indicators of prediction bias in GEBV. BAYESCÏ€ showed higher predictive ability to estimate SNP effects and GEBV for all traits.",2019,Theriogenology
Unravelling the effect of flow regime on macroinvertebrates and benthic algae in regulated versus unregulated streams,"Ecohydrology. 2018;11:e1996. https://doi.org/10.1002/eco.1996 Abstract Variability in riverine flow regimes is important for aquatic biodiversity. However, across the globe, management of water resources has altered natural flow dynamics. We explored relationships between flow regime (calculated from 3 years' daily averaged discharge), and water chemistry, benthic algae, as well as macroinvertebrate datasets from 64 sites across Germany and Norway. To deal with multicollinearity while maintaining interpretability, we performed principal component (PC) analyses for each dataset in each country and selected the metric with the highest absolute loading on each PC to represent that PC. We then used L1â€regularized (lasso) regression to link differences in water chemistry and hydrology to differences in ecology and compared this approach with the more popular bestâ€subsets ordinary least squares (OLS) regression. The results obtained using lasso regression were broadly comparable to those produced by bestâ€subsets OLS, but the lasso approach â€œrejectedâ€ more models than the bestâ€subsets approach. When lasso identified a plausible model, it was the same or similar to the best model found by bestâ€subsets OLS. The lasso method was more â€œdiscerningâ€, that is, it identified a smaller number of potentially interesting models, whereas bestâ€subsets regression seemed to find â€œtoo manyâ€ relationships. We identified two response variables that were potentially affected by regulation: (a) River regulation may lead to higher cyanobacterial abundance, possibly via a less variable flow regime; (b) reduced flow variability may lead to a reduced proportion of grazers and scrapers, possibly indicating a shift towards an increased importance of heterotrophic energy sources in ecosystems with less variable flows.",2018,Ecohydrology
Development and validation of an ultrasound-based nomogram to improve the diagnostic accuracy for malignant thyroid nodules,"ObjectivesThe aim of this study was to develop an ultrasound-based nomogram to improve the diagnostic accuracy of the identification of malignant thyroid nodules.MethodsA total of 1675 histologically proven thyroid nodules (1169 benign, 506 malignant) were included in this study. The nodules were grouped into the training dataset (n = 700), internal validation dataset (n = 479), or external validation dataset (n = 496). The grayscale ultrasound features included the nodule size, shape, aspect ratio, echogenicity, margins, and calcification pattern. We applied least absolute shrinkage and selection operator (lasso) regression to select the strongest features for the nomogram. Nomogram discrimination (area under the receiver operating characteristic curve, AUC) and calibration were assessed. The nomogram was subjected to bootstrapping validation (1000 bootstrap resamples) to calculate a mean AUC and 95% confidence interval (CI).ResultsThe nomogram showed good discrimination in the training dataset, with an AUC of 0.936 (95% CI: 0.918â€“0.953) and good calibration. Application of the nomogram to the internal validation dataset also resulted in good discrimination (AUC: 0.935; 95% CI, 0.915â€“0.954) and good calibration. The model tested in an external validation dataset demonstrated a lower AUC of 0.782 (95% CI: 0.776â€“0.789).ConclusionsThis ultrasound-based nomogram can be used to quantify the probability of malignant thyroid nodules.Key Pointsâ€¢ Ultrasound examination is helpful in the differential diagnosis of malignant and benign thyroid nodules.â€¢ However, ultrasound accuracy relies heavily on examiner experience.â€¢ A less subjective diagnostic model is desired, and the developed nomogram for thyroid nodules showed good discrimination and good calibration.",2018,European Radiology
Stability Feature Selection using Cluster Representative LASSO,"Variable selection in high dimensional regression problems with strongly correlated variables or with near 
 
linear dependence among few variables remains one of the most important issues. We propose to cluster the 
 
variables first and then do stability feature selection using Lasso for cluster representatives. The first step 
 
involves generation of groups based on some criterion and the second step mainly performs group selection 
 
with controlling the number of false positives. Thus, our primary emphasis is on controlling type-I error for 
 
group variable selection in high-dimensional regression setting. We illustrate the method using simulated and 
 
pseudo-real data, and we show that the proposed method finds an optimal and consistent solution.",2016,
Hierarchical Bayesian LASSO for a negative binomial regression,"ABSTRACT Numerous researches have been carried out to explain the relationship between the count data y and numbers of covariates x through a generalized linear model (GLM). This paper proposes a hierarchical Bayesian least absolute shrinkage and selection operator (LASSO) solution using six different prior models to the negative binomial regression. Latent variables Z have been introduced to simplify the GLM to a standard linear regression model. The proposed models regard two conjugate zero-mean Normal priors for the regression parameters and three independent priors for the variance: the Exponential, Inverse-Gamma and Scaled Inverse- distributions. Different types of priors result in different amounts of shrinkage. A Metropolisâ€“Hastings-within-Gibbs algorithm is used to compute the posterior distribution of the parameters of interest through a data augmentation process. Based on the posterior samples, an original double likelihood ratio test statistic have been proposed to choose the most relevant covariates and shrink the insignificant coefficients to zero. Numerical experiments on a real-life data set prove that Bayesian LASSO methods achieved significantly better predictive accuracy and robustness than the classical maximum likelihood estimation and the standard Bayesian inference.",2016,Journal of Statistical Computation and Simulation
Predictive Abilities of Machine Learning Techniques May Be Limited by Dataset Characteristics: Insights From the UNOS Database.,"BACKGROUND
Traditional statistical approaches to prediction of outcomes have drawbacks when applied to large clinical databases. It is hypothesized that machine learning methodologies might overcome these limitations by considering higher-dimensional and nonlinear relationships among patient variables.


METHODS AND RESULTS
The Unified Network for Organ Sharing (UNOS) database was queried from 1987 to 2014 for adult patients undergoing cardiac transplantation. The dataset was divided into 3 time periods corresponding to major allocation adjustments and based on geographic regions. For our outcome of 1-year survival, we used the standard statistical methods logistic regression, ridge regression, and regressions with LASSO (least absolute shrinkage and selection operator) and compared them with the machine learning methodologies neural networks, naÃ¯ve-Bayes, tree-augmented naÃ¯ve-Bayes, support vector machines, random forest, and stochastic gradient boosting. Receiver operating characteristic curves and C-statistics were calculated for each model. C-Statistics were used for comparison of discriminatory capacity across models in the validation sample. After identifying 56,477 patients, the major univariate predictors of 1-year survival after heart transplantation were consistent with earlier reports and included age, renal function, body mass index, liver function tests, and hemodynamics. Advanced analytic models demonstrated similarly modest discrimination capabilities compared with traditional models (C-statistic â‰¤0.66, all). The neural network model demonstrated the highest C-statistic (0.66) but this was only slightly superior to the simple logistic regression, ridge regression, and regression with LASSO models (C-statisticâ€¯=â€¯0.65, all). Discrimination did not vary significantly across the 3 historically important time periods.


CONCLUSIONS
The use of advanced analytic algorithms did not improve prediction of 1-year survival from heart transplant compared with more traditional prediction models. The prognostic abilities of machine learning techniques may be limited by quality of the clinical dataset.",2019,Journal of cardiac failure
Prescription-drug-related risk in driving: comparing conventional and lasso shrinkage logistic regressions.,"BACKGROUND
Large data sets with many variables provide particular challenges when constructing analytic models. Lasso-related methods provide a useful tool, although one that remains unfamiliar to most epidemiologists.


METHODS
We illustrate the application of lasso methods in an analysis of the impact of prescribed drugs on the risk of a road traffic crash, using a large French nationwide database (PLoS Med 2010;7:e1000366). In the original case-control study, the authors analyzed each exposure separately. We use the lasso method, which can simultaneously perform estimation and variable selection in a single model. We compare point estimates and confidence intervals using (1) a separate logistic regression model for each drug with a Bonferroni correction and (2) lasso shrinkage logistic regression analysis.


RESULTS
Shrinkage regression had little effect on (bias corrected) point estimates, but led to less conservative results, noticeably for drugs with moderate levels of exposure. Carbamates, carboxamide derivative and fatty acid derivative antiepileptics, drugs used in opioid dependence, and mineral supplements of potassium showed stronger associations.


CONCLUSION
Lasso is a relevant method in the analysis of databases with large number of exposures and can be recommended as an alternative to conventional strategies.",2012,Epidemiology
A Four-Pseudogene Classifier Identified by Machine Learning Serves as a Novel Prognostic Marker for Survival of Osteosarcoma,"Osteosarcoma is a common malignancy with high mortality and poor prognosis due to lack of predictive markers. Increasing evidence has demonstrated that pseudogenes, a type of non-coding gene, play an important role in tumorigenesis. The aim of this study was to identify a prognostic pseudogene signature of osteosarcoma by machine learning. A sample of 94 osteosarcoma patients' RNA-Seq data with clinical follow-up information was involved in the study. The survival-related pseudogenes were screened and related signature model was constructed by cox-regression analysis (univariate, lasso, and multivariate). The predictive value of the signature was further validated in different subgroups. The putative biological functions were determined by co-expression analysis. In total, 125 survival-related pseudogenes were identified and a four-pseudogene (RPL11-551L14.1, HR: 0.65 (95% CI: 0.44-0.95); RPL7AP28, HR: 0.32 (95% CI: 0.14-0.76); RP4-706A16.3, HR: 1.89 (95% CI: 1.35-2.65); RP11-326A19.5, HR: 0.52(95% CI: 0.37-0.74)) signature effectively distinguished the high- and low-risk patients, and predicted prognosis with high sensitivity and specificity (AUC: 0.878). Furthermore, the signature was applicable to patients of different genders, ages, and metastatic status. Co-expression analysis revealed the four pseudogenes are involved in regulating malignant phenotype, immune, and DNA/RNA editing. This four-pseudogene signature is not only a promising predictor of prognosis and survival, but also a potential marker for monitoring therapeutic schedule. Therefore, our findings may have potential clinical significance.",2019,Genes
Single-trial Connectivity Estimation through the Least Absolute Shrinkage and Selection Operator,"Methods based on the use of multivariate autoregressive models (MVAR) have proved to be an accurate tool for the estimation of functional links between the activity originated in different brain regions. A well-established method for the parameters estimation is the Ordinary Least Square (OLS) approach, followed by an assessment procedure that can be performed by means of Asymptotic Statistic (AS). However, the performances of both procedures are strongly influenced by the number of data samples available, thus limiting the conditions in which brain connectivity can be estimated. The aim of this paper is to introduce and test a regression method based on Least Absolute Shrinkage and Selection Operator (LASSO) to broaden the estimation of brain connectivity to those conditions in which current methods fail due to the limited data points available. We tested the performances of the LASSO regression in a simulation study under different levels of data points available, in comparison with a classical approach based on OLS and AS. Then, the two methods were applied to real electroencephalographic (EEG) signals, recorded during a motor imagery task. The simulation study and the application to real EEG data both indicated that LASSO regression provides better performances than the currently used methodologies for the estimation of brain connectivity when few data points are available. This work paves the way to the estimation and assessment of connectivity patterns with limited data amount and in on-line settings.",2019,2019 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)
Accuracy of genomic prediction using RR-BLUP and Bayesian LASSO,"We compared the accuracies of two genomic-selection prediction methods as affected by marker density and quantitative trait locus (QTL) number. Methods used to derive genomic estimated breeding values (GEBV) were random regression best linear unbiased prediction (RRâ€“BLUP) and a Bayesian LASSO (Least Absolute Shrinkage and Selection Operator). In this study the genome comprised four chromosomes of 250 cM each. Also considering the number of markers 1000, 2000 and 5000 and the number of QTLs 4, 10, 20 and 40 and heritability of 5, 10 and 25 percent were compared.. In all scenarios Bayesian LASSO was more accurate than RR-BLUP, also increasing the number of QTLs, the evaluation accuracy decreases slightly which this reduction is greater in the lower heritability. The correlation between true breeding value and the genomic estimated breeding value in target generations applying RR-BLUP and Bayesian LASSO decreased from 0.918 to 0.807 and 0.933 to 0.847 respectively.",2013,European Journal of Experimental Biology
Bayesian quantile regression using the skew exponential power distribution,"Traditonal Bayesian quantile regression relies on the Aymmetric Laplace Distribution (ALD) due primarily due to its satisfactory empirical and theoretical perforances. However, the ALD dispalys medium tails and it is not suitable for data characterized by strong deviations from the Gaussian hypotesis. In this paper we propose and extension of the ALD Bayesian quantile regression framework to account for fat tails using the Skew Exponential PowerÂ distribution (SEP). Linear and Additive model (AM) with penalized splines are used to show the felxibility of the SEP in the Bayesian quantile regression context. Lasso priors are used to acount for the problem of shrinking parameters when the parameters space becames wide. We propose a new adaptive Metropolis-Hastings algorithm in the linear model, and an adaptive Metropolis withing Gibbs one in the AM framework. Empirical evidence of the statistical properties of the model is provided through several examples based on both simulated and real data sets.",2018,Comput. Stat. Data Anal.
Estimation and HAC-based Inference for Machine Learning Time Series Regressions,"Time series regression analysis in econometrics typically involves a framework relying on a set of mixing conditions to establish consistency and asymptotic normality of parameter estimates and HAC-type estimators of the residual long-run variances to conduct proper inference. This article introduces structured machine learning regressions for high-dimensional time series data using the aforementioned commonly used setting. To recognize the time series data structures we rely on the sparse-group LASSO estimator. We derive a new Fuk-Nagaev inequality for a class of $\tau$-dependent processes with heavier than Gaussian tails, nesting $\alpha$-mixing processes as a special case, and establish estimation, prediction, and inferential properties, including convergence rates of the HAC estimator for the long-run variance based on LASSO residuals. An empirical application to nowcasting US GDP growth indicates that the estimator performs favorably compared to other alternatives and that the text data can be a useful addition to more traditional numerical data.",2019,arXiv: Econometrics
Application of shrinkage techniques in logistic regression analysis: a case study,"Logistic regression analysis may well be used to develop a predictive model for a dichotomous medical outcome, such as shortâ€term mortality. When the data set is small compared to the number of covariables studied, shrinkage techniques may improve predictions. We compared the performance of three variants of shrinkage techniques: 1) a linear shrinkage factor, which shrinks all coefficients with the same factor; 2) penalized maximum likelihood (or ridge regression), where a penalty factor is added to the likelihood function such that coefficients are shrunk individually according to the variance of each covariable; 3) the Lasso, which shrinks some coefficients to zero by setting a constraint on the sum of the absolute values of the coefficients of standardized covariables. Logistic regression models were constructed to predict 30â€day mortality after acute myocardial infarction. Small data sets were created from a large randomized controlled trial, half of which provided independent validation data. We found that all three shrinkage techniques improved the calibration of predictions compared to the standard maximum likelihood estimates. This study illustrates that shrinkage is a valuable tool to overcome some of the problems of overfitting in medical data.",2001,Statistica Neerlandica
Sharp Threshold Detection Based on Sup-norm Error rates in High-dimensional Models,"We propose a new estimator, the thresholded scaled Lasso, in high-dimensional threshold regressions. First, we establish an upper bound on the lâˆž estimation error of the scaled Lasso estimator of Lee, Seo, and Shin. This is a nontrivial task as the literature on high-dimensional models has focused almost exclusively on l1 and l2 estimation errors. We show that this sup-norm bound can be used to distinguish between zero and nonzero coefficients at a much finer scale than would have been possible using classical oracle inequalities. Thus, our sup-norm bound is tailored to consistent variable selection via thresholding. Our simulations show that thresholding the scaled Lasso yields substantial improvements in terms of variable selection. Finally, we use our estimator to shed further empirical light on the long-running debate on the relationship between the level of debt (public and private) and GDP growth. Supplementary materials for this article are available online.",2015,Journal of Business & Economic Statistics
Outcomes and predictors of response in steroid-refractory acute graft-versus-host disease: single-center results from a cohort of 203 patients.,"The prognosis of steroid-refractory acute graft-versus-host disease (aGVHD) is poor and predictors of response and survival are unclear. In an exploratory analysis of 203 steroid-refractory aGVHD patients with prospectively collected GVHD data who received antithymocyte globulin, etanercept, or mycophenolate mofetil (MMF) as second-line treatment, we determined the predictors of day 28 response, 2-year overall survival (OS), and 2-year non-relapse mortality (NRM). To minimize the risk of finding false positive results, we used lasso regression, aggressively eliminating variables that are unlikely to be associated with outcome. Day 28 response to second-line therapy was 38% (complete response [CR] 23%), with a 2-year OS of 25% and a 2-year NRM of 62%. Factors associated with response were GVHD prophylaxis, organ involvement, and initial aGVHD to steroid-refractory aGVHD interval. Specifically, compared with cyclosporine (CsA)/MMF as GVHD prophylaxis, the odds ratio (OR) for calcineurin inhibitor/methotrexate was 0.8 and for CsA/prednisone was 0.6. The OR for aGVHD to steroid-refractory aGVHD interval â‰¥14 vs. <14 days was 1.3. The ORs for skin only involvement and gut or liver only involvement when compared with multi-organ involvement were 1.4 and 1.2, respectively. The only variable associated with worse survival was age, with a hazard ratio (HR) per decade of 1.04 for overall mortality. Similarly, age was the only variable associated with NRM (HR 1.02 per decade). When compared with CR, no response at day 28 increased the risk of death (HR: 2.4, 95% confidence interval: 1.5-3.7). In conclusion, using an underutilized statistical technique in the field of transplantation, we identified predictors of response and survival in steroid-refractory aGVHD. Our results highlight the importance of developing novel treatment strategies as current treatments yield poor outcomes.",2019,Biology of blood and marrow transplantation : journal of the American Society for Blood and Marrow Transplantation
FMRI group studies of brain connectivity via a group robust Lasso,"Inferring effective brain connectivity from neuroimaging data such as functional Magnetic Resonance Imaging (fMRI) has been attracting increasing interest due to its critical role in understanding brain functioning. Incorporating sparsity into connectivity modeling to make models more biologically realistic and performing group analysis to deal with inter-subject variability are still challenges associated with fMRI brain connectivity modeling. To address the above two crucial challenges, the attractive computational and theoretical properties of the least absolute shrinkage and selection operator (LASSO) in sparse linear regression provide a suitable starting point. We propose a group robust LASSO (grpRLASSO) model by combining advantages of the popular group-LASSO and our recently developed robust-LASSO. Here group analysis is formulated as a grouped variable selection procedure. Superior performance of the proposed grpRLASSO in terms of group selection and robustness is demonstrated by simulations with large noise variance. The grpRLASSO is also applied to a real fMRI data set for brain connectivity study in Parkinson's disease, resulting in biologically plausible networks.",2010,2010 IEEE International Conference on Image Processing
