title,abstract,year,journal
"Best Subset, Forward Stepwise, or Lasso? Analysis and Recommendations Based on Extensive Comparisons","In exciting new work, Bertsimas et al. (2016) showed that the classical best-subset selection problem in regression modeling can be formulated as a mixed integer optimization (MIO) problem. Using recent advances in MIO algorithms, they demonstrated that bestsubset selection can now be solved at much larger problem sizes than what was thought possible in the statistics community. They presented empirical comparisons of best-subset with other popular variable selection procedures, in particular, the lasso and forward-stepwise selection. Surprisingly (to us), their simulations suggested that best subset consistently outperformed both methods in terms of prediction accuracy. Here we present an expanded set of simulations to shed more light on these comparisons. The summary is roughly as follows: â€¢ neither best subset nor the lasso uniformly dominate the other, with best Depts of Statistics and Biomedical Data Science, Stanford University (e-mail: hastie@stanford.edu; tibs@stanford.edu). Depts. of Statistics and Machine Learning, Carnegie-Mellon University (e-mail: ryantibs@stat.cmu.edu; url: ). âˆ—AMS 2010 subject classifications. Primary 62J05; secondary 62J07 1 imsart-sts ver. 2014/10/16 file: paper.tex date: June 26, 2019 2 HASTIE ET. AL. subset generally performing better in very high signal-to-noise (SNR) ratio regimes, and the lasso better in low SNR regimes; â€¢ for a large proportion of the settings considered, best subset and forward stepwise perform similarly, but in certain cases in the high SNR regime, best subset performs better. â€¢ forward-stepwise and best-subsets tend to yield sparser models, especially in the high SNR regime â€¢ the relaxed lasso (actually, a simplified version of the original relaxed estimator defined in Meinshausen 2007) is the overall winner, performing just about as well as the lasso in low-SNR scenarios, and nearly as well as best subset in high-SNR scenarios.",2018,
SÃ©lection de variables pour des processus ponctuels spatiaux,"Les applications recentes telles que les bases de donnees forestieres impliquent des observations de donnees spatiales associees a l'observation de nombreuses covariables spatiales. Nous considerons dans cette these le probleme de l'estimation d'une forme parametrique de la fonction d'intensite dans un tel contexte. Cette these developpe les procedures de selection des variables et donne des garanties quant a leur validite. En particulier, nous proposons deux approches differentes pour la selection de variables : les methodes de type lasso et les procedures de type Selecteur de Dantzig. Pour les methodes envisageant les techniques de type lasso, nous derivons les proprietes asymptotiques des estimations obtenues par les fontions d'estimation derivees par les vraisemblances de la Poisson et de la regression logistique penalisees par une grande classe de penalites. Nous prouvons que les estimations obtenues par de ces procedures satisfont la consistance, sparsite et la normalite asymptotique. Pour la partie selecteur de Dantzig, nous developpons une version modifiee du selecteur de Dantzig, que nous appelons le selecteur Dantzig lineaire adaptatif (ALDS), pour obtenir les estimations d'intensite. Plus precisement, les estimations ALDS sont definies comme la solution a un probleme d'optimisation qui minimise la somme des coefficients des estimations soumises a une approximation lineaire du vecteur score comme une contrainte. Nous constatons que les estimations obtenues par de ces methodes ont des proprietes asymptotiques semblables a celles proposees precedemment a l'aide de methode regularisation du lasso adaptatif. Nous etudions les aspects computationnels des methodes developpees en utilisant les procedures de type lasso et de type Selector Dantzig. Nous etablissons des liens entre l'estimation de l'intensite des processus ponctuels spatiaux et les modeles lineaires generalises (GLM), donc nous n'avons qu'a traiter les procedures de la selection des variables pour les GLM. Ainsi, des procedures de calcul plus faciles sont implementees et un algorithme informatique rapide est propose. Des etudes de simulation sont menees pour evaluer les performances des echantillons finis des estimations de chacune des deux approches proposees. Enfin, nos methodes sont appliquees pour modeliser les emplacements spatiaux, une espece d'arbre dans la foret observee avec un grand nombre de facteurs environnementaux.",2017,
sealasso: an R package for Standard Error Adjusted Adaptive Lasso (SEA-lasso),"inuence its variable selection performance. Qian and Yang (2010) proposed two versions of the adaptive lasso named SEA-lasso and NSEA-lasso that incorporate the OLS-standard errors to the l1 penalty term. It is shown that they outperform the usually used weight with OLS estimate only (we call the latter weight selection method OLS-adaptive lasso), especially when the condition index of the model matrix is large. In practice, it is advisable that NSEA-lasso is used when the condition index is greater than 10 (more specically, the condition index is dened as the natural logarithm of the ratio of the largest eigenvalue to the smallest eigenvalue for the matrix X T X, where X is the scaled predictor matrix). In fact, NSEA-lasso is the default method in sealasso package. It should be pointed out that this package applies only to linear regression setting, and the number of predictors must be greater than 1 and less than the sample size. The rest of this vignette will use diabetes data example to introduce the two functions sealasso and summary provided in this package.",2013,
High-Dimensional $L_2$Boosting: Rate of Convergence,"Boosting is one of the most significant developments in machine learning. This paper studies the rate of convergence of $L_2$Boosting, which is tailored for regression, in a high-dimensional setting. Moreover, we introduce so-called \textquotedblleft post-Boosting\textquotedblright. This is a post-selection estimator which applies ordinary least squares to the variables selected in the first stage by $L_2$Boosting. Another variant is \textquotedblleft Orthogonal Boosting\textquotedblright\ where after each step an orthogonal projection is conducted. We show that both post-$L_2$Boosting and the orthogonal boosting achieve the same rate of convergence as LASSO in a sparse, high-dimensional setting. We show that the rate of convergence of the classical $L_2$Boosting depends on the design matrix described by a sparse eigenvalue constant. To show the latter results, we derive new approximation results for the pure greedy algorithm, based on analyzing the revisiting behavior of $L_2$Boosting. We also introduce feasible rules for early stopping, which can be easily implemented and used in applied work. Our results also allow a direct comparison between LASSO and boosting which has been missing from the literature. Finally, we present simulation studies and applications to illustrate the relevance of our theoretical results and to provide insights into the practical aspects of boosting. In these simulation studies, post-$L_2$Boosting clearly outperforms LASSO.",2016,arXiv: Machine Learning
Low noise sensitivity analysis of Lq-minimization in oversampled systems,"The class of Lq-regularized least squares (LQLS) are considered for estimating a p-dimensional vector \b{eta} from its n noisy linear observations y = X\b{eta}+w. The performance of these schemes are studied under the high-dimensional asymptotic setting in which p grows linearly with n. In this asymptotic setting, phase transition diagrams (PT) are often used for comparing the performance of different estimators. Although phase transition analysis is shown to provide useful information for compressed sensing, the fact that it ignores the measurement noise not only limits its applicability in many application areas, but also may lead to misunderstandings. For instance, consider a linear regression problem in which n > p and the signal is not exactly sparse. If the measurement noise is ignored in such systems, regularization techniques, such as LQLS, seem to be irrelevant since even the ordinary least squares (OLS) returns the exact solution. However, it is well-known that if n is not much larger than p then the regularization techniques improve the performance of OLS. In response to this limitation of PT analysis, we consider the low-noise sensitivity analysis. We show that this analysis framework (i) reveals the advantage of LQLS over OLS, (ii) captures the difference between different LQLS estimators even when n > p, and (iii) provides a fair comparison among different estimators in high signal-to-noise ratios. As an application of this framework, we will show that under mild conditions LASSO outperforms other LQLS even when the signal is dense. Finally, by a simple transformation we connect our low-noise sensitivity framework to the classical asymptotic regime in which n/p goes to infinity and characterize how and when regularization techniques offer improvements over ordinary least squares, and which regularizer gives the most improvement when the sample size is large.",2017,ArXiv
"Predictive sparse modeling of fMRI data for improved classification, regression, and visualization using the k-support norm","We explore various sparse regularization techniques for analyzing fMRI data, such as the â„“1 norm (often called LASSO in the context of a squared loss function), elastic net, and the recently introduced k-support norm. Employing sparsity regularization allows us to handle the curse of dimensionality, a problem commonly found in fMRI analysis. In this work we consider sparse regularization in both the regression and classification settings. We perform experiments on fMRI scans from cocaine-addicted as well as healthy control subjects. We show that in many cases, use of the k-support norm leads to better predictive performance, solution stability, and interpretability as compared to other standard approaches. We additionally analyze the advantages of using the absolute loss function versus the standard squared loss which leads to significantly better predictive performance for the regularization methods tested in almost all cases. Our results support the use of the k-support norm for fMRI analysis and on the clinical side, the generalizability of the I-RISA model of cocaine addiction.",2015,Computerized medical imaging and graphics : the official journal of the Computerized Medical Imaging Society
Configurations of Wardrop's equilibrium and application to traffic analysis,"This thesis consists of two parts, and the connection between them is the so-called Wardropâ€™s equilibrium. In the first part of this thesis, which is the theoretical part, we study the congested transport dynamics arising from a non-autonomous traffic optimization problem. In this setting, we prove one can find an optimal traffic strategy with support on the trajectories of a DiPerna-Lions flow. The proof follows the scheme introduced by Brasco, Carlier, and Santambrogio in the autonomous setting, applied to the case of supercritical Sobolev dependence in the spatial variable. This requires both Lipschitz and weighted Sobolev apriori bounds for the minimizers of a class of integral functionals whose ellipticity bounds are satisfied only away from a ball of the gradient variable. We are then able to find the configuration of Wardropâ€™s equilibrium. In the second part of this thesis, which is the practical part, we use the established Wardropâ€™s equilibrium in the theoretical section, in order to optimize the traffic problem in the real-life application. New OD demand problem formulation is explored which allows the modeller to define structural similarity between the historical and estimated OD matrix while ensuring computationally fast and tractable solution. Shrinkage regression methods, such as Ridge and Lasso regression, are proposed to define distance function between historical and estimated OD matrix, in order to minimize estimation variance, and ensure the estimated OD matrix is close to true value. The presented OD estimation models reduce the dimensionality of the OD demand vector, which is crucial when the dimensionality of OD matrix is high, due to a high level of the zoning system. A new solution approach based on the well-known gradient descent algorithm is applied to solve the proposed models. Finally, results are tested out on a real life-size network.",2018,
Variable Selection in Bayesian Maximum Entropy 1 Quantile Regression 2,"4 Quantile regression has gained increasing popularity as it provides richer information than 5 the regular mean regression, and variable selection plays an important role in quantile regression 6 model building process, as it can improve the prediction accuracy by choosing an appropriate 7 subset of regression predictors. Unlike the traditional quantile regression, we consider the quan8 tile as an unknown parameter and estimate it jointly with other regression coefficients. In 9 particular, we consider the maximum entropy quantile regression and apply the Bayesian adap10 tive Lasso to it. A flat prior is put on the quantile parameter due to the lack of information 11 on it. Our proposed method not only addresses the problem about which quantile would be 12 the most probable one among all the candidates, but also reflects the inner relationship of the 13 data through the estimated quantile. We develop an efficient Gibbs sampler algorithm and show 14 that the performance of our proposed method is better than the Bayesian adaptive Lasso and 15 Bayesian Lasso through simulation studies and real data analysis. 16",2016,
Data-Driven Identification of Parametric Partial Differential Equations,"In this work we present a data-driven method for the discovery of parametric partial differential equations (PDEs), thus allowing one to disambiguate between the underlying evolution equations and their parametric dependencies. Group sparsity is used to ensure parsimonious representations of observed dynamics in the form of a parametric PDE, while also allowing the coefficients to have arbitrary time series, or spatial dependence. This work builds on previous methods for the identification of constant coefficient PDEs, expanding the field to include a new class of equations which until now have eluded machine learning based identification methods. We show that group sequentially thresholded ridge regression outperforms group LASSO in identifying the fewest terms in the PDE along with their parametric dependency. The method is demonstrated on four canonical models with and without the introduction of noise.",2019,SIAM J. Applied Dynamical Systems
Combined registration and motion correction of longitudinal retinal OCT data,"Optical coherence tomography (OCT) has become an important modality for examination of the eye. To measure layer thicknesses in the retina, automated segmentation algorithms are often used, producing accurate and reliable measurements. However, subtle changes over time are difficult to detect since the magnitude of the change can be very small. Thus, tracking disease progression over short periods of time is difficult. Additionally, unstable eye position and motion alter the consistency of these measurements, even in healthy eyes. Thus, both registration and motion correction are important for processing longitudinal data of a specific patient. In this work, we propose a method to jointly do registration and motion correction. Given two scans of the same patient, we initially extract blood vessel points from a fundus projection image generated on the OCT data and estimate point correspondences. Due to saccadic eye movements during the scan, motion is often very abrupt, producing a sparse set of large displacements between successive B-scan images. Thus, we use lasso regression to estimate the movement of each image. By iterating between this regression and a rigid point-based registration, we are able to simultaneously align and correct the data. With longitudinal data from 39 healthy control subjects, our method improves the registration accuracy by 43% compared to simple alignment to the fovea and 8% when using point-based registration only. We also show improved consistency of repeated total retina thickness measurements.",2016,
The degrees of freedom of the Lasso for general design matrix,"In this paper, we investigate the degrees of freedom ($\dof$) of penalized $\ell_1$ minimization (also known as the Lasso) for linear regression models. We give a closed-form expression of the $\dof$ of the Lasso response. Namely, we show that for any given Lasso regularization parameter $\lambda$ and any observed data $y$ belonging to a set of full (Lebesgue) measure, the cardinality of the support of a particular solution of the Lasso problem is an unbiased estimator of the degrees of freedom. This is achieved without the need of uniqueness of the Lasso solution. Thus, our result holds true for both the underdetermined and the overdetermined case, where the latter was originally studied in \cite{zou}. We also show, by providing a simple counterexample, that although the $\dof$ theorem of \cite{zou} is correct, their proof contains a flaw since their divergence formula holds on a different set of a full measure than the one that they claim. An effective estimator of the number of degrees of freedom may have several applications including an objectively guided choice of the regularization parameter in the Lasso through the $\sure$ framework. Our theoretical findings are illustrated through several numerical simulations.",2011,arXiv: Statistics Theory
Analysis of genotype by methylation interactions through sparsity-inducing regularized regression,"In this paper, we consider the use of the least absolute shrinkage and selection operator (LASSO)-type regression techniques to detect important genetic or epigenetic loci in genome-wide association studies (GWAS) and epigenome-wide association studies (EWAS). We demonstrate how these techniques can be adapted to provide quantifiable uncertainty using stability selection, including explicit control of the family-wise error rate. We also consider variants of the LASSO, such as the group LASSO, to study genetic and epigenetic interactions. We use these techniques to reproduce some existing results on the Genetics of Lipid Lowering Drugs and Diet Network (GOLDN) data set, which collects from 991 individuals blood triglyceride and differential methylation at 464,000 cytosine-phosphate-guanine (CpG) sites and 761,000 single-nucleotide polymorphisms (SNPs), and to identify new research directions. Epigenome-wide and genome-wide models based on the LASSO are considered, as well as an interaction model limited to chromosome 11. The analyses replicate findings concerning 2 CpGs in carnitine palmitoyltransferase 1A (CPT1A). Some suggestions are made regarding potentially interesting directions for the analysis of genetic and epigenetic interactions.",2018,BMC Proceedings
Multiple Change-Points Estimation in Linear Regression Models via Sparse Group Lasso,"We consider linear regression problems for which the underlying model undergoes multiple changes. Our goal is to estimate the number and locations of change-points that segment available data into different regions, and further produce sparse and interpretable models for each region. To address challenges of the existing approaches and to produce interpretable models, we propose a sparse group Lasso based approach for linear regression problems with change-points. Under certain mild assumptions and a properly chosen regularization term, we prove that the solution of the proposed approach is asymptotically consistent. In particular, we show that the estimation error of linear coefficients diminishes, and the locations of the estimated change-points are close to those of true change-points. We further propose a method to choose the regularization term so that the results mentioned above hold. In addition, we show that the complexity of the proposed algorithm is much smaller than those of existing approaches. Numerical examples are provided to validate the analytical results.",2015,IEEE Transactions on Signal Processing
"A Comparison of Lasso , Elastic Net , Ridge Regression , and Bayesian Methods for Large-scale Regression","Large-scale regression problems, where the number of predictors is large compared with the number of observations, arise in many practical applications. Commonly-used methods for this problem can be divided into two types: Likelihood-penalization techniques, which use penalties on the regression coefficients to shrink them, and Bayesian approaches which achieve the same goal by using sparse or shrinkage priors. Despite the fact that these different types of approaches aim to tackle the same problem, and have been around for nearly two decades, their performances have seldom been directly compared with one another. In this thesis, we compare prediction performance among three popular penalization techniques, i.e, the ridge regression (Hoerl and Kennard, 1970), the lasso (Tibshirani, 1996), the elastic net (Zou and Hastie, 2005), and a Bayesian approach, Bayesian Sparse Linear Mixed Models (BSLMM) (Zhou et al, 2012). In a simulation study, BSLMM consistently outperformed the other three methods in prediction accuracy. In a microarray classification study, BSLMM performed as well as the elastic net, which outperformed the other methods. For information about building access for persons with disabilities, please contact Matt Johnston at 773.7020541 or send an email to mhj@galton.uchicago.edu. If you wish to subscribe to our email list, please visit the following web site: https://lists.uchicago.edu/web/arc/statseminars.",2013,
Regularization Parameter Selection via Cross-Validation in the Presence of Dependent Regressors: A Simulation Study,"This letter reveals using simulation studies that regularization parameter selection via cross-validation (CV) in penalized regressions (e.g., Lasso) is valid even if the regressors are weakly dependent. In CV procedure, the time series structure of the data set is broken, meaning that there may occur a fatal problem unless the sample is i.i.d.; the estimation accuracy in the training step could be worse due to corruption of data continuity, which may furthermore lead to a bad choice of the regularization parameter. Even in such a situation, we find that CV works well as long as the sample size grows. These findings encourage us to apply the selection procedure via CV to macroeconomic empirical analyses with dependent regressors.",2015,Economics Bulletin
"Regional Climate Change: Downscaling, Prediction, and Impact Assessment","Although the issue of climate change is often dealt with in global perspective, the impact of climate change must be assessed at regional scales. While global climate models can provide projections of the average state of large-scale circulation of future climate, the downscaling of such projections to regional scale with improved spatial and temporal resolution for both the forcing fields and the climatic responses is the basis for assessing the societal impacts of climate change. Therefore, it is important to not only study climate change at the global scale but also study the regional manifestations of the climate system at spatial scales ranging from less than a hundred kilometers to thousands of kilometers with time scales from months to years to decades. This special issue publishes a collection of articles covering a wide range of topics of our understanding of â€œregional climateâ€ from downscaling the variability of extreme rainfall over the Yangtze River basin (T. Gao and L. Xie) and assessing the water resources in the Yellow River region (Z. Wu et al.) in China to forecasting the precipitation and water resources in the Lake Victoria region in East Africa (X. Sun et al., R. Argent et al., and K. A. Smith and F. H. M. Semazzi), from downscaling wind energy resources in the contiguous United States (B. Liu et al.) to characterizing the precipitation extremes in the Carpathian region in central and southern Europe (L. Gaal et al.), and from analyzing the energy balance in semiarid grasslands in China (Q. Jiang et al.) to detecting future climate change signals in central and eastern Europe from numerical model simulations (M. Belda et al.). This special issue also includes articles addressing the impacts of regional climate change on tropical cyclones over the Atlantic Ocean (K. Xie and B. Liu), on crop yields in North China (H. Liu et al.), and on litter production and nutrient dynamics in a plantation in China (X. Ge et al.), as well as rainfall and drought in Eastern Kenya (M. O. Kisaka et al.). Additionally, several articles with focus on regional climate downscaling methodologies are also included. S. Kim et al. studied the effects of geographic features in a mountainous area on the downscaling of global climate model data; T. R. Lee et al. demonstrated the feasibility of using PRISM (parameterelevation regression on independent slope model) to downscale maximum temperature to subkilometer scale; L. Gao et al. applied the LASSO algorithm to statistically downscale the ERA-interim precipitation forecast over complex terrain; and K.-H. Min and W.-Y. Sun explored the application of an atmosphere-cryosphere coupledmodel in regional climate applications. These articles reflect the recent advances and applications in â€œregional climate downscaling, prediction, and impact assessmentâ€ from a set of unique angles. We hope they are of interest to peers.",2015,Advances in Meteorology
A 3-mRNA-based prognostic signature of survival in oral squamous cell carcinoma,"Background
Oral squamous cell carcinoma (OSCC) is the most common type of head and neck squamous cell carcinoma with an unsatisfactory prognosis. The aim of this study was to identify potential prognostic mRNA biomarkers of OSCC based on analysis of The Cancer Genome Atlas (TCGA).


Methods
Expression profiles and clinical data of OSCC patients were collected from TCGA database. Univariate Cox analysis and the least absolute shrinkage and selection operator Cox (LASSO Cox) regression were used to primarily screen prognostic biomarkers. Then multivariate Cox analysis was performed to build a prognostic model based on the selected prognostic mRNAs. Nomograms were generated to predict the individual's overall survival at 3 and 5 years. The model performance was assessed by the time-dependent receiver operating characteristic (ROC) curve and calibration plot in both training cohort and validation cohort (GSE41613 from NCBI GEO databases). In addition, machine learning was used to assess the importance of risk factors of OSCC. Finally, in order to explore the potential mechanisms of OSCC, Kyoto Encyclopedia of Genes and Genomes (KEGG) pathway analysis was completed.


Results
Three mRNAs (CLEC3B, C6 and CLCN1) were finally identified as a prognostic biomarker pattern. The risk score was imputed as: (-0.38602 Ã— expression level of CLEC3B) + (-0.20632 Ã— expression level of CLCN1) + (0.31541 Ã— expression level of C6). In the TCGA training cohort, the area under the curve (AUC) was 0.705 and 0.711 for 3- and 5-year survival, respectively. In the validation cohort, AUC was 0.718 and 0.717 for 3- and 5-year survival. A satisfactory agreement between predictive values and observation values was demonstrated by the calibration curve in the probabilities of 3- and 5- year survival in both cohorts. Furthermore, machine learning identified the 3-mRNA signature as the most important risk factor to survival of OSCC. Neuroactive ligand-receptor interaction was most enriched mostly in KEGG pathway analysis.


Conclusion
A 3-mRNA signature (CLEC3B, C6 and CLCN1) successfully predicted the survival of OSCC patients in both training and test cohort. In addition, this signature was an independent and the most important risk factor of OSCC.",2019,PeerJ
Social Politics in Context: The Institutional Politics Theory and Social Spending at the End of the New Deal,"In this article, we develop an institutional politics theory of public social provision and examine U.S. social sp'ending programs at the end of the New Deal. This theory integrates key insights of institutional and political theories of social policy. Drawing on institutional arguments, our theory holds that the willingness or ability of prospending actors to promote social spending initiatives depends on institutional conditions, especially the extent of voting rights and the nature of political party systems. Furthermore, drawing on political arguments, the theory posits the importance of pro-spending actors, including progressive factions of political parties and organized challengers. To appraise the institutional politics theory, we analyze state-level outcomes for Old-Age Assistance pensions and Works Progress Administration wages, employing multiple regression and qualitative comparative analysis (QCA). All analyses support the institutional politics theory. Scholars often consider the U.S. exceptional in public social provision and its causes. The two major theories of public social provision - political and institutional - see the U.S. as peculiar. Political theorists often characterize U.S. political parties as nonideological and unlikely to propel public social provision. The same is true for the American labor movement, which is smaller and more divided than its Western European counterparts. For these reasons, political arguments explaining gains in U.S. public social spending often focus on challengers employing innovative forms of claims making. Institutional or statecentered theories portray U.S. state and political institutions as hindrances to * A previous version of this paper was presented at the 1993 annual meeting of the Eastern Sociological Society, Boston. Parts of this article were presented at the 1992 annual meeting of theAmerican SociologicalAssociation, Pittsburgh. Forhelpful comments on a previous draft of this paper, we thank Steven Barkan, Nancy X Cauthen, Jeff Goodwin, Brian Gran, James M.",1996,Social Forces
DÃ©tection de structures fines par traitement d'images et apprentissage statistique : application au contrÃ´le non destructif,"Dans cette these, nous presentons de nouvelles methodes de traitement dâ€™images pourextraire ou rehausser les elements fins dâ€™une image. Pour ces operateurs, issus de la morphologie mathematique,lâ€™accent a ete mis principalement sur la precision de detection et sur le temps de calcul,qui doivent etre optimises pour pouvoir repondre aux contraintes de temps imposees par differentesapplications industrielles. La premiere partie de ce memoire presente ces methodes, organisees enfonction de la tortuosite des objets a detecter. Nous commencons par proposer un algorithme rapidepour le calcul des ouvertures 1-D afin dâ€™extraire des structures rectilignes des images. Puis, nous etudionsune nouvelle classe dâ€™operateurs rapides avec les ouvertures parcimonieuses par chemins, permettantdâ€™analyser des structures ayant une tortuosite moderee. Enfin, nous proposons de nouveauxelements structurants adaptatifs et des filtres connexes construits avec des attributs geodesiques etgeometriques pour extraire des structures filiformes ayant une tortuosite quelconque.Dans un second temps, nous avons developpe une methode dâ€™analyse statistique en introduisantune nouvelle penalisation adaptative. Lâ€™objectif consiste a creer un modele predictif precis, quiminimise en meme temps une fonction de cout, independante des donnees. Lorsque cette fonctionde cout est liee au temps de calcul de chaque descripteur, il est alors possible de creer un modeleparcimonieux precis et qui minimise les temps de calcul. Cette methode est une generalisation desregressions lineaires et logistiques Ridge, Forward stagewise, Lar, ou Lasso.Les algorithmes developpes dans cette these ont ete utilises pour trois applications industrielles,tres differentes les unes des autres, mais toutes faisant intervenir une approche multidisciplinaire : letraitement dâ€™images et lâ€™analyse statistique. Lâ€™association de ces deux disciplines permet dâ€™ameliorerla genericite des strategies proposees puisque les operateurs de traitement dâ€™images allies a un apprentissagesupervise ou non supervise, permettent dâ€™adapter le traitement a chaque application.Mots cles : Traitement dâ€™images, morphologie mathematique, analyse statistique, caracterisation deformes, controles non destructifs, ouvertures parcimonieuses par chemins, region growing structuringelements, amincissements par attributs geodesiques et topologiques, adaptive coefficient shrinkage.",2012,
Elevated Neopterin Levels Predict Early Death in Older Hip-fracture Patients,"Our society faces a major challenge concerning management of the health and socio-economic burden caused by acute physical stress in the older population (+75years). In particular, hip-fracture surgery (HFS) represents a major health care preoccupation, affecting 1.6 million patients worldwide, resulting in a significant drop in life quality and autonomy. The trauma is associated with 20-30% one-year mortality in the elderly. In the present study, we aim to identify factors, which influence and/or predict the outcome of elderly hip- fracture patients (HFP) post-surgery. Our objective was to identify biomarkers with a prognostic capacity of one-year mortality. We employed an observational cohort of HFP (n=60) followed-up longitudinally during the first year post fracture. Clinical and biological data (n=136), collected at arrival to hospital, were then compared to healthy controls (n=42) and analyzed using a regularized logistic regression model with lasso penalty followed by 10-fold cross-validation of variables. We show that plasmatic neopterin levels, a molecule released by IFN-Î³-activated macrophages, is predictive of mortality in HFP (ROC-AUC=0.859). Moreover, neopterin measured at arrival to the hospital correlated negatively with the time of survival after HFS. Neopterin therefore represents a biomarker, which enables better follow-up of patients at risk of early death.",2017,EBioMedicine
