title,abstract,year,journal
The Doubly Adaptive LASSO for Vector Autoregressive Models,"The LASSO (Tibshirani, J R Stat Soc Ser B 58(1):267â€“288, 1996, [30]) and the adaptive LASSO (Zou, J Am Stat Assoc 101:1418â€“1429, 2006, [37]) are popular in regression analysis for their advantage of simultaneous variable selection and parameter estimation, and also have been applied to autoregressive time series models. We propose the doubly adaptive LASSO (daLASSO), or PLAC-weighted adaptive LASSO, for modelling stationary vector autoregressive processes. The procedure is doubly adaptive in the sense that its adaptive weights are formulated as functions of the norms of the partial lag autocorrelation matrix function (Heyse, 1985, [17]) and Yuleâ€“Walker or ordinary least squares estimates of a vector time series. The existing papers ignore the partial lag autocorrelation information inherent in a VAR process. The procedure shows promising results for VAR models. The procedure excels in terms of VAR lag order identification.",2016,
The use of nonnegative garrote for order selection of ARX models,"Order selection of linear regression models has been thoroughly researched in the statistical community for some time. Different shrinkage methods have been proposed, such as the Ridge and Lasso regression methods. Especially the Lasso regression has won fame because of its ability to set less important parameters exactly to zero. However, these methods do not take dynamical systems into account, where the regressors are ordered via the time lag. To this end, a modified variant of the nonnegative garrote method will be analyzed.",2008,2008 47th IEEE Conference on Decision and Control
Generalized and smooth James-Stein model selection,"The generalized and smooth James-Stein thresholding functions link and extend the thresholding functions employed by the James-Stein estimator, the block- and adaptive-lasso in variable selection, and the soft-, hard- and block-thresholding in wavelet smoothing. The estimator is indexed by two hyperparameters for more flexibility and a smoothness parameter for better estimation of its l2-risk with the Stein unbiased risk estimate (SURE). For blocks of a fixed size, a situation that arises when observing concomitant signals (e.g., gravitational wave bursts), we derive a universal threshold, an information criterion and an oracle inequality for block thresholding. Smooth James-Stein thresholding can also be employed in parametric regression for variable selection. In that case a unique smooth estimate is defined, its smooth SURE is derived, which provides the equivalent degrees of freedom of adaptive lasso as a side result. The new estimator enjoys smoothness like ridge regression and performs variable selection like lasso.",2010,
Shrinkage estimation of varying covariate effects based on quantile regression,"Varying covariate effects often manifest meaningful heterogeneity in covariate-response associations. In this paper, we adopt a quantile regression model that assumes linearity at a continuous range of quantile levels as a tool to explore such data dynamics. The consideration of potential non-constancy of covariate effects necessitates a new perspective for variable selection, which, under the assumed quantile regression model, is to retain variables that have effects on all quantiles of interest as well as those that influence only part of quantiles considered. Current work on l1-penalized quantile regression either does not concern varying covariate effects or may not produce consistent variable selection in the presence of covariates with partial effects, a practical scenario of interest. In this work, we propose a shrinkage approach by adopting a novel uniform adaptive LASSO penalty. The new approach enjoys easy implementation without requiring smoothing. Moreover, it can consistently identify the true model (uniformly across quantiles) and achieve the oracle estimation efficiency. We further extend the proposed shrinkage method to the case where responses are subject to random right censoring. Numerical studies confirm the theoretical results and support the utility of our proposals.",2014,Statistics and Computing
A Multi-Element Expression Score Is A Prognostic Factor In Glioblastoma Multiforme,"Purpose
Glioblastoma multiforme (GBM) is a highly malignant tumor of the central nervous system. Although primary GBM patients receive extensive therapies, tumors may recur within months, and there is no objective and scientific method to predict prognosis. Adoptive immunotherapy holds great promise for GBM treatment. However, the expression profiles of the tumor-associated antigens (TAAs) and tumor immune microenvironment (TME) genes used in immunotherapy of GBM patients have not been fully described. The present study aimed to develop a predictive tool to evaluate patient survival based on full analysis of the expression levels of TAAs and TME genes.


Methods
Expression profiles of a panel of 87 TAAs and 8 TME genes significantly correlated with poor prognosis were evaluated in 44 GBM patients and 10 normal brain tissues using quantitative real-time polymerase chain reaction (qRT-PCR). A linear formula (the LASSO algorithm based in the R package) weighted by regression coefficients was used to develop a multi-element expression score to predict prognosis; this formula was cross-validated by the leave-one-out method in different GBM cohorts.


Results
After analysis of gene expression, clinical features, and overall survival (OS), a total of 8 TAAs (CHI3L1, EZH2, TRIOBP, PCNA, PIK3R1, PRKDC, SART3 and EPCAM), 1 TME gene (FOXP3) and 4 clinical features (neutrophil-to-lymphocyte (NLR), number of basophils (BAS), age and treatment with standard radiotherapy and chemotherapy) were included in the formula. There were significant differences between high and low scoring groups identified using the formula in different GBM cohorts (TCGA (n=732) and GEO databases (n=84)), implying poor and good prognosis, respectively.


Conclusion
The multi-element expression score was significantly associated with OS of GBM patients. The improve understanding of TAAs and TMEs and well-defined formula could be implemented in immunotherapy for GBM to provide better care.",2019,Cancer Management and Research
Sparse semi-supervised learning on low-rank kernel,"Advances of modern science and engineering lead to unprecedented amount of data for information processing. Of particular interest is the semi-supervised learning, where very few training samples are available among large volumes of unlabeled data. Graph-based algorithms using Laplacian regularization have achieved state-of-the-art performance, but can induce huge memory and computational costs. In this paper, we introduce L""1-norm penalization on the low-rank factorized kernel for efficient, globally optimal model selection in graph-based semi-supervised learning. An important novelty is that our formulation can be transformed to a standard LASSO regression. On one hand, this makes it possible to employ advanced sparse solvers to handle large scale problems; on the other hand, a globally optimal subset of basis can be chosen adaptively given desired strength of penalizing model complexity, in contrast to some current endeavors that pre-determine the basis without coupling it with the learning task. Our algorithm performs competitively with state-of-the-art algorithms on a variety of benchmark data sets. In particular, it is orders of magnitude faster than exact algorithms and achieves a good trade-off between accuracy and scalability.",2014,Neurocomputing
A Machine Learning Approach Using Survival Statistics to Predict Graft Survival in Kidney Transplant Recipients: A Multicenter Cohort Study,"Accurate prediction of graft survival after kidney transplant is limited by the complexity and heterogeneity of risk factors influencing allograft survival. In this study, we applied machine learning methods, in combination with survival statistics, to build new prediction models of graft survival that included immunological factors, as well as known recipient and donor variables. Graft survival was estimated from a retrospective analysis of the data from a multicenter cohort of 3,117 kidney transplant recipients. We evaluated the predictive power of ensemble learning algorithms (survival decision tree, bagging, random forest, and ridge and lasso) and compared outcomes to those of conventional models (decision tree and Cox regression). Using a conventional decision tree model, the 3-month serum creatinine level post-transplant (cut-off, 1.65â€‰mg/dl) predicted a graft failure rate of 77.8% (index of concordance, 0.71). Using a survival decision tree model increased the index of concordance to 0.80, with the episode of acute rejection during the first year post-transplant being associated with a 4.27-fold increase in the risk of graft failure. Our study revealed that early acute rejection in the first year is associated with a substantially increased risk of graft failure. Machine learning methods may provide versatile and feasible tools for forecasting graft survival.",2017,Scientific Reports
The establishment of immune infiltration based novel recurrence predicting nomogram in prostate cancer,"Prostate cancer (PCa), a severe health burden for males, accounts for the second frequent cancer and fifth tumor specific death cancer around the world. Several studies on tumor-infiltrating immune cells (TIICs) have shown inconsistent and controversial results to PCa. We downloaded a gene expression matrix and clinical information from TCGA, and CIBERSORT was used to identify the proportion of TIICs. Wilcoxon's Sign Rank Test evaluated different gene expression levels in PCa and normal tissues. Kaplan-Meier curves were used to evaluate the associations of TIICs and recurrence-free survival (RFS). Finally, based on the preset P-value of .05, the distribution of TIICs in 73 PCa tissues and 11 normal tissues was illustrated. Activated CD4+ T cells and M0 macrophages account for a high proportion in PCa tissues, while neutrophils and monocytes were found at a high density in normal tissues. Further results showed that the density of plasma cells, Treg cells and resting mast cells were associated with advanced PCa. Additionally, M2 macrophages affected the RFS of PCa patients, and AR was also involved. In the current study, we first evaluated the immune infiltration among PCa and revealed that M2 macrophages could predict the prognosis of PCa patients. Meanwhile, based on the LASSO regression analysis, we established a novel nomogram to assess the recurrence risk of PCa based on immune cell proportions and clinical features.",2019,Cancer Medicine
Test data analytics â€” Exploring spatial and test-item correlations in production test data,"The discovery of patterns and correlations hidden in the test data could help reduce test time and cost. In this paper, we propose a methodology and supporting statistical regression tools that can exploit and utilize both spatial and inter-test-item correlations in the test data for test time and cost reduction. We first describe a statistical regression method, called group lasso, which can identify inter-test-item correlations from test data. After learning such correlations, some test items can be identified for removal from the test program without compromising test quality. An extended version of this method, weighted group lasso, allows taking into account the distinct test time/cost of each individual test item in the formulation as a weighted optimization problem. As a result, its solution would favor more costly test items for removal from the test program. We further integrate weighted group lasso with another statistical regression technique, virtual probe, which can learn spatial correlations of test data across a wafer. The integrated method could then utilize both spatial and inter-test-item correlations to maximize the number of test items whose values can be predicted without measurement. Experimental results of a high-volume industrial device show that utilizing both spatial and inter-test-item correlations can help reduce test time by up to 55%.",2013,2013 IEEE International Test Conference (ITC)
Simultaneous variable selection and parametric estimation for quantile regression,"Abstract In this paper, variable selection techniques in the linear quantile regression model are mainly considered. Based on the penalized quantile regression model, a one-step procedure that can simultaneously perform variable selection and coefficient estimation is proposed. The proposed procedure has three distinctive features: (1) By considering quantile regression, the set of relevant variables can vary across quantiles, thus making it more flexible to model heterogeneous data; (2) The one-step estimator has nice properties in both theory and practice. By applying SCAD penalty (Fan and Li, 2001) and Adaptive-LASSO penalty (Zou, 2006), we establish the oracle property for the sparse quantile regression under mild conditions. Computationally, the one-step estimator is fast, dramatically reducing the computation cost; (3) We suggest a BIC-like tuning parameter selector for the penalized quantile regression and demonstrate the consistency of this criterion. That is to say the true model can be identified consistently based on the BIC-like criterion, making our one-step estimator more reliable practically. Monte Carlo simulation studies are conducted to examine the finite-sample performance of this procedure. Finally, we conclude with a real data analysis. The results are promising.",2015,Journal of The Korean Statistical Society
Can peritumoral radiomics increase the efficiency of the prediction for lymph node metastasis in clinical stage T1 lung adenocarcinoma on CT?,"ObjectivesTo evaluate the efficiency of radiomics model on CT images of intratumoral and peritumoral lung parenchyma for preoperative prediction of lymph node (LN) metastasis in clinical stage T1 peripheral lung adenocarcinoma patients.MethodsThree hundred sixty-six peripheral lung adenocarcinoma patients with clinical stage T1 were evaluated using five CT scanners. For each patient, two volumes of interest (VOIs) on CT were defined as the gross tumor volume (GTV) and the peritumoral volume (PTV, 1.5Â cm around the tumor). One thousand nine hundred forty-six radiomic features were obtained from each VOI, and then refined for reproducibility and redundancy. The refined features were investigated for usefulness in building radiomic signatures by mRMR feature ranking method and LASSO classifier. Multivariable logistic regression analysis was used to develop a radiomic nomogram incorporating the radiomic signature and clinical parameters. The prediction performance was evaluated on the validation cohort.ResultsThe radiomic signatures using the features of GTV and PTV showed a good ability in predicting LN metastasis with an AUC of 0.829 (95% CI, 0.745â€“0.913) and 0.825 (95% CI, 0.733â€“0.918), respectively. By incorporating the features of GTV and PTV, the AUC of radiomic signature increased to 0.843 (95% CI, 0.770â€“0.916). The AUC of radiomic nomogram was 0.869 (95% CI, 0.800â€“0.938).ConclusionsRadiomic signatures of GTV and PTV both had a good prediction ability in the prediction of LN metastasis, and there is no significant difference of AUC between the two groups. The proposed nomogram can be conveniently used to facilitate the preoperative prediction of LN metastasis in T1 peripheral lung adenocarcinomas.Key Pointsâ€¢ Radiomics from peritumoral lung parenchyma increase the efficiency of the prediction for lymph node metastasis in clinical stage T1 lung adenocarcinoma on CT.â€¢ A radiomic nomogram was developed and validated to predict LN metastasis.â€¢ Different scan parameters on CT showed that radiomics signature had good predictive performance.",2019,European Radiology
"Impact of environmental factors on the dependency of litter biomass in carbon cycling of Hooghly estuary, India","Abstract Litterfall of the mangroves and its subsequent decomposition is an important mechanism in terms of productivity and nutrient cycle of that ecosystem. Present study emphasizes on the significance of litter biomass and role of environmental factors impacting this process. Mangrove litter undergoes degradation and decomposition and serves as the main source of carbon in different forms within the system, mangrove forests adjacent to the creeks at Sagar Island of the Hooghlyâ€“Matla estuarine ecosystem. This system receives a major load of carbon from adjacent mangrove forest in the form of litterfall throughout the year. Keeping in view the effect of environmental factors on litterfall and dynamics of carbon, machine learning method has been applied for this study. Different forms of carbon and environmental factors like temperature, salinity, pH, dissolved oxygen are estimated following standard procedure. Correlation, redundancy analysis and LASSO (Least Absolute Shrinkage and Selection Operator) regression are done in order to know the impact of environmental variables on carbon pool dynamics and effect of litterfall on the carbon pools in soil and water. The results reflect a strong correlation among the studied environmental factors and carbon pool dynamics. It has been revealed from the LASSO prediction results that each carbon pool is sensitive to a separate set of environmental factors.",2019,Ecol. Informatics
Using Lasso for Predictor Selection and to Assuage Overfitting: A Method Long Overlooked in Behavioral Sciences.,"Ordinary least squares and stepwise selection are widespread in behavioral science research; however, these methods are well known to encounter overfitting problems such that R(2) and regression coefficients may be inflated while standard errors and p values may be deflated, ultimately reducing both the parsimony of the model and the generalizability of conclusions. More optimal methods for selecting predictors and estimating regression coefficients such as regularization methods (e.g., Lasso) have existed for decades, are widely implemented in other disciplines, and are available in mainstream software, yet, these methods are essentially invisible in the behavioral science literature while the use of sub optimal methods continues to proliferate. This paper discusses potential issues with standard statistical models, provides an introduction to regularization with specific details on both Lasso and its related predecessor ridge regression, provides an example analysis and code for running a Lasso analysis in R and SAS, and discusses limitations and related methods.",2015,Multivariate behavioral research
Detection of early stage pancreatic cancer using 5-hydroxymethylcytosine signatures in circulating cell free DNA,"Pancreatic cancers are typically diagnosed at late stage where disease prognosis is poor as exemplified by a 5-year survival rate of 8.2%. Earlier diagnosis would be beneficial by enabling surgical resection or earlier application of therapeutic regimens. We investigated the detection of pancreatic ductal adenocarcinoma (PDAC) in a non-invasive manner by interrogating changes in 5-hydroxymethylation cytosine status (5hmC) of circulating cell free DNA in the plasma of a PDAC cohort (n=51) in comparison with a non-cancer cohort (n=41). We found that 5hmC sites are enriched in a disease and stage specific manner in exons, 3â€™UTRs and transcription termination sites. Our data show that 5hmC density is reduced in promoters and histone H3K4me3-associated sites with progressive disease suggesting increased transcriptional activity. 5hmC density is differentially represented in thousands of genes, and a stringently filtered set of the most significant genes points to biology related to pancreas (GATA4, GATA6, PROX1, ONECUT1) and/or cancer development (YAP1, TEAD1, PROX1, ONECUT1, ONECUT2, IGF1 and IGF2). Regularized regression models were built using 5hmC densities in statistically filtered genes or a comprehensive set of highly variable 5hmC counts in genes and performed with an AUC = 0.94-0.96 on training data. We were able to test the ability to classify PDAC and non-cancer samples with the Elastic net and Lasso models on two external pancreatic cancer 5hmC data sets and found validation performance to be AUC = 0.74-0.97. The findings suggest that 5hmC changes enable classification of PDAC patients with high fidelity and are worthy of further investigation on larger cohorts of patient samples.",2018,bioRxiv
The SIFK score: a validated predictive model for arthroplasty progression after subchondral insufficiency fractures of the knee,"The purpose of this study was to create a predictive model utilizing baseline demographic and radiographic characteristics for the likelihood that a patient with subchondral insufficiency fracture of the knee will progress to knee arthroplasty with emphasis on clinical interpretability and usability. A retrospective review of baseline and final radiographs in addition to MRIs were reviewed for evaluation of insufficiency fractures and associated injuries. Patient and radiographic factors were used in building predictive models for progression to arthroplasty with Train: Validation: Test subsets. Multiple models were compared with emphasis on clinical utility. Total of 249 patients with a mean age of 64.6 (SD 10.5) years were included. Knee arthroplasty rate was 27% at mean of 4 years of follow-up. Lasso Regression was non-inferior to other models and was chosen for ease of interpretability. In order of importance, predictors for progression to arthroplasty included lateral meniscus extrusion, Kellgrenâ€“Lawrence Grade 4, SIFK on MFC, lateral meniscus root tear, and medial meniscus extrusion. The final SIFK Score stratified patients into low-, medium-, and high-risk categories with arthroplasty rates of 8.8%, 40.4%, and 78.9% (pâ€‰<â€‰0.001) and an area under the curve of 82.5%. In this validated model, lateral meniscus extrusion, K-L Grade 4, SIFK on MFC, lateral meniscus root tear, and medial meniscus extrusion were the most important factors in predicting progression to arthroplasty (in that order). This model assists in patient treatment and counseling in providing prognostic information based on patient-specific risk factors by classifying them into a low-, medium-, and high-risk categories. This model can be used both by medical professionals treating musculoskeletal injuries in guiding patient decision making. Level III.",2019,"Knee Surgery, Sports Traumatology, Arthroscopy"
Variational Bayes Group Sparse Time-Adaptive Parameter Estimation With Either Known or Unknown Sparsity Pattern,"In this paper, we study the problem of time-adaptive group sparse signal estimation from a Bayesian viewpoint. We propose two online variational Bayes schemes that are specifically designed to estimate and track group sparse signals in time. The proposed schemes address both the cases where the grouping information of the signal is either known or not. For the case of known group sparsity pattern, the proposed scheme builds on a novel hierarchical model for the Bayesian adaptive group lasso. Utilizing the variational Bayes framework, update equations for all model parameters are given, for both the batch and time adaptive estimation scenarios. To address the case where the group sparsity pattern is unknown, the hierarchical Bayesian model of the former scheme is extended by organizing the penalty parameters of the Bayesian lasso in a conditional autoregressive model. Intrinsic conditional autoregression is exploited to penalize the signal coefficients in a structured manner and thus obtain group sparse solutions automatically. Again, a robust and computationally efficient online variational Bayes estimator is developed, capitalizing on the conjugacy of the proposed hierarchical Bayesian formulation. Experimental results are reported that corroborate the superior estimation performance of the proposed online schemes, when compared with state-of-the-art methods.",2016,IEEE Transactions on Signal Processing
Discussion of Least Angle Regression,"Algorithms for simultaneous shrinkage and selection in regression and classifi cation provide attractive solutions to knotty old statistical challenges. N vertheless, as far as we can tell, Tibshiraniâ€™s Lasso algorithm has had little impac t on statistical practice. Two particular reasons for this may be the relative inefficienc y of the original Lasso algorithm, and the relative complexity of more recent Lasso algor ithms (e.g., Osbornet al., 2000). Efron, Hastie, Johnstone, and Tibshirani have provided an efficient, simple algorithm for the Lasso as well as algorithms for stage wiseregression and the new least angle regression. As such this paper is an important contribution to statistical computing.",2003,
Bayesian variable selection in additive partial linear models,"Many studies in recent time include a large number of predictor variables, but typically only a few of the predictors have significant roles. Variable selection techniques have been developed using both non-Bayesian and Bayesian approaches. Additive partial linear models (APLM) provide a flexible yet manageable extension of linear models, where some variables can have non-linear effects. We develop a Bayesian method for variable selection for APLM by expanding the non-linear functions in a polynomial basis and introducing sparsity by allowing point masses in the prior distribution of regression coefficients. We address variable selection for both linear and non-linear parts. The nonsingular part of the prior is given by a Laplace or multivariate Laplace density depending on whether the predictor has only a linear effect or a general effect. However, instead of using Markov Chain Monte Carlo methods, which are extremely slow in high dimensional models, we use Laplace approximation technique around posterior mode, which can be identified with the group lasso solution. We conduct a simulation study and present real data analysis for a nutritional epidemiology study and prostate cancer data.",2013,
Short-Term Forecasting of CO2 Emission Intensity in Power Grids by Machine Learning,"A machine learning algorithm is developed to forecast the CO2 emission intensities in electrical power grids in the Danish bidding zone DK2, distinguishing between average and marginal emissions. The analysis was done on data set comprised of a large number (473) of explanatory variables such as power production, demand, import, weather conditions etc. collected from selected neighboring zones. The number was reduced to less than 50 using both LASSO (a penalized linear regression analysis) and a forward feature selection algorithm. Three linear regression models that capture different aspects of the data (non-linearities and coupling of variables etc.) were created and combined into a final model using Softmax weighted average. Cross-validation is performed for debiasing and autoregressive moving average model (ARIMA) implemented to correct the residuals, making the final model the variant with exogenous inputs (ARIMAX). The forecasts with the corresponding uncertainties are given for two time horizons, below and above six hours. Marginal emissions came up independent of any conditions in the DK2 zone, suggesting that the marginal generators are located in the neighbouring zones. 
The developed methodology can be applied to any bidding zone in the European electricity network without requiring detailed knowledge about the zone.",2020,ArXiv
The Bayesian Group Lasso for Confounded Spatial Data,"Generalized linear mixed models for spatial processes are widely used in applied statistics. In many applications of the spatial generalized linear mixed model (SGLMM), the goal is to obtain inference about regression coefficients while achieving optimal predictive ability. When implementing the SGLMM, multicollinearity among covariates and the spatial random effects can make computation challenging and influence inference. We present a Bayesian group lasso prior with a single tuning parameter that can be chosen to optimize predictive ability of the SGLMM and jointly regularize the regression coefficients and spatial random effect. We implement the group lasso SGLMM using efficient Markov chain Monte Carlo (MCMC) algorithms and demonstrate how multicollinearity among covariates and the spatial random effect can be monitored as a derived quantity. To test our method, we compared several parameterizations of the SGLMM using simulated data and two examples from plant ecology and disease ecology. In all examples, problematic levels multicollinearity occurred and influenced sampling efficiency and inference. We found that the group lasso prior resulted in roughly twice the effective sample size for MCMC samples of regression coefficients and can have higher and less variable predictive accuracy based on out-of-sample data when compared to the standard SGLMM.Supplementary materials accompanying this paper appear online.",2017,"Journal of Agricultural, Biological and Environmental Statistics"
Prioritize Transcription Factor Binding Sites for Multiple Co-Expressed Gene Sets Based on Lasso Multinomial Regression Models,Computational prediction of cis-regulatory elements for a set of co-expressed genes based on sequence analysis provides an overwhelming volume of potential transcription factor binding sites. It presents a challenge to prioritize a set of functional transcription factors and their binding sites on the regulatory regions of the target genes that are relevant to the gene expression study. A novel approach based on the use of lasso multinomial regression models is proposed to address this problem. We examine the ability of the lasso models using a time-course microarray data obtained from a comprehensive study of gene expression profiles in skin and mucosal in mouse over all stages of wound healing.,2016,
A predictive maintenance system based on regularization methods for ion-implantation,"Ion Implantation is one of the most sensitive processes in Semiconductor Manufacturing. It consists in impacting accelerated ions with a material substrate and is performed by an Implanter tool. The major maintenance issue of such tool concerns the breaking of the tungsten filament contained within the ion source of the tool. This kind of fault can happen on a weekly basis, and the associated maintenance operations can last up to 3 hours. It is important to optimize the maintenance activities by synchronizing the Filament change operations with other minor maintenance interventions. In this paper, a Predictive Maintenance (PdM) system is proposed to tackle such issue; the filament lifetime is estimated on a statistical basis exploiting the knowledge of physical variables acting on the process. Given the high-dimensionality of the data, the statistical modeling has been based on Regularization Methods: Lasso, Ridge Regression and Elastic Nets. The predictive performances of the aforementioned regularization methods and of the proposed PdM module have been tested on actual productive semiconductor data.",2012,2012 SEMI Advanced Semiconductor Manufacturing Conference
Comparison of whole genome prediction accuracy across generations using parametric and semi parametric methods,"Accuracy of genomic prediction was compared using three parametric and semi parametric methods, including BayesA, Bayesian LASSO and Reproducing kernel Hilbert spaces regression under various levels of heritability (0.15, 0.3 and 0.45), different number of markers (500, 750 and 1000) and generation intervals of validating set. A historical population of 1000 individuals with equal sex ratio was simulated for 100 generations at constant size. It followed by 100 extra generations of gradually reducing size down to 500 individuals in generation 200. Individuals of generation 200 were mated randomly for 10 more generations applying litter size of 5 to expand the historical generation. Finally, 50 males and 500 females chosen from generation 210 were randomly mated to generate 10 more generations of recent population. Individuals born in generation 211 considered as the training set while the validation set was composed of individuals either from generations 213, 215 or 217. The genome comprised one chromosome of 100 cM length carrying 50 QTLs. There was no significant difference between accuracy of investigated methods (p > 0.05) but among three methods, the highest mean accuracy (0.659) was observed for BayesA. By increasing the heritability, the average genomic accuracy increased from 0.53 to 0.75 (p < 0.05). The number of SNPs affected the accuracy and accuracies increased as number of SNPs increased; therefore, the highest accuracy was for the case number of SNPs=1000. With getting away from validating set, the accuracies decreased and the most severe decay observed in the case of low heritability. Decreasing the accuracy across generations affected by marker density but was independent from investigated methods.",2016,Acta Scientiarum. Animal Sciences
3 Greedy Variance Estimation â€“ The Orthonormal Case,"Recent results have proven the minimax optimality of LASSO and related algorithms for noisy linear regression. However, these results tend to rely on variance estimators that are inefficient or optimizations that are slower than LASSO itself. We propose an efficient estimator for the noise variance in high dimensional linear regression that is faster than LASSO, only requiring p matrix-vector multiplications. We prove this estimator is consistent with a good rate of convergence, under the condition that the design matrix satisfies the Restricted Isometry Property (RIP). In practice, our estimator scales incredibly well into high dimensions, is highly parallelizable, and only incurs a modest bias.",2018,
Genome-wide prediction of traits with different genetic architecture through efficient variable selection.,"In genome-based prediction there is considerable uncertainty about the statistical model and method required to maximize prediction accuracy. For traits influenced by a small number of quantitative trait loci (QTL), predictions are expected to benefit from methods performing variable selection [e.g., BayesB or the least absolute shrinkage and selection operator (LASSO)] compared to methods distributing effects across the genome [ridge regression best linear unbiased prediction (RR-BLUP)]. We investigate the assumptions underlying successful variable selection by combining computer simulations with large-scale experimental data sets from rice (Oryza sativa L.), wheat (Triticum aestivum L.), and Arabidopsis thaliana (L.). We demonstrate that variable selection can be successful when the number of phenotyped individuals is much larger than the number of causal mutations contributing to the trait. We show that the sample size required for efficient variable selection increases dramatically with decreasing trait heritabilities and increasing extent of linkage disequilibrium (LD). We contrast and discuss contradictory results from simulation and experimental studies with respect to superiority of variable selection methods over RR-BLUP. Our results demonstrate that due to long-range LD, medium heritabilities, and small sample sizes, superiority of variable selection methods cannot be expected in plant breeding populations even for traits like FRIGIDA gene expression in Arabidopsis and flowering time in rice, assumed to be influenced by a few major QTL. We extend our conclusions to the analysis of whole-genome sequence data and infer upper bounds for the number of causal mutations which can be identified by LASSO. Our results have major impact on the choice of statistical method needed to make credible inferences about genetic architecture and prediction accuracy of complex traits.",2013,Genetics
Two-Layer Feature Reduction for Sparse-Group Lasso via Decomposition of Convex Sets,"Sparse-Group Lasso (SGL) has been shown to be a powerful regression technique for simultaneously discovering group and within-group sparse patterns by using a combination of the l1 and l2 norms. However, in large-scale applications, the complexity of the regularizers entails great computational challenges. In this paper, we propose a novel two-layer feature reduction method (TLFre) for SGL via a decomposition of its dual feasible set. The two-layer reduction is able to quickly identify the inactive groups and the inactive features, respectively, which are guaranteed to be absent from the sparse representation and can be removed from the optimization. Existing feature reduction methods are only applicable for sparse models with one sparsity-inducing regularizer. To our best knowledge, TLFre is the first one that is capable of dealing with multiple sparsity-inducing regularizers. Moreover, TLFre has a very low computational cost and can be integrated with any existing solvers. Experiments on both synthetic and real data sets show that TLFre improves the efficiency of SGL by orders of magnitude.",2014,ArXiv
Diffusion tensor imaging for characterizing tumor microstructure and improving diagnostic performance on breast MRI: a prospective observational study,"BackgroundDiffusion-weighted imaging (DWI) can increase breast MRI diagnostic specificity due to the tendency of malignancies to restrict diffusion. Diffusion tensor imaging (DTI) provides further information over conventional DWI regarding diffusion directionality and anisotropy. Our study evaluates DTI features of suspicious breast lesions detected on MRI to determine the added diagnostic value of DTI for breast imaging.MethodsWith IRB approval, we prospectively enrolled patients over a 3-year period who had suspicious (BI-RADS category 4 or 5) MRI-detected breast lesions with histopathological results. Patients underwent multiparametric 3â€‰T MRI with dynamic contrast-enhanced (DCE) and DTI sequences. Clinical factors (age, menopausal status, breast density, clinical indication, background parenchymal enhancement) and DCE-MRI lesion parameters (size, type, presence of washout, BI-RADS category) were recorded prospectively by interpreting radiologists. DTI parameters (apparent diffusion coefficient [ADC], fractional anisotropy [FA], axial diffusivity [Î»1], radial diffusivity [(Î»2â€‰+â€‰Î»3)/2], and empirical difference [Î»1â€‰âˆ’â€‰Î»3]) were measured retrospectively. Generalized estimating equations (GEE) and least absolute shrinkage and selection operator (LASSO) methods were used for univariate and multivariate logistic regression, respectively. Diagnostic performance was internally validated using the area under the curve (AUC) with bootstrap adjustment.ResultsThe study included 238 suspicious breast lesions (95 malignant, 143 benign) in 194 women. In univariate analysis, lower ADC, axial diffusivity, and radial diffusivity were associated with malignancy (ORâ€‰=â€‰0.37â€“0.42 per 1-SD increase, pâ€‰<â€‰0.001 for each), as was higher FA (ORâ€‰=â€‰1.45, pâ€‰=â€‰0.007). In multivariate analysis, LASSO selected only ADC (ORâ€‰=â€‰0.41) as a predictor for a DTI-only model, while both ADC (ORâ€‰=â€‰0.41) and FA (ORâ€‰=â€‰0.88) were selected for a model combining clinical and imaging parameters. Post-hoc analysis revealed varying association of FA with malignancy depending on the lesion type. The combined model (AUCâ€‰=â€‰0.81) had a significantly better performance than Clinical/DCE-MRI-only (AUCâ€‰=â€‰0.76, pâ€‰<â€‰0.001) and DTI-only (AUCâ€‰=â€‰0.75, pâ€‰=â€‰0.002) models.ConclusionsDTI significantly improves diagnostic performance in multivariate modeling. ADC is the most important diffusion parameter for distinguishing benign and malignant breast lesions, while anisotropy measures may help further characterize tumor microstructure and microenvironment.",2019,Breast Cancer Research : BCR
"LASSOPACK and PDSLASSO: Prediction, model selection and causal inference with regularized regression","The field of machine learning is attracting increasing attention among social scientists and economists. At the same time, Stata offers to date only a very limited set of machine learning tools. This one-hour session introduces two Stata packages, lassopack and pdslasso, which implement regularized regression methods, including but not limited to the lasso (Tibshirani 1996 Journal of the Royal Statistical Society Series B), for Stata. The packages include features intended for prediction, model selection and causal inference, and are thus applicable in a wide range of settings. The commands allow for high-dimensional models, where the number of regressors may be large or even exceed the number of observations under the assumption of sparsity. The package lassopack implements lasso, square-root lasso (Belloni et al. 2011 Biometrika; 2014 Annals of Statistics), elastic net (Zou and Hastie 2005 Journal of the Royal Statistical Society Series B), ridge regression (Hoerl and Kennard 1970 Technometrics), adaptive lasso (Zou 2006 Journal of the American Statistical Association) and post-estimation OLS. These methods rely on tuning parameters, which determine the degree and type of penalization. lassopack supports three approaches for selecting these tuning parameters: information criteria (implemented in lasso2), K-fold and h-step ahead rolling cross-validation (cvlasso), and theory-driven penalization (rlasso) due to Belloni et al. (2012 Econometrica). In addition, rlasso implements the Chernozhukov et al. (2013 Annals of Statistics) sup-score test of joint significance of the regressors.",2018,
