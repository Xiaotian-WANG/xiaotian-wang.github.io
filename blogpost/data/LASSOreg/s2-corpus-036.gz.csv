title,abstract,year,journal
Measurement Error in LASSO - Analytical Results and a Simulation Study,"The LASSO (Tibshirani, 1996) is a powerful method which due to the fact that is uses a l 1-penalty allows for the estimation of regression coefficients and variable selection at the same time. An important property of the LASSO is that it can be applied even when the number of covariates p is larger than the number of observations n. This differs the LASSO from the popular OLS method which requires p < n. An assumption that is common to the vast majority of studies on the LASSO is that the design matrix X passed to the LASSO for performing linear regression contains perfect covariate measurements that do not suffer from additive measurement error. However, in practice where data corrupted by measurement errors or errors-invariable data are rather the norm than the exception, this assumption does not meet the truth. In this work, we studied the LASSO in the presence of additive measurement error in the design matrix. In doing so, we allowed for analytical results on the estimation and variable selection consistency of both the LASSO with perfect design and the naive LASSO with additive covariate measurement error. We performed a Monte Carlo simulation study to assess the finite sample performance of the OLS and the LASSO under matrix uncertainty. Thereby, we also computed the corresponding corrected estimates. In particular, we used the well-known reliability ratio (Fuller, 1987) for the OLS estimates and a reliability ratio-like factor according to SÃ¸rensen et al. (2014) for the naive LASSO estimates in the presence of measurement error. In summary, we found that the MSE values of both the naive LASSO and the naive OLS increase with growing measurement error variance and covariate correlation. With respect to the corrected LASSO and OLS estimates, our results suggest that there does not exist any overall evidence of the efficacy of the applied measurement error correction factors. Especially, the MSE values of the corrected estimates tend to be larger than the ones for the naive estimators. However, we found that the empirical averages of the MSE values were inflated by a few outliers and that the occurrence of outliers was due to the bad conditioning of a matrix which contributes to the correction factor. Moreover, our simulation results suggest that the coincidence of high covariate correlation and large measurement error variance leads the LASSO to be more or less unable to differ between important and unimportant â€¦",2015,
Consensus Ensemble System for Traffic Flow Prediction,"Traffic flow prediction is a key component of an intelligent transportation system. Accurate traffic flow prediction provides a foundation for other tasks, such as signal coordination and travel time forecasting. There are many known methods in literature for the short-term traffic flow prediction problem, but their efficacy depends heavily on the traffic characteristics. It is difficult, if not impossible, to pick a single method that works well over time. In this paper, we present an automated framework to address this practical issue. Instead of selecting a single method, we combine predictions from multiple methods to generate a consensus traffic flow prediction. We propose an ensemble learning model that exploits the temporal characteristics of the data, and balances the accuracy of individual models and their mutual dependence through a covariance-regularizer. We additionally use a pruning scheme to remove anomalous individual predictions. We apply our proposed model to multi-step-ahead arterial roadway flow prediction. In tests, our method consistently outperforms recently published ensemble prediction methods based on ridge regression and lasso. Our method also produces steady results even when the standalone models and other ensemble methods make wildly exaggerated predictions.",2018,IEEE Transactions on Intelligent Transportation Systems
Opinion polarity detection in Twitter data combining shrinkage regression and topic modeling,"We propose a method to analyze public opinion about political issues online by automatically detecting polarity in Twitter data. Previous studies have focused on the polarity classification of individual tweets. However, to understand the direction of public opinion on a political issue, it is important to analyze the degree of polarity on the major topics at the center of the discussion in addition to the individual tweets. The first stage of the proposed method detects polarity in tweets using the Lasso and Ridge models of shrinkage regression. The models are beneficial in that the regression results provide sentiment scores for the terms that appear in tweets. The second stage identifies the major topics via a latent Dirichlet analysis (LDA) topic model and estimates the degree of polarity on the LDA topics using term sentiment scores. To the best of our knowledge, our study is the first to predict the polarities of public opinion on topics in this manner. We conducted an experiment on a mayoral election in Seoul, South Korea and compared the total detection accuracy of the regression models with five support vector machine (SVM) models with different numbers of input terms selected by a feature selection algorithm. The results indicated that the performance of the Ridge model was approximately 7% higher on average than that of the SVM models. Additionally, the degree of polarity on the LDA topics estimated using the proposed method was compared with actual public opinion responses. The results showed that the polarity detection accuracy of the Lasso model was 83%, indicating that the proposed method was valid in most cases.",2016,J. Informetrics
Transparent Reporting of a multivariable prediction model for Individual Prognosis Or Diagnosis (TRIPOD): The TRIPOD Statement,"Editors' Note: In order to encourage dissemination of the TRIPOD Statement, this article is freely accessible on the Annals of Internal Medicine Web site ( www.annals.org ) and will be also published in BJOG, British Journal of Cancer, British Journal of Surgery, BMC Medicine, British Medical Journal, Circulation, Diabetic Medicine, European Journal of Clinical Investigation, European Urology, and Journal of Clinical Epidemiology. The authors jointly hold the copyright of this article. An accompanying explanation and elaboration article is freely available only at www.annals.org ; Annals of Internal Medicine holds copyright for that article. In medicine, patients with their care providers are confronted with making numerous decisions on the basis of an estimated risk or probability that a specific disease or condition is present (diagnostic setting) or a specific event will occur in the future (prognostic setting) (Figure 1). In the diagnostic setting, the probability that a particular disease is present can be used, for example, to inform the referral of patients for further testing, initiate treatment directly, or reassure patients that a serious cause for their symptoms is unlikely. In the prognostic setting, predictions can be used for planning lifestyle or therapeutic decisions based on the risk for developing a particular outcome or state of health within a specific period (1, 2). Such estimates of risk can also be used to risk-stratify participants in therapeutic clinical trials (3, 4). Figure 1. Schematic representation of diagnostic and prognostic prediction modeling studies. The nature of the prediction in diagnosis is estimating the probability that a specific outcome or disease is present (or absent) within an individual, at this point in timethat is, the moment of prediction (T= 0). In prognosis, the prediction is about whether an individual will experience a specific event or outcome within a certain time period. In other words, in diagnostic prediction the interest is in principle a cross-sectional relationship, whereas prognostic prediction involves a longitudinal relationship. Nevertheless, in diagnostic modeling studies, for logistical reasons, a time window between predictor (index test) measurement and the reference standard is often necessary. Ideally, this interval should be as short as possible and without starting any treatment within this period. In both the diagnostic and prognostic setting, estimates of probabilities are rarely based on a single predictor (5). Doctors naturally integrate several patient characteristics and symptoms (predictors, test results) to make a prediction (see Figure 2 for differences in common terminology between diagnostic and prognostic studies). Prediction is therefore inherently multivariable. Prediction models (also commonly called prognostic models, risk scores, or prediction rules [6]) are tools that combine multiple predictors by assigning relative weights to each predictor to obtain a risk or probability (1, 2). Well-known prediction models include the Framingham Risk Score (7), Ottawa Ankle Rules (8), EuroScore (9), Nottingham Prognostic Index (10), and the Simplified Acute Physiology Score (11). Figure 2. Similarities and differences between diagnostic and prognostic prediction models. Prediction Model Studies Prediction model studies can be broadly categorized as model development (12), model validation (with or without updating) (13) or a combination of both (Figure 3). Model development studies aim to derive a prediction model by selecting the relevant predictors and combining them statistically into a multivariable model. Logistic and Cox regression are most frequently used for short-term (for example, disease absent vs. present, 30-day mortality) and long-term (for example, 10-year risk) outcomes, respectively (1214). Studies may also focus on quantifying the incremental or added predictive value of a specific predictor (for example, newly discovered) to a prediction model (18). Figure 3. Types of prediction model studies covered by the TRIPOD Statement. D = development data; V = validation data. Quantifying the predictive ability of a model on the same data from which the model was developed (often referred to as apparent performance) will tend to give an optimistic estimate of performance, owing to overfitting (too few outcome events relative to the number of candidate predictors) and the use of predictor selection strategies (19). Studies developing new prediction models should therefore always include some form of internal validation to quantify any optimism in the predictive performance (for example, calibration and discrimination) of the developed model. Internal validation techniques use only the original study sample and include such methods as bootstrapping or cross-validation. Internal validation is a necessary part of model development (2). Overfitting, optimism, and miscalibration may also be addressed and accounted for during the model development by applying shrinkage (for example, heuristic or based on bootstrapping techniques) or penalization procedures (for example, ridge regression or lasso) (20). After developing a prediction model, it is strongly recommended to evaluate the performance of the model in other participant data than was used for the model development. Such external validation requires that for each individual in the new data set, outcome predictions are made using the original model (that is, the published regression formula) and compared with the observed outcomes (13, 14). External validation may use participant data collected by the same investigators, typically using the same predictor and outcome definitions and measurements, but sampled from a later period (temporal or narrow validation); by other investigators in another hospital or country, sometimes using different definitions and measurements (geographic or broad validation); in similar participants but from an intentionally different setting (for example, model developed in secondary care and assessed in similar participants but selected from primary care); or even in other types of participants (for example, model developed in adults and assessed in children, or developed for predicting fatal events and assessed for predicting nonfatal events) (13, 15, 17, 21, 22). In case of poor performance, the model can be updated or adjusted on the basis of the validation data set (13). Reporting of Multivariable Prediction Model Studies Studies developing or validating a multivariable prediction model share specific challenges for researchers (6). Several reviews have evaluated the quality of published reports that describe the development or validation prediction models (2328). For example, Mallett and colleagues (26) examined 47 reports published in 2005 presenting new prediction models in cancer. Reporting was found to be poor, with insufficient information described in all aspects of model development, from descriptions of patient data to statistical modeling methods. Collins and colleagues (24) evaluated the methodological conduct and reporting of 39 reports published before May 2011 describing the development of models to predict prevalent or incident type 2 diabetes. Reporting was also found to be generally poor, with key details on which predictors were examined, the handling and reporting of missing data, and model-building strategy often poorly described. Bouwmeester and colleagues (23) evaluated 71 reports, published in 2008 in 6 high-impact general medical journals, and likewise observed an overwhelmingly poor level of reporting. These and other reviews provide a clear picture that, across different disease areas and different journals, there is a generally poor level of reporting of prediction model studies (6, 2327, 29). Furthermore, these reviews have shown that serious deficiencies in the statistical methods, use of small data sets, inappropriate handling of missing data, and lack of validation are common (6, 2327, 29). Such deficiencies ultimately lead to prediction models that are not or should not be used. It is therefore not surprising, and fortunate, that very few prediction models, relative to the large number of models published, are widely implemented or used in clinical practice (6). Prediction models in medicine have proliferated in recent years. Health care providers and policy makers are increasingly recommending the use of prediction models within clinical practice guidelines to inform decision making at various stages in the clinical pathway (30, 31). It is a general requirement of reporting of research that other researchers can, if required, replicate all the steps taken and obtain the same results (32). It is therefore essential that key details of how a prediction model was developed and validated be clearly reported to enable synthesis and critical appraisal of all relevant information (14, 3336). Reporting Guidelines for Prediction Model Studies: The TRIPOD Statement We describe the development of the TRIPOD (Transparent Reporting of a multivariable prediction model for Individual Prognosis or Diagnosis) Statement, a guideline specifically designed for the reporting of studies developing or validating a multivariable prediction model, whether for diagnostic or prognostic purposes. TRIPOD is not intended for multivariable modeling in etiologic studies or for studies investigating single prognostic factors (37). Furthermore, TRIPOD is also not intended for impact studies that quantify the impact of using a prediction model on participant or doctors' behavior and management, participant health outcomes, or cost-effectiveness of care, compared with not using the model (13, 38). Reporting guidelines for observational (the STrengthening the Reporting of OBservational studies in Epidemiology [STROBE]) (39), tumor marker (REporting recommendations for tumour MARKer prognostic studies [REMARK]) (37), diagnostic accuracy (STAndards f",2015,Annals of Internal Medicine
The adaptive BerHu penalty in robust regression,"We intend to combine Huber's loss with an adaptive reversed version as a penalty function. The purpose is twofold: first we would like to propose an estimator that is robust to data subject to heavy-tailed errors or outliers. Second we hope to overcome the variable selection problem in the presence of highly correlated predictors. For instance, in this framework, the adaptive least absolute shrinkage and selection operator (lasso) is not a very satisfactory variable selection method, although it is a popular technique for simultaneous estimation and variable selection. We call this new penalty the adaptive BerHu penalty. As for elastic net penalty, small coefficients contribute through their norm to this penalty while larger coefficients cause it to grow quadratically (as ridge regression). We will show that the estimator associated with Huber's loss combined with the adaptive BerHu penalty enjoys theoretical properties in the fixed design context. This approach is compared to existing regularisation methods such as adaptive elastic net and is illustrated via simulation studies and real data.",2016,Journal of Nonparametric Statistics
62 Predictors of Tardive Dyskinesia in Psychiatric Patients Taking Concomitant Antipsychotics.,"BACKGROUND
Tardive dyskinesia (TD) is typically caused by exposure to antipsychotics, is often irreversible, and can be debilitating. TD symptoms can increase the social stigma of patients with comorbid psychiatric disorders, negatively impact quality of life, and potentially increase medical morbidity and mortality. An increased risk of developing TD has been associated with factors such as older age, female sex, underlying mental illness, and long-term use and higher doses of antipsychotics. The association of TD with the use of typical versus atypical antipsychotics has also been evaluated, with mixed results. To date, predictive models assessing the joint effect of clinical characteristics on TD risk have not been developed and validated in the US population.Study ObjectiveTo develop a prediction model to identify patient and treatment characteristics associated with the occurrence of TD among patients with psychiatric disorders taking antipsychotic medications, using a retrospective database analysis.


METHODS
Adult patients with schizophrenia, major depressive disorder, or bipolar disorder who were taking oral antipsychotics, and who had 6months of data prior to the index date were identified from Medicaid claims from six US states. The index date was defined as the date of the first claim for an antipsychotic drug after a claim for the underlying disorder but before TD diagnosis. A multivariate Cox prediction model was developed using a cross-validated version of the least absolute shrinkage and selection operator (LASSO) regression method to improve prediction accuracy and interpretability of the model. The predictive performance was assessed in a separate validation set via model discrimination (concordance) and calibration.


RESULTS
A total of 189,415 patients were identified: 66,723 with bipolar disorder, 68,573 with depressive disorder, and 54,119 with schizophrenia. The selected prediction model had a clinically meaningful concordance of 70% and was well calibrated (P=0.46 for Hosmer-Leme show goodness-of-fit test). Patient's age at index date (hazard ratio [HR]: 1.03), diagnosis of schizophrenia (HR: 1.73), dosage of antipsychotic at index date (up to 100mg/day chlorpromazine equivalent; HR: 1.40), and presence of bipolar and related disorders (HR: 1.16) were significantly associated with an increased risk of TD diagnosis. Use of atypical antipsychotics at index date was associated with a modest reduction in the risk of TD (HR=0.94).


CONCLUSIONS
This study identified a group of factors associated with the development of TD among patients with psychiatric disorders treated with antipsychotics. This may allow physicians to better monitor their patients receiving antipsychotics, allowing for the prompt identification and treatment of TD to help maintain quality of life.Presented at: American Psychiatric Association Annual Meeting; May 5-9, 2018, New York, New York, USAFunding Acknowledgements: This study was supported by Teva Pharmaceuticals, Petach Tikva, Israel.",2019,CNS spectrums
Lasso penalized model selection criteria for high-dimensional multivariate linear regression analysis,"This paper proposes two model selection criteria for identifying relevant predictors in the high-dimensional multivariate linear regression analysis. The proposed criteria are based on a Lasso type penalized likelihood function to allow the high-dimensionality. Under the asymptotic framework that the dimension of multiple responses goes to infinity while the maximum size of candidate models has smaller order of the sample size, it is shown that the proposed criteria have the model selection consistency, that is, they can asymptotically pick out the true model. Simulation studies show that the proposed criteria outperform existing criteria when the dimension of multiple responses is large.",2014,J. Multivar. Anal.
Regularized Estimation and Feature Selection in Mixtures of Gaussian-Gated Experts Models,"Mixtures-of-Experts models and their maximum likelihood estimation (MLE) via the EM algorithm have been thoroughly studied in the statistics and machine learning literature. They are subject of a growing investigation in the context of modeling with high-dimensional predictors with regularized MLE. We examine MoE with Gaussian gating network, for clustering and regression, and propose an $\ell_1$-regularized MLE to encourage sparse models and deal with the high-dimensional setting. We develop an EM-Lasso algorithm to perform parameter estimation and utilize a BIC-like criterion to select the model parameters, including the sparsity tuning hyperparameters. Experiments conducted on simulated data show the good performance of the proposed regularized MLE compared to the standard MLE with the EM algorithm.",2019,ArXiv
Identification of a Five-Pseudogene Signature for Predicting Survival and Its ceRNA Network in Glioma,"Background: Glioma is the most common primary brain tumor with a dismal prognosis. It is urgent to develop novel molecular biomarkers and conform to individualized schemes. Methods: Differentially expressed pseudogenes between low grade glioma (LGG) and glioblastoma multiforme (GBM) were identified in the training cohort. Least absolute shrinkage and selection operator (LASSO) regression and multivariate Cox proportional hazards regression analyses were used to select pseudogenes associated with prognosis of glioma. A risk signature was constructed based on the selected pseudogenes for predicting the survival of glioma patients. A pseudogene-miRNA-mRNA regulatory network was established and visualized using Cytoscape 3.5.1. Gene Oncology (GO) and signaling pathway analyses were performed on the targeted genes to investigate functional roles of the risk signature. Results: Five pseudogenes (ANXA2P2, EEF1A1P9, FER1L4, HILS1, and RAET1K) correlating with glioma survival were selected and used to establish a risk signature. Time-dependent receiver operating characteristic (ROC) curves revealed that the risk signature could accurately predict the 1, 3, and 5-year survival of glioma patients. GO and signaling pathway analyses showed that the risk signature was involved in regulation of proliferation, migration, angiogenesis, and apoptosis in glioma. Conclusions: In this study, a risk signature with five pseudogenes was constructed and shown to accurately predict 1-, 3-, and 5-year survival for glioma patient. The risk signature may serve as a potential target against glioma.",2019,Frontiers in Oncology
Sparse Point Estimation for Bayesian Regression via Simulated Annealing,"In the context of variable selection in a regression model, the classical Lasso based optimization approach provides a sparse estimate with respect to regression coefficients but is unable to provide more information regarding the distribution of regression coefficients. Alternatively, using a Bayesian approach is more advantageous since it gives direct access to the distribution which is usually summarized by estimating the expectation (not sparse) and variance. Additionally, to support frequent application requirements, heuristics like thresholding are generally used to produce sparse estimates for variable selection purposes. In this paper, we provide a more principled approach for generating a sparse point estimate in a Bayesian framework. We extend an existing Bayesian framework for sparse regression to generate a MAP estimate by using simulated annealing. We then justify this extension by showing that this MAP estimate is also sparse in the regression coefficients. Experiments on real world applications like the splice site detection and diabetes progression demonstrate the usefulness of the extension.",2012,
Sharp oracle inequalities in aggregation and shape restricted regression,"This PhD thesis studies two fields of Statistics: Aggregation of estimatorsand shape constrained regression.Shape constrained regression studies the regression problem (find a function that approximates well a set of points) with an underlying shape constraint, that is, the function must have a specific ""shape"". For instance, this function could be nondecreasing of convex: These two shape examples are the most studied. We study two estimators: an estimator based on aggregation methods and the Least Squares estimator with a convex shape constraint. Oracle inequalities are obtained for both estimators, and we construct confidence sets that are adaptive and honest.Aggregation of estimators studies the following problem. If several methods are proposed for the same task, how to construct a new method that mimics the best method among the proposed methods? We will study these problems in three settings: aggregation of density estimators, aggregation of affine estimators and aggregation on the regularization path of the Lasso.",2016,
Comparison of predictive modeling approaches for 30-day all-cause non-elective readmission risk,"BackgroundThis paper explores the importance of electronic medical records (EMR) for predicting 30-day all-cause non-elective readmission risk of patients and presents a comparison of prediction performance of commonly used methods.MethodsThe data are extracted from eight Advocate Health Care hospitals. Index admissions are excluded from the cohort if they are observation, inpatient admissions for psychiatry, skilled nursing, hospice, rehabilitation, maternal and newborn visits, or if the patient expires during the index admission. Data are randomly and repeatedly divided into fitting and validating sets for cross validations. Approaches including LACE, STEPWISE logistic, LASSO logistic, and AdaBoost, are compared with sample sizes varying from 2,500 to 80,000.ResultsOur results confirm that LACE has moderate discrimination power with the area under receiver operating characteristic curve (AUC) around 0.65-0.66, which can be improved to 0.73-0.74 when additional variables from EMR are considered. These variables include Inpatient in the last six months, Number of emergency room visits or inpatients in the last year, Braden score, Polypharmacy, Employment status, Discharge disposition, Albumin level, and medical condition variables such as Leukemia, Malignancy, Renal failure with hemodialysis, History of alcohol substance abuse, Dementia and Trauma. When sample size is small (â‰¤5000), LASSO is the best; when sample size is large (â‰¥20,000), the predictive performance is similar. The STEPWISE method has a slightly lower AUC (0.734) comparing to LASSO (0.737) and AdaBoost (0.737). More than one half of the selected predictors can be false positives when using a single method and a single division of fitting/validating data.ConclusionsTrue predictors can be identified by repeatedly dividing data into fitting/validating subsets and referring the final model based on summarizing results. LASSO is a better alternative to the STEPWISE logistic regression, especially when sample size is not large. The evidence for adequate sample size can be explored by fitting models on gradually reduced samples. Our model comparison strategy is not only good for 30-day all-cause non-elective readmission risk predictions, but also applicable to other types of predictive models in clinical studies.",2016,BMC Medical Research Methodology
Research on Bayesian Model Averaging for Lasso Based on Analysis of Scientific Materials,"The Lasso (least absolute shrinkage and selection operator) estimates a vector of regression coeï¬ƒcients by minimizing the residual sum of squares subject to a constraint on the -norm of coeï¬ƒcient vector, which has been an attractive technique for regularization and variable selection. In this paper, we study the Bayesian Model Averaging(BMA) for Lasso, which accounts for the uncertainty about the best model to choose by averaging over multiple models. Experimental results on simulated data show that BMA has signiï¬cant advantage over the model selection method based on Bayesian information criterion (BIC).",2011,Advanced Materials Research
MATHEMATICAL ENGINEERING TECHNICAL REPORTS An Estimation Procedure for Contingency Table Models Based on the Nested Geometry,"We propose a geometrical method for estimating the parameters of contingency tables. Our methodâ€“bisector regression for contingency tablesâ€“is based on a nested structure of models. The nested structure represents the variables that are independent. This means that a model includes smaller models allowing stronger independence, which also means that more parameters are eliminated in smaller models. Our method estimates parameters corresponding to the interactions of lower orders after those of higher orders are estimated or eliminated. Bisector regression generates a sequence of parameter estimates, each element of which represents a model and an estimate. The length of the sequence is much smaller than the total number of models. We describe the algorithm and show examples. In this paper, contingency tables are considered. We introduce parametrization of multinomial distributions and propose an algorithm for estimating parameters. The proposed algorithm is bisector regression for contingency tables (BRCT). The main idea of BRCT comes from our previous works. In [6, 7], we proposed the bisector regression algorithm, which is an extension of least angle regression [4]. Least angle regression is an algorithm for parameter estimation, which is related to the l1-regularization method (lasso, [3, 5, 11, 12]). In problems of contingency tables, our interest is to estimate parameters corresponding to interactions between factors. Factors, or random variables, are qualitative variables. Parameters are separated into groups depending on how many factors are involved. We apply the main idea of bisector regression for generalized linear regression ([6]) and Gaussian graphical models [7] to these parameter groups. We provide âˆ—hirose@stat.t.u-tokyo.ac.jp",2012,
Smooth-Threshold Multivariate Genetic Prediction with Unbiased Model Selection.,"We develop a new genetic prediction method, smooth-threshold multivariate genetic prediction, using single nucleotide polymorphisms (SNPs) data in genome-wide association studies (GWASs). Our method consists of two stages. At the first stage, unlike the usual discontinuous SNP screening as used in the gene score method, our method continuously screens SNPs based on the output from standard univariate analysis for marginal association of each SNP. At the second stage, the predictive model is built by a generalized ridge regression simultaneously using the screened SNPs with SNP weight determined by the strength of marginal association. Continuous SNP screening by the smooth thresholding not only makes prediction stable but also leads to a closed form expression of generalized degrees of freedom (GDF). The GDF leads to the Stein's unbiased risk estimation (SURE), which enables data-dependent choice of optimal SNP screening cutoff without using cross-validation. Our method is very rapid because computationally expensive genome-wide scan is required only once in contrast to the penalized regression methods including lasso and elastic net. Simulation studies that mimic real GWAS data with quantitative and binary traits demonstrate that the proposed method outperforms the gene score method and genomic best linear unbiased prediction (GBLUP), and also shows comparable or sometimes improved performance with the lasso and elastic net being known to have good predictive ability but with heavy computational cost. Application to whole-genome sequencing (WGS) data from the Alzheimer's Disease Neuroimaging Initiative (ADNI) exhibits that the proposed method shows higher predictive power than the gene score and GBLUP methods.",2016,Genetic epidemiology
Absolute penalty and shrinkage estimation strategies in linear and partially linear models with correlated errors,"In this dissertation we propose shrinkage estimators and absolute penalty estimators (APEs) in linear models, partially linear models (PLM) and quasi-likelihood models. We study the asymptotic properties of shrinkage estimators both analytically and through simulation studies, and compare their performance with APEs. In Chapter 2, we propose shrinkage estimators for a multiple linear regression with first order random coefficient autoregressive (RCAR(1)) error term. We also present two APEs for this models which are modified versions of lasso and adaptive lasso estimators. We compare the performance of shrinkage estimators and APEs through the mean squared error criterion. Monte Carlo studies were conducted to compare the estimators in two situations: when p > n and when p < n. A data example is presented to illustrate the usefulness of the suggested methods. In Chapter 3, we develop shrinkage estimators for a PLM with RCAR(1) error term. The nonparametric function is estimated using a kernel function. We also compare the performance of shrinkage estimators with a modified version of lasso for correlated data. Monte Carlo studies were conducted to compare the behavior of the proposed estimators. A data example is presented to illustrate the application of the suggested methods. In Chapter 4, we propose pretest and shrinkage estimators for quasi-likelihood models. We investigate the asymptotic properties of these estimators both analytically and through simulation studies. We also apply a lasso estimator and compare its performance with the other proposed estimators.",2013,
Attribute Efficient Linear Regression with Distribution-Dependent Sampling,"We consider a budgeted learning setting, where the learner can only choose and observe a small subset of the attributes of each training example. We develop efficient algorithms for Ridge and Lasso linear regression, which utilize the geometry of the data by a novel distribution-dependent sampling scheme, and have excess risk bounds which are better a factor of up to O(âˆšd/k) over the state-of-the-art, where d is the dimension and k + 1 is the number of observed attributes per example. Moreover, under reasonable assumptions, our algorithms are the first in our setting which can provably use less attributes than full-information algorithms, which is the main concern in budgeted learning. We complement our theoretical analysis with experiments which support our claims.",2015,
A weighted nearest mean classifier for sparse subspaces,"In this paper we focus on high dimensional data sets for which the number of dimensions is an order of magnitude higher than the number of objects. From a classifier design standpoint, such small sample size problems have some interesting challenges. First, in any subspace with as many dimensions as objects the data set can be separated with an almost arbitrary linear hyperplane. Second, another important issue is to determine which features are responsible for the phenomenon under consideration. This problem comes down to finding as few features as possible that still can discriminate the classes involved. To attack these problems, we propose the LESS (lowest error in a sparse subspace) classifier. The LESS classifier is a weighted nearest mean classifier that efficiently finds linear discriminants in sparse subspaces, where the subspace is found automatically. In the experiments we compare LESS to related state-of-the-art classifiers like among others linear ridge regression with the LASSO and the support vector machine. It turns out that LESS performs competitively while it uses the fewest features.",2005,2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)
Directing Power Towards Subspaces of the Alternative Hypothesis,"This paper treats two problems in high-dimensional testing that have received attention in the recent literature. The first problem is that tests based on a quadratic statistic (such as the Wald statistic) lose power against subspaces of the alternative hypothesis as the dimension of the parameter vector of interest increases. The second problem is that the Wald statistic is not defined in high-dimensional settings, as it requires an invertible sample covariance matrix. I simultaneously address these issues by generalizing the Wald statistic to a statistic that is large in a subspace of the alternative hypothesis chosen by the econometrician. The existence of the statistic depends on a restricted eigenvalue condition that is tied directly to the size of the subspace. I show that if the conventional sample covariance matrix is used, then the statistic can be computed using linear regression with a constant dependent variable, where coefficient vector is restricted to the subspace of interest. As a demonstration, I consider subspaces that contain sparse or nearly-sparse vectors. For these cases, the computation reduces to $\ell_0$-regularized regression (best subset selection) and $\ell_1$-regularized regression (Lasso), respectively.",2019,
Selection and Fusion of Categorical Predictors with L0-Type Penalties:,"In regression modelling, categorical covariates have to be coded. Depending on the number of categorical covariates and on the number of levels they have, the number of coefficients can become huge. To reduce the model complexity, coefficients of similar categories should be fused and coefficients of non-influential categories should be set to zero. To this end, Lasso-type penalties on the differences of coefficients are a standard approach. However, the clustering/selection performance of this approach is sometimes poorâ€“especially when the adaptive weights are badly conditioned or not existing. In some situations, there is no incentive to cluster similar categories. To overcome this, a L0 penalty on the differences of coefficients is proposed, whereby the L0 â€˜normâ€™ is defined as the number of non-zero entries in a vector. The proposed penalty favours to find clusters of categories that share the same effect on the response variable while the estimation accuracy is comparable to Lasso-type penalties. Numerical experiments within the framework of generalized linear models are promising. For illustration, data on the unemployment rates in Germany is analyzed.",2015,Statistical Modelling
Quantile regression and variable selection for the single-index model,"In this paper, we propose a new full iteration estimation method for quantile regression (QR) of the single-index model (SIM). The asymptotic properties of the proposed estimator are derived. Furthermore, we propose a variable selection procedure for the QR of SIM by combining the estimation method with the adaptive LASSO penalized method to get sparse estimation of the index parameter. The oracle properties of the variable selection method are established. Simulations with various non-normal errors are conducted to demonstrate the finite sample performance of the estimation method and the variable selection procedure. Furthermore, we illustrate the proposed method by analyzing a real data set.",2014,Journal of Applied Statistics
Sparse calibration of an extreme Adaptive Optics system,"Adaptive optics systems are extensively used in astronomy to obtain high resolution pictures of stars and galaxies with ground telescopes. The crucial point is to shape deformable mirrors in order to compensate for the incoming wave distorted by the atmospheric turbulence. The calibration of the system is the cornerstone to obtain good performance. The next generation of adaptive optics system, eXtreme Adaptive Optics (XAO), will have a very large number of actuators and sensors (âˆ¼ 104) in order to guarantee high Strehl ratio and contrast levels; as such computational burden could become a serious bottleneck. For this reason several iterative methods have been proposed in the last decade. Since convergence and computational complexity of these methods depend on the sparsity of the interaction matrix (matrix projecting commands into measurements), the problem of calibrating an XAO system forcing the interaction matrix to be as sparse as possible is clearly important. In this paper we propose a method based on the LASSO regression algorithm that solves efficiently this problem.",2010,49th IEEE Conference on Decision and Control (CDC)
An integrated mRNA-lncRNA signature for relapse prediction in laryngeal cancer.,"Patients with laryngeal cancer with early relapse usually have a poor prognosis. In this study, we aimed to identify a multi-gene signature to improve the relapse prediction in laryngeal cancer. One microarray data set GSE27020 (training set, Nâ€‰=â€‰109) and one RNA-sequencing data set (validation set, Nâ€‰=â€‰85) were included into the analysis. In the training set, the microarray expression profile was re-annotated into an mRNA-long noncoding RNA (lncRNA) biphasic profile. Then, LASSO Cox regression model identified nine relapse-related RNA (eight mRNA and one lncRNA), and a risk score was calculated for each sample according to the model coefficients. Patients with high-risk showed poorer relapse-free survival than patients with low risk (hazard ratios (HR): 6.189, 95% confidence interval (CI): 3.075-12.460, Pâ€‰<â€‰0.0001). The risk score demonstrated good accuracy in predicting the relapse (area under time-dependent receiver-operating characteristic (AUC): 0.859 at 1 year, 0.822 at 3 years, and 0.815 at 5 years). The results were validated in the validation set (HR: 3.762, 95% CI: 1.594-8.877, Pâ€‰=â€‰0.011; AUC: 0.770 at 1 year, 0.769 at 3 years, and 0.728 at 5 years). The multivariate analysis reached consistent results after adjustment by multiple confounders. When compared with a 27-gene signature, a 2-lncRNA signature, and Tumor-Node-Metastasis stage, the risk score also showed better performance (Pâ€‰<â€‰0.05). In conclusion, we successfully developed a robust mRNA-lncRNA signature that can accurately predict the relapse in laryngeal cancer.",2019,Journal of cellular biochemistry
Constructions dÃ©terministes pour la rÃ©gression parcimonieuse,"Dans cette these nous etudions certains designs deterministes pour la regression parcimonieuse. Notre problematique est largement inspiree du "" Compressed Sensing "" ou l'on cherche a acquerir et compresser simultanement un signal de grande taille a partir d'un petit nombre de mesures lineaires. Plus precisement, nous faisons le lien entre l'erreur d'estimation et l'erreur de prediction des estimateurs classiques (lasso, selecteur Dantzig et basis pursuit) et la distorsion (qui mesure l'"" ecart "" entre la norme 1 et la norme Euclidienne) du noyau du design considere. Notre etude montre que toute construction de sous-espaces de faibles distorsions (appeles sous-espaces "" presque ""- Euclidiens) conduit a de "" bons "" designs. Dans un second temps, nous nous interessons aux designs construits a partir de graphes expanseurs desequilibres. Nous en etablissons de maniere precise les performances en termes d'erreur d'estimation et d'erreur de prediction. Enfin, nous traitons la reconstruction exacte de mesures signees sur la droite reelle. Nous demontrons que tout systeme de Vandermonde generalise permet la reconstruction fidele de n'importe quel vecteur parcimonieux a partir d'un tres faible nombre d'observations. Dans une partie independante, nous etudions la stabilite de l'inegalite isoperimetrique sur la droite reelle pour des mesures log-concaves.",2011,
Bayesian generalized fused lasso modeling via NEG distribution,"Abstract The fused lasso penalizes a loss function by the L1 norm for both the regression coefficients and their successive differences to encourage sparsity of both. In this paper, we propose a Bayesian generalized fused lasso modeling based on a normal-exponential-gamma (NEG) prior distribution. The NEG prior is assumed into the difference of successive regression coefficients. The proposed method enables us to construct a more versatile sparse model than the ordinary fused lasso using a flexible regularization term. Simulation studies and real data analyses show that the proposed method has superior performance to the ordinary fused lasso.",2016,Communications in Statistics - Theory and Methods
Locating mental toughness in factor models of personality,"Abstract We examined the degree to which two factor models of personality capture mental toughness. We focused on item and facet-level correlates of mental toughness in three widely used personality inventories â€“ HEXACO and Hogan Personality Inventory (bright side personality) and Hogan Development Survey (dark side personality). US-based college students and NCAA athletes (Sample 1; Nâ€¯=â€¯285) and working adults (Sample 2; Nâ€¯=â€¯218) took the Sports Mental Toughness Questionnaire and the HEXACO-60 (Sample 1) and the HPI and HDS (Sample 2). We used lasso regression with cross-validation to identify the HEXACO-60's items and HPI and HDS' facets predicting mental toughness. In Study 1 the strongest predictors of mental toughness were 12 items coming from the eXtraversion, Emotionality, and Conscientiousness scales. Item content examinations indicated that people scoring high on mental toughness tended to endorse items related to pushing for results, having elevated energy levels, and a high degree of self-confidence. In Study 2, emotional control, ambition, self-efficacy, creativity, and low anxiety emerged as the strongest facet correlates of mental toughness. Mental toughness is located between factors of the HEXACO model, but much more squarely aligned with Ambition in the HPI inventory.",2019,Personality and Individual Differences
Variational Inference for Quantile Rgression,"of the Thesis Variational Inference for Quantile Rgression by Bufei Guo A.M. in Statistics Washington University in St. Louis, 2019. Professor Nan Lin, Chair Quantile regression (QR) (Koenker and Bassett, 1978), is an alternative to classic linear regression with extensive applications in many fields. This thesis studies Bayesian quantile regression (Yu and Moyeed, 2001) using variational inference, which is one of the alternative methods to the Markov chain Monte Carlo (MCMC) in approximating intractable posterior distributions. The lasso regularization is shown to be effective in improving the accuracy of quantile regression (Li and Zhu, 2008). This thesis developed variational inference for quantile regression and regularized quantile regression with the lasso penalty. Simulation results show that variational inference is a computationally more efficient alternative to the MCMC, while providing a comparable accuracy.",2019,
An expectationâ€“maximization algorithm for the Lasso estimation of quantitative trait locus effects,"The least absolute shrinkage and selection operator (Lasso) estimation of regression coefficients can be expressed as Bayesian posterior mode estimation of the regression coefficients under various hierarchical modeling schemes. A Bayesian hierarchical model requires hyper prior distributions. The regression coefficients are parameters of interest. The normal distribution assigned to each regression coefficient is a prior distribution. The variance parameter in the normal prior distribution is further assigned a hyper prior distribution so that the variance parameter can be estimated from the data. We developed an expectationâ€“maximization (EM) algorithm to estimate the variance parameter of the prior distribution for each regression coefficient. Performance of the EM algorithm was evaluated through simulation study and real data analysis. We found that the Jeffreysâ€™ hyper prior for the variance component usually performs well with regard to generating the desired sparseness of the regression model. The EM algorithm can handle not only the usual regression models but it also conveniently deals with linear models in which predictors are defined as classification variables. In the context of quantitative trait loci (QTL) mapping, this new EM algorithm can estimate both genotypic values and QTL effects expressed as linear contrasts of the genotypic values.",2010,Heredity
Prediction of cardiac death after adenosine myocardial perfusion SPECT based on machine learning,"BackgroundWe developed machine-learning (ML) models to estimate a patientâ€™s risk of cardiac death based on adenosine myocardial perfusion SPECT (MPS) and associated clinical data, and compared their performance to baseline logistic regression (LR). We demonstrated an approach to visually convey the reasoning behind a patientâ€™s risk to provide insight to clinicians beyond that of a â€œblack box.â€MethodsWe trained multiple models using 122 potential clinical predictors (features) for 8321 patients, including 551 cases of subsequent cardiac death. Accuracy was measured by area under the ROC curve (AUC), computed within a cross-validation framework. We developed a method to display the modelâ€™s rationale to facilitate clinical interpretation.ResultsThe baseline LR (AUCâ€‰=â€‰0.76; 14 features) was outperformed by all other methods. A least absolute shrinkage and selection operator (LASSO) model (AUCâ€‰=â€‰0.77; pâ€‰=â€‰.045; 6 features) required the fewest features. A support vector machine (SVM) model (AUCâ€‰=â€‰0.83; pâ€‰<â€‰.0001; 49 features) provided the highest accuracy.ConclusionsLASSO outperformed LR in both accuracy and simplicity (number of features), with SVM yielding best AUC for prediction of cardiac death in patients undergoing MPS. Combined with presenting the reasoning behind the risk scores, our results suggest that ML can be more effective than LR for this application.",2018,Journal of Nuclear Cardiology
