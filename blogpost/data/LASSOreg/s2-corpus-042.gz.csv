title,abstract,year,journal
Radiomics nomogram for differentiating between benign and malignant soft-tissue masses of the extremities.,"BACKGROUND
Preoperative differentiation between malignant and benign tumors is important for treatment decisions.


PURPOSE/HYPOTHESIS
To investigate/validate a radiomics nomogram for preoperative differentiation between malignant and benign masses.


STUDY TYPE
Retrospective.


POPULATION
Imaging data of 91 patients.


FIELD STRENGTH/SEQUENCE
T1 -weighted images (570 msec repetition time [TR]; 17.9 msec echo time [TE], 200-400 mm field of view [FOV], 208-512â€‰Ã—â€‰208-512 matrix), fat-suppressed fast-spin-echo (FSE) T2 -weighted images (T2 WIs) (4331 msec TR; 87.9 msec TE, 200-400 mm FOV, 312â€‰Ã—â€‰312 matrix), slice thickness 4 mm, and slice spacing 1 mm.


ASSESSMENT
Fat-suppressed FSE T2 WIs were selected for extraction of features. Radiomics features were extracted from fat-suppressed T2 WIs. A radiomics signature was generated from the training dataset using least absolute shrinkage and selection operator algorithms. Independent risk factors were identified by multivariate logistic regression analysis and a radiomics nomogram was constructed. Nomogram capability was evaluated in the training dataset and validated in the validation dataset. Performance of the nomogram, radiomics signature, and clinical model were compared.


STATISTICAL TESTS
1) Independent t-test or Mann-Whitney U-test: for continuous variables. Fisher's exact test or Ï‡2 test: comparing categorical variables between two groups. Univariate analysis: evaluating associations between clinical/morphological characteristics and malignancy. 2) Least absolute shrinkage and selection operator (LASSO)-logistic regression model: selection of malignancy features. 3) Significant clinical/morphological characteristics and radiomics signature were input variables for multiple logistic regression analysis. Area under the curve (AUC): evaluation of ability of the nomogram to identify malignancy. Hosmer-Lemeshow test and decision curve: evaluation and validation of nomogram results.


RESULTS
The radiomics nomogram was able to differentiate malignancy from benignity in the training and validation datasets with an AUC of 0.94. The nomogram outperformed both the radiomics signature and clinical model alone.


DATA CONCLUSION
This radiomics nomogram is a noninvasive, low-cost preoperative prediction method combining the radiomics signature and clinical model.


LEVEL OF EVIDENCE
3 Technical Efficacy: Stage 2 J. Magn. Reson. Imaging 2019.",2019,Journal of magnetic resonance imaging : JMRI
Now You See Me: High School Dropout and Machine Learning,"In this paper, we create an algorithm to predict which students are eventually going to drop out of US high school using information available in 9th grade. We show that using a naive model - as implemented in many schools - leads to poor predictions. In addition to this, we explain how schools can obtain more precise predictions by exploiting the big data available to them, as well as more sophisticated quantitative techniques. We also compare the performances of econometric techniques like Logistic Regression with Machine Learning tools such as Support Vector Machine, Boosting and LASSO. We offer practical advice on how to apply the new Machine Learning codes available in Stata to the high dimensional datasets available in education. Model parameters are calibrated by taking into account policy goals and budget constraints.",2017,
"Contribution of 30 biomarkers to 10-year cardiovascular risk estimation in 2 population cohorts: the MONICA, risk, genetics, archiving, and monograph (MORGAM) biomarker project.","BACKGROUND
Cardiovascular risk estimation by novel biomarkers needs assessment in disease-free population cohorts, followed up for incident cardiovascular events, assaying the serum and plasma archived at baseline. We report results from 2 cohorts in such a continuing study.


METHODS AND RESULTS
Thirty novel biomarkers from different pathophysiological pathways were evaluated in 7915 men and women of the FINRISK97 population cohort with 538 incident cardiovascular events at 10 years (fatal or nonfatal coronary or stroke events), from which a biomarker score was developed and then validated in the 2551 men of the Belfast Prospective Epidemiological Study of Myocardial Infarction (PRIME) cohort (260 events). No single biomarker consistently improved risk estimation in FINRISK97 men and FINRISK97 women and the Belfast PRIME Men cohort after allowing for confounding factors; however, the strongest associations (with hazard ratio per SD in FINRISK97 men) were found for N-terminal pro-brain natriuretic peptide (1.23), C-reactive protein (1.23), B-type natriuretic peptide (1.19), and sensitive troponin I (1.18). A biomarker score was developed from the FINRISK97 cohort with the use of regression coefficients and lasso methods, with selection of troponin I, C-reactive protein, and N-terminal pro-brain natriuretic peptide. Adding this score to a conventional risk factor model in the Belfast PRIME Men cohort validated it by improved c-statistics (P=0.004) and integrated discrimination (P<0.0001) and led to significant reclassification of individuals into risk categories (P=0.0008).


CONCLUSIONS
The addition of a biomarker score including N-terminal pro-brain natriuretic peptide, C-reactive protein, and sensitive troponin I to a conventional risk model improved 10-year risk estimation for cardiovascular events in 2 middle-aged European populations. Further validation is needed in other populations and age groups.",2010,Circulation
Comparison of variable selection methods for PLS-based soft sensor modeling,"Abstract Data-driven soft sensors have been widely used in both academic research and industrial applications for predicting hard-to-measure variables or replacing physical sensors to reduce cost. It has been shown that the performance of these data-driven soft sensors could be greatly improved by selecting only the vital variables that strongly affect the primary variables, rather than using all the available process variables. In this work, a comprehensive evaluation of different variable selection methods for PLS-based soft sensor development is presented, and a new metric is proposed to assess the performance of different variable selection methods. The following seven variable selection methods are compared: stepwise regression (SR), partial least squares with regression coefficients (PLS-BETA), PLS with variable importance in projection (PLS-VIP), uninformative variable elimination with PLS (UVE-PLS), genetic algorithm with PLS (GA-PLS), least absolute shrinkage and selection operator (Lasso), and competitive adaptive reweighted sampling with PLS (CARS-PLS). Their strengths and limitations for soft sensor development are demonstrated by a simulated case study and an industrial case study.",2015,Journal of Process Control
QSAR analysis of Multimodal Antidepressants Vortioxetine Analogs using Physicochemical Descriptors and MLR Modeling.,"BACKGROUND
Vortioxetine is a multimodal antidepressant drug with combined effects on SERT as inhibitor, 5-HT1A as agonist and 5-HT3A as antagonist. Series of vortioxetine analogs have been reported as multi antidepressant compounds and they block serotonin transport into the neuronal cells, activate the postsynaptic 5-HT1A receptors and eliminate the low activity of 5-HT3A receptors.


OBJECTIVE
To explore the important properties of vortioxetine analogs involved in antidepressant activity by developing 2D QSAR models.


METHODS
Selections of significant descriptors were performed by least absolute shrinkage and selection operator (LASSO) method and, the multiple linear regression (MLR) method and All Subsets and GA algorithm included in QSARINS software were used for generating QSAR models. Further, the virtual screening was performed based on bioactivity and structure similarity using the PubChem database.


RESULTS
The four descriptor model of complementary information content (CIC2), solubility (bcutp3), mass (bcutm8) and partial charge in van der Waals surface area (PEOEVSA7) of the molecules is obtained for SERT inhibition with the significant statistics of R2= 0.69, RMSEtr= 0.44, R2ext= 0.62 and CCCext= 0.78. For 5-HT1A agonist, the two descriptor model of molecular shape (Kappm3) and van der Waals volume of the atoms (bcutv11) with R2= 0.78, RMSEtr= 0.33, R2ext = 0.83, and CCCext= 0.87 is established. The three descriptor model of information content (IC3), solubility (bcutp9) and electronegativity (GATSe5) of the molecules with R2= 0.61, RMSEtr= 0.34, R2ext= 0.69 and CCCext= 0.72 is obtained for 5-HT3A antagonist. The antidepressant activities of 16 virtual screened compounds were predicted using the developed models.


CONCLUSIONS
The developed QSAR models may be useful to predict antidepressant activity for the newly synthesized vortioxetine analogs.",2018,Current computer-aided drug design
"Understanding electricity consumption: A comparative contribution of building factors, socio-demographics, appliances, behaviours and attitudes","This paper tests to what extent different types of variables (building factors, socio-demographics, appliance ownership and use, attitudes and self-reported behaviours) explain annualized electricity consumption in residential buildings with gas-fuelled space and water heating. It then shows which individual variables have the highest explanatory power. In contrast to many other studies, the study recognizes the problem of multicollinearity between predictors in regression analysis and uses Lasso regression to address this issue.",2016,Applied Energy
Acceleration Methods for Classic Convex Optimization Algorithms,"Most Machine Learning models are defined in terms of a convex optimization problem. Thus, developing algorithms to quickly solve such problems its of great interest to the field. We focus in this thesis on two of the most widely used models, the Lasso and Support Vector Machines. The former belongs to the family of regularization methods, and it was introduced in 1996 to perform both variable selection and regression at the same time. This is accomplished by adding a l1-regularization term to the least squares model, achieving interpretability and also a good generalization error. Support Vector Machines were originally formulated to solve a classification problem by finding the maximum-margin hyperplane, that is, the hyperplane which separates two sets of points and its at equal distance from both of them. SVMs were later extended to handle non-separable classes and non-linear classification problems, applying the kernel-trick. A first contribution of this work is to carefully analyze all the existing algorithms to solve both problems, describing not only the theory behind them but also pointing out possible advantages and disadvantages of each one. Although the Lasso and SVMs solve very different problems, we show in this thesis that they are both equivalent. Following a recent result by Jaggi, given an instance of one model we can construct an instance of the other having the same solution, and vice versa. This equivalence allows us to translate theoretical and practical results, such as algorithms, from one field to the other, that have been otherwise being developed independently. We will give in this thesis not only the theoretical result but also a practical application, that consists on solving the Lasso problem using the SMO algorithm, the state-of-the-art solver for non-linear SVMs. We also perform experiments comparing SMO to GLMNet, one of the most popular solvers for the Lasso. The results obtained show that SMO is competitive with GLMNet, and sometimes even faster. Furthermore, motivated by a recent trend where classical optimization methods are being re-discovered in improved forms and successfully applied to many problems, we have also analyzed two classical momentum-based methods: the Heavy Ball algorithm, introduced by Polyak in 1963 and Nesterovâ€™s Accelerated Gradient, discovered by Nesterov in 1983. In this thesis we develop practical versions of Conjugate Gradient, which is essentially equivalent to the Heavy Ball method, and Nesterovâ€™s Acceleration for the SMO algorithm. Experiments comparing the convergence of all the methods are also carried out. The results show that the proposed algorithms can achieve a faster convergence both in terms of iterations and execution time.",2017,
The development and validation of a CT-based radiomics signature for the preoperative discrimination of stage I-II and stage III-IV colorectal cancer,"OBJECTIVES
To investigative the predictive ability of radiomics signature for preoperative staging (I-IIvs.III-IV) of primary colorectal cancer (CRC).


METHODS
This study consisted of 494 consecutive patients (training dataset: n=286; validation cohort, n=208) with stage I-IV CRC. A radiomics signature was generated using LASSO logistic regression model. Association between radiomics signature and CRC staging was explored. The classification performance of the radiomics signature was explored with respect to the receiver operating characteristics(ROC) curve.


RESULTS
The 16-feature-based radiomics signature was an independent predictor for staging of CRC, which could successfully categorize CRC into stage I-II and III-IV (p <0.0001) in training and validation dataset. The median of radiomics signature of stage III-IV was higher than stage I-II in the training and validation dataset. As for the classification performance of the radiomics signature in CRC staging, the AUC was 0.792(95%CI:0.741-0.853) with sensitivity of 0.629 and specificity of 0.874. The signature in the validation dataset obtained an AUC of 0.708(95%CI:0.698-0.718) with sensitivity of 0.611 and specificity of 0.680.


CONCLUSIONS
A radiomics signature was developed and validated to be a significant predictor for discrimination of stage I-II from III-IV CRC, which may serve as a complementary tool for the preoperative tumor staging in CRC.",2016,Oncotarget
Variables that influence basic prosthetic mobility in people with non-vascular lower limb amputation.,"BACKGROUND
There exists a dearth of evidence on rehabilitation factors that influence prosthetic mobility in people with lower limb amputation (LLA). Examining variables that contribute to prosthetic mobility can inform rehabilitation interventions, providing guidance in developing more comprehensive care for these individuals.


OBJECTIVE
Determine the influence of modifiable and non-modifiable variables related to LLA and their impact on prosthetic mobility, using the International Classification of Functioning, Disability and Health (ICF) model. Secondarily, determining if personal factors and responses on self-report measures of balance and mobility are predictive of component timed-up-and-go (cTUG) performance.


DESIGN
Cross-sectional study of a convenience sample.


SETTING
National conference PARTICIPANTS: People (N=68) with non-vascular causes of unilateral LLA.


METHODS
Assessment of anthropometrics, mobility, bilateral hip extensor strength, hip range of motion, single limb balance, and self-report measures. Lasso linear regression and extreme gradient boosting analyses were used to determine influence of variables on prosthetic mobility.


MAIN OUTCOME MEASURE
Timed performance of the cTUG.


RESULTS
The following five variables were found to influence basic prosthetic mobility (p â‰¤.05) in people with transtibial (TT) amputation: hip extensor strength, hip range of motion, single limb balance, waist circumference, and age. In the TF cohort, number of comorbidities and waist circumference primarily influenced prosthetic mobility. Additionally, 66% of the variance in cTUG total time for the entire sample could be explained by simply regressing on level of amputation, number of comorbidities, age and Activities-specific Balance Confidence (ABC) scale score, all variables easily collected in a waiting room.


CONCLUSION
Variables that are modifiable with physical therapy intervention including hip extensor strength, hip range of motion, single limb balance, and waist circumference significantly influenced basic prosthetic mobility. These variables can be affected by targeted rehabilitation interventions and lifestyle changes.


LEVEL OF EVIDENCE
II This article is protected by copyright. All rights reserved.",2019,"PM & R : the journal of injury, function, and rehabilitation"
Big Data Study about the Effects of Weather Factors on Food Poisoning Incidence,"Abstract This research attempts an analysis that fuses the big data concerning weather variation and health care from January 1, 2011 to December 31, 2014; it gives the weather factor as to what kind of influence there is for the incidence of food poisoning, and also endeavors to be helpful regarding national health prevention. By using R, the Logistic and Lasso Logistic Regression were analyzed. The main factor germ generating the food poisoning was classified and the incidence was confirmed for the germ of bacteria and virus. According to the result of the analysis of Logistic Regression, we found that the incidence of bacterial food poisoning was affected by the following influences: the average temperature, amount of sunshine deviation, and deviation of temperature. Furthermore, the weather factors, having an effect on the incidence of viral food poisoning, were: the minimum vapor pressure, amount of sunshine deviation and deviation of temperature. This study confirmed the correlation of meteorological factors and incidence of food poisoning. It was also found out that even if the incidence from two causes were influenced by the same weather factor, the incidence might be oppositely affected by the characteristic of the germs.",2016,Journal of Digital Convergence
Equivalent Partial Correlation Selection for High Dimensional Gaussian Graphical Models,"Gaussian graphical models (GGMs) are frequently used to explore networks, such as gene regulatory networks, among a set of variables. Under the classical theory of GGMs, the graph construction amounts to finding the pairs of variables with nonzero partial correlation coefficients. However, this is infeasible for high dimensional problems for which the number of variables is larger than the sample size. We propose a surrogate of partial correlation coefficient, which is evaluated with a reduced conditional set and thus feasible for high dimensional problems. Under the faithfulness condition, we show that the surrogate partial correlation coefficient is equivalent to the true one in graph construction. The proposed method is not only computational efficient, but also outperforms the existing methods, such as graphical Lasso and node-wise regression, in graph construction, especially for the problems for which a large number of indirect associations are present. The proposed method is computationally efficient, and very flexible in data integration, covariate adjustment, network comparison, etc. Gaussian Graphical Model Consider a set of Gaussian random variables, there are two measures for their dependency: I Correlation Coefficient I Partial Correlation Coefficient: Compared to the partial correlation coefficient, the correlation coefficient is much weaker as marginally, i.e. directly or indirectly, all variables in a system are more or less correlated. The goal of GGM learning is to distinguish direct from indirect dependencies for all variables in a system. The partial correlation coefficient provides a measure for direct dependence as it will vanish for the indirect case. Applications I Gene regulatory networks: molecular mechanism underlying cancer with data available at The Cancer Genome Atlas (TCGA). I Causal networks: time course data. I Stock network I Mutual fund network Covariance Selection let X = (X (1), . . .X (p)) denote a p-dimensional random vector drawn from a multivariate Gaussian distribution Np(Î¼,Î£). Denote the concentration matrix by C = Î£âˆ’1 = (Cij). Then Ïij |V\{i ,j} = âˆ’ Ci ,j âˆš Ci ,iCj ,j , i , j = 1, . . . , p. (1) The covariance selection method (Dempster, 1972; Lauritzen, 1996) is to identify the non-zero elements in the concentration matrix. However, it is not feasible for high dimensional problems with p > n. Limited order partial correlations The idea is to use low-order partial correlation as a surrogate of the full-order partial correlation, and it is widely acknowledged that the limited order partial correlation methods can result in something inbetween the full GGM (with correlations conditioned on all p âˆ’ 2 variables) and the correlation graph. Related work: Spirtes et al. (2000), Magwene and Kim (2004), Wille and BÃ¼hlmann (2006), Castelo and Roverato (2006, 2009). Nodewise Regression It is to use Lasso (Tibshirani, 1996) as a variable selection method to identify the neighborhood of each variable, and thus the nonzero elements of the concentration matrix. Consider a linear regression X (j) = Î² (j) i X (i) + âˆ‘ râˆˆV\{j ,i} Î² (j) r X (r) + 2, (2) where 2(j) is a zero-mean Gaussian random error. Let S (j) = {r : Î² r 6= 0} denote the set of explanatory variables identified by Lasso for X (j). The GGM can then be constructed using the â€œorâ€-rule: estimate an edge between vertices i and j â‡â‡’ i âˆˆ S (j) or j âˆˆ S (i), or the â€œandâ€-rule: estimate an edge between vertices i and j â‡â‡’ i âˆˆ S (j) and j âˆˆ S (i). Graphical Lasso Yuan and Lin (2007) proposed to directly estimate the concentration matrix by minimizing âˆ’ log(det(C)) + trace(Î£Ì‚MLEC) + Î»â€–Câ€–, (3) where Î£Ì‚MLE denotes the maximum likelihood estimator of Î£, â€–Câ€– denotes the norm of C, and Î» is the regularization parameter. Later, the algorithm is accelerated by Friedman et al. (2008) and Banerjee et al. (2008). Graph Theory An undirected graph is a pair G = (V,E), where V is the set of vertices and E = (eij) is the adjacency matrix. I If two vertices i , j âˆˆ V forms an edge then we say that i and j are adjacent and set eij = 1. I The boundary set of a vertex v âˆˆ V, denoted by bG(v), is the set of vertices adjacent to v , i.e., bG(v) = {j : evj = 1}. I A path of length l > 0 from v0 to vl is a sequence v0, v1, . . . , vl of distinct vertices such that evkâˆ’1,vk = 1 for all k = 1, . . . , l . I The subset U âŠ‚ V is said to separate I âŠ‚ V from J âŠ‚ V if for every i âˆˆ I and j âˆˆ J, all paths from i to j have at least one vertex in U. I For a pair of vertices i 6= j with eij = 0, a set U âŠ‚ V is called a {i , j}-separator if it separates {i} and {j} in G. I Let Gij be a reduced graph of G with eij being set to zero. Then both the boundary sets bGij (i) and bGij (j) are {i , j}-separators in Gij . Graph Theory (continuation) Definition 1 We say that PV satisfies the Markov property with respect to G if for every triple of disjoint sets I, J,U âŠ‚ V, it holds that XI âŠ¥ XJ|XU whenever U separates I and J in G. Definition 2 We say that PV satisfies the adjacency faithfulness condition with respect to G: If two variables X (i) and X (j) are adjacent in G, then they are dependent conditioned on any subset of XV\{i ,j}. Graph Theory (continuation) The adjacency faithfulness condition implies that if there exists a subset U âŠ† V \ {i , j} such that X (i) âŠ¥ X |XU, then X (i) and X (j) are not adjacent in G. Further, by the Markov property, we have X (i) âŠ¥ X |XU =â‡’ X (i) âŠ¥ X |XV\{i ,j}, for any U âŠ† V \ {i , j} . (4) In particular, if U = âˆ…, we have X (i) and X (j) are marginally independent =â‡’ X (i) âŠ¥ X |XV\{i ,j}, (5) or, equivalently, Ïij |V\{i ,j} 6= 0 =â‡’ Corr{X ,X ) 6= 0. (6) Hence, to infer the conditional independence structure for a GGM, one may perform a correlation screening which can often reduce the dimensionality of the problem by a substantial amount. Equivalent Measure I Let G = (V, E) denote the correlation graph of X1, . . . ,Xp, where E = (áº½ij) is the edge set, and áº½ij = 1 if rij 6= 0 and 0 otherwise. I Let ÃŠÎ³i ,i ,âˆ’j = {v : |rÌ‚iv | > Î³i} \ {j} denote a reduced neighborhood of node i in Gij , where Gij denotes a reduced graph of G with áº½ij being set to 0, and Î³i denotes a threshold value. I ÃŠÎ³i ,i = {v : |rÌ‚iv | > Î³i} as a reduced neighborhood of node i in G. For any pair of vertices i and j , we define the partial correlation coefficient Ïˆij by Ïˆij = Ïij |Sij , (7) where Sij = ÃŠÎ³i ,i ,âˆ’j if |ÃŠÎ³i ,i ,âˆ’j | < |ÃŠÎ³j ,j ,âˆ’i | and Sij = ÃŠÎ³i ,i ,âˆ’j otherwise.",2015,
Predicting seizures in pregnant women with epilepsy: Development and external validation of a prognostic model,"BACKGROUND
Seizures are the main cause of maternal death in women with epilepsy, but there are no tools for predicting seizures in pregnancy. We set out to develop and validate a prognostic model, using information collected during the antenatal booking visit, to predict seizure risk at any time in pregnancy and until 6 weeks postpartum in women with epilepsy on antiepileptic drugs.


METHODS AND FINDINGS
We used datasets of a prospective cohort study (EMPiRE) of 527 pregnant women with epilepsy on medication recruited from 50 hospitals in the UK (4 November 2011-17 August 2014). The model development cohort comprised 399 women whose antiepileptic drug doses were adjusted based on clinical features only; the validation cohort comprised 128 women whose drug dose adjustments were informed by serum drug levels. The outcome was epileptic (non-eclamptic) seizure captured using diary records. We fitted the model using LASSO (least absolute shrinkage and selection operator) regression, and reported the performance using C-statistic (scale 0-1, values > 0.5 show discrimination) and calibration slope (scale 0-1, values near 1 show accuracy) with 95% confidence intervals (CIs). We determined the net benefit (a weighted sum of true positive and false positive classifications) of using the model, with various probability thresholds, to aid clinicians in making individualised decisions regarding, for example, referral to tertiary care, frequency and intensity of monitoring, and changes in antiepileptic medication. Seizures occurred in 183 women (46%, 183/399) in the model development cohort and in 57 women (45%, 57/128) in the validation cohort. The model included age at first seizure, baseline seizure classification, history of mental health disorder or learning difficulty, occurrence of tonic-clonic and non-tonic-clonic seizures in the 3 months before pregnancy, previous admission to hospital for seizures during pregnancy, and baseline dose of lamotrigine and levetiracetam. The C-statistic was 0.79 (95% CI 0.75, 0.84). On external validation, the model showed good performance (C-statistic 0.76, 95% CI 0.66, 0.85; calibration slope 0.93, 95% CI 0.44, 1.41) but with imprecise estimates. The EMPiRE model showed the highest net proportional benefit for predicted probability thresholds between 12% and 99%. Limitations of this study include the varied gestational ages of women at recruitment, retrospective patient recall of seizure history, potential variations in seizure classification, the small number of events in the validation cohort, and the clinical utility restricted to decision-making thresholds above 12%. The model findings may not be generalisable to low- and middle-income countries, or when information on all predictors is not available.


CONCLUSIONS
The EMPiRE model showed good performance in predicting the risk of seizures in pregnant women with epilepsy who are prescribed antiepileptic drugs. Integration of the tool within the antenatal booking visit, deployed as a simple nomogram, can help to optimise care in women with epilepsy.",2019,PLoS Medicine
Functional Linear Regression That â€™ s Interpretable,"Regression models to relate a scalar Y to a functional predictor X(t) are becoming increasingly common. Work in this area has concentrated on estimati ng a coefficient function, Î²(t), with Y related toX(t) through âˆ« Î²(t)X(t)dt. Regions whereÎ²(t) 6= 0 correspond to places where there is a relationship between X(t) andY. Alternatively, points whereÎ²(t) = 0 indicate no relationship. Hence, for interpretation purposes, it is de irable for a regression procedure to be capable of producing estimates of Î²(t) that are exactly zero over regions with no apparent relationship and have simple structures over the remaining regions. Unfortunately, most fitting procedures result in an estimate for Î²(t) that is rarely exactly zero and has unnatural wiggles making the curve hard to interpret. In this article we introd uce a new approach which uses variable selection ideas, applied to various derivatives o f Î²(t), to produce estimates that are both interpretable, flexible and accurate. We call our metho d â€œFunctional Linear Regression Thatâ€™s Interpretableâ€ (FLiRTI) and demonstrate it on simul ated and real world data sets. In addition, non-asymptotic theoretical bounds on the estima tion error are presented. The bounds provide strong theoretical motivation for our approach. Some key words : Interpretable Regression; Functional Linear Regression ; Da tzig Selector; Lasso.",2008,
The value of statistical or bioinformatics annotation for rare variant association with quantitative trait.,"In the past few years, a plethora of methods for rare variant association with phenotype have been proposed. These methods aggregate information from multiple rare variants across genomic region(s), but there is little consensus as to which method is most effective. The weighting scheme adopted when aggregating information across variants is one of the primary determinants of effectiveness. Here we present a systematic evaluation of multiple weighting schemes through a series of simulations intended to mimic large sequencing studies of a quantitative trait. We evaluate existing phenotype-independent and phenotype-dependent methods, as well as weights estimated by penalized regression approaches including Lasso, Elastic Net, and SCAD. We find that the difference in power between phenotype-dependent schemes is negligible when high-quality functional annotations are available. When functional annotations are unavailable or incomplete, all methods suffer from power loss; however, the variable selection methods outperform the others at the cost of increased computational time. Therefore, in the absence of good annotation, we recommend variable selection methods (which can be viewed as ""statistical annotation"") on top of regions implicated by a phenotype-independent weighting scheme. Further, once a region is implicated, variable selection can help to identify potential causal single nucleotide polymorphisms for biological validation. These findings are supported by an analysis of a high coverage targeted sequencing study of 1,898 individuals.",2013,Genetic epidemiology
Assoziation zwischen Restharnvolumen und Harnwegsinfektion,"ZusammenfassungHintergrundHarnwegsinfektionen (HWI) kÃ¶nnen Folge einer Blasenauslassobstruktion mit Restharnbildung sein. In einer aktuell publizierten Studie wurde hinsichtlich einer HWI asymptomatischen MÃ¤nnern ein Restharngrenzwert von â‰¥180Â ml definiert, der eine SensitivitÃ¤t von 87% und eine SpezifitÃ¤t von 98,5% bezÃ¼glich des Auftretens einer signifikanten Bakteriurie aufwies. Ziel der vorliegenden Untersuchung war, die Assoziation zwischen Restharnbildung und HWI bei asymptomatischen MÃ¤nnern zu Ã¼berprÃ¼fen und verschiedene Restharngrenzwerte zu validieren.Material und MethodeIn einer prospektiven Studie wurden 225 asymptomatische mÃ¤nnliche Patienten (medianes Alter 66Â Jahre) hinsichtlich folgender Kriterien untersucht: prostataspezifisches Antigen (PSA), Prostatavolumen, Internationaler Prostatasymptomenscore, maximaler Uroflow, Urinkultur, Urinschnelltest und Restharnvolumen. Mittels ROC-Analyse wurde ein Restharngrenzwert diskriminiert, aus dessen Assoziation mit einer signifikanten Bakteriurie bzw. einem positiven Urinschnelltest der hÃ¶chste AUC-Wert (â€žarea under the curve â€ž) resultierte. Der unabhÃ¤ngige Einfluss der erfassten Variablen auf die Entwicklung einer HWI (in Urinkultur oder Urinschnelltest) wurde mittels logistischer Regressionsanalyse Ã¼berprÃ¼ft.ErgebnisseBei 60% der asymptomatischen MÃ¤nner erfolgte die Miktion restharnfrei (â‰¤10Â ml); 31% der Probanden zeigten eine positive Urinkultur (n=69). Escherichia coli wurde bei 59 der 69Â MÃ¤nner (86%) mit positiver Urinkultur identifiziert. Patienten mit nachgewiesener Bakteriurie hatten ein signifikant hÃ¶heres Restharnvolumen im Vergleich zu Patienten mit negativer Urinkultur (113 vs. 41Â ml; p<0,001); 29Â MÃ¤nner (13%) hatten ein Restharnvolumen â‰¥180Â ml. Dieser Grenzwert besaÃŸ bezÃ¼glich des Nachweises einer positiven Urinkultur eine SensitivitÃ¤t und SpezifitÃ¤t von 28% bzw. 94% (AUC=0,606; p=0,012). In der Diskriminationsanalyse wies ein Restharngrenzwert von 150Â ml den hÃ¶chsten AUC-Wert auf (0,617). Restharnbildung zeigte in der multivariaten Regressionsanalyse einen unabhÃ¤ngigen Einfluss auf den Nachweis einer HWI (Urinkultur: p=0,006; Urinschnelltest: p<0,001).SchlussfolgerungenEs konnte kein Restharnvolumen als Grenzwert diskriminiert werden, das mit ausreichender SensitivitÃ¤t und SpezifitÃ¤t eine signifikant positive Urinkultur wahrscheinlich macht. Aus den Ergebnissen dieser Studie und den gegenwÃ¤rtig verfÃ¼gbaren Daten kann nicht sicher geschlussfolgert werden, ab welchem Restharnvolumen das Auftreten eines HWI wahrscheinlich und somit eine medikamentÃ¶se oder operative Intervention sinnvoll ist. Die Indikationsstellung zu einer therapeutischen Intervention muss sich folglich an weiteren Kriterien orientieren.AbstractPurposeUrinary tract infections can result from bladder outlet obstruction and consecutive post-void residual urine. In a recent publication, a cutoff for post-void residual urine of 180Â ml was calculated, revealing sensitivity and specificity of 87 and 98.5%, respectively, regarding occurrence of significant bacteriuria in asymptomatic men. In the present study the association between post-void residual urine volume and urinary tract infection was evaluated, and different cutoff values were validated.Materials und methodsA total of 225 asymptomatic patients (median age 66Â years) were prospectively evaluated regarding the following criteria: prostate-specific antigen, prostate volume, International Prostate Symptom Score, peak urinary flow rate, urine culture results, urinary test strip, and post-void residual urine volume. By ROC analysis a cutoff predicting significant bacteriuria was calculated, and different cutoff values were validated. The independent influence of several parameters on the incidence of urinary tract infection was measured using multivariate regression analyses.ResultsOf the patients, 60% were able to completely empty the bladder (post-void residual urine volume â‰¤10Â ml); 31% (n=69) had significant bacteriuria in the urine culture. Escherichia coli was identified in 59 of 69 patients (86%) with positive urine culture. Patients presenting with urinary tract infection had significantly higher mean post-void residual urine volumes than patients with negative urine culture (113 vs 41Â ml, p<0.001). In 29 men (13%) residual volume was 180Â ml or greater. Regarding the coincidence of urinary tract infection, this cutoff value showed sensitivity and specificity of 28 and 94%, respectively (AUC: 0.606, p=0.012). By ROC analysis a cutoff value of 150Â ml revealed the highest AUC value (0.617). Post-void residual volume had an independent significant influence on detection of urinary tract infection in multivariate regression analysis (urine culture: p=0.006; urinary test strip: p<0.001).ConclusionsNo cutoff value could be determined to predict positive urine culture with sufficient sensitivity and specificity. Based on the results of the present study and currently available data from the literature we are not able to recommend a cutoff value leading to therapeutic consequences. Hence, to establish the indication for treatment further criteria should be taken into consideration.",2010,Der Urologe
Geographically weighted lasso (GWL) study for modeling the diarrheic to achieve open defecation free (ODF) target,"Diarrhea has been one main cause of morbidity and mortality to children around the world, especially in the developing countries According to available data that was mentioned. It showed that sanitary and healthy lifestyle implementation by the inhabitants was not good yet. Inadequacy of environmental influence and the availability of health services were suspected factors which influenced diarrhea cases happened followed by heightened percentage of the diarrheic. This research is aimed at modelling the diarrheic by using Geographically Weighted Lasso method. With the existence of spatial heterogeneity was tested by Breusch Pagan, it was showed that diarrheic modeling with weighted regression, especially GWR and GWL, can explain the variation in each location. But, the absence of multi-collinearity cases on predictor variables, which were affecting the diarrheic, resulted in GWR and GWL modelling to be not different or identical. It is shown from the resulting MSE value. While from R2 value which usually higher on GWL model showed a significant variable predictor based on more parametric shrinkage value.",2014,
Controlling the false discovery rate via knockoffs,"In many fields of science, we observe a response variable together with a large number of potential explanatory variables, and would like to be able to discover which variables are truly associated with the response. At the same time, we need to know that the false discovery rate (FDR) - the expected fraction of false discoveries among all discoveries - is not too high, in order to assure the scientist that most of the discoveries are indeed true and replicable. This paper introduces the knockoff filter, a new variable selection procedure controlling the FDR in the statistical linear model whenever there are at least as many observations as variables. This method achieves exact FDR control in finite sample settings no matter the design or covariates, the number of variables in the model, or the amplitudes of the unknown regression coefficients, and does not require any knowledge of the noise level. As the name suggests, the method operates by manufacturing knockoff variables that are cheap - their construction does not require any new data - and are designed to mimic the correlation structure found within the existing variables, in a way that allows for accurate FDR control, beyond what is possible with permutation-based methods. The method of knockoffs is very general and flexible, and can work with a broad class of test statistics. We test the method in combination with statistics from the Lasso for sparse regression, and obtain empirical results showing that the resulting method has far more power than existing selection rules when the proportion of null variables is high.",2015,Annals of Statistics
A Hybrid Supervised Approach to Human Population Identification Using Genomics Data.,"Single nucleotide polymorphisms (SNPs) are one type of genetic variations and each SNP represents a difference in a single DNA building block, namely a nucleotide. Previous research demonstrated that SNPs can be used to identify the correct source population of an individual. In addition, variations in the DNA sequences have an influence on human diseases. In this regard, SNPs studies are helpful for personalised medicine and treatment. In the literature, unsupervised clustering methods especially principal component analysis (PCA) have been popular for studying population structure. In this study, we investigate supervised approaches, particularly the LASSO multinomial regression classification method, for recognizing individuals' origin genetic population. Then, we introduce PCA-LASSO as an extension of LASSO method that benefits from advantageous characteristics of both PCA and LASSO regression. The experimental results obtained on the 1000 genome project dataset show PCA-LASSO's significantly high accuracy in prediction of individual's origin population.",2019,IEEE/ACM transactions on computational biology and bioinformatics
m6A RNA methylation regulators can contribute to malignant progression and impact the prognosis of bladder cancer,"N6-methyladenosine (m6A) is the most common form of mRNA modification. An increasing number of studies have provenÂ that m6A RNA methylation regulators are overexpressed in many cancers and participate in the development of cancer through the dynamic regulation of m6A RNA methylation regulators. However, the prognosticÂ role of m6A RNA methylation regulators in bladder cancer (BC) is poorly understood. In this study, we downloaded the mRNA expressionÂ data from The Cancer Genome Atlas (TCGA) database and the corresponding clinical and prognostic information. The relationship between m6A RNA methylation regulators and clinicopathologicalÂ variables was assessed by the Kolmogorov-Smirnov test. The expression of the m6A RNA methylation regulatorsÂ was differentially associated with different clinicopathological variables of BC patients. The least absolute shrinkage and selection operator (LASSO) Cox regression modelÂ was then applied to identify three m6A RNA methylation regulators. The riskÂ signatureÂ was constructed as follows: 0.164FTO - (0.081YTHDC1+0.032WTAP). Based on the risk signature, the risk score of each patient was calculated, and the patients were divided into a high-risk group and a low-risk group. TheÂ overall survival (OS) rate of the high-risk group was significantly lower than that of the low-risk group. The risk signature wasÂ not only an independent prognostic markerÂ for BC patients but also a predictor of clinicopathologicalÂ variables. In conclusion, m6A RNA methylation regulators can participate in the malignant progression of BC, and a risk signatureÂ with three selected m6A RNA methylation regulators may be a promising prognostic biomarker to guide personalized treatment for BC patients.",2019,Bioscience Reports
Identification of Novel Genes Associated with Platelet Activation Signalling Pathways in High Dimensional Data Using an Alternative Regression Approach,"Platelet activation involves different signalling pathways in the underlying thrombus formation process. These pathways are the result of platelet responses to agonists9 activation. Previous analyses involving four pathways (P-selectin in response to adenosine diphosphate (ADP), P-selectin in response to cross-linked polypeptide (CRP), fibrinogen binding stimulated with ADP and fibrinogen binding stimulated with CRP) revealed genomic associations regulating these pathways. These analyses were performed on single nucleotide polymorphisms data (SNPs) in which the underlying characteristic of these data normally contains small number of observations (N) and large number of variables or features (p). However, the methodologies used in analysing these genomic data involved linear models using stepwise regression. We argue that this approach deemed to be sub optimal for linear modelling analyses. We propose an alternative approach using more rob ust methods such as ridge regression and LASSO that would produce previously unknown novel SNPs describing their effects on the four pathways in the available large pool of SNPs. Methodology The genome-wide association (GWA) data containing 1554 single nucleotide polymorphisms (SNPs) for 512 individuals describing their effects on four signalling pathways were previously analysed statistically using stepwise regression[1] which is sub optimal[2]. We statistically re-analysed these data using both stepwise regression and shrinkage approaches. Results Several of the SNPs and their associated genes identified using our new stepwise approach were not previously selected, though are now found to be significant. Conclusion We propose shrinkage approach for linear models using ridge regression and LASSO for statistical analysis of genomic data with large p and small N.",2014,Heart
Using imaging biomarkers to predict radiation induced xerostomia in head and neck cancer,"In this study, we analyzed baseline CT- and MRI-based image features of salivary glands to predict radiation-induced xerostomia after head-and-neck cancer (HNC) radiotherapy. A retrospective analysis was performed on 216 HNC patients who were treated using radiotherapy at a single institution between 2009 and 2016. CT and T1 post-contrast MR images along with NCI-CTCAE xerostomia grade (3-month follow-up) were prospectively collected at our institution. Image features were extracted for ipsilateral/contralateral parotid and submandibular glands relative to the location of the primary tumor. Dose-volume-histogram (DVH) parameters were also acquired. Features that were correlated with xerostomia (p<0.05) were further reduced using a LASSO logistic regression. Generalized Linear Model (GLM) and the Support Vector Machine (SVM) classifiers were used to predict xerostomia under five conditions (DVH-only, CT-only, MR-only, CT+MR, and DVH+CT+MR) using a ten-fold cross validation. The prediction performance was determined using the area under the receiver operator characteristic curve (ROC-AUC). DeLongâ€™s test was used to determine the difference between the ROC curves. Among extracted features, 13 CT, 6 MR, and 4 DVH features were selected. The ROC-AUC values for GLM/SVM classifiers with DVH, CT, MR, CT+MR and all features were 0.72Â±0.01/0.72Â±0.01, 0.73Â±0.01/0.68Â±0.01, 0.68Â±0.01/0.63Â±0.01, 0.74Â±0.01/0.75Â±0.01, and 0.78Â±0.01/0.79Â±0.01, respectively. DeLongâ€™s test demonstrated an improved in AUC for both classifiers with the addition of all features compared to DVH, CT, and MR-alone (p<0.05) and the SVM CT+MR model (p=0.03). The integration of baseline image features into prediction models has the potential to improve xerostomia risk stratification with the ultimate goal of personalized HNC radiotherapy.",2019,
A Data Adaptive Biological Sequence Representation for Supervised Learning,"Proper expression of the genes plays a vital role in the function of an organism. Recent advancements in DNA microarray technology allow for monitoring the expression level of thousands of genes. One of the important tasks in this context is to understand the underlying mechanisms of gene regulation. Recently, researchers have focused on identifying local DNA elements, or motifs to infer the relation between the expression and the nucleotide sequence of the gene. This study proposes a novel data adaptive representation approach for supervised learning to predict the response associated with the biological sequences. Biological sequences such as DNA and protein are a class of categorical sequences. In machine learning, categorical sequences are generally mapped to a lower dimensional representation for learning tasks to avoid problems with high dimensionality. The proposed method, namely SW-RF (sliding window-random forest), is a feature-based approach requiring two main steps to learn a representation for categorical sequences. In the first step, each sequence is represented by overlapping subsequences of constant length. Then a tree-based learner on this representation is trained to obtain a bag-of-words like representation which is the frequency of subsequences on the terminal nodes of the tree for each sequence. After representation learning, any classifier can be trained on the learned representation. A lasso logistic regression is trained on the learned representation to facilitate the identification of important patterns for the classification task. Our experiments show that proposed approach provides significantly better results in terms of accuracy on both synthetic data and DNA promoter sequence data. Moreover, a common problem for microarray datasets, namely missing values, is handled efficiently by the tree learners in SW-RF. Although the focus of this paper is on biological sequences, SW-RF is flexible in handling any categorical sequence data from different applications.",2018,Journal of Healthcare Informatics Research
Should Penalized Least Squares Regression be Interpreted as Maximum A Posteriori Estimation?,"Penalized least squares regression is often used for signal denoising and inverse problems, and is commonly interpreted in a Bayesian framework as a Maximum a posteriori (MAP) estimator, the penalty function being the negative logarithm of the prior. For example, the widely used quadratic program (with an <i>l</i><sup>1</sup> penalty) associated to the LASSO/basis pursuit denoising is very often considered as MAP estimation under a Laplacian prior in the context of additive white Gaussian noise (AWGN) reduction. This paper highlights the fact that, while this is one possible Bayesian interpretation, there can be other equally acceptable Bayesian interpretations. Therefore, solving a penalized least squares regression problem with penalty Ï†(<i>x</i>) need not be interpreted as assuming a prior <i>C</i>Â·exp(-Ï†(<i>x</i>)) and using the MAP estimator. In particular, it is shown that for any prior <i>PX</i>, the minimum mean-square error (MMSE) estimator is the solution of a penalized least square problem with some penalty Ï†(<i>x</i>) , which can be interpreted as the MAP estimator with the prior <i>C</i>Â·exp(-Ï†(<i>x</i>)). Vice versa, for certain penalties Ï†(<i>x</i>), the solution of the penalized least squares problem is indeed the MMSE estimator, with a certain prior <i>PX</i> . In general <i>dPX</i>(<i>x</i>) â‰  <i>C</i>Â·exp(-Ï†(<i>x</i>))<i>dx</i>.",2011,IEEE Transactions on Signal Processing
Genomic Selection using Multiple Populations,"Using different populations in genomic selection raises the possibility of marker effects varying across populations. However, common models for genomic selection only account for the main marker effects, assuming that they are consistent across populations. We present an approach in which the main plus population-specific marker effects are simultaneously estimated in a single mixed model. Cross-validation is used to compare the predictive ability of this model to that of the ridge regression best linear unbiased prediction (RR-BLUP) method involving only either the main marker effects or the population-specific marker effects. We used a maize (Zea mays L.) data set with 312 genotypes derived from five biparental populations, which were genotyped with 39,339 markers. A combined analysis incorporating genotypes for all the populations and hence using a larger training set was better than separate analyses for each population. Modeling the main plus the population-specific marker effects simultaneously improved predictive ability only slightly compared with modeling only the main marker effects. The performance of the RR-BLUP method was comparable to that of two regularization methods, namely the ridge regression and the elastic net, and was more accurate than that of the least absolute shrinkage and selection operator (LASSO). Overall, combining information from related populations and increasing the number of genotypes improved predictive ability, but further allowing for population-specific marker effects made minor improvement.",2012,Crop Science
Smoothing proximal gradient method for general structured sparse regression,"We study the problem of estimating high-dimensional regression models regularized by a structured sparsity-inducing penalty that encodes prior structural information on either the input or output variables. We consider two widely adopted types of penalties of this kind as motivating examples: (1) the general overlapping-group-lasso penalty, generalized from the group-lasso penalty; and (2) the graph-guided-fused-lasso penalty, generalized from the fused-lasso penalty. For both types of penalties, due to their nonseparability and nonsmoothness, developing an efficient optimization method remains a challenging problem. In this paper we propose a general optimization approach, the smoothing proximal gradient (SPG) method, which can solve structured sparse regression problems with any smooth convex loss under a wide spectrum of structured sparsity-inducing penalties. Our approach combines a smoothing technique with an effective proximal gradient method. It achieves a convergence rate significantly faster than the standard first-order methods, subgradient methods, and is much more scalable than the most widely used interior-point methods. The efficiency and scalability of our method are demonstrated on both simulation experiments and real genetic data sets.",2012,The Annals of Applied Statistics
SÃ©lection des prÃ©dicteurs pour la mise Ã  l'Ã©chelle des donnÃ©es MCG par la mÃ©thode Lasso.,"Au cours des 10 dernieres annees, les techniques de reduction d'echelle (dynamiques ou 
statistiques) ont ete largement developpees afin de fournir une information sur le 
changement climatique a une resolution plus fine que celle fournie par les modeles 
climatiques globaux (MCG). Vu que la plus grande preoccupation des techniques de 
reduction d'echelle est de fournir l'information la plus precise possible, les analystes ont 
essaye de nombreuses methodes pour ameliorer la selection des predicteurs, etape 
cruciale en reduction d'echelle statistique. 
Des methodes classiques sont utilisees, telles que les methodes de regression simple en 
particulier la methode de regression pas a pas. Cependant, cette derniere presente 
quelques limites en traitant les problemes de colinearite des variables ainsi qu'en 
fournissant des modeles complexes difficiles a interpreter. La methode lasso est utilisee 
comme une deuxieme alternative. L'objectif de cette etude est la comparaison des 
performances d'une methode classique de regression (regression pas a pas) et de la 
methode lasso. Pour ce faire, des series de donnees de 9 stations situees au sud du Quebec 
ainsi que 25 predicteurs, s'etalant sur la periode de 1961-1990 sont exploites. Les 
resultats indiquent qu'en raison de ses avantages de calcul et de sa facilite 
d'implementation, lasso donne de meilleurs resultats en se basant sur le coefficient de 
determination et l'erreur quadratique moyenne (EQM) utilises comme outils de 
comparaison des performances.",2012,
Correlation Research of Centralities on Complex Network by Statistical Learning,"In network theory and network analysis, indicators of centrality identify the most important vertices on complex networks. In this paper, we perform analysis on correlations of 13 centralities on ER random network and research how the Radial centralities interpret the Medial centralities adequately by statistical learning approaches such as linear regression, forwardand backward-stepwise selection and lasso. As a result, it is illustrated that some centralities on ER random networks with different connecting probability p always display strong correlations, and the Medial centrality can be interpreted by the Radial centralities. Furthermore, the linear regression is used to fit the relationship and retain some centralities to describe a medial centrality in our example, which will help to solve the problem that a centrality we donâ€™t have a ready algorithm and compute difficultly. The methods proposed by statistical learning provide an alternative way to obtain better understanding of the centralities and reveal the relationship among them. Introduction The study of random networks started with the influential work of ErdÃ¶s and RÃ©nyi in the 1950s and 1960s. In the study the assumption has been made that the presence or absence of an edge between two vertices is independent with the presence or absence of any other edge, so that each edge may be considered to be present with independent probability p. If there are N vertices in a network, and each is connected to an average of z edges, it is easy to show that p=z/(N-1), which for lager N is usually approximated by z/N[1]. The number of edges connected to any particular vertex is called the degree k of that vertex, and has a probability distribution pk given by (1 ) , ! k z k N k k N z e p p p k k ï€­ ï€­ ïƒ¦ ïƒ¶ ï€½ ï€­ ï‚» ïƒ§ ïƒ· ïƒ¨ ïƒ¸ (1) where the second equality becomes exact in the limit of lager N. To view a complex network, a direct way is to identify the most influential nodes and many centralities are proposed to locate important ones. Degree centrality of a node v is the fraction of nodes it is connected to. Closeness centrality of a node u is the reciprocal of the average shortest path distance to u over n-1 reachable nodes. Betweenness centrality of a node v is the sum of the fraction of all-pairs shortest paths that pass through v. Eigenvector centrality computes the centrality for a node based on the centrality of its neighbors. PageRank computes a ranking of the nodes in the network G based on the structure of the incoming links. It was originally designed as an algorithm to rank web pages. Till now, there are hundreds of ways to define the centralities. The centrality can be classified into Radial or Medial classes according to its construction. Centralities are Radial centralities counting walks which start/end from the given vertex[2][3]. The degree and eigenvector centralities are examples of radial centralities. Medial centralities count walks which pass through the given vertex. The canonical example is betweenness centrality, counting the number of shortest paths which pass through the given vertex.",2018,DEStech Transactions on Computer Science and Engineering
La radiomique du cancer du rein mÃ©tastatique,"Le developpement de l'analyse de donnees massives en medecine a suscite un interet pour la quantification des donnees d'imagerie. La radiomique est un nouveau champ de recherche base sur les donnees qui extrait un grand nombre de caracteristiques des images medicales. Cependant, il n'y a pas de consensus sur la facon d'extraire ou d'analyser les descripteurs. Il est donc necessaire de centrer la recherche sur la validation des techniques et processus. Notre travail etait centre sur une population de patients porteurs de cancer du rein metastatiques traites par anti-angiogeniques et pour: 1 / extraire des donnees cliniques et d'imagerie a partir de dossiers medicaux electroniques en utilisant des techniques bio-informatiques; 2 / le developpement et le controle qualite d'un logiciel d'extraction de caracteristiques; 3 / le developpement d'une strategie de reduction des descripteurs basee sur la reproductibilite et la redondance et l'analyse des facteurs impactant cette etape tels que le nombre de pixels; 4 / une application clinique explorant l'association des descripteurs radiomiques avec la survie sans progression et la survie globale en utilisant une regression de Cox avec penalisation LASSO et analyse bootstrap pour prendre en compte le sur-ajustement. Nos resultats ont montre que les etapes de reduction des descripteurs etaient reproductibles lorsqu'elles etaient testees dans deux populations independantes. Certains descripteurs etaient tres sensibles au nombre de pixels dans la lesion, montrant que les resultats ne sont pas toujours generalisables entre les populations. Enfin, une signature radiomique composee de trois parametres etait predictive de la survie sans progression dans notre population.",2018,
Does generalization performance of lq regularization learning depend on q? A negative example,"$l^q$-regularization has been demonstrated to be an attractive technique in machine learning and statistical modeling. It attempts to improve the generalization (prediction) capability of a machine (model) through appropriately shrinking its coefficients. The shape of a $l^q$ estimator differs in varying choices of the regularization order $q$. In particular, $l^1$ leads to the LASSO estimate, while $l^{2}$ corresponds to the smooth ridge regression. This makes the order $q$ a potential tuning parameter in applications. To facilitate the use of $l^{q}$-regularization, we intend to seek for a modeling strategy where an elaborative selection on $q$ is avoidable. In this spirit, we place our investigation within a general framework of $l^{q}$-regularized kernel learning under a sample dependent hypothesis space (SDHS). For a designated class of kernel functions, we show that all $l^{q}$ estimators for $0< q < \infty$ attain similar generalization error bounds. These estimated bounds are almost optimal in the sense that up to a logarithmic factor, the upper and lower bounds are asymptotically identical. This finding tentatively reveals that, in some modeling contexts, the choice of $q$ might not have a strong impact in terms of the generalization capability. From this perspective, $q$ can be arbitrarily specified, or specified merely by other no generalization criteria like smoothness, computational complexity, sparsity, etc..",2013,ArXiv
Regularization and feature selection for large dimensional data.,"Feature selection has evolved to be an important step in several machine learning paradigms. In domains like bio-informatics and text classification which involve data of high dimensions, feature selection can help in drastically reducing the feature space. In cases where it is difficult or infeasible to obtain sufficient number of training examples, feature selection helps overcome the curse of dimensionality which in turn helps improve performance of the classification algorithm. The focus of our research here are five embedded feature selection methods which use either the ridge regression, or Lasso regression, or a combination of the two in the regularization part of the optimization function. We evaluate five chosen methods on five large dimensional datasets and compare them on the parameters of sparsity and correlation in the datasets and their execution times.",2019,arXiv: Learning
