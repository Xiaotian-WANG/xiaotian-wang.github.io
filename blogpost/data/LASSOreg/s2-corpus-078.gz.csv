title,abstract,year,journal
Explanatory and Causal Analysis of the MIBEL Electricity Market Spot Price,"This paper analyzes the electricity prices of the MIBEL electricity spot market with respect to a set of possible explanatory variables. Understanding the main drivers of the electricity price is a key aspect in understanding price formation and in developing forecasting models, which are essential for the selling and buying strategies of market agents. For this analysis, different techniques have been applied in this work, including standard and lasso regression models, causal analysis based on bayesian networks and classification trees. Results from the different approaches are coherent and show strong dependency of the electricity prices with the Portuguese imported coal for lower non-dispatchable net demands, which has been progressively replaced by gas for larger non-dispatchable net demands. Hydro reservoirs and hydro production are also main explanatory variables of the electricity price for all non-dispatchable net demand levels.",2019,2019 IEEE Milan PowerTech
A small review and further studies on the LASSO,"Abstract High-dimensional data analysis arises from almost all scienti c areas, evolving withdevelopment of computing skills, and has encouraged penalized estimations that playimportant roles in statistical learning. For the past years, various penalized estimationshave been developed, and the least absolute shrinkage and selection operator (LASSO)proposed by Tibshirani (1996) has shown outstanding ability, earning the rst place onthe development of penalized estimation. In this paper, we rst introduce a number ofrecent advances in high-dimensional data analysis using the LASSO. The topics includevarious statistical problems such as variable selection and grouped or structured variableselection under sparse high-dimensional linear regression models. Several unsupervisedlearning methods including inverse covariance matrix estimation are presented. In ad-dition, we address further studies on new applications which may establish a guidelineon how to use the LASSO for statistical challenges of high-dimensional data analysis.Keywords: High dimension, LASSO, penalized estimation, review.",2013,
Replicability of Machine Learning Models in the Social Sciences: A Case Study in Variable Selection,"Machine learning tools are increasingly used in social sciences and policy fields due to their increase in predictive accuracy. However, little research has been done on how well the models of machine learning methods replicate across samples. We compare machine learning methods with regression on the replicability of variable selection, along with predictive accuracy, using an empirical dataset as well as simulated data with additive, interaction, and non-linear squared terms added as predictors. Methods analyzed include support vector machines (SVM), random forests (RF), multivariate adaptive regression splines (MARS), and the regularized regression variants, least absolute shrinkage and selection operator (LASSO), and elastic net. In simulations with additive and linear interactions, machine learning methods performed similarly to regression in replicating predictors; they also performed mostly equal or below regression on measures of predictive accuracy. In simulations with square terms, machine learning methods SVM, RF, and MARS improved predictive accuracy and replicated predictors better than regression. Thus, in simulated datasets, the gap between machine learning methods and regression on predictive measures foreshadowed the gap in variable selection. In replications on the empirical dataset, however, improved prediction by machine learning methods was not accompanied by a visible improvement in replicability in variable selection. This disparity is explained by the overall explanatory power of the models. When predictors have small effects and noise predominates, improved global measures of prediction in a sample by machine learning methods may not lead to the robust selection of predictors; thus, in the presence of weak predictors and noise, regression remains a useful tool for model building and replication.",2018,Zeitschrift fÃ¼r Psychologie
Pretest and Stein-Type Estimations in Quantile Regression Model,"In this study, we consider preliminary test and shrinkage estimation strategies for quantile regression models. In classical Least Squares Estimation (LSE) method, the relationship between the explanatory and explained variables in the coordinate plane is estimated with a mean regression line. In order to use LSE, there are three main assumptions on the error terms showing white noise process of the regression model, also known as Gauss-Markov Assumptions, must be met: (1) The error terms have zero mean, (2) The variance of the error terms is constant and (3) The covariance between the errors is zero i.e., there is no autocorrelation. However, data in many areas, including econometrics, survival analysis and ecology, etc. does not provide these assumptions. First introduced by Koenker, quantile regression has been used to complement this deficiency of classical regression analysis and to improve the least square estimation. The aim of this study is to improve the performance of quantile regression estimators by using pre-test and shrinkage strategies. A Monte Carlo simulation study including a comparison with quantile $L_1$--type estimators such as Lasso, Ridge and Elastic Net are designed to evaluate the performances of the estimators. Two real data examples are given for illustrative purposes. Finally, we obtain the asymptotic results of suggested estimators",2017,arXiv: Statistics Theory
Quantile Regression Applied to Genome-Enabled Prediction of Traits Related to Flowering Time in the Common Bean,"Genomic selection (GS) aims to incorporate molecular information directly into the prediction of individual genetic merit. Regularized quantile regression (RQR) can be used to fit models for all portions of a probability distribution of the trait, enabling the conditional quantile that â€œbestâ€ represents the functional relationship between dependent and independent variables to be chosen. The objective of this study was to predict the individual genetic merits of the traits associated with flowering time (DFFâ€”days to first flower; DTFâ€”days to flower) in the common bean using RQR and to compare the predictive abilities obtained from Random Regression Best Linear Unbiased Predictor (RR-BLUP), Bayesian LASSO (BLASSO), BayesB, and RQR for predicting the genetic merit. GS was performed using 80 genotypes of common beans genotyped for 380 single nucleotide polymorphism (SNP) markers. Considering the â€œbestâ€ RQR fit models (RQR0.3 for DFF, and RQR0.2 for DTF), the gains in predictive ability in relation to BLASSO, BayesB, and RR-BLUP were 18.75%, 22.58%, and 15.15% for DFF, respectively, and 15.20%, 24.65%, and 12.55% for DTF, respectively. The potential cultivars selected, considering the RQR â€œbestâ€ models, were among the 5% of cultivars with the lowest genomic estimated breeding value (GEBV) for the DFF and DTF traitsâ€”the IAC Imperador, IPR Colibri, Capixaba Precoce, and IPR Andorinha were included in the list of early cycle cultivars.",2019,Agronomy
Selective Sequential Model Selection,"Many model selection algorithms produce a path of fits specifying a sequence of increasingly complex models. Given such a sequence and the data used to produce them, we consider the problem of choosing the least complex model that is not falsified by the data. Extending the selected-model tests of Fithian et al. (2014), we construct p-values for each step in the path which account for the adaptive selection of the model path using the data. In the case of linear regression, we propose two specific tests, the max-t test for forward stepwise regression (generalizing a proposal of Buja and Brown (2014)), and the next-entry test for the lasso. These tests improve on the power of the saturated-model test of Tibshirani et al. (2014), sometimes dramatically. In addition, our framework extends beyond linear regression to a much more general class of parametric and nonparametric model selection problems. 
To select a model, we can feed our single-step p-values as inputs into sequential stopping rules such as those proposed by G'Sell et al. (2013) and Li and Barber (2015), achieving control of the familywise error rate or false discovery rate (FDR) as desired. The FDR-controlling rules require the null p-values to be independent of each other and of the non-null p-values, a condition not satisfied by the saturated-model p-values of Tibshirani et al. (2014). We derive intuitive and general sufficient conditions for independence, and show that our proposed constructions yield independent p-values.",2015,arXiv: Methodology
Wafer yield prediction using derived spatial variables,"Unreliable chips tend to form spatial clusters on semiconductor wafers. The spatial patterns of these defects are largely reflected in functional testing results. However, the spatial cluster information of unreliable chips has not been fully used to predict the performance in field use in the literature. This paper proposes a novel wafer yield prediction model that incorporates the spatial clustering information in functional testing. Fused LASSO is first adopted to derive variables based on the spatial distribution of defect clusters. Then, a logistic regression model is used to predict the final yield (ratio of chips that remain functional until expected lifetime) with derived spatial covariates and functional testing values. The proposed model is evaluated both on real production wafers and in an extensive simulation study. The results show that by explicitly considering the characteristics of defect clusters, our proposed model provides improved performance compared to existing methods. Moreover, the cross-validation experiments prove that our approach is capable of using historical data to predict yield on newly produced wafers.",2017,Quality and Reliability Eng. Int.
A Nodewise Regression Approach to Estimating Large Portfolios,"This paper investigates the large sample properties of the variance, weights, and risk of high-dimensional portfolios where the inverse of the covariance matrix of excess asset returns is estimated using a technique called nodewise regression. Nodewise regression provides a direct estimator for the inverse covariance matrix using the Least Absolute Shrinkage and Selection Operator (Lasso) of Tibshirani (1994) to estimate the entries of a sparse precision matrix. We show that the variance, weights, and risk of the global minimum variance portfolios and the Markowitz mean-variance portfolios are consistently estimated with more assets than observations. We show, empirically, that the nodewise regression-based approach performs well in comparison to factor models and shrinkage methods.",2016,arXiv: Statistics Theory
Grouped graphical Granger modeling methods for temporal causal modeling,"We develop and evaluate an approach to causal modeling based on time series data, collectively referred to as ""grouped graphical Granger modeling methods."" Graphical Granger modeling uses graphical modeling techniques on time series data and invokes the notion of ""Granger causality"" to make assertions on causality among a potentially large number of time series variables through inference on time-lagged effects. The present paper proposes a novel enhancement to the graphical Granger methodology by developing and applying families of regression methods that are sensitive to group information among variables, to leverage the group structure present in the lagged temporal variables according to the time series they belong to. Additionally, we propose a new family of algorithms we call group boosting, as an improved component of grouped graphical Granger modeling over the existing regression methods with grouped variable selection in the literature (e.g group Lasso). The introduction of group boosting methods is primarily motivated by the need to deal with non-linearity in the data. We perform empirical evaluation to confirm the advantage of the grouped graphical Granger methods over the standard (non-grouped) methods, as well as that specific to the methods based on group boosting. This advantage is also demonstrated for the real world application of gene regulatory network discovery from time-course microarray data.",2009,
Serum Newborn Screening Blood Metabolites Are not Associated With Childhood-onset Inflammatory Bowel Disease: A Population-based Matched Case-control Study.,"BACKGROUND
Originally used for screening of inborn errors of metabolism, routine metabolite profiles of newborns have also been associated with prematurity and some childhood diseases. We sought to determine whether metabolites measured during routine newborn screening could identify infants who develop inflammatory bowel disease (IBD) in childhood.


METHODS
We conducted a population-based matched case-control study using health administrative data from Ontario, Canada. Children born 2006 to 2015 with IBD were identified using a validated algorithm and matched to 5 controls based on birth date, sex, rural/urban household, and mean neighborhood income quintile at birth. Cases and controls were linked deterministically to metabolic profiles from Newborn Screening Ontario. We fit a lasso penalized logistic regression model and used 10-fold cross-validation to obtain internally valid performance measures. Models included metabolites, amino acids, and endocrine markers. Models also included ratios of metabolites, gestational age, birth weight, mode of delivery, age at serum collection, maternal age at delivery, maternal history of IBD, and parity.


RESULTS
Three hundred eight cases of IBD, diagnosed at 5.5 Â± 2.8 years, were matched to 1540 controls. No individual metabolites were associated with IBD. The c-statistic was 0.50 for the training data. After 10-fold cross-validation the C statistic was 0.50, indicating no significant association between metabolites and IBD diagnosis.


CONCLUSIONS
Newborn screening serum metabolites could not identify children who will develop IBD in this population-based cohort. Future studies with an expanded panel of metabolites may provide improved prediction of IBD.",2019,Inflammatory bowel diseases
Introducing complex dependency structures into supervised components-based models,"Une forte redondance des variables explicatives cause de gros problemes d'identifiabilite et d'instabilite des coefficients dans les modeles de regression. Meme lorsque l'estimation est possible, l'interpretation des resultats est donc extremement delicate. Il est alors indispensable de combiner a leur vraisemblance un critere supplementaire qui regularise l'estimateur. Dans le sillage de la regression PLS, la strategie de regularisation que nous considerons dans cette these est fondee sur l'extraction de composantes supervisees. Contraintes a l'orthogonalite entre elles, ces composantes doivent non seulement capturer l'information structurelle des variables explicatives, mais aussi predire autant que possible les variables reponses, qui peuvent etre de types divers (continues ou discretes, quantitatives, ordinales ou nominales). La regression sur composantes supervisees a ete developpee pour les GLMs multivaries, mais n'a jusqu'alors concerne que des modeles a observations independantes.Or dans de nombreuses situations, les observations sont groupees. Nous proposons une extension de la methode aux GLMMs multivaries, pour lesquels les correlations intra-groupes sont modelisees au moyen d'effets aleatoires. A chaque etape de l'algorithme de Schall permettant l'estimation du GLMM, nous procedons a la regularisation du modele par l'extraction de composantes maximisant un compromis entre qualite d'ajustement et pertinence structurelle. Compare a la regularisation par penalisation de type ridge ou LASSO, nous montrons sur donnees simulees que notre methode non seulement permet de reveler les dimensions explicatives les plus importantes pour l'ensemble des reponses, mais fournit souvent une meilleure prediction. La methode est aussi evaluee sur donnees reelles.Nous developpons enfin des methodes de regularisation dans le contexte specifique des donnees de panel (impliquant des mesures repetees sur differents individus aux memes dates). Deux effets aleatoires sont introduits : le premier modelise la dependance des mesures relatives a un meme individu, tandis que le second modelise un effet propre au temps (possedant donc une certaine inertie) partage par tous les individus. Pour des reponses Gaussiennes, nous proposons d'abord un algorithme EM pour maximiser la vraisemblance du modele penalisee par la norme L2 des coefficients de regression. Puis nous proposons une alternative consistant a donner une prime aux directions les plus ""fortes"" de l'ensemble des predicteurs. Une extension de ces approches est egalement proposee pour des donnees non-Gaussiennes, et des tests comparatifs sont effectues sur donnees Poissonniennes.",2019,
Development and assessment of linear regression techniques for modeling multisensor data for non-invasive continuos glucose monitoring,"Solianis Monitoring AG (Zurigo, Svizzera) ha recentemente proposto un multisensore non invasivo per il monitoraggio continuo della glicemia, basato su una combinazione di sensori dielettrici e ottici. Lo scopo del progetto di ricerca in collaborazione con Solianis Monitoring AG consiste nello sviluppo e nella valutazione di un modello per la stima della glicemia a partire da dati del multisensore. In questo lavoro di tesi tre differenti metodi per la stima di un modello multivariato di regressione lineare saranno valutati e confrontati: Ordinary Least Squares (OLS), Partial Least Squares (PLS) and Least Absolute Shrinkage and Selection Operator (LASSO). Prima verranno descritti i tre metodi dal punto di vista metodologico e algoritmico. Successivamente, i tre metodi saranno applicati ad un database di 32 esperimenti nei quali misure del multisensore e valori reali della glicemia sono aquisite in parallelo. Infine saranno proposti alcuni metodi per un ulteriore miglioramento delle stime dei profili glicemici.",2011,
Selection of ordinally scaled independent variables with applications to international classification of functioning core sets,"Summary.â€‚ Ordinal categorial variables arise commonly in regression modelling. Although the analysis of ordinal response variables has been well investigated, less work has been done concerning ordinal predictors. We consider so-called international classfication of functioning core sets for chronic widespread pain, in which many ordinal covariates are collected. The effect of specific international classification of functioning variables on a subjective measure of physical health is investigated, which requires strategies for variable selection. In this context, we propose methods for the selection of ordinally scaled independent variables in the classical linear model. The ordinal structure is taken into account by use of a difference penalty on adjacent dummy coefficients. It is shown how the group lasso can be used for the selection of ordinal predictors, and an alternative blockwise boosting procedure is proposed. Both methods are discussed in general, and applied to international classification of functioning core sets for chronic widespread pain.",2011,Journal of The Royal Statistical Society Series C-applied Statistics
Fast Rate Analysis of Some Stochastic Optimization Algorithms,"In this paper, we revisit three fundamental and popular stochastic optimization algorithms (namely, Online Proximal Gradient, Regularized Dual Averaging method and ADMM with online proximal gradient) and analyze their convergence speed under conditions weaker than those in literature. In particular, previous works showed that these algorithms converge at a rate of O(ln T/T) when the loss function is strongly convex, and O(1/âˆšT) in the weakly convex case. In contrast, we relax the strong convexity assumption of the loss function, and show that the algorithms converge at a rate O(ln T/T) if the expectation of the loss function is locally strongly convex. This is a much weaker assumption and is satisfied by many practical formulations including Lasso and Logistic Regression. Our analysis thus extends the applicability of these three methods, as well as provides a general recipe for improving analysis of convergence rate for stochastic and online optimization algorithms.",2016,
Variable Selection for Confounding Adjustment in High-dimensional Covariate Spaces When Analyzing Healthcare Databases,"Background: Data-adaptive approaches to confounding adjustment may improve performance beyond expert knowledge when analyzing electronic healthcare databases and have additional practical advantages for analyzing multiple databases in rapid cycles. Improvements seemed possible if outcome predictors were reliably identified empirically and adjusted. Methods: In five cohort studies from diverse healthcare databases, we implemented a base-case high-dimensional propensity score algorithm with propensity score decile-adjusted outcome models to estimate treatment effects among prescription drug initiators. The original variable selection procedure based on the estimated bias of each variable using unadjusted associations between confounders and exposure (RRCE) and disease outcome (RRCD) was augmented by alternative strategies. These included using increasingly adjusted RRCD estimates, including models considering >1,500 variables jointly (Lasso, Bayesian logistic regression); using prediction statistics or likelihood-ratio statistics for covariate prioritization; directly estimating the propensity score with >1,500 variables (Lasso, Bayesian regression); or directly fitting an outcome model using all covariates jointly (Lasso, Ridge). Results: In five example studies, most tested augmentations of the base-case hdPS did not meaningfully change estimates in light of wide confidence intervals except for Bayesian regression and Lasso to estimate RRCD, which moved estimates minimally closer to the expectation in three of five examples. The direct outcome estimation with Lasso performed worst. Conclusion: Overall, the basic heuristic of variable reduction in high-dimensional propensity score adjustment performed, as well as alternative approaches in diverse settings. Minor improvements in variable selection may be possible using Bayesian outcome regression to prioritize variables for propensity score estimation when outcomes are rare. See video abstract at, http://links.lww.com/EDE/B162.",2017,Epidemiology
Ubiquitous Parameter Anthology For Energy In Wireless Sensor Network: A Statistical Analysis,"In this paper, statistical and machine learning tools are used to reduce the variety of parameters by analyzing the dependency between these parameters and the average energy consumption of a specific application, permitting very appropriate kinds to be chosen. The few techniques are used for correlation analysis, namely Pearson correlation, Spearman correlation; Lasso regularization as well as P-value are used for statistical analysis and suggested a model. Afterward, the random forest regression is applied to evaluate the exactness of prediction for each original and even decreased parameter in estimating the average energy consumption of the wireless sensor network.",2019,
Abstract 1479: A miRNA signature distinguishing low-grade and high-grade gliomas shows miR-21 and 210 as promising biomarkers of aggressive phenotype and prognosis,"Proceedings: AACR Annual Meeting 2014; April 5-9, 2014; San Diego, CA

BACKGROUND. Gliomas account for approximately 80% of all primary malignant brain tumors and, despite advances in clinical care, remain still associated with poor prognosis. They are currently classified by the WHO system in Low Grade Gliomas (LGGs, WHO I, II) and High Grade Gliomas (HGGs, WHO grade III, IV) based on histological features such as nuclear atypia, mitotic figures, microvascular proliferation and necrosis. LGGs and HGGs share several morphological traits and pathways abnormalities but have a different clinical behaviour. miRNAs have emerged as key regulators of many biological processes mediating genesis and dissemination of cancer. Molecular analyses based on miRNA measurements have shown to be able to better stratify and discriminate the two tumor types. We hypothesize the comparison of miRNA profile between LGGs and HGGs may lead to the identification of miRNAs associated with the most aggressive form, that is GBM (Glioblastoma Multiforme, WHO IV).

MATERIAL AND METHODS. miRNA expression profiling was performed in 8 LGGs, 24 HGGs, and 4 Normal Brain Tissues (NBT) by using the Affymetrix GeneChipÂ® miRNA Array 1.0. Data analysis was performed by Partek Genomic Suite software, setting a significative p-value â‰¤ 0.01 and a fold change cutoff of 2. A relative quantification method (RT-qPCR) with standard curve was used to validate the 22 miRNA signature resulted by array analysis. The prognostic performance of the 13 validated miRNAs was estimated by using the Tumor Cancer Genome Atlas (TCGA). RESULTS. miRNA profiling identified 80 miRNAs differentially expressed in LGGs vs NBT and 71 in HGGs vs NBT. A panel of 22 miRNAs clearly differentiated HGGs and LGGs. RT-qPCR assay confirmed differential expression for 13 out of the 22 miRNAs in LGG vs HGG. In addition, 6 among our 13-miRNA signature (miR-21, miR-210, miR-22, miR-155, miR-223, miR-219-2-3p) were found to be significantly associated with GBM molecular subtypes when compared on TCGA dataset. Moreover miR-21 and miR-210 show correlation with worse overall survival in both univariate and multivariate Cox Regression analysis (HR 1.19 95% CI 1.008-1.406 p=0.04; and 1.18 95%C 1.018-1.375 p=0.03).

CONCLUSIONS. We show the comparison of LGGs and HGGs profiles is able to identify miRNAs associated with invasive phenotype. Our results support a direct involvement of miR-21 and miR-210 in glioma progression, suggesting they may represent promising targets for new therapeutic approaches in gliomas.

Citation Format: Raffaela Barbano, Barbara Pasculli, Orazio Palumbo, Marco Galasso, Stefano Volinia, Vincenzo D'Angelo, Michelina Coco, Lucia Dimitri, Massimiliano Copetti, Vanna Maria Valori, Evaristo Maiello, Massimo Carella, Vito Michele Fazio, Paola Parrella. A miRNA signature distinguishing low-grade and high-grade gliomas shows miR-21 and 210 as promising biomarkers of aggressive phenotype and prognosis. [abstract]. In: Proceedings of the 105th Annual Meeting of the American Association for Cancer Research; 2014 Apr 5-9; San Diego, CA. Philadelphia (PA): AACR; Cancer Res 2014;74(19 Suppl):Abstract nr 1479. doi:10.1158/1538-7445.AM2014-1479",2014,Cancer Research
The Best Model of LASSO With The LARS (Least Angle Regression and Shrinkage) Algorithm Using Mallowâ€™s Cp,"Multicollinearity often occurs in regression analysis. Multicollinearity is a condition of correlation between independent variables which is a problem. One method that can overcome multicollinearity is the LASSO (Least Absolute Shrinkage and Selection Operator) method. LASSO is able to help to shrink multicollinearity and improve the accuracy of linear regression models. Estimators of LASSO parameters can be solved by the LARS (Least Angle Regression and Shrinkage) algorithm by algorithm which calculates the correlation vector, the largest absolute correlation value, equiangular vector, inner product vector, and determines the LARS algorithm limiter for LASSO. Selecting the best model using the Mallowâ€™s Cp statistics. The smallest Mallowâ€™s Cp value will be selected as the best model. LASSO method with a more detailed procedure with LARS algorithm and selecting the best model using the Mallowâ€™s Cp statistics is discussed in this paper. World Scientific News 116 (2019) 245-252 -246",2019,
Abstract 2893: Integrated genomic meta-analysis of colorectal cancer by elastic-net.,"To identify combinatorial sets of known, putative and new cancer drivers responsible for colorectal cancer (CRC) development and other associated specific clinical outcomes, we have developed an integrative analysis method for cancer genome data. This approach is based on the elastic-net algorithm that we have applied to genomic data from the Cancer Genome Atlas (TCGA) Project. Our supervised analysis simultaneously assesses the contribution of i) copy number variation (CNV), ii) gene expression, iii) miRNA, iv) methylation and v) cancer mutations to clinical features. The ongoing TCGA project is generating genomic and clinical data sets from different tumor types including CRC. These detailed catalogues of genetic changes in cancer genomes will continue to provide us with new insights about cancer development. However, extracting biologically/clinically relevant information from TCGA9s diverse and large cancer genome data remains a challenge. In attempt to overcome this challenge, we use regularized regression method: elastic-net that improves on â€œthe least absolute shrinkage and selection operatorâ€ (Lasso). To demonstrate the performance and validity of this approach, we showed that elastic-net successfully identified i) synthetic genes that have their CNVs perfectly associated with stages from a simulated data ii) TGFBR2 and other driver genes that have mutations associated with CRCs that demonstrate microsatellite instability CRC from TCGA data, and iii) IDH1 that obtained mutations associated with survival according to glioblastoma (GBM) data. In the next phase of the study, we identified the top ranked genes that delineate key clinical features such as TNM stages of CRC. We have identified a series of candidate genes that may indicate clinical stages including novel candidates on chromosome 8. Overall, we have successfully demonstrated that our approach allows for an integrative and highly robust supervised analysis of TCGA data. Citation Format: HoJoon Lee, Patrick Flaherty, Hanlee P. Ji. Integrated genomic meta-analysis of colorectal cancer by elastic-net. [abstract]. In: Proceedings of the 104th Annual Meeting of the American Association for Cancer Research; 2013 Apr 6-10; Washington, DC. Philadelphia (PA): AACR; Cancer Res 2013;73(8 Suppl):Abstract nr 2893. doi:10.1158/1538-7445.AM2013-2893",2013,Cancer Research
Least Squares Approximation for a Distributed System,"In this work, we develop a distributed least squares approximation (DLSA) method that is able to solve a large family of regression problems (e.g., linear regression, logistic regression, and Cox's model) on a distributed system. By approximating the local objective function using a local quadratic form, we are able to obtain a combined estimator by taking a weighted average of local estimators. The resulting estimator is proved to be statistically as efficient as the global estimator. Moreover, it requires only one round of communication. We further conduct shrinkage estimation based on the DLSA estimation using an adaptive Lasso approach. The solution can be easily obtained by using the LARS algorithm on the master node. It is theoretically shown that the resulting estimator possesses the oracle property and is selection consistent by using a newly designed distributed Bayesian information criterion (DBIC). The finite sample performance and the computational efficiency are further illustrated by an extensive numerical study and an airline dataset. The airline dataset is 52 GB in size. The entire methodology has been implemented in Python for a de-facto standard Spark system. The proposed DLSA algorithm on the Spark system takes 26 minutes to obtain a logistic regression estimator, whereas a full likelihood algorithm takes 15 hours to obtain an inferior result.",2019,ArXiv
Re-evaluation of the comparative effectiveness of bootstrap-based optimism correction methods in the development of multivariable clinical prediction models.,"Multivariable predictive models are important statistical tools for providing synthetic diagnosis and prognostic algorithms based on multiple patients' characteristics. Their apparent discriminant and calibration measures usually have overestimation biases (known as 'optimism') relative to the actual performances for external populations. Existing statistical evidence and guidelines suggest that three bootstrap-based bias correction methods are preferable in practice, namely Harrell's bias correction and the .632 and .632+ estimators. Although Harrell's method has been widely adopted in clinical studies, simulation-based evidence indicates that the .632+ estimator may perform better than the other two methods. However, there is limited evidence and these methods' actual comparative effectiveness is still unclear. In this article, we conducted extensive simulations to compare the effectiveness of these methods, particularly using the following modern regression models: conventional logistic regression, stepwise variable selections, Firth's penalized likelihood method, ridge, lasso, and elastic-net. Under relatively large sample settings, the three bootstrap-based methods were comparable and performed well. However, all three methods had biases under small sample settings, and the directions and sizes of the biases were inconsistent. In general, the .632+ estimator is recommended, but we provide several notes concerning the operating characteristics of each method.",2020,arXiv: Applications
Complex Network Short-Term Traffic Forecasting Based on Lasso-NN Model,"Traditional traffic forecasting models are transforming from single section historical data processing to multi-section and multi-timing historical data processing.However,when considering the influence between each section,the fickle traffic condition tends to complicate the forecasting model.Therefore,this paper introduced the Lasso method used in multivariable linear regression and utilized its excellent ability of variable selection.It selected partial high correction sections from a complex multi-section road network.Combined with the non-linear neural network,a new Lasso-NN model was proposed.The result shows that the Lasso-NN model has an overall lower error rate.In the intersection region of the road network,the error rate is less than 9.2% and in the non-intersection region,it is less than 6.7%.",2015,Journal of Shanghai Jiaotong University
High dimensional regression using the sparse matrix transform (SMT),"Regression from high dimensional observation vectors is particularly difficult when training data is limited. More specifically, if the number of sample vectors n is less than dimension of the sample vectors p, then accurate regression is difficult to perform without prior knowledge of the data covariance. In this paper, we propose a novel approach to high dimensional regression for application when n â‰ª p. The approach works by first decorrelating the high dimensional observation vector using the sparse matrix transform (SMT) estimate of the data covariance. Then the decorrelated observations are used in a regularized regression procedure such as Lasso or shrinkage. Numerical results demonstrate that the proposed regression approach can significantly improve the prediction accuracy, especially when n is small and the signal to be predicted lies in the subspace of the observations corresponding to the small eigenvalues.",2010,"2010 IEEE International Conference on Acoustics, Speech and Signal Processing"
DNA methylation-based forensic age estimation in human bone,"DNA methylation is an epigenetic modification of cytosine nucleotides that represents a promising suite of aging markers with broad potential applications. In particular, determining an individualâ€™s age from their skeletal remains is an enduring problem in the field of forensic anthropology, and one that epigenetic markers are particularly well-suited to address. However, all DNA methylation-based age prediction methods published so far focus on tissues other than bone. While high accuracy has been achieved for saliva, blood and sperm, which are easily accessible in living individuals, the highly tissue-specific nature of DNA methylation patterns means that age prediction models trained on these particular tissues may not be directly applicable to other tissues. Bone is a prime target for the development of DNA methylation-based forensic identification tools as skeletal remains are often recoverable for years post-mortem, and well after soft tissues have decomposed. In this study, we generate genome-wide DNA methylation data from 32 individual bone samples. We analyze this new dataset alongside published data from 133 additional bone donors, both living and deceased. We perform an epigenome-wide association study on this combined dataset to identify 108 sites of DNA methylation that show a significant relationship with age (FDR < 0.05). We also develop an age-prediction model using lasso regression that produces highly accurate estimates of age from bone spanning an age range of 49-112 years. Our study demonstrates that DNA methylation levels at specific CpG sites can serve as powerful markers of aging, and can yield more accurate predictions of chronological age in human adults than morphometric markers.",2019,bioRxiv
Prediction of breed composition in an admixed cattle population.,"Swiss Fleckvieh was established in 1970 as a composite of Simmental (SI) and Red Holstein Friesian (RHF) cattle. Breed composition is currently reported based on pedigree information. Information on a large number of molecular markers potentially provides more accurate information. For the analysis, we used Illumina BovineSNP50 Genotyping Beadchip data for 90 pure SI, 100 pure RHF and 305 admixed bulls. The scope of the study was to compare the performance of hidden Markov models, as implemented in structure software, with methods conventionally used in genomic selection [BayesB, partial least squares regression (PLSR), least absolute shrinkage and selection operator (LASSO) variable selection)] for predicting breed composition. We checked the performance of algorithms for a set of 40Â 492 single nucleotide polymorphisms (SNPs), subsets of evenly distributed SNPs and subsets with different allele frequencies in the pure populations, using F(ST) as an indicator. Key results are correlations of admixture levels estimated with the various algorithms with admixture based on pedigree information. For the full set, PLSR, BayesB and structure performed in a very similar manner (correlations of 0.97), whereas the correlation of LASSO and pedigree admixture was lower (0.93). With decreasing number of SNPs, correlations decreased substantially only for 5% or 1% of all SNPs. With SNPs chosen according to F(ST) , results were similar to results obtained with the full set. Only when using 96 and 48 SNPs with the highest F(ST) , correlations dropped to 0.92 and 0.90 respectively. Reducing the number of pure animals in training sets to 50, 20 and 10 each did not cause a drop in the correlation with pedigree admixture.",2012,Animal genetics
Subset Selection with Shrinkage: Sparse Linear Modeling when the SNR is low,"We study the behavior of a fundamental tool in sparse statistical modeling --the best-subset selection procedure (aka ""best-subsets""). Assuming that the underlying linear model is sparse, it is well known, both in theory and in practice, that the best-subsets procedure works extremely well in terms of several statistical metrics (prediction, estimation and variable selection) when the signal to noise ratio (SNR) is high. However, its performance degrades substantially when the SNR is low -- it is outperformed in predictive accuracy by continuous shrinkage methods, such as ridge regression and the Lasso. We explain why this behavior should not come as a surprise, and contend that the original version of the classical best-subsets procedure was, perhaps, not designed to be used in the low SNR regimes. We propose a close cousin of best-subsets, namely, its $\ell_{q}$-regularized version, for $q \in\{1, 2\}$, which (a) mitigates, to a large extent, the poor predictive performance of best-subsets in the low SNR regimes; (b) performs favorably and generally delivers a substantially sparser model when compared to the best predictive models available via ridge regression and the Lasso. Our estimator can be expressed as a solution to a mixed integer second order conic optimization problem and, hence, is amenable to modern computational tools from mathematical optimization. We explore the theoretical properties of the predictive capabilities of the proposed estimator and complement our findings via several numerical experiments.",2017,arXiv: Methodology
Lasso Regression Based on Empirical Mode Decomposition,"The Hilbertâ€“Huang transform uses the empirical mode decomposition (EMD) method to analyze nonlinear and nonstationary data. This method breaks a time series of data into several orthogonal sequences based on differences in frequency. These data components include the intrinsic mode functions (IMFs) and the final residue. Although IMFs have been used in the past as predictors for other variables, very little effort has been devoted to identifying the most effective predictors among IMFs. As lasso is a widely used method for feature selection within complex datasets, the main objective of this article is to present a lasso regression based on the EMD method for choosing decomposed components that exhibit the strongest effects. Both numerical experiments and empirical results show that the proposed modeling process can use time-frequency structure within data to reveal interactions between two variables. This allows for more accurate predictions concerning future events.",2016,Communications in Statistics - Simulation and Computation
Prediction of malignant glioma grades using contrast-enhanced T1-weighted and T2-weighted magnetic resonance images based on a radiomic analysis,"We conducted a feasibility study to predict malignant glioma grades via radiomic analysis using contrast-enhanced T1-weighted magnetic resonance images (CE-T1WIs) and T2-weighted magnetic resonance images (T2WIs). We proposed a framework and applied it to CE-T1WIs and T2WIs (with tumor region data) acquired preoperatively from 157 patients with malignant glioma (grade III: 55, grade IV: 102) as the primary dataset and 67 patients with malignant glioma (grade III: 22, grade IV: 45) as the validation dataset. Radiomic features such as size/shape, intensity, histogram, and texture features were extracted from the tumor regions on the CE-T1WIs and T2WIs. The Wilcoxonâ€“Mannâ€“Whitney (WMW) test and least absolute shrinkage and selection operator logistic regression (LASSO-LR) were employed to select the radiomic features. Various machine learning (ML) algorithms were used to construct prediction models for the malignant glioma grades using the selected radiomic features. Leave-one-out cross-validation (LOOCV) was implemented to evaluate the performance of the prediction models in the primary dataset. The selected radiomic features for all folds in the LOOCV of the primary dataset were used to perform an independent validation. As evaluation indices, accuracies, sensitivities, specificities, and values for the area under receiver operating characteristic curve (or simply the area under the curve (AUC)) for all prediction models were calculated. The mean AUC value for all prediction models constructed by the ML algorithms in the LOOCV of the primary dataset was 0.902â€‰Â±â€‰0.024 (95% CI (confidence interval), 0.873â€“0.932). In the independent validation, the mean AUC value for all prediction models was 0.747â€‰Â±â€‰0.034 (95% CI, 0.705â€“0.790). The results of this study suggest that the malignant glioma grades could be sufficiently and easily predicted by preparing the CE-T1WIs, T2WIs, and tumor delineations for each patient. Our proposed framework may be an effective tool for preoperatively grading malignant gliomas.",2019,Scientific Reports
Efficient Clustering of Correlated Variables and Variable Selection in High-Dimensional Linear Models,"In this paper, we introduce Adaptive Cluster Lasso(ACL) method for variable selection in high dimensional sparse regression models with strongly correlated variables. To handle correlated variables, the concept of clustering or grouping variables and then pursuing model fitting is widely accepted. When the dimension is very high, finding an appropriate group structure is as difficult as the original problem. The ACL is a three-stage procedure where, at the first stage, we use the Lasso(or its adaptive or thresholded version) to do initial selection, then we also include those variables which are not selected by the Lasso but are strongly correlated with the variables selected by the Lasso. At the second stage we cluster the variables based on the reduced set of predictors and in the third stage we perform sparse estimation such as Lasso on cluster representatives or the group Lasso based on the structures generated by clustering procedure. We show that our procedure is consistent and efficient in finding true underlying population group structure(under assumption of irrepresentable and beta-min conditions). We also study the group selection consistency of our method and we support the theory using simulated and pseudo-real dataset examples.",2016,ArXiv
Identification of a 5â€‘microRNA signature and hub miRNAâ€‘mRNA interactions associated with pancreatic cancer.,"miRNAâ€‘gene axes have been reported to serve an important role in the carcinogenesis of pancreatic cancer (PC). The aim of the present study was to systematically identity the microRNA signature and hub molecules, as well as hub miRNAâ€‘gene axes, and to explore the potential biomarkers and mechanisms associated with the carcinogenesis of PC. Eleven microRNA profile datasets were obtained from the National Center for Biotechnology Information (NCBI) Gene Expression Omnibus (GEO) and ArrayExpress databases, and a metaâ€‘analysis was performed to identify the differentially expressed miRNAs (DEMs) between tumor tissue and normal tissue. Subsequently, a diagnostic regression model was constructed to identify PC based on The Cancer Genome Atlas (TCGA) miRNA sequence data by using the least absolute shrinkage and selection operator (LASSO) method. In addition, GSE41368 was downloaded, and a weighted gene coâ€‘expression network analysis (WGCNA) was performed to obtain the gene module associated with carcinogenesis by using the TCGAbiolinks and WGCNA packages, respectively. Finally, miRNAâ€‘gene networks were constructed and visualized using Cytoscape software, followed by Gene Ontology (GO) and Kyoto Encyclopedia of Genes and Genomes (KEGG) analyses based on the Database for Annotation, Visualization, and Integrated Discovery (DAVID). A total of 14 DEMs were identified, and a 5â€‘microRNAâ€‘based score generated by the LASSO regression model provided a high accuracy for identifying PC [area under the curve (AUC)=0.918]. In addition, 44 miRNAâ€‘mRNA interactions were constructed, and 4Â hub genes were screened on the basis of the above bioinformatic tools and databases. Furthermore, 14 biological process (BP) functions and 6 KEGG pathways were identified according to gene set enrichment analysis (GSEA). In summary, the present study applied integrated bioinformatics approaches to generate a holistic view of PC, thereby providing a basis for further clinical application of the 5â€‘miRNA signature and the identified hub molecules, as well as the miRNAâ€‘gene axes, which could serve as diagnostic markers and potential treatment targets.",2019,Oncology reports
A Multi-parametric MRI-Based Radiomics Signature and a Practical ML Model for Stratifying Glioblastoma Patients Based on Survival Toward Precision Oncology,"Purpose: Predicting patients' survival outcomes is recognized of key importance to clinicians in oncology toward determining an ideal course of treatment and patient management. This study applies radiomics analysis on pre-operative multi-parametric MRI of patients with glioblastoma from multiple institutions to identify a signature and a practical machine learning model for stratifying patients into groups based on overall survival. Methods: This study included 163 patients' data with glioblastoma, collected by BRATS 2018 Challenge from multiple institutions. In this proposed method, a set of 147 radiomics image features were extracted locally from three tumor sub-regions on standardized pre-operative multi-parametric MR images. LASSO regression was applied for identifying an informative subset of chosen features whereas a Cox model used to obtain the coefficients of those selected features. Then, a radiomics signature model of 9 features was constructed on the discovery set and it performance was evaluated for patients stratification into short- (<10 months), medium- (10-15 months), and long-survivors (>15 months) groups. Eight ML classification models, trained and then cross-validated, were tested to assess a range of survival prediction performance as a function of the choice of features. Results: The proposed mpMRI radiomics signature model had a statistically significant association with survival (P < 0.001) in the training set, but was not confirmed (P = 0.110) in the validation cohort. Its performance in the validation set had a sensitivity of 0.476 (short-), 0.231 (medium-), and 0.600 (long-survivors), and specificity of 0.667 (short-), 0.732 (medium-), and 0.794 (long-survivors). Among the tested ML classifiers, the ensemble learning model's results showed superior performance in predicting the survival classes, with an overall accuracy of 57.8% and AUC of 0.81 for short-, 0.47 for medium-, and 0.72 for long-survivors using the LASSO selected features combined with clinical factors. Conclusion: A derived GLCM feature, representing intra-tumoral inhomogeneity, was found to have a high association with survival. Clinical factors, when added to the radiomics image features, boosted the performance of the ML classification model in predicting individual glioblastoma patient's survival prognosis, which can improve prognostic quality a further step toward precision oncology.",2019,Frontiers in Computational Neuroscience
Behavioral responses to pre-planned road capacity reduction based on smartphone GPS trajectory data: A functional data analysis approach,"Abstract Pre-planned events such as constructions or special events lead to road capacity reductions and create bottlenecks in the traffic network. The traffic impact of such events goes beyond local areas, as informed drivers may detour to alternative corridors and consequently the traffic congestion may divert or propagate to other corridors. Due to the lack of real observation data, traditional traffic impact analyses are typically based on simulation models, fixed-location sensor data or survey questionnaires. In this research, we use high-resolution vehicle trajectory data collected via a smartphone app, which is capable of keeping track of individual driverâ€™s behavior before and after road capacity reduction, to investigate travelersâ€™ behavioral responses to pre-planned events and the contribution factors. For this purpose, a functional data analysis (FDA) approach-based clustering method is firstly proposed to cluster trajectory data and identify detour patterns, and two logistic and a least absolute shrinkage and selection operator (LASSO) regression models are used to explain driversâ€™ detour behavior choice for each pattern with spatial and temporal features of interest. A case study based on a lane closure event on MoPac expressway in Austin, TX is used as an example in this research. The case study demonstrates that: (1) the freeway capacity reduction triggered heterologous behavior responses, (2) driver detour behavior exhibits three major patterns and (3) each detour pattern highly depends on spatial features such as trip length, distance to freeway entrance and distance to other alternative freeways, in addition to the temporal features when the trip happens.",2019,Journal of Intelligent Transportation Systems
Shrinkage Estimation of Common Breaks in Panel Data Models via Adaptive Group Fused Lasso,"In this paper we consider estimation and inference of common breaks in panel data models via adaptive group fused Lasso. We consider two approachesâ€”penalized least squares (PLS) for first-differenced models without endogenous regressors, and penalized GMM (PGMM) for first-differenced models with endogeneity. We show that with probability tending to one, both methods can correctly determine the unknown number of breaks and estimate the common break dates consistently. We establish the asymptotic distributions of the Lasso estimators of the regression coefficients and their post Lasso versions. We also propose and validate a data-driven method to determine the tuning parameter used in the Lasso procedure. Monte Carlo simulations demonstrate that both the PLS and PGMM estimation methods work well in finite samples. We apply our PGMM method to study the effect of foreign direct investment (FDI) on economic growth using a panel of 88 countries and regions from 1973 to 2012 and find multiple breaks in the model.",2016,Journal of Econometrics
A linear programming approach for estimating the structure of a sparse linear genetic network from transcript profiling data,"BackgroundA genetic network can be represented as a directed graph in which a node corresponds to a gene and a directed edge specifies the direction of influence of one gene on another. The reconstruction of such networks from transcript profiling data remains an important yet challenging endeavor. A transcript profile specifies the abundances of many genes in a biological sample of interest. Prevailing strategies for learning the structure of a genetic network from high-dimensional transcript profiling data assume sparsity and linearity. Many methods consider relatively small directed graphs, inferring graphs with up to a few hundred nodes. This work examines large undirected graphs representations of genetic networks, graphs with many thousands of nodes where an undirected edge between two nodes does not indicate the direction of influence, and the problem of estimating the structure of such a sparse linear genetic network (SLGN) from transcript profiling data.ResultsThe structure learning task is cast as a sparse linear regression problem which is then posed as a LASSO (l1-constrained fitting) problem and solved finally by formulating a Linear Program (LP). A bound on the Generalization Error of this approach is given in terms of the Leave-One-Out Error. The accuracy and utility of LP-SLGNs is assessed quantitatively and qualitatively using simulated and real data. The Dialogue for Reverse Engineering Assessments and Methods (DREAM) initiative provides gold standard data sets and evaluation metrics that enable and facilitate the comparison of algorithms for deducing the structure of networks. The structures of LP-SLGNs estimated from the IN SILICO 1, IN SILICO 2 and IN SILICO 3 simulated DREAM2 data sets are comparable to those proposed by the first and/or second ranked teams in the DREAM2 competition. The structures of LP-SLGNs estimated from two published Saccharomyces cerevisae cell cycle transcript profiling data sets capture known regulatory associations. In each S. cerevisiae LP-SLGN, the number of nodes with a particular degree follows an approximate power law suggesting that its degree distributions is similar to that observed in real-world networks. Inspection of these LP-SLGNs suggests biological hypotheses amenable to experimental verification.ConclusionA statistically robust and computationally efficient LP-based method for estimating the topology of a large sparse undirected graph from high-dimensional data yields representations of genetic networks that are biologically plausible and useful abstractions of the structures of real genetic networks. Analysis of the statistical and topological properties of learned LP-SLGNs may have practical value; for example, genes with high random walk betweenness, a measure of the centrality of a node in a graph, are good candidates for intervention studies and hence integrated computational â€“ experimental investigations designed to infer more realistic and sophisticated probabilistic directed graphical model representations of genetic networks. The LP-based solutions of the sparse linear regression problem described here may provide a method for learning the structure of transcription factor networks from transcript profiling and transcription factor binding motif data.",2008,Algorithms for Molecular Biology : AMB
L G ] 2 8 O ct 2 01 9 Variable Selection with Copula Entropy,"Variable selection is of significant importance for classification and regression tasks in machine learning and statistical applications where both predictability and explainability are needed. In this paper, a Copula Entropy (CE) based method for variable selection which use CE based ranks to select variables is proposed. The method is both model-free and tuning-free. Comparison experiments between the proposed method and traditional variable selection methods, such as Stepwise Selection, regularized generalized linear models and Adaptive LASSO, were conducted on the UCI heart disease data. Experimental results show that CE based method can select the â€˜rightâ€™ variables out effectively and derive better interpretable results than traditional methods do without sacrificing accuracy performance. It is believed that CE based variable selection can help to build more explainable models.",2019,
"Comparison of linear regression models Ordinary Lasso, Adaptive Group Lasso and Ordinary Least Squares models in selecting effective characteristics to predict the expected return","In this study, for the selection of the characteristics of the company that provides the incremental information to investors and financial analysts, the linear models are adapted by the ordinary Lasso method (Tibshirani, 1996), Adaptive Group LASSO (Zu, 2006) and the least squares method (OLS). The main objective of this research is to determine which method can predict the expected return on stock portfolios in the shortest time and using the least effective features. The research sample is1340observations, including 134companies listed in Tehran Stock Exchange, and the research variables from the financial statements of the companies and the stock market reports between 2008and 2018. The results of this study show that by employing the least squares regression method, 7 characteristics, the typical 5- characteristics LASSO method and in the Adaptive Group LASSO method, only 4characteristics, contain incremental information to predict the expected returns of stock portfolios. In the second place, by applying the Adaptive Group LASSO regression method, one can achieve the same results with using the least characteristics.",2018,
Supervised learning via the â€œhubNetâ€ procedure,"We propose a new method for supervised learning. The hubNet procedure fits a hub-based graphical model to the predictors, to estimate the amount of â€œconnectionâ€ that each predictor has with other predictors. This yields a set of predictor weights that are then used in a regularized regression such as the lasso or elastic net. The resulting procedure is easy to implement, can often yield higher or competitive prediction accuracy with fewer features than the lasso, and can give insight into the underlying structure of the predictors. HubNet can be generalized seamlessly to supervised problems such as regularized logistic regression (and other GLMs), Coxâ€™s proportional hazards model, and nonlinear procedures such as random forests and boosting. We prove recovery results under a specialized model and illustrate the method on real and simulated data. HubNet; Adaptive Lasso; Graphical Model; Unsupervised Weights",2018,Statistica Sinica
PREDICTING PREFERENCES Analyzing Reading Behavior and News Preferences,"News reading has gradually become a significant activity in our daily life as the world is saturated with new information. We consume so much information every day that it becomes extremely difficult to filter and extract only the interesting and relevant news to us. To improve readersâ€™ experience, we need to be able to accurately predict the new stories that are most probable for them to read. This reduces to predicting preferences which is a common problem of finding out which items are most relevant to each user and essentially rank or feed them to tailored users. In our study, we in particular look at the stories users click and read previously in a news-reading mobile application Pulse in order to predict which stories users are most likely to read in the next few days. Similar challenge has been recently tackled by researchers in data mining and natural language processing because its implications can be applied to other popular areas such as search engine recommendation system. This paper considers applying different machine learning algorithms in the realm of supervised learning and unsupervised learning in attempt to predict the news stories that each user are most likely to read from a set of all available stories, which are much too large for the average users to parse and find the most relevant ones. In particular, using usersâ€™ click and read history, we explore text categorization algorithms by comparing the accuracy of several supervised learning methods such as a simple Naive Bayes, L2-norm regularized logistic regression, and L1-norm regularized logistic regression with Lasso algorithm. We then move on to investigate common unsupervised learning technique such as k-mean clustering of users and a simple implementation of collaborative filtering.",2011,
Impact of statistical models on the prediction of type 2 diabetes using non-targeted metabolomics profiling,"OBJECTIVE
Characterizing specific metabolites in sub-clinical phases preceding the onset of type 2 diabetes to enable efficient preventive and personalized interventions.


RESEARCH DESIGN AND METHODS
We developed predictive models of type 2 diabetes using two strategies. One strategy focused on the probability of incidence only and was based on logistic regression (MRS1); the other strategy accounted for the age at diagnosis of diabetes and was based on Cox regression (MRS2). We assessed 293 metabolites using non-targeted metabolomics in fasting plasma samples of 1,044 participants (including 231 incident cases over 9 years) used as training population; and fasting serum samples of 128 participants (64 incident cases versus 64 controls) used as validation population. We applied a LASSO-based variable selection aiming at maximizing the out-of-sample area under the receiver operating characteristic curve (AROC) and integrated AROC.


RESULTS
Sixteen and 17 metabolites were selected for MRS1 and MRS2, respectively, with AROCÂ =Â 90% and 73% in the training and validation populations, respectively for MRS1. MRS2 had a similar performance and was significantly associated with a younger age of onset of type 2 diabetes (Î²Â =Â -3.44 years per MRS2 SD in the training population, pÂ =Â 1.56Â Ã—Â 10(-7); Î²Â =Â -4.73 years per MRS2 SD in the validation population, pÂ =Â 4.04Â Ã—Â 10(-3)).


CONCLUSIONS
Overall, this study illustrates that metabolomics improves prediction of type 2 diabetes incidence of 4.5% on top of known clinical and biological markers, reaching 90% in total AROC, which is considered the threshold for clinical validity, suggesting it may be used in targeting interventions to prevent type 2 diabetes.",2016,Molecular Metabolism
"Pixl Prediction Accuracies for Ni , Mn , S , and Major Elements : a Comparative Study Using the Same Standards","Introduction: Calibration models for detection and accurate quantification of elements on Mars are necessary to understand the composition of its surface. Mars 2020 will carry Planetary Instrument for X-ray Lithochemistry (PIXL), an x-ray fluorescence (XRF) instrument, and a laser-induced breakdown spectroscopy (LIBS) instrument as part of the SuperCam instrument for geochemical analyses. No comparison of prediction accuracies between these two techniques using identical standards has been undertaken to date, making it difficult to compare results from the two methods. Another issue with Mars geochemical accuracies is that most calibration models are made using terrestrial geologic standards with concentrations that may be significantly lower than those of, especially, Ni, Mn, and S in Martian soils [1], where these elements may be enriched by contributions from meteorites and volcanic gasses. Accordingly, this study uses standards created from several different rock types doped with up to 1 wt% (10,000 ppm) of these elements to create appropriate calibration models. Background: With XRF, samples are bombarded with high-energy X-rays, resulting in ejection of inner shell electrons. The resultant holes in the inner-shell orbitals are filled by electrons from outer shells, in the process ejecting a photon with energy diagnostic of each individual element. In contrast, LIBS uses energy from a laser pulse to excite electrons into higher energy orbitals. When electrons return to their ground state, they release photons detected by spectrometers from the UV to the NIR. These transitions occur at longer wavelengths and lower energies than those measured in XRF, due to differences in the energy of the two excitation sources (plasma heat and x-rays, respectively). So these two techniques are highly complementary, and each has its strengths and weaknesses. Methods: Doped samples consisted of 7 matrices with different bulk compositions, including three basalts, one granite, one rhyolitic volcanic glass, sea sand, and a 50:50 mixture of diopside and forsteritic olivine. Standard preparation and analyses are described in [2]. These powders were pressed into pellets and a subset of the 84 samples was analyzed under Mars conditions with the Mount Holyoke College ChemLIBS-analog instrument as well as with the Stony Brook University PIXL-analog instrument in air. Spectral Preprocessing: XRF spectra from three pellet locations were summed over a total dwell time of one or two hours using data from two spectrometers. X-axis resampling and baseline calculations for later removal were performed by PIQUANT, a software created specifically to analyze PIXL spectra [3-4]. To remedy temporal count differences, the spectra were normalized by the emission counts at 2.697 keV, which derive from Rh-anode L-emission lines in the PIXL Xray tube. LIBS spectra were averages of 36 individual shots taken on 6 locations across the pelletsâ€™ surfaces. Spectra were preprocessed using the same method as the ChemLIBS Curiosity team and normalized by the total intensity of each of the three spectrometers [5]. The baselines were removed using the Kajfosz-Kwiatek method (bottom width of 50 and top width of zero) [6]. Modeling and Analysis: Both datasets were uploaded to a web tool that utilizes the SciKit-learn library and allows for convenient multivariate analysis with partial-least squares (PLS) and the least absolute shrinkage and selection operator (lasso) regression methods [7]. Calibration models for the dopants (Ni, Mn, S) and the major oxides (SiO2, TiO2, Al2O3, Fe2O3, MgO, CaO, Na2O, K2O, P2O5) were made using both the entire available spectral range (0-40 keV for PIXL and ~240-850 nm for ChemLIBS) as well as limited regions of the spectra that contain emission peaks specific to the element of interest (Table 1). XRF regions used each elementâ€™s k-ï¡ fluorescence peak, while LIBS focused on regions with the most intense peaks or clusters of peaks found with the NIST LIBS spectral database [8]. PIXL calibrations for Na and Mg were not possible because their XRF emissions are not measurable in air.",2018,
Comparing the performance of propensity score methods in healthcare database studies with rare outcomes.,"Nonrandomized studies of treatments from electronic healthcare databases are critical for producing the evidence necessary to making informed treatment decisions, but often rely on comparing rates of events observed in a small number of patients. In addition, studies constructed from electronic healthcare databases, for example, administrative claims data, often adjust for many, possibly hundreds, of potential confounders. Despite the importance of maximizing efficiency when there are many confounders and few observed outcome events, there has been relatively little research on the relative performance of different propensity score methods in this context. In this paper, we compare a wide variety of propensity-based estimators of the marginal relative risk. In contrast to prior research that has focused on specific statistical methods in isolation of other analytic choices, we instead consider a method to be defined by the complete multistep process from propensity score modeling to final treatment effect estimation. Propensity score model estimation methods considered include ordinary logistic regression, Bayesian logistic regression, lasso, and boosted regression trees. Methods for utilizing the propensity score include pair matching, full matching, decile strata, fine strata, regression adjustment using one or two nonlinear splines, inverse propensity weighting, and matching weights. We evaluate methods via a 'plasmode' simulation study, which creates simulated datasets on the basis of a real cohort study of two treatments constructed from administrative claims data. Our results suggest that regression adjustment and matching weights, regardless of the propensity score model estimation method, provide lower bias and mean squared error in the context of rare binary outcomes. Copyright Â© 2017 John Wiley & Sons, Ltd.",2017,Statistics in medicine
