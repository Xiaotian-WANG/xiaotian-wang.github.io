title,abstract,year,journal
Does SLOPE outperform bridge regression?,"A recently proposed SLOPE estimator (arXiv:1407.3824) has been shown to adaptively achieve the minimax $\ell_2$ estimation rate under high-dimensional sparse linear regression models (arXiv:1503.08393). Such minimax optimality holds in the regime where the sparsity level $k$, sample size $n$, and dimension $p$ satisfy $k/p \rightarrow 0$, $k\log p/n \rightarrow 0$. In this paper, we characterize the estimation error of SLOPE under the complementary regime where both $k$ and $n$ scale linearly with $p$, and provide new insights into the performance of SLOPE estimators. We first derive a concentration inequality for the finite sample mean square error (MSE) of SLOPE. The quantity that MSE concentrates around takes a complicated and implicit form. With delicate analysis of the quantity, we prove that among all SLOPE estimators, LASSO is optimal for estimating $k$-sparse parameter vectors that do not have tied non-zero components in the low noise scenario. On the other hand, in the large noise scenario, the family of SLOPE estimators are sub-optimal compared with bridge regression such as the Ridge estimator.",2019,ArXiv
Approche protÃ©omique par Â« label-free Â» des peptides associÃ©s Ã  la rÃ©ponse Ã  la radio chimiothÃ©rapie prÃ©opÃ©ratoire du cancer du rectum localement avancÃ© (Ã©tude PROTEORECTUM),"Introduction Le cancer du rectum localement avance justifie dâ€™un traitement preoperatoire par radiochimiotherapie. Les patients bons repondeurs (un sur deux) ont un meilleur pronostic que les mauvais repondeurs, mais ce statut nâ€™est determine quâ€™apres analyse de la piece chirurgicale, sans possibilite dâ€™adaptation therapeutique au cours du traitement. Lâ€™objectif de PROTEORECTUM etait de comparer par spectrometrie de masse (selon la procedure label-free) lâ€™expression des proteines seriques chez les patients bons et mauvais repondeurs afin dâ€™identifier des biomarqueurs precoces pour adapter au plus tot la strategie therapeutique. Le but de ce travail est de presenter lâ€™approche statistique choisie pour trier les peptides potentiellement associes a la reponse au traitement tout en tenant compte de variables dâ€™ajustement liees a la technique notamment. Methodes Il sâ€™agit dâ€™une etude de cohorte prospective, monocentrique chez des patients atteints de cancer du rectum localement avance et soumis a un traitement par radiochimiotherapie avant exerese rectale. Tous les patients eligibles ont ete inclus de maniere consecutive. La determination de lâ€™expression des proteines a ete realisee a posteriori sur les serums congeles qui avaient ete preleves a lâ€™inclusion (T0) et 48Â h apres la premiere cure de radiochimiotherapie (T1), en insu du statut bon ou mauvais repondeur determine par anatomopathologie de la piece operatoire. La correction de lâ€™effet de la calibration (variable technique expliquant 42Â % de la variabilite de la mesure) sur la mesure des abondances (quantification des peptides) a ete faite par la methode COMBAT (Biostatistics 8:118) apres transformation des donnees par la fonction log2. La determination des peptides differentiellement exprimes entre bons et mauvais repondeurs a ete realisee a lâ€™aide dâ€™un modele de regression logistique avec un critere de penalisation de type LASSO permettant de selectionner les variables dâ€™interet. La variable a expliquer etait le statut repondeur du patient et les variables explicatives etaientÂ : lâ€™abondance des differents peptides (apres transformation log2Â et correction par la methode COMBAT). Le modele a ete ajuste sur la taille tumorale. Resultats Quarante-six patients ont ete inclus dans lâ€™etude. Lâ€™Ã¢ge moyen etait de 67Â ans. Lâ€™IMC moyen etait de 25Â kg/m2, 31Â patients (67Â %) etaient des hommes et 25 (54Â %) etaient bons repondeurs au traitement. Au total, 40Â 409Â ions ont ete detectes par spectrometrie de masse, parmi lesquels 4785Â peptides ont ete identifies et quantifies de facon relative par la methode Â«Â label-freeÂ Â». Lâ€™analyse a T0Â par le modele logistique avec critere de penalisation de type LASSO a permis de selectionner 11Â peptides associes au statut du patient. Lâ€™analyse a T1Â nâ€™a permis dâ€™identifier aucun peptide associe au statut du patient. Conclusions Lâ€™analyse proteomique des serums precoces de patients traites par radiochimiotherapie preoperatoire pour cancer du rectum a necessite de prendre en compte lâ€™effet important dâ€™une variable technique et le nombre eleve de facteurs predictifs a tester dans le modele. Onze peptides mesures a T0Â etaient significativement associes a la reponse a la radiochimiotherapie.",2014,Revue D Epidemiologie Et De Sante Publique
Efficient Shrinkage for Generalized Linear Mixed Models Under Linear Restrictions,"Abstract In this paper, we consider the pretest, shrinkage, and penalty estimation procedures for generalized linear mixed models when it is conjectured that some of the regression parameters are restricted to a linear subspace. We develop the statistical properties of the pretest and shrinkage estimation methods, which include asymptotic distributional biases and risks. We show that the pretest and shrinkage estimators have a significantly higher relative efficiency than the classical estimator. Furthermore, we consider the penalty estimator LASSO (Least Absolute Shrinkage and Selection Operator), and numerically compare its relative performance with that of the other estimators. A series of Monte Carlo simulation experiments are conducted with different combinations of inactive predictors, and the performance of each estimator is evaluated in terms of the simulated mean squared error. The study shows that the shrinkage and pretest estimators are comparable to the LASSO estimator when the number of inactive predictors in the model is relatively large. The estimators under consideration are applied to a real data set to illustrate the usefulness of the procedures in practice.",2018,
An extensive experimental survey of regression methods,"Regression is a very relevant problem in machine learning, with many different available approaches. The current work presents a comparison of a large collection composed by 77 popular regression models which belong to 19 families: linear and generalized linear models, generalized additive models, least squares, projection methods, LASSO and ridge regression, Bayesian models, Gaussian processes, quantile regression, nearest neighbors, regression trees and rules, random forests, bagging and boosting, neural networks, deep learning and support vector regression. These methods are evaluated using all the regression datasets of the UCI machine learning repository (83 datasets), with some exceptions due to technical reasons. The experimental work identifies several outstanding regression models: the M5 rule-based model with corrections based on nearest neighbors (cubist), the gradient boosted machine (gbm), the boosting ensemble of regression trees (bstTree) and the M5 regression tree. Cubist achieves the best squared correlation ( R2) in 15.7% of datasets being very near to it, with difference below 0.2 for 89.1% of datasets, and the median of these differences over the dataset collection is very low (0.0192), compared e.g. to the classical linear regression (0.150). However, cubist is slow and fails in several large datasets, while other similar regression models as M5 never fail and its difference to the best R2 is below 0.2 for 92.8% of datasets. Other well-performing regression models are the committee of neural networks (avNNet), extremely randomized regression trees (extraTrees, which achieves the best R2 in 33.7% of datasets), random forest (rf) and Îµ-support vector regression (svr), but they are slower and fail in several datasets. The fastest regression model is least angle regression lars, which is 70 and 2,115 times faster than M5 and cubist, respectively. The model which requires least memory is non-negative least squares (nnls), about 2 GB, similarly to cubist, while M5 requires about 8 GB. For 97.6% of datasets there is a regression model among the 10 bests which is very near (difference below 0.1) to the best R2, which increases to 100% allowing differences of 0.2. Therefore, provided that our dataset and model collection are representative enough, the main conclusion of this study is that, for a new regression problem, some model in our top-10 should achieve R2 near to the best attainable for that problem.",2019,Neural networks : the official journal of the International Neural Network Society
Infant feeding modes and determinants among HIV-1-infected African Women in the Kesho Bora Study.,"OBJECTIVE
To assess breastfeeding modes and determinants in a prevention of mother-to-child transmission study.


DESIGN
HIV-1-infected pregnant women from 5 sites in Burkina Faso, Kenya, and South Africa were enrolled in the study that comprised 2 prospective cohorts and 1 randomized controlled trial. Women were counseled to either breastfeed exclusively up to 6 months or formula feed from birth.


METHODS
Determinants of breastfeeding initiation and continuation by 3 months postpartum were investigated using multiple logistic regression analysis. Neonatal morbidity was defined as mother-reported fever, diarrhea, or vomiting during the first month of life.


RESULTS
Among 1028, 781 women (76%) initiated breastfeeding and 565 of 995 (56%) were still breastfeeding at 3 months postpartum (30% exclusively, 18% predominantly, and 8% partially). Study site (Durban, Mombasa, and Nairobi compared with Bobo-Dioulasso), CD4 cell count (<200 cells/mm), secondary schooling (compared with none), and emergency cesarean delivery (compared with vaginal delivery) were independently associated with a lower probability of ever breastfeeding. The odds of still breastfeeding by 3 months postpartum (among those breastfeeding by 1 month) were lower in Mombasa, Nairobi, and Somkhele (compared with Bobo-Dioulasso) and among infants with neonatal morbidity [0.60 (0.37-0.976)]. The odds of exclusive breastfeeding (EBF) by 3 months (if EBF by 1 month) were lower in Mombasa and Nairobi, in ill neonates [0.54 (0.31-0.93)] and boys [0.51 (0.34-0.77)].


CONCLUSIONS
EBF was of short duration, particularly for boys. The importance of neonatal morbidity for breastfeeding cessation requires further investigation. Infant feeding counseling might need adaptation to better support mothers of boys and ill neonates.",2013,Journal of acquired immune deficiency syndromes
Incorporating Intra-Operative Medication Information for Prediction of Post-Operative Atrial Fibrillation,"This study aimed to construct and evaluate a novel prediction model for postoperative atrial fibrillation (PoAF) with the addition of intraoperative medications. The study patient population included 4731 patients who underwent CABG surgery, of which 1363 developed PoAF, and the prediction methods included three logistic regression models. Multivariate logistic regression was performed with traditional clinical variables only for the first model, intraoperative medications added in the second model, and a subset of all variables chosen by the Least Absolute Shrinkage and Selection Operator (LASSO) in the third model. Age and prior AF diagnosis were consistently the strongest predictors for PoAF across all three models. The specific intra-operative medications moderately improved predictive accuracy as compared to the clinical feature-only model.",2019,2019 IEEE International Conference on Healthcare Informatics (ICHI)
Understanding Emergency Medicine Providers' Perceptions of the ACA in a Renewed Era of Health Care Reform: National Survey and Qualitative Mixedâ€Methods Approach: 2EMF,"Study Objectives: Acute kidney injury (AKI) is strongly associated with adverse clinical outcomes including prolonged hospitalization, progression to CKD, and death. Diagnosis of AKI relies on detection of changes in serum creatinine (sCr) and urine output, both of which lag days behind renal injury and are unreliable at initial presentation. Here, we utilized data mining and machine learning methods to develop a predictive model for AKI with capacity for identifying ED patients at high risk for development of AKI within 7 days of their ED visit. Methods: A retrospective cross-sectional cohort of ED visits from 3 hospitals over 2 years was generated and used for model derivation and out-of-sample validation. Clinical data for all adult ED visits where initial sCr measurements were available at index visit and again within 7 days of EDdeparturewere extracted froma relational database that underlies our electronic health record (EHR) by an experienced data user. Primary outcome for prediction was Stage 2 AKI within 7 days of ED visit, defined according to sCr-based Kidney Disease Improving Global Outcomes (KDIGO) criteria (sCr increase to 2 times baseline). Secondary outcomes included KDIGO Stage 1 AKI (sCr increase of 0.3mg/dl above baseline or 1.5 times baseline) and Stage 3 AKI (sCr increase to 3 times baseline or to 4.0 mg/dl). Predictor variables extracted from the EHR included vital signs, laboratory results, chief complaints, demographics, past medical history, active problems, home medications and EDmedication administrations. Only EHR data available prior to prediction, made at time of first metabolic panel result, was included. Predictor variables were normalized as follows: ED vital signs and laboratory results were processed to minimum and maximum values, nephrotoxic and nephroprotective medications were grouped by pharmacologic class and least absolute shrinkage and selection operator (LASSO) feature selection processing applied to chief complaints and active problems identify variables with predictive value for AKI.Multiple machine learning models (logistic regression, decision tree, linear discriminant analysis, support vector machine, and random forest) were generated and tested in the prediction of our primary outcome. All were developed using a training dataset comprised of 90% of encounters and evaluated in the remaining encounters using 10-fold cross validation. Performance of each model was assessed using binary classification measures and receiver operator curve (ROC) analyses. Results: Our final cohort included 127,183 ED visits by 72,539 unique patients. Median age was 58 years (IQR: 43-71) and most common high-risk comorbidities were hypertension (51.8%) and heart failure (9.8%). Incidence of AKI in our cohort was as follows: Stage 1: 12.4%, Stage 2: 1.5%, Stage 3: 1.0%. Predictive model performance as measured by area under the ROC analysis ranged from 0.661 (95% CI: 0.637 0.685) using decision tree to 0.771 (95% CI: 0.759 0.783) using random forest. Conclusions: Machine learning methods applied to EHR data identified ED patients at high risk for AKI well before patients met diagnostic criteria. The model developed here, when paired with nephroprotective point-of-care clinical decision support, has potential to improve outcomes for this patient population.",2018,Annals of Emergency Medicine
Identification of a four-gene metabolic signature predicting overall survival for hepatocellular carcinoma.,"While hundreds of consistently altered metabolic genes had been identified in hepatocellular carcinoma (HCC), the prognostic role of them remains to be further elucidated. Messenger RNA expression profiles and clinicopathological data were downloaded from The Cancer Genome Atlas-Liver Hepatocellular Carcinoma and GSE14520 data set from the Gene Expression Omnibus database. Univariate Cox regression analysis and lasso Cox regression model established a novel four-gene metabolic signature (including acetyl-CoA acetyltransferase 1, glutamic-oxaloacetic transaminase 2, phosphatidylserine synthase 2, and uridine-cytidine kinase 2) for HCC prognosis prediction. Patients in the high-risk group shown significantly poorer survival than patients in the low-risk group. The signature was significantly correlated with other negative prognostic factors such as higher Î±-fetoprotein. The signature was found to be an independent prognostic factor for HCC survival. Nomogram including the signature shown some clinical net benefit for overall survival prediction. Furthermore, gene set enrichment analyses revealed several significantly enriched pathways, which might help explain the underlying mechanisms. Our study identified a novel robust four-gene metabolic signature for HCC prognosis prediction. The signature might reflect the dysregulated metabolic microenvironment and provided potential biomarkers for metabolic therapy and treatment response prediction in HCC.",2019,Journal of cellular physiology
A Path Algorithm for the Fused Lasso Signal Approximator,"The Lasso is a very well-known penalized regression model, which adds an L1 penalty with parameter Î»1 on the coefficients to the squared error loss function. The Fused Lasso extends this model by also putting an L1 penalty with parameter Î»2 on the difference of neighboring coefficients, assuming there is a natural ordering. In this article, we develop a path algorithm for solving the Fused Lasso Signal Approximator that computes the solutions for all values of Î»1 and Î»2. We also present an approximate algorithm that has considerable speed advantages for a moderate trade-off in accuracy. In the Online Supplement for this article, we provide proofs and further details for the methods developed in the article.",2009,Journal of Computational and Graphical Statistics
Abstract 19631: Application of Machine Learning Methods for Prediction of Outcomes After Cardiac Transplantation: Insights From the UNOS Database,"Objectives: It has been hypothesized that applying advanced analytic methodologies to large patient datasets can revolutionize patient care but the relative performance and comparative effectiveness of various statistical approaches remains unclear. Methods: The United Network for Organ Sharing (UNOS) database was queried to identify initial adult heart transplants performed in the United States from 1987-2014 (N=50,453). We assessed prediction of 1 year survival using all donor and recipient variables except those with >20% missingness using traditional logistic regression and a comprehensive number of machine learning methodologies (ridge regression, regression with LASSO, support vector machines, naive Bayesian, tree-augmented Bayesian, neural network, random forest, and stochastic gradient boosting). We tested the predictive accuracy of these methods across UNOS regions for the following time periods [Period 1 (1987-1996); Period 2 (1996-2005); Period 3 (2006-2014)]. Results: The most powerful univari...",2016,Circulation
Sparse model identification and learning for ultra-high-dimensional additive partially linear models,"Abstract The additive partially linear model (APLM) combines the flexibility of nonparametric regression with the parsimony of regression models, and has been widely used as a popular tool in multivariate nonparametric regression to alleviate the â€œcurse of dimensionalityâ€. A natural question raised in practice is the choice of structure in the nonparametric part, i.e., whether the continuous covariates enter into the model in linear or nonparametric form. In this paper, we present a comprehensive framework for simultaneous sparse model identification and learning for ultra-high-dimensional APLMs where both the linear and nonparametric components are possibly larger than the sample size. We propose a fast and efficient two-stage procedure. In the first stage, we decompose the nonparametric functions into a linear part and a nonlinear part. The nonlinear functions are approximated by constant spline bases, and a triple penalization procedure is proposed to select nonzero components using adaptive group LASSO. In the second stage, we refit data with selected covariates using higher order polynomial splines, and apply spline-backfitted local-linear smoothing to obtain asymptotic normality for the estimators. The procedure is shown to be consistent for model structure identification. It can identify zero, linear, and nonlinear components correctly and efficiently. Inference can be made on both linear coefficients and nonparametric functions. We conduct simulation studies to evaluate the performance of the method and apply the proposed method to a dataset on the Shoot Apical Meristem (SAM) of maize genotypes for illustration.",2019,J. Multivar. Anal.
Combining Clinical and Omics data: hope or illusion?,"In modern biomedicine the combination of clinical information with that of multiple biomarkers coming form transcriptomic-wide experiments is a theme of central interest. New clinical trials design try to combine the evaluation of the effect of new drugs with that of the identification of subgroups with maximum benefit. The subgroups are identified with measurements from omic data (genomics, proteomics, lipidomics, etc.). A general concern regards the magnitude of the effects from omic data which is a central point when designing a trial. In this work we present a simulation strategy to investigate the impact on some measures of prognostic impact (namely the (integrated) prediction error and the C-index) of the magnitude of the effects from omic covariates. We hypothesise the presence of clinical covariates with large impact on prognosis and not correlated with the omic data. We adopt as a method of analysis the Cox regression with lasso regularisation.",2016,
A race-DC in Big Data,"The strategy of divide-and-combine (DC) has been widely used in the area of big data. Bias-correction is crucial in the DC procedure for validly aggregating the locally biased estimators, especial for the case when the number of batches of data is large. This paper establishes a race-DC through a residual-adjustment composition estimate (race). The race-DC applies to various types of biased estimators, which include but are not limited to Lasso estimator, Ridge estimator and principal component estimator in linear regression, and least squares estimator in nonlinear regression. The resulting global estimator is strictly unbiased under linear model, and is acceleratingly bias-reduced in nonlinear model, and can achieve the theoretical optimality, for the case when the number of batches of data is large. Moreover, the race-DC is computationally simple because it is a least squares estimator in a pro forma linear regression. Detailed simulation studies demonstrate that the resulting global estimator is significantly bias-corrected, and the behavior is comparable with the oracle estimation and is much better than the competitors.",2019,arXiv: Methodology
Robust Estimation of High-Dimensional Mean Regression,"Data subject to heavy-tailed errors are commonly encountered in various scientific fields, especially in the modern era with explosion of massive data. To address this problem, procedures based on quantile regression and Least Absolute Deviation (LAD) regression have been devel- oped in recent years. These methods essentially estimate the conditional median (or quantile) function. They can be very different from the conditional mean functions when distributions are asymmetric and heteroscedastic. How can we efficiently estimate the mean regression functions in ultra-high dimensional setting with existence of only the second moment? To solve this problem, we propose a penalized Huber loss with diverging parameter to reduce biases created by the traditional Huber loss. Such a penalized robust approximate quadratic (RA-quadratic) loss will be called RA-Lasso. In the ultra-high dimensional setting, where the dimensionality can grow exponentially with the sample size, our results reveal that the RA-lasso estimator produces a consistent estimator at the same rate as the optimal rate under the light-tail situation. We further study the computational convergence of RA-Lasso and show that the composite gradient descent algorithm indeed produces a solution that admits the same optimal rate after sufficient iterations. As a byproduct, we also establish the concentration inequality for estimat- ing population mean when there exists only the second moment. We compare RA-Lasso with other regularized robust estimators based on quantile regression and LAD regression. Extensive simulation studies demonstrate the satisfactory finite-sample performance of RA-Lasso.",2014,arXiv: Statistics Theory
Application of ImmunoScore Model for the Differentiation between Active Tuberculosis and Latent Tuberculosis Infection as Well as Monitoring Anti-tuberculosis Therapy,"Tuberculosis (TB) is a leading global public health problem. To achieve the end TB strategy, non-invasive markers for diagnosis and treatment monitoring of TB disease are urgently needed, especially in high-endemic countries such as China. Interferon-gamma release assays (IGRAs) and tuberculin skin test (TST), frequently used immunological methods for TB detection, are intrinsically unable to discriminate active tuberculosis (ATB) from latent tuberculosis infection (LTBI). Thus, the specificity of these methods in the diagnosis of ATB is dependent upon the local prevalence of LTBI. The pathogen-detecting methods such as acid-fast staining and culture, all have limitations in clinical application. ImmunoScore (IS) is a new promising prognostic tool which was commonly used in tumor. However, the importance of host immunity has also been demonstrated in TB pathogenesis, which implies the possibility of using IS model for ATB diagnosis and therapy monitoring. In the present study, we focused on the performance of IS model in the differentiation between ATB and LTBI and in treatment monitoring of TB disease. We have totally screened five immunological markers (four non-specific markers and one TB-specific marker) and successfully established IS model by using Lasso logistic regression analysis. As expected, the IS model can effectively distinguish ATB from LTBI (with a sensitivity of 95.7% and a specificity of 92.1%) and also has potential value in the treatment monitoring of TB disease.",2017,Frontiers in Cellular and Infection Microbiology
Predicting asthma in later childhood: a general and highâ€risk population approach: S59,"Introduction Young children commonly wheeze but only some have asthma later in life. Asthma prediction tools have poor predictive performance and few have been validated. We aimed to develop a robust tool for the prediction of asthma at age 10â€“14 years using readily available information. Methods we studied 5 UK birth cohorts (the STELAR consortium) and considered two groups: 1. all children recruited at birth and 2. high-risk children on the basis of reported wheezing at 2/3 or 5 years. Two comparable cohorts (Ashford and ALSPAC) were used to select predictors (training sample) and the SEATON, MAAS and Isle of Wight studies to assess predictive performance (validation sample). We included 16â€‰187 and 814 children from groups 1 and 2 respectively in the training sample and validated the developed predictive tools in 5320 and 285 children from the validation sample. We considered 40 potential predictors collected at recruitment and at 1, 2/3 and 5 years of age: demographic and perinatal information, eczema, hay-fever, respiratory symptoms, environmental and family-related factors. We defined asthma at 10â€“14 years by the presence of both current wheeze and asthma treatment. We compared 5 statistical methods to select variables and estimate coefficients: stepwise regression, classical (LASSO and Elastic-Net, EN), empirical Bayes (EB) and Bayesian (BM) regularisation Methods Predictive performance was assessed using calibration and discrimination measures including area under the ROC curve (AUC). Results Asthma prevalence at age 10â€“14 ranged from 7%â€“18% in group 1 and from 32%â€“52% in group 2. Frequency of early wheezing, eczema, and paternal asthma were important predictors in all models and both groups. Other selected predictors included birth order, maternal asthma and domestic pets. Specificity and negative predictive value (NPV) were higher in the general population, while sensitivity and positive predictive value (PPV) were higher in high-risk group. BM (AUC 0.77, specificity 0.84 and NPV 0.93) and EN (AUC 0.74, sensitivity 0.71 and PPV 0.65) provided the highest accuracy and discriminative ability predictive ability in the 2 groups, respectively. Conclusion The use of sophisticated statistical methods in a large, multicentre population demonstrated promising Results in developing an asthma predictive tool.",2017,Thorax
Fast global convergence of gradient methods for high-dimensional statistical recovery,"Many statistical $M$-estimators are based on convex optimization problems formed by the combination of a data-dependent loss function with a norm-based regularizer. We analyze the convergence rates of projected gradient and composite gradient methods for solving such problems, working within a high-dimensional framework that allows the data dimension $\pdim$ to grow with (and possibly exceed) the sample size $\numobs$. This high-dimensional structure precludes the usual global assumptions---namely, strong convexity and smoothness conditions---that underlie much of classical optimization analysis. We define appropriately restricted versions of these conditions, and show that they are satisfied with high probability for various statistical models. Under these conditions, our theory guarantees that projected gradient descent has a globally geometric rate of convergence up to the \emph{statistical precision} of the model, meaning the typical distance between the true unknown parameter $\theta^*$ and an optimal solution $\hat{\theta}$. This result is substantially sharper than previous convergence results, which yielded sublinear convergence, or linear convergence only up to the noise level. Our analysis applies to a wide range of $M$-estimators and statistical models, including sparse linear regression using Lasso ($\ell_1$-regularized regression); group Lasso for block sparsity; log-linear models with regularization; low-rank matrix recovery using nuclear norm regularization; and matrix decomposition. Overall, our analysis reveals interesting connections between statistical precision and computational efficiency in high-dimensional estimation.",2011,ArXiv
Asynchronous Doubly Stochastic Sparse Kernel Learning,"Kernel methods have achieved tremendous success in the past two decades. In the current big data era, data collection has grown tremendously. However, existing kernel methods are not scalable enough both at the training and predicting steps. To address this challenge, in this paper, we first introduce a general sparse kernel learning formulation based on the random feature approximation, where the loss functions are possibly non-convex. Then we propose a new asynchronous parallel doubly stochastic algorithm for large scale sparse kernel learning (AsyDSSKL). To the best our knowledge, AsyDSSKL is the first algorithm with the techniques of asynchronous parallel computation and doubly stochastic optimization. We also provide a comprehensive convergence guarantee to AsyDSSKL. Importantly, the experimental results on various large-scale real-world datasets show that, our AsyDSSKL method has the significant superiority on the computational efficiency at the training and predicting steps over the existing kernel methods. Introduction Kernel methods have achieved tremendous success in the past two decades for non-linear learning problems. There are a large number of successful and popular kernel methods (Vapnik 1998; Zhu and Hastie 2005; Zhu et al. 2004; Baudat and Anouar 2000; Vovk 2013; Li, Yang, and Xing 2005) for various learning problems. We take binary classification and regression for example. Support vector classification (SVC) (Vapnik 1998), kernel logistic regression (Zhu and Hastie 2005) and 1-norm SVC (Zhu et al. 2004) are the popular kernel methods for binary classification. Support vector regression (Vapnik 1998), kernel ridge regression (Vovk 2013) and kernel Lasso (Li, Yang, and Xing 2005) are the popular kernel methods for regression. These kernel methods have been successfully applied to solve various real-world applications (such as computational biology (SchÃ¶lkopf, Tsuda, and Vert 2004) and remote sensing data analysis (CampsValls and Bruzzone 2009)). However, traditional kernel methods need to store and compute the kernel matrix with the size of O(l) where l is the training sample size. When l is large, the kernel matrix can be neither stored in local memory nor computed. Even *To whom all correspondence should be addressed. Copyright Â© 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. worse, the kernel methods normally have the computational complexity O(l) for the training. To address the scalability issue of kernel methods in the training step, several decomposition algorithms (Takahashi and Nishi 2006) have been proposed for training the kernel methods. However, even for the state-of-the-art implementations (e.g. LIBSVM software package), the observed computational complexity is O(l) where 1 < Îº < 2.3 (Chang and Lin 2011a). More related works of training kernel methods are discussed in the next section. To sum up, in the current big data era, the existing kernel methods are not scalable enough at the training step. Besides the scalability issue at the training step, traditional kernel methods are also not scalable at the predicting step. Specifically, the computational complexity for predicting a testing sample is O(l), because normally the number of support vectors grows linearly with the sample size. Thus, the computational complexity of predicting a testing set with similar size of training set is O(l). To address the scalability issue of kernel methods in the predicting step, a compact (or sparse) model is preferred. The direct method for obtaining a compact model is adding sparse constraint (normally 1norm) on the coefficients of the model. For example, (Zhu et al. 2004) imposed the 1-norm on the SVC formulation. (Yen et al. 2014) imposed 1-norm on the random features. Li, Yang, and Xing (2005) reformulated Lasso into a form isomorphic to SVC, which generates a sparse solution in the nonlinear feature space. However, these methods are not scalable at the training step. To address the scalability issues of kernel methods at the training and predicting steps, in this paper, we first introduce a general sparse kernel learning formulation with the random feature approximation, where the loss functions are possibly non-convex. Then, we propose a new asynchronous parallel doubly stochastic algorithm for sparse kernel learning (AsyDSSKL). We believe this is very important for kernel methods for four reasons. 1) Generality: AsyDSSKL works for a general class of sparse kernel methods based on the random feature approximation, where the loss function is possibly non-convex. 2) Efficient computation: The pivotal step of AsyDSSKL is to compute the doubly stochastic gradient on a mini-batch of samples and a selected coordinate, which is quite efficient. 3) Comprehensive convergence guarantees: AsyDSSKL achieves a sublinear rate when the smooth loss functions are convex or non-convex. The Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18)",2018,
High-dimensional robust approximated M-estimators for mean regression with asymmetric data,"Asymmetry along with heteroscedasticity or contamination often occurs with the growth of data dimensionality. In ultra-high dimensional data analysis, such irregular settings are usually overlooked for both theoretical and computational convenience. In this paper, we establish a framework for estimation in high-dimensional regression models using Penalized Robust Approximated quadratic M-estimators (PRAM). This framework allows general settings such as random errors lack of symmetry and homogeneity, or the covariates are not sub-Gaussian. To reduce the possible bias caused by the data's irregularity in mean regression, PRAM adopts a loss function with a flexible robustness parameter growing with the sample size. Theoretically, we first show that, in the ultra-high dimension setting, PRAM estimators have local estimation consistency at the minimax rate enjoyed by the LS-Lasso. Then we show that PRAM with an appropriate non-convex penalty in fact agrees with the local oracle solution, and thus obtain its oracle property. Computationally, we demonstrate the performances of six PRAM estimators using three types of loss functions for approximation (Huber, Tukey's biweight and Cauchy loss) combined with two types of penalty functions (Lasso and MCP). Our simulation studies and real data analysis demonstrate satisfactory finite sample performances of the PRAM estimator under general irregular settings.",2019,arXiv: Statistics Theory
The Constrained Lasso,"Motivated by applications in areas as diverse as finance, image reconstruction, and curve estimation, we introduce the constrained lasso problem, where the underlying parameters satisfy a collection of linear constraints. We show that many statistical methods, such as the fused lasso, monotone curve estimation and the generalized lasso, are all special cases of the constrained lasso. Computing the constrained lasso poses some technical challenges but we develop an efficient algorithm for fitting it over a grid of tuning parameters. Non-asymptotic error bounds are developed which suggest that the constrained lasso should outperform the lasso in situations where the true parameters satisfy the underlying constraints. Extensive numerical experiments show that our method performs well, both computationally and statistically. Finally, we apply the constrained lasso to a real data set to estimate the demand curve for a particular type of auto loan as a function of interest rate, and demonstrate that it can outperform more standard approaches. Some key words: Lasso; Linear constraints; Penalized regression; Demand function; Generalized lasso",2012,
Review of Modern Logistic Regression Methods with Application to Small and Medium Sample Size Problems,"Logistic regression is one of the most widely applied machine learning tools in binary classification problems. Traditionally, inference of logistic models has focused on stepwise regression procedures which determine the predictor variables to be included in the model. Techniques that modify the log-likelihood by adding a continuous penalty function of the parameters have recently been used when inferring logistic models with a large number of predictor variables. This paper compares and contrasts three popular penalized logistic regression methods: ridge regression, the Least Absolute Shrinkage and Selection Operator (LASSO) and the elastic net. The methods are compared in terms of prediction accuracy using simulated data as well as real data sets.",2010,
Utilizing longitudinal microbiome taxonomic profiles to predict food allergy via Long Short-Term Memory networks,"Food allergy is usually difficult to diagnose in early life, and the inability to diagnose patients with atopic diseases at an early age may lead to severe complications. Numerous studies have suggested an association between the infant gut microbiome and development of allergy. In this work, we investigated the capacity of Long Short-Term Memory (LSTM) networks to predict food allergies in early life (0-3 years) from subjects' longitudinal gut microbiome profiles. Using the DIABIMMUNE dataset, we show an increase in predictive power using our model compared to Hidden Markov Model, Multi-Layer Perceptron Neural Network, Support Vector Machine, Random Forest, and LASSO regression. We further evaluated whether the training of LSTM networks benefits from reduced representations of microbial features. We considered sparse autoencoder for extraction of potential latent representations in addition to standard feature selection procedures based on Minimum Redundancy Maximum Relevance (mRMR) and variance prior to the training of LSTM networks. The comprehensive evaluation reveals that LSTM networks with the mRMR selected features achieve significantly better performance compared to the other tested machine learning models.",2019,PLoS Computational Biology
"Statistical methods in disease risk analysis, disease testing and nutrition epidemiology","The thesis is composed of three separated projects: disease risk scoring systems (chapter 2), statistical tests for proportion difference in one-to-two matched binary data (chapter 3) and bivariate measurement error model for nutrition epidemiology (chapter 4). In the first project, we propose to use group lasso algorithm for logistic regression to construct a risk scoring system for predicting disease in swine. We choose the penalty parameter for the group lasso through leave-one-out cross validation and use the area under the receiver operating characteristic curve as criterion. We show our proposed scoring system is superior to existing methods. The second project was originally motivated by the pooling of diagnostic tests. We proposed exact and asymptotic tests for one-to-two matched binary data. Unlike other existing methods, our procedure doesnâ€™t rely on a mutual independence assumption. The emphasis on dependence among observations from the same matched set is natural and appealing, as much in human health as it is in veterinary medicine. It can be applied to many kinds of diagnostic studies with a one-to-two matched data structure. Our method can also be generalized to one-to-N matched case in a straightforward manner. In the third paper we consider the problem of estimating the joint distribution of two correlated random variables where one of the variables is observed with error. DKM is first used to adjust the univariate measurement error. A Gaussian copula is then used to model the correlation structure between the two variables after error adjustment.",2018,
Inference on Large-scale Structures,"The emergence of â€˜Big Dataâ€™ has driven a new statistical branch which Efron [11] calls â€˜Large-Scale Inferenceâ€™ (LSI). In many LSI problems, the data are structured. Due to proximity in geography, time, etc., the data matrix may contain graphical structures, low-rank structures, clustering structures, etc.. These structure pose great challenges as well as great opportunities: by carefully exploiting such structures, we are able to significantly improve the inference. In this dissertation, we illustrate the point by exploiting two types of structures: sparse graphical structures and homogeneous clustering structures. Consider a linear regression model Y = XÎ² + , where X âˆˆ RnÃ—p is the design matrix and is the noise vector. Our study consists of two parts. The first part is largely motivated by the recent interest in DNA copy number variation (CNV) and long-memory financial data, where the focus is â€˜variable selectionâ€™: we assume the effects are â€˜rare and weakâ€™ in the sense that only a small fraction of the entries of Î² are nonzero and the nonzeros are individually small; the main interest is to identify this small fraction of nonzeros. We consider the very challenging case where the columns of the design matrix X are heavily correlated. We recognize that in many of such situations, adjacent rows of the Gram matrix G = X â€²X are similar, and such structures can be exploited for better variable selection. We develop a new variable selection procedure called â€˜Covariate Assisted Screening and Estimation (CASE)â€™. CASE is a two-stage Screen and Clean method, at the heart of which is a graph-guided multivariate screening procedure. We show that in a broad context, CASE achieves the minimax Hamming selection errors. We have successfully applied CASE to a change-point problem and long-memory time series, and derived explicit forms of the so-called â€˜phase diagramsâ€™. The phase diagram is a recent way to visualize variable selection that is especially appropriate for rare and weak effects. iii CASE has advantages over the more well-known L/L-penalization methods and marginal screening. CASE is a flexible idea and is readily extendable to many different settings. The second part of the study is largely motivated by the recent interest on gene network [32] and the study of housing price [16]. We assume the vector Î² is homogeneous in the sense that its entries cluster into a few groups, and entries in each group share a common value; the main interest is to take advantage of such structures to estimate Î². We propose a new method called â€˜Clustering Algorithm in Regression via Datadriven Segmentationâ€™ (CARDS). We show that under mild conditions, CARDS successfully recovers the groups of variables and achieves the oracle estimation errors. The study provides additional insights on how to exploit low-dimensional structures in high-dimensional regression problems. CARDS was successfully applied to predicting Polyadenylation signals and the S&P500 stock returns. Compared to methods not taking advantage of homogeneity, CARDS has much smaller estimation errors. Compared to other methods that attempt to exploit homogeneity such as the fused lasso, CARDS has the advantage of not requiring any given ordering or graph on the variables. CARDS is also a flexible idea and can be adapted to many different settings.",2014,
Application of shrinkage estimation in linear regression models with autoregressive errors,"In this paper, we consider the shrinkage and penalty estimation procedures in the linear regression model with autoregressive errors of order p when it is conjectured that some of the regression parameters are inactive. We develop the statistical properties of the shrinkage estimation method including asymptotic distributional biases and risks. We show that the shrinkage estimators have a significantly higher relative efficiency than the classical estimator. Furthermore, we consider the two penalty estimators: least absolute shrinkage and selection operator (LASSO) and adaptive LASSO estimators, and numerically compare their relative performance with that of the shrinkage estimators. A Monte Carlo simulation experiment is conducted for different combinations of inactive predictors and the performance of each estimator is evaluated in terms of the simulated mean-squared error. This study shows that the shrinkage estimators are comparable to the penalty estimators when the number of inactive predictors in the model is relatively large. The shrinkage and penalty methods are applied to a real data set to illustrate the usefulness of the procedures in practice.",2015,Journal of Statistical Computation and Simulation
Model selection consistency of U-statistics with convex loss and weighted lasso penalty,"ABSTRACT In the paper we consider minimisation of U-statistics with the weighted Lasso penalty and investigate their asymptotic properties in model selection and estimation. We prove that the use of appropriate weights in the penalty leads to the procedure that behaves like the oracle that knows the true model in advance, i.e. it is model selection consistent and estimates nonzero parameters with the standard rate. For the unweighted Lasso penalty, we obtain sufficient and necessary conditions for model selection consistency of estimators. The obtained results strongly based on the convexity of the loss function that is the main assumption of the paper. Our theorems can be applied to the ranking problem as well as generalised regression models. Thus, using U-statistics we can study more complex models (better describing real problems) than usually investigated linear or generalised linear models.",2017,Journal of Nonparametric Statistics
Development and internal validation of the multivariable CIPHER (Collaborative Integrated Pregnancy High-dependency Estimate of Risk) clinical risk prediction model,"BackgroundIntensive care unit (ICU) outcome prediction models, such as Acute Physiology And Chronic Health Evaluation (APACHE), were designed in general critical care populations and their use in obstetric populations is contentious. The aim of the CIPHER (Collaborative Integrated Pregnancy High-dependency Estimate of Risk) study was to develop and internally validate a multivariable prognostic model calibrated specifically for pregnant or recently delivered women admitted for critical care.MethodsA retrospective observational cohort was created for this study from 13 tertiary facilities across five high-income and six low- or middle-income countries. Women admitted to an ICU for more thanâ€‰24Â h during pregnancy orÂ less thanÂ 6Â weeks post-partum from 2000 to 2012 were included in the cohort. A composite primary outcome was defined as maternal death or need for organ support for more than 7Â days or acute life-saving intervention. Model development involved selection of candidate predictor variables based on prior evidence of effect, availability across study sites, and use of LASSO (Least Absolute Shrinkage and Selection Operator) model building after multiple imputation using chained equations to address missing data for variable selection. The final model was estimated using multivariable logistic regression. Internal validation was completed using bootstrapping to correct for optimism in model performance measures of discrimination and calibration.ResultsOverall, 127 out of 769 (16.5%) women experienced an adverse outcome. Predictors included in the final CIPHER model were maternal age, surgery in the preceding 24Â h, systolic blood pressure, Glasgow Coma Scale score, serum sodium, serum potassium, activated partial thromboplastin time, arterial blood gas (ABG) pH, serum creatinine, and serum bilirubin. After internal validation, the model maintained excellent discrimination (area under the curve of the receiver operating characteristic (AUROC) 0.82, 95% confidence interval (CI) 0.81 to 0.84) and good calibration (slope of 0.92, 95% CI 0.91 to 0.92 and intercept of âˆ’0.11, 95% CI âˆ’0.13 to âˆ’0.08).ConclusionsThe CIPHER model has the potential to be a pragmatic risk prediction tool. CIPHER can identify critically ill pregnant women at highest risk for adverse outcomes, inform counseling of patients about risk, and facilitate bench-marking of outcomes between centers by adjusting for baseline risk.",2018,Critical Care
CS 294-1 : Assignment 2 A Large-Scale Linear Regression Sentiment Model,"The primary objective of this assignment was to build a linear regression sentiment model based on amazon.com reviews. The main challenge comprised of handling moderately large amounts of data on a single machine. The different variations that I tried include the following: exact solution (L2 loss and ridge regularization), stochastic gradient with different training schemes and initialization, lasso regularization and unigram and bigram features .",2012,
Improving Lasso for model selection and prediction,"It is known that the Thresholded Lasso (TL), SCAD or MCP correct intrinsic estimation bias of the Lasso. In this paper we propose an alternative method of improving the Lasso for predictive models with general convex loss functions which encompass normal linear models, logistic regression, quantile regression or support vector machines. For a given penalty we order the absolute values of the Lasso non-zero coefficients and then select the final model from a small nested family by the Generalized Information Criterion. We derive exponential upper bounds on the selection error of the method. These results confirm that, at least for normal linear models, our algorithm seems to be the benchmark for the theory of model selection as it is constructive, computationally efficient and leads to consistent model selection under weak assumptions. Constructivity of the algorithm means that, in contrast to the TL, SCAD or MCP, consistent selection does not rely on the unknown parameters as the cone invertibility factor. Instead, our algorithm only needs the sample size, the number of predictors and an upper bound on the noise parameter. We show in numerical experiments on synthetic and real-world data sets that an implementation of our algorithm is more accurate than implementations of studied concave regularizations. Our procedure is contained in the R package ""DMRnet"" and available on the CRAN repository.",2019,arXiv: Statistics Theory
OC-097 An externally validated nomogram to predict the risk of bowel dysfunction following an anterior resection,"Introduction Almost half of the patients undergoing an anterior resection for rectal cancer report quality of life impairment due to bowel dysfunction. The Low Anterior Resection Syndrome (LARS) score is an internationally validated patient reported outcome measure (PROM) designed to assess the severity of these symptoms. 1 The aims of this study were to (i) produce a LARS score prediction model based on the risk factors for bowel dysfunction (ii) to validate the model externally and (iii) to incorporate findings into a nomogram for use in clinical practice. Method Patients who were recurrence free and more than one-year post restorative anterior resection (UK, 2001â€“2011; Denmark, 2001â€“2007 (median 54 and 56 months since surgery respectively), were invited to complete EORTC QLQ-C30, LARS and Wexner incontinence scores. Demographics, tumour characteristics, pre/post-operative treatment, and surgical procedures were recorded. Advanced linear regression shrinkage techniques (i.e. LASSO) were used independently for both datasets (UK, DK) for an unbiased selection of the most influential factors. Finally a nomogram based on linear regression was generated. Results The model was developed from 463 British patients and validated in 938 Danes. The respective cohorts were similar in gender (females 40% [184/463] versus 43% [402/938] NS) but differed by age (65 [13] versus 64 [13], p = 0.013), pre-operative radiotherapy (31.6% [145/463] versus 20.4% [191/938], p = 0.001), tumour height (median [IQR] 9 cm [4] versus 11 [4], p = 0.001), global quality of life (mean (SD) QLQ C30 Scale, 77 (19) versus 78 (21), p = 0.001) and LARS score (mean (SD) 26.1 (11) versus 24.5 (12), p = 0.012). Tumour height (from the anal verge) and pre-operative radiotherapy were the key variables in the nomogram with several additional factors including age, gender and temporary stoma influencing the predicted LARS score. Conclusion This is the first nomogram to predict patient reported bowel dysfunction following an anterior resection. Despite differences between cohorts, with the UK tending to restore lower tumours and use more pre-operative radiotherapy, we were able to externally validate the model in a Danish population. Colorectal Surgeons and Nurse Specialists may use this predictive tool to help patients understand their risk of bowel dysfunction, it is intended to aid the consent process and highlight patients that may need additional post-operative support. Disclosure of interest None Declared. Reference Juul T, Ahlberg M, Biondo S, et al . International validation of the low anterior resection syndrome (LARS) score. Ann Surg. 2014;259(4):728â€“34",2015,Gut
The Effect of Arterial Stiffness on Recurrence of Paroxysmal Atrial Fibrillation after Catheter Ablation,"Purpose: The aim of this study was to investigate the relationship between arterial stiffness and atrial fibrillation (AF) recurrence after catheter ablation (CA). Methods: We enrolled 94 consecutive patients (mean age: 62Â±9.5) undergoing circumferential pulmonary vein electrical isolation with a double lasso technique using CARTO system for paroxysmal AF. Cardio-ankle vascular index (CAVI) was determined as an index of arterial stiffness. Plasma BNP, serum aldosterone, and high sensitivity CRP were assayed, and Left atrial volume (LAV) was measured by multi-slice CT. Results: Among the 94 patients, 26 (28%) had metabolic syndrome. In simple regression analysis, CAVI were correlated with age, eGFR, BNP, and LAV (p<0.001, p<0.05, p<0.01, p<0.05, respectively). During a follow-up of 7.9Â±2.3 months, 21 patients experienced AF recurrence. Patients with AF recurrence had a increased CAVI (recurrence 8.9 vs non-recurrence 8.2 p<0.05) and a larger LAV (recurrence 81.3 ml vs non-recurrence 70.6 ml p<0.05) compared with patients without AF recurrence. Multivariate analysis revealed that LAV, BNP, CAVI were independently associated with AF recurrence (t-value: 2.76, 4.65, 2.28, respectively). Conclusions: CAVI was associated with age, BNP and LAV, suggesting a significant role of arterial stiffness in AF after CA.",2011,Journal of Arrhythmia
Homology-based radiomic features for prediction of the prognosis of lung cancer based on CT-based radiomics.,"PURPOSE
Radiomics is a new technique that enables noninvasive prognostic prediction by extracting features from medical images. Homology is a concept used in many branches of algebra and topology that can quantify the contact degree. In the present study, we developed homology-based radiomic features to predict the prognosis of non-small-cell lung cancer (NSCLC) patients and then evaluated the accuracy of this prediction method.


METHODS
Four data sets were used: two to provide training and test data and two for the selection of robust radiomic features. All the data sets were downloaded from The Cancer Imaging Archive (TCIA). In two-dimensional cases, the Betti numbers consist of two values: b0 (zero-dimensional Betti number), which is the number of isolated components, and b1 (one-dimensional Betti number), which is the number of one-dimensional or ""circular"" holes. For homology-based evaluation, CT images must be converted to binarized images in which each pixel has two possible values: 0 or 1. All CT slices of the gross tumor volume were used for calculating the homology histogram. First, by changing the threshold of the CT value (range: -150 to 300 HU) for all its slices, we developed homology-based histograms for b0 , b1 , and b1 /b0 using binarized images All histograms were then summed, and the summed histogram was normalized by the number of slices. 144 homology-based radiomic features were defined from the histogram. To compare the standard radiomic features, 107 radiomic features were calculated using the standard radiomics technique. To clarify the prognostic power, the relationship between the values of the homology-based radiomic features and overall survival was evaluated using LASSO Cox regression model and the Kaplan-Meier method. The retained features with non-zero coefficients calculated by the LASSO Cox regression model were used for fitting the regression model. Moreover, these features were then integrated into a radiomics signature. An individualized rad score was calculated from a linear combination of the selected features, which were weighted by their respective coefficients.


RESULTS
When the patients in the training and test data sets were stratified into high-risk and low-risk groups according to the rad scores, the overall survival of the groups was significantly different. The C-index values for the homology-based features (rad score), standard features (rad score), and tumor size were 0.625, 0.603, and 0.607, respectively, for the training data sets and 0.689, 0.668, and 0.667 for the test data sets. This result showed that homology-based radiomic features had slightly higher prediction power than the standard radiomic features.


CONCLUSIONS
Prediction performance using homology-based radiomic features had a comparable or slightly higher prediction power than standard radiomic features. These findings suggest that homology-based radiomic features may have great potential for improving the prognostic prediction accuracy of CT-based radiomics. In this result, it is noteworthy that there are some limitations.",2020,Medical physics
A comparative study of variable selection methods in the context of developing psychiatric screening instruments.,"The development of screening instruments for psychiatric disorders involves item selection from a pool of items in existing questionnaires assessing clinical and behavioral phenotypes. A screening instrument should consist of only a few items and have good accuracy in classifying cases and non-cases. Variable/item selection methods such as Least Absolute Shrinkage and Selection Operator (LASSO), Elastic Net, Classification and Regression Tree, Random Forest, and the two-sample t-test can be used in such context. Unlike situations where variable selection methods are most commonly applied (e.g., ultra high-dimensional genetic or imaging data), psychiatric data usually have lower dimensions and are characterized by the following factors: correlations and possible interactions among predictors, unobservability of important variables (i.e., true variables not measured by available questionnaires), amount and pattern of missing values in the predictors, and prevalence of cases in the training data. We investigate how these factors affect the performance of several variable selection methods and compare them with respect to selection performance and prediction error rate via simulations. Our results demonstrated that: (1) for complete data, LASSO and Elastic Net outperformed other methods with respect to variable selection and future data prediction, and (2) for certain types of incomplete data, Random Forest induced bias in imputation, leading to incorrect ranking of variable importance. We propose the Imputed-LASSO combining Random Forest imputation and LASSO; this approach offsets the bias in Random Forest and offers a simple yet efficient item selection approach for missing data. As an illustration, we apply the methods to items from the standard Autism Diagnostic Interview-Revised version.",2014,Statistics in medicine
High-Dimensional Metrics in R,"The package High-dimensional Metrics (\Rpackage{hdm}) is an evolving collection of statistical methods for estimation and quantification of uncertainty in high-dimensional approximately sparse models. It focuses on providing confidence intervals and significance testing for (possibly many) low-dimensional subcomponents of the high-dimensional parameter vector. Efficient estimators and uniformly valid confidence intervals for regression coefficients on target variables (e.g., treatment or policy variable) in a high-dimensional approximately sparse regression model, for average treatment effect (ATE) and average treatment effect for the treated (ATET), as well for extensions of these parameters to the endogenous setting are provided. Theory grounded, data-driven methods for selecting the penalization parameter in Lasso regressions under heteroscedastic and non-Gaussian errors are implemented. Moreover, joint/ simultaneous confidence intervals for regression coefficients of a high-dimensional sparse regression are implemented, including a joint significance test for Lasso regression. Data sets which have been used in the literature and might be useful for classroom demonstration and for testing new estimators are included. \R and the package \Rpackage{hdm} are open-source software projects and can be freely downloaded from CRAN: \texttt{http://cran.r-project.org}.",2016,arXiv: Machine Learning
Prognostic alternative mRNA splicing signature in hepatocellular carcinoma: a study based on large-scale sequencing data.,"Most genes are alternatively spliced and increasing number of evidences show that alternative splicing (AS) is modified and related to tumor progression. Systematic profiles of AS signature in hepatocellular carcinoma (HCC) is absent and urgently needed. Here, differentially spliced AS transcripts between HCC and non-HCC tissues were compared, prognosis-associated AS events by using univariate Cox regression analysis were selected. Our gene functional enrichment analysis demonstrated the potential pathways enriched by survival-associated AS. Prognostic AS signatures were then constructed for HCC prognosis prediction by Lasso regression model. We also analyzed splicing factors (SFs) regulating underlying mechanisms by Pearson correlation and then built corresponding regulatory networks. In addition, we explored the performance of AS signature in the mutated HCC samples. Genome-wide AS events in 377 HCC patients from TCGA were profiled. Among 34 163 AS events in 8985 genes, 3950 AS events in 2403 genes associated with overall survival (OS) significantly for HCC were detected. In addition, computational algorithm results showed that metabolic and ribosome pathways may be the potential molecular mechanisms regulating the poor prognosis. More importantly, survival-associated AS signatures revealed high performance in predicting HCC prognosis. The area under curve for AS signature was 0.806 in all HCC and 0.944 in TP53 mutated HCC samples at 2000 days of OS. We submitted prognostic SFs to build the AS regulatory network, from which we found prognostic AS events were significantly enriched in metabolism-related pathways. A robust AS signature for HCC patients and revealed the regulatory splicing networks contributing to the potential significantly enriched metabolism-related pathways.",2019,Carcinogenesis
Selection consistency of Lasso-based procedures for misspecified high-dimensional binary model and random regressors,We consider selection of random predictors for high-dimensional regression problem with binary response for a general loss function. Important special case is when the binary model is semiparametric and the response function is misspecified under parametric model fit. Selection for such a scenario aims at recovering the support of the minimizer of the associated risk with large probability. We propose a two-step selection procedure which consists of screening and ordering predictors by Lasso method and then selecting a subset of predictors which minimizes Generalized Information Criterion on the corresponding nested family of models. We prove consistency of the selection method under conditions which allow for much larger number of predictors than number of observations. For the semiparametric case when distribution of random predictors satisfies linear regression conditions the true and the estimated parameters are collinear and their common support can be consistently identified.,2020,ArXiv
