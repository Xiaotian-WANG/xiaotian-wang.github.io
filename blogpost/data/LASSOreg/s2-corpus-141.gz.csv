title,abstract,year,journal
Lack of direct correlation between CD4 T-lymphocyte counts and induration sizes of the tuberculin skin test in human immunodeficiency virus type 1 seropositive patients.,"SETTING
The study was conducted in Bobo-Dioulasso, Burkina Faso, where Mycobacterium tuberculosis infection and human immunodeficiency virus type 1 (HIV-1) infection are prevalent.


OBJECTIVE
To identify proportions of representative (test) populations who are reactive to the tuberculin skin test, and to study the relationship between CD4 T-lymphocyte counts and the induration size of the tuberculin skin test in these groups.


DESIGN
A group of 435 healthy students was tuberculin skin tested in order to evaluate the intensity of skin testing in a 'normal' population. The study group consisted of 195 subjects with or without tuberculosis, and with or without HIV-1 infection, who received a tuberculin skin test and a CD4 T lymphocyte count on the same day.


RESULTS
In total, 90% of the control (nontuberculous, HIV negative) subjects, 32% of the HIV-1 seropositive subjects, 76.5% of the tuberculous patients and 57% of the tuberculous HIV-1 seropositive patients were tuberculin positive. There was no direct correlation between the induration size of reactions to the tuberculin skin test and CD4 T-lymphocyte count in these study groups using linear regression analysis.


CONCLUSION
In vivo skin testing using tuberculin yields clinically significant information on the degree of immunodeficiency which is different from that of CD4 T-lymphocyte counts. The tuberculin skin test should therefore be used as an independent marker of the weakened immunological status of HIV-1 seropositive subjects.",1998,The international journal of tuberculosis and lung disease : the official journal of the International Union against Tuberculosis and Lung Disease
Exploration and Mining of Educational data for Analyzing Student's Engagement,"In the recent years data mining and analytical tools are used to thoroughly observe and find unanticipated relationships with in the data. They are customized into useful, novel ways for the data owners by extracting insights. A lack of appropriate methods, findings for exploring the student-learning data using advanced feature selection methods has paved way for this idea. Events occurring in the most common sources of diverse learning platforms were identified where ever common engagements are observed facilitating Student participation. Completely anonymous data is used for analysis keeping ethical concerns in mind. LIWC is used for breaking down the text into numerical measures, helped to pop out any interesting patterns in the review comments. This research showcases different available advanced feature selection options while trying to exclude predictors with least impact on the dependant variable. Best subsets, Lasso, Ridge and Elastic net were evaluated with lowest MSE of 0.46 achieved for Ridge regression validated by k-fold technique. The data depicting most common student personalities is classified into 3 groups based on discrete learning events. The contribution of this work is an accurate prediction of linear models entities via validating different feature selection techniques. Student personalities were identified based on most common student participation platforms where both numerical and textual measures were considered for analysis.",2017,
Quantitative EEG (QEEG) Measures Differentiate Parkinson's Disease (PD) Patients from Healthy Controls (HC),"Objectives: To find out which Quantitative EEG (QEEG) parameters could best distinguish patients with Parkinson's disease (PD) with and without Mild Cognitive Impairment from healthy individuals and to find an optimal method for feature selection. Background: Certain QEEG parameters have been seen to be associated with dementia in Parkinson's and Alzheimer's disease. Studies have also shown some parameters to be dependent on the stage of the disease. We wanted to investigate the differences in high-resolution QEEG measures between groups of PD patients and healthy individuals, and come up with a small subset of features that could accurately distinguish between the two groups. Methods: High-resolution 256-channel EEG were recorded in 50 PD patients (age 68.8 Â± 7.0 year; female/male 17/33) and 41 healthy controls (age 71.1 Â± 7.7 year; female/male 20/22). Data was processed to calculate the relative power in alpha, theta, delta, beta frequency bands across the different regions of the brain. Median, peak frequencies were also obtained and alpha1/theta ratios were calculated. Machine learning methods were applied to the data and compared. Additionally, penalized Logistic regression using LASSO was applied to the data in R and a subset of best-performing features was obtained. Results: Random Forest and LASSO were found to be optimal methods for feature selection. A group of six measures selected by LASSO was seen to have the most effect in differentiating healthy individuals from PD patients. The most important variables were the theta power in temporal left region and the alpha1/theta ratio in the central left region. Conclusion: The penalized regression method applied was helpful in selecting a small group of features from a dataset that had high multicollinearity.",2017,Frontiers in Aging Neuroscience
Multiple m6A RNA methylation modulators promote the malignant progression of hepatocellular carcinoma and affect its clinical prognosis,"Hepatocellular carcinoma (HCC) is the second most common cause of cancer-related death in the world. N6-methyladenosine (m6A) RNA methylation is dynamically regulated by m6A RNA methylation modulators (â€œwriter,â€ â€œeraser,â€ and â€œreaderâ€ proteins), which are associated with cancer occurrence and development. The purpose of this study was to explore the relationships between m6A RNA methylation modulators and HCC. First, using data from The Cancer Genome Atlas (TCGA) and International Cancer Genome Consortium (ICGC) databases, we compared the expression levels of 13 major m6A RNA methylation modulators between HCC and normal tissues. Second, we applied consensus clustering to the expression data on the m6A RNA methylation modulators to divide the HCC tissues into two subgroups (clusters 1 and 2), and we compared the clusters in terms of overall survival (OS), World Health Organization (WHO) stage, and pathological grade. Third, using least absolute shrinkage and selection operator (LASSO) regression, we constructed a risk signature involving the m6A RNA methylation modulators that affected OS in TCGA and ICGC analyses. We found that the expression levels of 12 major m6A RNA methylation modulators were significantly different between HCC and normal tissues. After dividing the HCC tissues into clusters 1 and 2, we found that cluster 2 had poorer OS, higher WHO stage, and higher pathological grade. Four m6A RNA methylation modulators (YTHDF1, YTHDF2, METTL3, and KIAA1429) affecting OS in the TCGA and ICGC analyses were selected to construct a risk signature, which was significantly associated with WHO stage and was also an independent prognostic marker of OS. In summary, m6A RNA methylation modulators are key participants in the malignant progression of HCC and have potential value in prognostication and treatment decisions.",2020,BMC Cancer
Bayesian accelerated failure time models based on penalized mixtures of Gaussians: regularization and variable selection,"In many biostatistical applications concerned with the analysis of duration times and especially those including high-dimensional genetic information, the following three extensions of classical accelerated failure time (AFT) models are required: (1) a flexible, nonparametric estimate of the survival time distribution, (2) a structured additive predictor including linear as well as nonlinear effects of continuous covariates and possibly further types of effects such as random or spatial effects, and (3) regularization and variable selection of high-dimensional effect vectors. Although a lot of research has dealt with these features separately, the development of AFT models combining them in a unified framework has not been considered yet. We present a Bayesian approach for modeling and inference in such flexible AFT models, incorporating a penalized Gaussian mixture error distribution, a structured additive predictor with Bayesian P-splines as a main ingredient, and Bayesian versions of ridge and LASSO as well as a spike and slab priors to enforce sparseness. Priors for regression coefficients are conditionally Gaussian, facilitating Markov chain Monte Carlo inference. The proposed model class is extensively tested in simulation studies and applied in the analysis of acute myeloid leukemia survival times considering microarray information as well as clinical covariates as prognostic factors.",2015,AStA Advances in Statistical Analysis
Online Adaptive Estimation of Sparse Signals: Where RLS Meets the $\ell_1$ -Norm,"Using the â„“1-norm to regularize the least-squares criterion, the batch least-absolute shrinkage and selection operator (Lasso) has well-documented merits for estimating sparse signals of interest emerging in various applications where observations adhere to parsimonious linear regression models. To cope with high complexity, increasing memory requirements, and lack of tracking capability that batch Lasso estimators face when processing observations sequentially, the present paper develops a novel time-weighted Lasso (TWL) approach. Performance analysis reveals that TWL cannot estimate consistently the desired signal support without compromising rate of convergence. This motivates the development of a time- and norm-weighted Lasso (TNWL) scheme with â„“1-norm weights obtained from the recursive least-squares (RLS) algorithm. The resultant algorithm consistently estimates the support of sparse signals without reducing the convergence rate. To cope with sparsity-aware recursive real-time processing, novel adaptive algorithms are also developed to enable online coordinate descent solvers of TWL and TNWL that provably converge to the true sparse signal in the time-invariant case. Simulated tests compare competing alternatives and corroborate the performance of the novel algorithms in estimating time-invariant signals, and tracking time-varying signals under sparsity constraints.",2010,IEEE Transactions on Signal Processing
On scaling of soft-thresholding estimator,"LASSO is known to have a problem of excessive shrinkage at a sparse representation. To analyze this problem in detail, in this paper, we consider a positive scaling for soft-thresholding estimators that are LASSO estimators in an orthogonal regression problem. We especially consider a non-parametric orthogonal regression problem which includes wavelet denosing. We first gave a risk (generalization error) of LARS (least angle regression) based soft-thresholding with a single scaling parameter. We then showed that an optimal scaling value that minimizes the risk under a sparseness condition is 1 + O ( log n / n ) , where n is the number of samples. The important point is that the optimal value of scaling is larger than one. This implies that expanding soft-thresholding estimator shows a better generalization performance compared to a naive soft-thresholding. This also implies that a risk of LARS-based soft-thresholding with the optimal scaling is smaller than without scaling. We then showed their difference is O ( log n / n ) . This also shows an effectiveness of the introduction of scaling. Through simple numerical experiments, we found that LARS-based soft-thresholding with scaling can improve both of sparsity and generalization performance compared to a naive soft-thresholding.",2016,Neurocomputing
Prediction of risk scores for colorectal cancer patients from the concentration of proteins involved in mitochondrial apoptotic pathway,"One of the major challenges in managing the treatment of colorectal cancer (CRC) patients is to predict risk scores or level of risk for CRC patients. In past, several biomarkers, based on concentration of proteins involved in type-2/intrinsic/mitochondrial apoptotic pathway, have been identified for prognosis of colorectal cancer patients. Recently, a prognostic tool DR_MOMP has been developed that can discriminate high and low risk CRC patients with reasonably high accuracy (Hazard Ratio, HR = 5.24 and p-value = 0.0031). This prognostic tool showed an accuracy of 59.7% when used to predict favorable/unfavorable survival outcomes. In this study, we developed knowledge based models for predicting risk scores of CRC patients. Models were trained and evaluated on 134 stage III CRC patients. Firstly, we developed multiple linear regression based models using different techniques and achieved a maximum HR value of 6.34 with p-value = 0.0032 for a model developed using LassoLars technique. Secondly, models were developed using a parameter optimization technique and achieved a maximum HR value of 38.13 with p-value 0.0006. We also predicted favorable/unfavorable survival outcomes and achieved maximum prediction accuracy value of 71.64%. A further enhancement in the performance was observed if clinical factors are added to this model. Addition of age as a variable to the model improved the HR to 40.11 with p-value as 0.0003 and also boosted the accuracy to 73.13%. The performance of our models were evaluated using five-fold cross-validation technique. For providing service to the community we also developed a web server 'CRCRpred', to predict risk scores of CRC patients, which is freely available at https://webs.iiitd.edu.in/raghava/crcrpred.",2019,PLoS ONE
Bridge estimators and the adaptive lasso under heteroscedasticity,"In this paper we investigate penalized least squares methods in linear regression models with heteroscedastic error structure. It is demonstrated that the basic properties with respect to model selection and parameter estimation of bridge estimators, Lasso and adaptive Lasso do not change if the assumption of homoscedasticity is violated. However, these estimators do not have oracle properties in the sense of Fan and Li (2001) if the oracle is based on weighted least squares. In order to address this problem we introduce weighted penalized least squares methods and demonstrate their advantages by asymptotic theory and by means of a simulation study.",2012,Mathematical Methods of Statistics
Dynamic Screening: Accelerating First-Order Algorithms for the Lasso and Group-Lasso,"Recent computational strategies based on screening tests have been proposed to accelerate algorithms addressing penalized sparse regression problems such as the Lasso. Such approaches build upon the idea that it is worth dedicating some small computational effort to locate inactive atoms and remove them from the dictionary in a preprocessing stage so that the regression algorithm working with a smaller dictionary will then converge faster to the solution of the initial problem. We believe that there is an even more efficient way to screen the dictionary and obtain a greater acceleration: inside each iteration of the regression algorithm, one may take advantage of the algorithm computations to obtain a new screening test for free with increasing screening effects along the iterations. The dictionary is henceforth dynamically screened instead of being screened statically, once and for all, before the first iteration. We formalize this dynamic screening principle in a general algorithmic scheme and apply it by embedding inside a number of first-order algorithms adapted existing screening tests to solve the Lasso or new screening tests to solve the Group-Lasso. Computational gains are assessed in a large set of experiments on synthetic data as well as real-world sounds and images. They show both the screening efficiency and the gain in terms of running times.",2015,IEEE Transactions on Signal Processing
Plasticity of Least Tern and Piping Plover nesting behaviors in response to sand temperature,"Abstract Birds that nest on the ground in open areas, like Piping Plovers (Charadrius melodus) and interior Least Terns (Sternula antillarum athalassos), are exposed to high temperatures in thermally stressful environments. As a result, some ground-nesting avian species have adapted behavioral strategies to maintain thermal regulation of eggs and themselves. We assessed the impact of sand temperature on shorebird nesting behaviors by installing video cameras and thermocouples at 52 Least Tern and 55 Piping Plover nests on the Missouri River in North Dakota during the 2014â€“2015 breeding seasons. Daily duration and frequency of shading behaviors exhibited a nonlinear relationship with temperature; therefore, we used segmented regressions to determine at what threshold temperature (mean temperatureâ€¯=â€¯25.7â¸°C for shading behavior daily frequency and mean temperatureâ€¯=â€¯25.1â¸°C for shading behavior daily duration) shorebird adults exhibited a behavioral response to rising sand temperatures. Daily nest attendance of both species decreased with increasing sand temperatures in our system. Frequency and duration of daily shading behaviors were positively correlated with sand temperatures above the temperature threshold. Piping Plovers exhibited more and longer shading behaviors above and below the temperature thresholds (below: frequencyâ€¯=â€¯10.30â€¯Â±â€¯1.69 se, durationâ€¯=â€¯7.29â€¯minâ€¯Â±â€¯2.35 se; above: frequencyâ€¯=â€¯59.27â€¯Â±â€¯6.87 se) compared to Least Terns (below: frequencyâ€¯=â€¯âˆ’1.37â€¯Â±â€¯1.98 se, durationâ€¯=â€¯âˆ’0.73â€¯minâ€¯Â±â€¯1.51 se; above: frequencyâ€¯=â€¯31.32â€¯Â±â€¯7.29 se). The effects of sand temperature on avian ground-nesting behavior will be critical to understand in order to adapt or develop recovery plans in response to climate change.",2020,Journal of Thermal Biology
Independently Interpretable Lasso for Generalized Linear Models,"Sparse regularization such as â„“1 regularization is a quite powerful and widely used strategy for high-dimensional learning problems. The effectiveness of sparse regularization has been supported practically and theoretically by several studies. However, one of the biggest issues in sparse regularization is that its performance is quite sensitive to correlations between features. Ordinary â„“1 regularization selects variables correlated with each other under weak regularizations, which results in deterioration of not only its estimation error but also interpretability. In this letter, we propose a new regularization method, independently interpretable lasso (IILasso), for generalized linear models. Our proposed regularizer suppresses selecting correlated variables, so that each active variable affects the response independently in the model. Hence, we can interpret regression coefficients intuitively, and the performance is also improved by avoiding overfitting. We analyze the theoretical property of the IILasso and show that the proposed method is advantageous for its sign recovery and achieves almost minimax optimal convergence rate. Synthetic and real data analyses also indicate the effectiveness of the IILasso.",2020,Neural Computation
Brazil or Germany - who will win the trophy? Prediction of the FIFA World Cup 2014 based on team-specific regularized Poisson regression,"In this article an approach for the analysis and prediction of soc- cer match results is proposed. It is based on a regularized Poisson regression model that includes various potentially influential covariates describing the national teams' success in previous FIFA World Cups. Additionally, sim- ilar to Bradley-Terry-Luce models, dierences of team-specific eects of the competing teams are included. It is discussed that within the gener- alized linear model (GLM) framework the team-specific eects can either be incorporated in the form of fixed or random eects. In order to achieve variable selection and shrinkage, we use tailored Lasso approaches. Based on the three preceding FIFA World Cups, two competing models for the prediction of the FIFA World Cup 2014 are fitted and investigated. Keywords Football, FIFA World Cup 2014, Sports tournaments, Gener- alized linear model, Lasso, Variable selection.",2014,
On the data-driven inference of modulatory networks in climate science: An application to West African rainfall,"Abstract. Decades of hypothesis-driven and/or first-principles research have been applied towards the discovery and explanation of the mechanisms that drive climate phenomena, such as western African Sahel summer rainfall~variability. Although connections between various climate factors have been theorized, not all of the key relationships are fully understood. We propose a data-driven approach to identify candidate players in this climate system, which can help explain underlying mechanisms and/or even suggest new relationships, to facilitate building a more comprehensive and predictive model of the modulatory relationships influencing a climate phenomenon of interest. We applied coupled heterogeneous association rule mining (CHARM), Lasso multivariate regression, and dynamic Bayesian networks to find relationships within a complex system, and explored means with which to obtain a consensus result from the application of such varied methodologies. Using this fusion of approaches, we identified relationships among climate factors that modulate Sahel rainfall. These relationships fall into two categories: well-known associations from prior climate knowledge, such as the relationship with the El Ninoâ€“Southern Oscillation (ENSO) and putative links, such as North Atlantic Oscillation, that invite further research.",2014,Nonlinear Processes in Geophysics
New In-Depth Analytical Approach of the Porcine Seminal Plasma Proteome Reveals Potential Fertility Biomarkers.,"A complete characterization of the proteome of seminal plasma (SP) is an essential step to understand how SP influences sperm function and fertility after artificial insemination (AI). The purpose of this study was to identify which among characterized proteins in boar SP were differently expressed among AI boars with significantly different fertility outcomes. A total of 872 SP proteins, 390 of them belonging specifically to Sus Scrofa taxonomy, were identified (Experiment 1) by using a novel proteomic approach that combined size exclusion chromatography and solid-phase extraction as prefractionation steps prior to Nano LC-ESI-MS/MS analysis. The SP proteomes of 26 boars showing significant differences in farrowing rate (n = 13) and litter size (n = 13) after the AI of 10â€¯526 sows were further analyzed (Experiment 2). A total of 679 SP proteins were then quantified by the SWATH approach, where the penalized linear regression LASSO revealed differentially expressed SP proteins for farrowing rate (FURIN, AKR1B1, UBA1, PIN1, SPAM1, BLMH, SMPDL3A, KRT17, KRT10, TTC23, and AGT) and litter size (PN-1, THBS1, DSC1, and CAT). This study extended our knowledge of the SP proteome and revealed some SP proteins as potential biomarkers of fertility in AI boars.",2018,Journal of proteome research
Simultaneous Channel and Feature Selection of Fused EEG Features Based on Sparse Group Lasso,"Feature extraction and classification of EEG signals are core parts of brain computer interfaces (BCIs). Due to the high dimension of the EEG feature vector, an effective feature selection algorithm has become an integral part of research studies. In this paper, we present a new method based on a wrapped Sparse Group Lasso for channel and feature selection of fused EEG signals. The high-dimensional fused features are firstly obtained, which include the power spectrum, time-domain statistics, AR model, and the wavelet coefficient features extracted from the preprocessed EEG signals. The wrapped channel and feature selection method is then applied, which uses the logistical regression model with Sparse Group Lasso penalized function. The model is fitted on the training data, and parameter estimation is obtained by modified blockwise coordinate descent and coordinate gradient descent method. The best parameters and feature subset are selected by using a 10-fold cross-validation. Finally, the test data is classified using the trained model. Compared with existing channel and feature selection methods, results show that the proposed method is more suitable, more stable, and faster for high-dimensional feature fusion. It can simultaneously achieve channel and feature selection with a lower error rate. The test accuracy on the data used from international BCI Competition IV reached 84.72%.",2015,BioMed Research International
Stabilising terminal cost and terminal controller for â„“asso-MPC: enhanced optimality and region of attraction,"In recent literature, â„“<sub>1</sub>-regularised MPC, or â„“<sub>asso</sub>-MPC, has been recommended for control tasks involving complex requirements on the control signals, for instance, the simultaneous solution of regulation and sharp control allocation for redundantly-actuated systems. This is due to the implicit thresholding ability of LASSO regression. In this paper, a stabilising terminal cost featuring a mixed â„“<sub>1</sub>/â„“<sub>2</sub><sup>2</sup> penalty is presented. Then, a candidate terminal controller is computed, with the aim of enlarging the region of attraction.",2013,2013 European Control Conference (ECC)
Robust combination of model selection methods for prediction,"One important goal of regression analysis is prediction. In recent years, the idea of combining different statistical methods has attracted an increasing atten- tion. In this work, we propose a method, l1-ARM (adaptive regression by mixing), to robustly combine model selection methods that performs well adaptively. In numerical work, we consider the LASSO, SCAD, and adaptive LASSO in represen- tative scenarios, as well as in cases of randomly generated models. The l1-ARM automatically performs like the best among them and consequently provides a bet- ter estimation/prediction in an overall sense, especially when outliers are likely to occur.",2012,Statistica Sinica
Quantitative structure-(chromatographic) retention relationship models for dissociating compounds.,"The aim of this work was to develop mathematical models relating the hydrophobicity and dissociation constant of an analyte with its structure, which would be useful in predicting analyte retention times in reversed-phase liquid chromatography. For that purpose a large and diverse group of 115 drugs was used to build three QSRR models combining retention-related parameters (logkw-chromatographic measure of hydrophobicity, S-slope factor from Snyder-Soczewinski equation, and pKa) with structural descriptors calculated by means of molecular modeling for both dissociated and nondissociated forms of analytes. Lasso, Stepwise and PLS regressions were used to build statistical models. Moreover a simple QSRR equations based on lipophilicity and dissociation constant parameters calculated in the ACD/Labs software were proposed and compared with quantum chemistry-based QSRR equations. The obtained relationships were further used to predict chromatographic retention times. The predictive performances of the obtained models were assessed using 10-fold cross-validation and external validation. The QSRR equations developed were simple and were characterized by satisfactory predictive performance. Application of quantum chemistry-based and ACD-based descriptors leads to similar accuracy of retention times' prediction.",2016,Journal of pharmaceutical and biomedical analysis
Development and validation of an objective risk scoring system for assessing the likelihood of virus introduction in porcine reproductive and respiratory syndrome virus-free sow farms in the US.,"The lack of validated tools to predict how 
long sow farms will remain PRRS virus-free following successful elimination of 
the virus has deterred veterinarians and producers from attempting to eliminate 
the PRRS virus from sow farms. The aim of this study was to use the database of 
PRRS Risk Assessments for the Breeding Herd in PADRAP to develop and validate 
an objective risk scoring system for predicting the likelihood of virus 
introduction in PRRS virus-free sow farms in the US. To overcome the 
challenges of dealing with a large number of variables, group lasso for 
logistic regression (GLLR) was applied to a retrospective dataset of PRRS Risk 
Assessment for the Breeding Herd surveys completed for 704 farms to develop the 
risk scoring system. The validity of the GLLR risk scoring system 
was then evaluated by testing its predictive ability on a dataset from a 
long-term prospective study of 196 sow farms to assess risk factors associated 
with how long PRRS virus-free sow farms remained PRRS virus-free. Receiver 
operator characteristic(ROC) curves were estimated to compare the performance 
of the GLLR risk scoring system to the risk scoring system based on expert 
opinion (EO), currently used in the PRRS Risk Assessment for the Breeding Herd, for 
predicting whether herds remained PRRS virus-free for 130 weeks. The GLLR risk 
scoring system (AUC, 0.76; 95% CI, 0.67 - 0.84) 
performed significantly better than the EO risk scoring system (AUC, 0.36; 95% 
CI, 0.27 - 0.46) for predicting whether to sow farms in the 
prospective study survived for 130 weeks (p 0.001). Dividing 
farms into 3 risk groups (low, medium and high) using a low and high cutoff 
values for the GLLR risk score was informative as the differences in the KM 
survival curves for the 3 groups were both clinically meaningful and 
statistically significant. The GLLR risk scoring system used in conjunction 
with the PRRS Risk Assessment for the Breeding Herd survey delivered through 
PADRAP appears to have the potential to help veterinarians predict the 
likelihood of virus introduction in PRRS virus-free sow farms in the US.",2013,Open Journal of Veterinary Medicine
Radiomics score: a potential prognostic imaging feature for postoperative survival of solitary HCC patients,"BackgroundRadiomics is an emerging field in oncological research. In this study, we aimed at developing a radiomics score (rad-score) to estimate postoperative recurrence and survival in patients with solitary hepatocellular carcinoma (HCC).MethodsA total of 319 solitary HCC patients (training cohort: nâ€‰=â€‰212; validation cohort: nâ€‰=â€‰107) were enrolled. Radiomics features were extracted from the artery phase of preoperatively acquired computed tomography (CT) in all patients. A rad-score was generated by using the least absolute shrinkage and selection operator (lasso) logistic model. Kaplan-Meier and Coxâ€™s hazard regression analyses were used to evaluate the prognostic significance of the rad-score. Final nomograms predicting recurrence and survival of solitary HCC patients were established based on the rad-score and clinicopathological factors. C-index and calibration statistics were used to assess the performance of nomograms.ResultsSix potential radiomics features were selected out of 110 texture features to formulate the rad-score. Low rad-score positively correlated with aggressive tumor phenotypes, like larger tumor size and vascular invasion. Meanwhile, low rad-score was significantly associated with increased recurrence and reduced survival. In addition, multivariate analysis identified the rad-score as an independent prognostic factor (recurrence: Hazard ratio (HR): 2.472, 95% confident interval (CI): 1.339â€“4.564, pâ€‰=â€‰0.004;survival: HR: 1.558, 95%CI: 1.022â€“2.375, pâ€‰=â€‰0.039). Notably, the nomogram integrating rad-score had a better prognostic performance as compared with traditional staging systems. These results were further confirmed in the validation cohort.ConclusionsThe preoperative CT image based rad-score was an independent prognostic factor for the postoperative outcome of solitary HCC patients. This score may be complementary to the current staging system and help to stratify individualized treatments for solitary HCC patients.",2018,BMC Cancer
How News Organizations Paraphrase Their Stories on Social Media? Computational Text Analysis Approach,"Social media has become one of major sources of news. As information overload prevails, news organizations need to form social media strategies to reach news readersâ€™ limited attention (Lanham, 2006; Anderson & de Palma, 2013). This study aims to investigate one of news organizationsâ€™ potential strategies â€“ paraphrasing a news story on a social media post. 
Previous literature on the choice of news headlines found that commercial news media relying on advertising for their revenue tend to frame their news story as sensational in its headline (Reah 1998; Molek-Kozakowska, 2013). Similarly, recent studies on search engine optimization (SEO) show that news media carefully choose titles and keywords tagged in URL and HTML to maximize chances for their stories to be searched online monitoring their competitors (Dick, 2011). If these strategies are effective, news organizations are likely to adopt a similar strategy on social media. In particular, they may paraphrase their news information to make it: 
(a) concise enough to fit into a text limit on a social media platform, 
(b) informative enough to signal news content, 
(c) and appealing to the news demand. 
This strategy can influence news readersâ€™ perception of a news topic because news consumption via social media tends to be relatively instant and less-lasting (Mitchell, Jurkowitz & Olmstead, 2014). Many news readers may learn about a news topic from a social media post rather than an original news story, as they used to do with headlines and leads of traditional news (Andrew, 2007). This implies that how news organizations paraphrase news for social media may frame news readersâ€™ perception of a public issue. 
To reveal news organizationsâ€™ paraphrasing strategy, I apply computational information retrieval and text analysis methods. Previous studies based on hand-coding approaches often analyze only social media posts (Newman, 2011) due to the large amount of data from social media posts themselves, original news articles and relationships between the two. This approach targets only information after paraphrasing, but does not allow for looking at how the paraphrase is related to the original text. Instead, I crawled news articles from 117 news organizationsâ€™ websites and their official Twitter accounts for a week (Feb 27, 2017â€“Mar 5, 2017), which amount to 13,773 news stories and 61,219 tweets. Also, I could identify news articles shared in each social media post matching URLs from an article and from a tweet. 
I analyze how news organizations paraphrase their news articles by looking at which word in an original text is likely to make it on a social media post. This task can be carried out by discriminating words algorithms such as Logistic Lasso regression (Mitra & Gilbert, 2014) or Multinomial Inverse regression (Taddy, 2013) recently developed in statistics and machine learning fields. Unlike popular dictionary methods such as LIWC, these algorithms allow for words likely to be in a social media post to emerge as an outcome of the empirical analysis without pre-assigning psychological meanings to dictionary words.",2017,
Preventing Heterotopic Ossification in Combat Casualtiesâ€”Which Models Are Best Suited for Clinical Use?,"BackgroundTo prevent symptomatic heterotopic ossification (HO) and guide primary prophylaxis in patients with combat wounds, physicians require risk stratification methods that can be used early in the postinjury period. There are no validated models to help guide clinicians in the treatment for this common and potentially disabling condition.Questions/purposesWe developed three prognostic models designed to estimate the likelihood of wound-specific HO formation and compared them using receiver operating characteristic (ROC) curve analysis and decision curve analysis (DCA) to determine (1) which model is most accurate; and (2) which technique is best suited for clinical use.MethodsWe obtained muscle biopsies from 87 combat wounds during the first dÃ©bridement in the United States, all of which were evaluated radiographically for development of HO at a minimum of 2Â months postinjury. The criterion for determining the presence of HO was the ability to see radiographic evidence of ectopic bone formation within the zone of injury. We then quantified relative gene expression from 190 wound healing, osteogenic, and vascular genes. Using these data, we developed an Artificial Neural Network, Random Forest, and a Least Absolute Shrinkage and Selection Operator (LASSO) Logistic Regression model designed to estimate the likelihood of eventual wound-specific HO formation. HO was defined as any HO visible on the plain film within the zone of injury. We compared the models accuracy using area under the ROC curve (area under the curve [AUC]) as well as DCA to determine which model, if any, was better suited for clinical use. In general, the AUC compares models based solely on accuracy, whereas DCA compares their clinical utility after weighing the consequences of under- or overtreatment of a particular disorder.ResultsBoth the Artificial Neural Network and the LASSO logistic regression models were relatively accurate with AUCs of 0.78 (95% confidence interval [CI], 0.72â€“0.83) and 0.75 (95% CI, 0.71â€“0.78), respectively. The Random Forest model returned an AUC of only 0.53 (95% CI, 0.48â€“0.59), marginally better than chance alone. Using DCA, the Artificial Neural Network model demonstrated the highest net benefit over the broadest range of threshold probabilities, indicating that it is perhaps better suited for clinical use than the LASSO logistic regression model. Specifically, if only patients with greater than 25% risk of developing HO received prophylaxis, for every 100 patients, use of the Artificial Network Model would result in six fewer patients who unnecessarily receive prophylaxis compared with using the LASSO regression model while not missing any patients who might benefit from it.ConclusionsOur findings suggest that it is possible to risk-stratify combat wounds with regard to eventual HO formation early in the dÃ©bridement process. Using these data, the Artificial Neural Network model may lead to better patient selection when compared with the LASSO logistic regression approach. Future prospective studies are necessary to validate these findings while focusing on symptomatic HO as the endpoint of interest.Level of EvidenceLevel III, prognostic study.",2015,Clinical Orthopaedics and Related ResearchÂ®
3.2 Regularization Methods for Categorical Predictors,The majority of regularization methods in regression analysis has been designed for metric predictors and can not be used for categorical predictors. A rare exception is the group lasso which allows for categorical predictors or factors. We will consider alternative approaches based on penalized liâ€¦,2017,
Which bridge estimator is optimal for variable selection?,"We study the problem of variable selection for linear models under the high-dimensional asymptotic setting, where the number of observations $n$ grows at the same rate as the number of predictors $p$. We consider two-stage variable selection techniques (TVS) in which the first stage uses bridge estimators to obtain an estimate of the regression coefficients, and the second stage simply thresholds this estimate to select the ""important"" predictors. The asymptotic false discovery proportion (AFDP) and true positive proportion (ATPP) of these TVS are evaluated. We prove that for a fixed ATPP, in order to obtain a smaller AFDP, one should pick a bridge estimator with smaller asymptotic mean square error in the first stage of TVS. Based on such principled discovery, we present a sharp comparison of different TVS, via an in-depth investigation of the estimation properties of bridge estimators. Rather than ""order-wise"" error bounds with loose constants, our analysis focuses on precise error characterization. Various interesting signal-to-noise ratio and sparsity settings are studied. Our results offer new and thorough insights into high-dimensional variable selection. For instance, we prove that a TVS with Ridge in its first stage outperforms TVS with other bridge estimators in large noise settings; two-stage LASSO becomes inferior when the signal is rare and weak. As a by-product, we show that two-stage methods outperform some standard variable selection techniques, such as LASSO and Sure Independence Screening, under certain conditions.",2017,ArXiv
Abstract 774: Proteogenomic characterization of high-grade serous ovarian cancer,"High grade serous ovarian carcinoma (HGSOC) is a highly lethal gynecological malignancy, in large part because most patients develop resistance to the standard of care chemotherapy with platinum and taxanes. We performed deep proteomic and phosphoproteomic characterization on ovarian high-grade serous carcinomas (HGSC) previously characterized genomically by the The Cancer Genome Atlas (TCGA) as part of the Clinical Proteomic Tumor Analysis Consortium (CPTAC; http://proteomics.cancer.gov/programs/cptacnetwork). We constructed an integrated proteogenomic landscape of HGSC and identified functional modules statistically associated with outcomes including platinum resistance and overall survival. Integration of genomic data such as copy number alterations (CNA) with global protein abundance data revealed several chromosomal regions that have significant trans effects on protein expression. These genetically affected proteins were used to construct statistical models capable of predicting survival outcomes. Trans-affected proteins were enriched in proliferation, cell motility and invasion, and immune regulation, hallmarks of cancer. We used statistical methods to identify functional pathways able to discriminate between short surviving and long surviving patients. We found that protein abundance and phosphorylation indicated pathway activity that was able to clearly discriminate between these two groups, but that transcriptional expression and CNAs were not. We integrated proteomic abundance and phosphorylation state to elucidate specific signaling interactions and pathways differentially active in tumors from patients with short overall survival as well as to predict novel kinase substrate relationships important in HGSC. Finally, we identified druggable pathways that are differentially active across proteomic subtypes. A key point arising from our multimodal analysis is that combining different types of molecular data from these tumors greatly increases the ability to identify biologically relevant differences between groups. Using a Lasso-based regression approach to integrate data we show that the most robustly predictive survival models can be obtained using integrated CNA, transcriptome, proteome, and phosphoproteome data than with any of the individual sources of data. Citation Format: Jason E. McDermott, Samuel Payne, Debjit Ray, Vladislav Petyuk, Ronald Moore, Marina Gritsenko, Richard Smith, Karin Rodland. Proteogenomic characterization of high-grade serous ovarian cancer. [abstract]. In: Proceedings of the 107th Annual Meeting of the American Association for Cancer Research; 2016 Apr 16-20; New Orleans, LA. Philadelphia (PA): AACR; Cancer Res 2016;76(14 Suppl):Abstract nr 774.",2016,Cancer Research
â„“1-penalization for mixture regression models,"We consider a finite mixture of regressions (FMR) model for high-dimensional inhomogeneous data where the number of covariates may be much larger than sample size. We propose an â„“1-penalized maximum likelihood estimator in an appropriate parameterization. This kind of estimation belongs to a class of problems where optimization and theory for non-convex functions is needed. This distinguishes itself very clearly from high-dimensional estimation with convex loss- or objective functions as, for example, with the Lasso in linear or generalized linear models. Mixture models represent a prime and important example where non-convexity arises.For FMR models, we develop an efficient EM algorithm for numerical optimization with provable convergence properties. Our penalized estimator is numerically better posed (e.g., boundedness of the criterion function) than unpenalized maximum likelihood estimation, and it allows for effective statistical regularization including variable selection. We also present some asymptotic theory and oracle inequalities: due to non-convexity of the negative log-likelihood function, different mathematical arguments are needed than for problems with convex losses. Finally, we apply the new method to both simulated and real data.",2010,TEST
Statistical Tests for Optimization Efficiency,"Learning problems, such as logistic regression, are typically formulated as pure optimization problems defined on some loss function. We argue that this view ignores the fact that the loss function depends on stochastically generated data which in turn determines an intrinsic scale of precision for statistical estimation. By considering the statistical properties of the update variables used during the optimization (e.g. gradients), we can construct frequentist hypothesis tests to determine the reliability of these updates. We utilize subsets of the data for computing updates, and use the hypothesis tests for determining when the batch-size needs to be increased. This provides computational benefits and avoids overfitting by stopping when the batch-size has become equal to size of the full dataset. Moreover, the proposed algorithms depend on a single interpretable parameter -the probability for an update to be in the wrong direction - which is set to a single value across all algorithms and datasets. In this paper, we illustrate these ideas on three L1 regularized coordinate descent algorithms: L1-regularized L2-loss SVMs, L1-regularized logistic regression, and the Lasso, but we emphasize that the underlying methods are much more generally applicable.",2011,
Ultrasound radimoics nomogram for predicting lymph node metastasis in papillary thyroid carcinoma,"Objective To establish an ultrasound (US) radimoics nomogram for predicting central neck lymph node metastasis (LNM) in papillary thyroid carcinoma (PTC). Method A training cohort of 300 patients and validation cohort of 143 patients were established from January to December 2017. US radiomics score was built by using radiomics to analyze ultrasound image of PTC lesion. A nomogram was established by logistic regression from selection of gender, age, thyroglobulin (TG), thyroglobulin antibodies (TGAB), thyroid peroxidase antibody (TPOAB), US radiomics score and US report LNM status. C-index was calculated to evaluate discrimination of this nomogram. Also, calibration curve was drawn to evaluate calibration of it. Results The positive rates of LNM in two cohorts were 29.7% and 34.6%, respectively. In this study, 23 predictive US radiomics features (26:1) were screened out from 609 cases in the training set, which were nonzero coefficients in the LASSO logistic regression model. US radiomics score yield a C-index of 0.802 (95% CI, 0.791 to 0.815) in training cohort and 0.809 (95% CI, 0.801 to 0.816). Through binary logistic regression, indexes were filtered as age, TPOAB, US radiomics score and US report LNM status. The area under curve, accuracy, sensitivity and specificity of this nomogram was 0.801, 0.812, 0.628 and 0861, respectively, from validation cohort. The C-index was 0.811 (95% CI, 0.804 to 0.818). Conclusion US radiomics nomogram can predict LNM in PTC effectively.",2019,Ultrasound in Medicine and Biology
Generalized Sparse Regularization with Application to fMRI Brain Decoding,"Many current medical image analysis problems involve learning thousands or even millions of model parameters from extremely few samples. Employing sparse models provides an effective means for handling the curse of dimensionality, but other propitious properties beyond sparsity are typically not modeled. In this paper, we propose a simple approach, generalized sparse regularization (GSR), for incorporating domain-specific knowledge into a wide range of sparse linear models, such as the LASSO and group LASSO regression models. We demonstrate the power of GSR by building anatomically-informed sparse classifiers that additionally model the intrinsic spatiotemporal characteristics of brain activity for fMRI classification. We validate on real data and show how prior-informed sparse classifiers outperform standard classifiers, such as SVM and a number of sparse linear classifiers, both in terms of prediction accuracy and result interpretability. Our results illustrate the added-value in facilitating flexible integration of prior knowledge beyond sparsity in large-scale model learning problems.",2011,Information processing in medical imaging : proceedings of the ... conference
Total Variation Regularization Enhances Regression-Based Brain Activity Prediction,"While medical imaging typically provides massive amounts of data, the automatic extraction of relevant information in a given applicative context remains a difficult challenge in general. With functional MRI (fMRI), the data provide an indirect measurement of brain activity, that can be related to behavioral information. It is now standard to formulate this relation as a machine learning problem where the signal from the entire brain is used to predict a target, typically a behavioral variable. In order to cope with the high dimensionality of the data, the learning method requires a regularization procedure. Among other alternatives, l1 regularization achieves simultaneously a selection of the most predictive features. One limitation of the latter method, also referred to as Lasso in the case of regression, is that the spatial structure of the image is not taken into account, so that the extracted features are often hard to interpret. To obtain more informative and interpretable results, we propose to use the l1 norm of the image radient, a.k.a., the Total Variation (TV), as regularization. TV extracts few predictive regions with piecewise constant weights over the whole brain, and is thus more consistent with traditional brain mapping. We show on real fMRI data that this method yields more accurate predictions in inter-subject analysis compared to voxel-based reference methods, such as Elastic net or Support Vector Regression.",2010,2010 First Workshop on Brain Decoding: Pattern Recognition Challenges in Neuroimaging
A transcriptional metabolic gene-set based prognostic signature is associated with clinical and mutational features in head and neck squamous cell carcinoma,"Head and neck squamous cell carcinoma (HNSCC) is a common cancer with high mortality and poor prognosis partially owing to lack of application of predictive markers. Increasing evidence has suggested that metabolic dysregulation plays an important part in tumorigenesis. We aim to identify a prognostic metabolic pathway (MP) signature in HNSCC. Single sample gene-set enrichment analysis (ssGSEA) was used in metabolic gene sets to develop a metabolism-based prognostic risk score (MPRS) for HNSCC using Cox regression analysis (univariate, LASSO, and stepwise multiple cox analysis), which was then validated in different subgroups, and association with clinical and mutational features was analyzed. Seventy-two dysregulated metabolic pathways were identified, and a six-MP signature (6MPS) was constructed which can effectively distinguish between the high- and low-risk patients in both training and testing sets, accompanied with high sensitivity and specificity (AUCâ€‰=â€‰0.7) in prognosis prediction. 6MPS was also applicable to patients of different subgroups. Furthermore, 6MPS is not only an independent prognostic predictor but also associated with clinicopathological and mutational features. Higher tumor stage and tumor mutation burden (TMB) have a higher MPRS. 6MPS functions not only as a promising predictor of prognosis and survival but also as potential marker for therapeutic schedule monitoring.",2020,Journal of Cancer Research and Clinical Oncology
Dynamic Edge Fabric EnvironmenT: Seamless and Automatic Switching among Resources at the Edge of IoT Network and Cloud,"The number of IoT devices at the edge of the network is increasing rapidly. Data from IoT devices can be analysed locally at the edge, or they may be sent to the cloud. Currently, the decision to deploy a task for execution at the edge or in the cloud is not decided as the data are received. Instead the decision is usually based on pre-defined system design and corresponding assumptions about locality and connectivity. However, the mobile environment has rapid, sometimes unpredictable changes and requires a system that can dynamically adapt to these changes. An intelligent platform is required that can discover available resources (both nearby and in the cloud) and autonomously orchestrate a seamless and transparent task allocation at runtime to help the IoT devices achieve their best performance given the available resources. We propose a new platform, DEFT (Dynamic Edge-Fabric environmenT), that can automatically learn where best to execute each task based on real-time system status and task requirements, along with learned behavior from past performance of the available resources. The task allocation decision in this platform is powered by machine learning techniques such as regression models (linear, ridge, Lasso) and ensemble models (random forest, extra trees). We have implemented this platform on heterogeneous devices and run various IoT tasks on the devices. The results reveal that choosing proper machine learning approaches based on the tasks properties and priorities can significantly improve the overall performance of selecting resources (either from the edge or cloud) dynamically at runtime.",2019,2019 IEEE International Conference on Edge Computing (EDGE)
Book Reviews: Introduction to Multivariate Statistical Analysis in Chemometrics,"Introduction to Multivariate Statistical Analysis in Chemometrics. Kurt Varmuza and Peter Filzmoser. CRC Press, Boca Raton, FL, 2009. Pp: 321. Price: USD$119.95. ISBN 13 978-14200-5947-2. The new book by Kurt Varmuza, a chemometrician, and Peter Filzmoser, a statistician, takes the reader on a brisk walk though modern data analysis. The collaboration here gives a somewhat unconventional, integrated perspective on the treatment of multivariate data. Like the bookâ€™s coverâ€”a panorama of the Monument Valley area in the American Southwestâ€”the book provides a wide-ranging overview of many â€˜â€˜monumentsâ€™â€™ of chemometrics and modern multivariate statistics, without a tight focus on any one. The tone is introductory throughout. After a first chapter with extensive, up-to-date lists of other texts in chemometrics and multivariate statistics, the authors begin with introductory examples from near-infrared spectroscopy and archaeological glasses, followed by a brief examination of univariate statistical methods. In most cases, only a short discussion is provided, along with a brief mention of the R language object or an R command that performs the test. The use of R, while now common in statistical texts, is less frequent in chemometrics. Moving from this univariate approach to data pretreatment, the authors take on multivariate data pretreatment and outlier detection. Unlike many introductory texts in chemometrics, they cover data transformations and robust methodology here as well as the more conventional centering and scaling. As with univariate methods, the authors provide a brief mention of the R object for performing the task. This chapter shows their focus on graphical depiction of the methods, another attribute of the book that sets it apart from others available. The usual topics that are common to most texts in chemometrics follow: principal components analysis, calibration, classification, and cluster analysis. What is unusual is the inclusion of robust methods throughout, the extensive use of graphical representations of the methods to aid in understanding, and the software examples implementing the methods in R. Readers comfortable with R can follow along by obtaining the libraries and datasets for the examples from the Comprehensive R Archive Network (CRAN, at http://cran.r-project.org), as I did. They are all free, as is the R software to run them. While many of the topics covered in calibration and classification are standard, some of the topics considered are unconventional, possibly reflecting field-based differences in the authorsâ€™ perspectives. For example, in calibration, ridge regression gets more than the usual attention, along with the lasso method, as well as ordinary (classical) least squares (OLS) and canonical correlation. Given the recent resurgence of interest in OLS methods by spectroscopists, detailed discussion of OLS is both timely and highly useful. The treatment of the more conventional partial least squares (PLS) and allied methods is both substantial and upto-date: even the recent controversy over orthogonal-scores-based and orthogonal-loadings-based calculation of latent variables in PLS is covered here. When needed, the authors delve into some of the mathematical details, but most of the treatment is not heavily oriented to theory, a feature that those just starting out will surely appreciate. As noted above, classification is considered from a slightly different perspective. The linear discriminant is discussed in detail, but Gaussian mixture models, decision trees, support vector machines, and logistic regression share space with the more commonly presented k-NN and SIMCA classifiers that are routine in commercial chemometric software packages. Similarly, the chapter on clustering covers the usual hierarchical methods present in the commercial packages but also briefly examines fuzzy clustering. A chapter on data preprocessing ends the book. This chapter provides very brief coverage of basic methods such as differentiation, multiplicative scatter correction, and mass spectral normalization methods. It is somewhat less up to date than the other chapters. The authors mention but do not discuss wavelet processing, for example. With all of these methods receiving attention in the span of about 300 pages, readers should not expect comprehensive coverage of all of the popular topics. Some aspects of modern chemometrics donâ€™t make the list: multivariate curve resolution is not covered here, nor are other recent preprocessing methods that are implemented in some software packages, such as orthogonal signal correction. Higher-order methods receive only the briefest of treatment. The Appendices covering matrix algebra and the R language are also quite short; those needing help will likely find these too brief, but there is a good amount of web-based help available on these topics. The spectroscopist will find this a useful reference to modern chemometrics. The integration of chemometric methods with modern multivariate statistical methods should prove particularly useful. The motivated student seeking to gain experience in data analysis will find this a balanced, insightful, and accessible introduction to the field. Given the use of R software to implement the methods discussed in the text, and the high overlap of multivariate statistics and chemometrics in recent literature, this is a text that offers a good deal. It offers even more to the reader who invests a bit of time learning enough R to put the accompanying software library to use. There is a great deal of useful chemometrics available here for the price of a textbook. I highly recommend the book.",2010,Applied Spectroscopy
A lasso based ensemble empirical mode decomposition approach to designing adaptive clutter suppression filters,"Accurate estimation of the blood flow velocity in ultrasound imaging is an important tool for medical diagnostics. In this paper, we adopt an improved empirical mode decomposition (EMD) framework called ensemble EMD (EEMD). To reduce the errors caused by the outliers in data when using a uniform weight in conventional EEMD, a regularized LASSO EEMD algorithm is proposed to solve for the multiple regression weights. An adaptive clutter rejection filter can then be designed to remove the clutter components. According to our simulation study, the proposed LASSO EEMD approach performs better than the state-of-the-art eigen-based and EMD method in estimating the blood flow velocity. Although the LASSO EEMD derived filter only achieves slightly better results than the cubic regression derived filters at most part of the simulated blood flow center frequencies, the proposed LASSO EEMD algorithm achieves much improved performance over cubic regression at extreme cases when the blood flow center frequency is close to or much higher than that of the clutter.",2012,"2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
Identifying incident dementia by applying machine learning to a very large administrative claims dataset,"INTRODUCTION Alzheimerâ€™s disease and related dementias (ADRD) are highly prevalent conditions, and prior efforts to develop predictive models have relied on demographic and clinical risk factors using traditional logistical regression methods. We hypothesized that machine-learning algorithms using administrative claims data may represent a novel approach to predicting ADRD. METHODS Using a national de-identified dataset of more than 125 million patients including over 10,000 clinical, pharmaceutical, and demographic variables, we developed a cohort to train a machine learning model to predict ADRD 4-5 years in advance. RESULTS The Lasso algorithm selected a 50-variable model with an area under the curve (AUC) of 0.693. Top diagnosis codes in the model were memory loss (780.93), Parkinsonâ€™s disease (332.0), mild cognitive impairment (331.83) and bipolar disorder (296.80), and top pharmacy codes were psychoactive drugs. DISCUSSION Machine learning algorithms can rapidly develop predictive models for ADRD with massive datasets, without requiring hypothesis-driven feature engineering. RESEARCH IN CONTEXT Systematic review: Previous attempts to predict incident dementia have relied on extensive clinical evaluations, cognitive testing, laboratory testing, neuro-imaging, genetic factors, demographics, and lifestyle variables. Applying machine learning to a large administrative claims dataset to identify individuals at increased likelihood for near-term diagnosis of dementia had not been tested. Interpretation: A 50-variable model to identify those at risk for near-term diagnosis of dementia was created and validated. Based on AUC analysis, the model compared favorably with other historical attempts at modeling more traditional forms of data. Future direction: Models, such as the one developed here, could be used to identify populations of higher prior probability for near-term diagnosis of dementia. These could then be subjected to more in-depth scrutiny for intervention or dementia-related research eligibility.",2018,bioRxiv
Penalized regression for interval-censored times of disease progression: Selection of HLA markers in psoriatic arthritis.,"Times of disease progression are interval-censored when progression status is only known at a series of assessment times. This situation arises routinely in clinical trials and cohort studies when events of interest are only detectable upon imaging, based on blood tests, or upon careful clinical examination. We consider the problem of selecting important prognostic biomarkers from a large set of candidates when disease progression status is only known at irregularly spaced and individual-specific assessment times. Penalized regression techniques (e.g., LASSO, adaptive LASSO, and SCAD) are adapted to handle interval-censored time of disease progression. An expectation-maximization algorithm is described which is empirically shown to perform well. Application to the motivating study of the development of arthritis mutilans in patients with psoriatic arthritis is given and several important human leukocyte antigen (HLA) variables are identified for further investigation.",2015,Biometrics
An Efficient Augmented Lagrangian Based Method for Constrained Lasso,"Variable selection is one of the most important tasks in statistics and machine learning. To incorporate more prior information about the regression coefficients, the constrained Lasso model has been proposed in the literature. In this paper, we present an inexact augmented Lagrangian method to solve the Lasso problem with linear equality constraints. By fully exploiting second-order sparsity of the problem, we are able to greatly reduce the computational cost and obtain highly efficient implementations. Furthermore, numerical results on both synthetic data and real data show that our algorithm is superior to existing first-order methods in terms of both running time and solution accuracy.",2019,ArXiv
