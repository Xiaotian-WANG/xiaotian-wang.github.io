title,abstract,year,journal
Adaptive multi-view clustering via cross trace lasso,"We propose a novel multi-view clustering method by learning auto-regression problems under structural constraints and treating the regression coefficients as new feature representations for the cluster partition. In particular, we take the data intrinsic correlation structure into account. Correlated data under one view tend to be also related under another view and are likely to fall into the same group. Therefore we pair the data matrix from one view and the regression coefficient from a different view together to meet a trace Lasso constraint, which adaptively adjusts the sparsity of regression coefficients in order to promote consistent data correlations across views. Then a joint low-rank constraint is further imposed to encourage similar regression coefficients for the same samples under distinct views. Finally, we develop an effective algorithm to optimize the objective function. And experimental results demonstrate that our method is useful and fairly competitive compared with other state-of-the-art multi-view clustering methods.",2015,2015 3rd IAPR Asian Conference on Pattern Recognition (ACPR)
Sparsity considerations for dependent variables,"The aim of this paper is to provide a comprehensive introduc- tion for the study of `1-penalized estimators in the context of dependent observations. We define a general `1-penalized estimator for solving prob- lems of stochastic optimization. This estimator turns out to be the LASSO (Tib96) in the regression estimation setting. Powerful theoretical guaran- tees on the statistical performances of the LASSO were provided in recent papers, however, they usually only deal with the iid case. Here, we study this estimator under various dependence assumptions.",2011,Electronic Journal of Statistics
Stagewise Estimating Equations,"Stagewise estimation is a slow-brewing approach for model building that has recently experienced a revival due to its computational efficiency, its flexibility in handling complex data structures, and its intrinsic connections with penalized estimation. Synthesizing generalized estimating equations to handle correlated non-Gaussian data with stagewise techniques, this thesis proposes general stagewise estimation approaches that perform model selection in the presence of complex covariate structures. First, the setting where there is a prior covariate grouping structure or hierarchy is considered. As the grouping structure in practice is often not ideal as even important groups may contain unimportant variables, the key is to simultaneously conduct group selection and within-group variable selection, or in other words, bi-level selection. This thesis presents two approaches to address the challenge. The first is the bi-level stagewise estimating equations (BiSEE) approach, which is shown to correspond to the sparse group lasso penalized regression. The second is the hierarchical stagewise estimating equations (HiSEE) approach that can handle a more general hierarchical grouping structure, in which each stagewise estimation step itself is executed as a hierarchical selection process based on the grouping structure. The second setting explored is regression with interaction terms. As it is often required that main effect terms be included when an interaction term is part of a model, the goal is to perform variable selection that maintains the variable hierarchy. Two approaches are proposed by this thesis. The first is a hierarchical lasso stagewise estimating equations approach, which is shown to directly correspond to the hierarchical lasso penalized regression. The second is a stagewise active set approach, which enforces the variable hierarchy by conforming the selection to a properly growing active set in each stagewise estimation step. Simulation studies are presented to show the efficacy and superior computational efficiency of the proposed approaches. The approaches are also used to study the association between the suicide-related hospitalization rates among 15â€“19 year olds in Connecticut and the characteristics of the school districts in which they reside. Stagewise Estimating Equations Gregory Phillip Lucas Vaughan B.S., Mathematics, Trinity College, CT, USA, 2012 B.S., Computer Science, Trinity College, CT, USA, 2012 A Dissertation Submitted in Partial Fulfillment of the Requirements for the Degree of Doctor of Philosophy at the University of Connecticut 2017",2017,
Prioritizing predicted cis-regulatory elements for co-expressed gene sets based on Lasso regression models,Computational prediction of cis-regulatory elements for a set of co-expressed genes based on sequence analysis provides an overwhelming volume of potential transcription factor binding sites. It presents a challenge to prioritize transcription factors for regulatory functional studies. A novel approach based on the use of Lasso regression models is proposed to address this problem. We examine the ability of the Lasso model using time-course microarray data obtained from a comprehensive study of gene expression profiles in skin and mucosal wounds in mouse over all stages of wound healing.,2011,2011 Annual International Conference of the IEEE Engineering in Medicine and Biology Society
A modified multinomial baseline logit model with logit functions having different covariates,"The multinomial logistic regression is a useful tool in the health and life sciences. In this paper, we propose a modified multinomial baseline logit model for nominal polychotomous data. The modified model is suitable for use in the situation where separate logistic models may be functions of different covariates. An estimation procedure is presented. The modified model is an alternative to the multinomial baseline logit model and the multivariate sparse group lasso. Simulation shows that this modified model outperforms the multinomial baseline logit model and the multinomial sparse group lasso. A real data set about an adolescent placement study is analyzed to demonstrate flexibility and efficiency of the modified model.",2019,Communications in Statistics - Simulation and Computation
Prediction models for solitary pulmonary nodules based on curvelet textural features and clinical parameters.,"Lung cancer, one of the leading causes of cancer-related deaths, usually appears as solitary pulmonary nodules (SPNs) which are hard to diagnose using the naked eye. In this paper, curvelet-based textural features and clinical parameters are used with three prediction models [a multilevel model, a least absolute shrinkage and selection operator (LASSO) regression method, and a support vector machine (SVM)] to improve the diagnosis of benign and malignant SPNs. Dimensionality reduction of the original curvelet-based textural features was achieved using principal component analysis. In addition, non-conditional logistical regression was used to find clinical predictors among demographic parameters and morphological features. The results showed that, combined with 11 clinical predictors, the accuracy rates using 12 principal components were higher than those using the original curvelet-based textural features. To evaluate the models, 10-fold cross validation and back substitution were applied. The results obtained, respectively, were 0.8549 and 0.9221 for the LASSO method, 0.9443 and 0.9831 for SVM, and 0.8722 and 0.9722 for the multilevel model. All in all, it was found that using curvelet-based textural features after dimensionality reduction and using clinical predictors, the highest accuracy rate was achieved with SVM. The method may be used as an auxiliary tool to differentiate between benign and malignant SPNs in CT images.",2013,Asian Pacific journal of cancer prevention : APJCP
A Penalized Regression Approach for Integrative Analysis in Gen ome-Wide Association Studies,"Over one thousand genome-wide association studies (GWAS) have been conducted in the past decade. Increasing biological evidence suggests the polygenic genetic architecture of complex traits: a complex trait is affected by many risk variants with small or moderate effects jointly. Meanwhile, recent progress in GWAS suggests that complex human traits may share common genetic bases, which is known as â€œpleiotropyâ€. To further improve statistical power of detecting risk genetic variants in GWAS, we propose a penalized regression method to analyze the GWAS dataset of primary interest by incorporating information from other related GWAS. The proposed method does not require the individual-level of genotype and phenotype data from other related GWAS, making it useful when only summary statistics are available. The key idea of the proposed approach is that related traits may share common genetic basis. Specifically, we propose a linear model for integrative analysis of multiple GWAS, in which risk genetic variants can be detected via identification of nonzero coefficients. Due to the pleiotropy effect, there exist genetic variants affecting multiple traits, which correspond to a consistent nonzero pattern of coefficients across multiple GWAS. To achieve this, we use a group Lasso penalty to identify this nonzero pattern in our model, and then develop an efficient algorithm based on the proximal gradient method. Simulation studies showed that the proposed approach had satisfactory performance. We applied the proposed method to analyze a body mass index (BMI) GWAS dataset from a European American (EA) population and achieved improvement over single GWAS analysis.",2015,Journal of biometrics & biostatistics
"Exploring the associations of serum concentrations of PCBs, PCDDs, and PCDFs with walking speed in the U.S. general population: Beyond standard linear models.","Studies have shown that persistent organic pollutants (POPs) can have various health effects. However, little is known about the effects of multiple chemicals with possible common sources of exposure on walking speed, a proxy index reflecting lower limb neuromuscular function and physical function. We simultaneously applied multiple linear and nonlinear statistical models to explore the complex exposure-response relationship between a mixture of 22 selected POPs and walking speed. A total of 14 polychlorinated biphenyls (PCBs), 3 polychlorinated dibenzo-p-dioxins (PCDDs), and 5 polychlorinated dibenzofurans (PCDFs) were measured in the serum of participants in the National Health and Nutrition Examination Survey (NHANES) from 1999 to 2002. Walking speed was measured during a physical examination. Linear regression (LR), least absolute shrinkage and selection operator (LASSO), and group LASSO were used to evaluate the linearity of mixtures, while restricted cubic spline (RCS) regression, random forest (RF), and Bayesian kernel machine regression (BKMR) models were used to evaluate the nonlinearity of mixtures. Potential confounders were adjusted in the above models. A total of 436 subjects were included in our final analysis. The results of the LR model did not identify any POP exposure that was significantly associated with walking speed. The LASSO results revealed an inverse association of one PCDD congener and two PCDF congeners with walking speed, while the group LASSO analysis identified PCDFs at the exposure level and at the group level. In the RCS analysis, two PCB congeners presented significant overall associations with walking speed. The PCB congener PCB194 showed statistically significant effects on the outcome (Pâ€¯=â€¯0.01) when a permutation-based RF was used. The BKMR analysis suggested that PCBs and PCDFs (probabilitiesâ€¯=â€¯0.887 and 0.909, respectively) are potentially associated with walking speed. Complex statistical models, such as RCS regression, RF and BKMR models, can detect the nonlinear and nonadditive relationships between PCBs and walking speed, while LASSO and group LASSO can identify only the linear relationships between PCDFs and walking speed. Fully considering the influence of collinearity in each method during modelling can increase the comprehensiveness and reliability of conclusions in studies of multiple chemicals.",2019,Environmental research
Oracle Inequalities for High-dimensional Prediction,"The abundance of high-dimensional data in the modern sciences has generated tremendous interest in penalized estimators such as the lasso, scaled lasso, square-root lasso, elastic net, and many others. In this paper, we establish a general oracle inequality for prediction in high-dimensional linear regression with such methods. Since the proof relies only on convexity and continuity arguments, the result holds irrespective of the design matrix and applies to a wide range of penalized estimators. Overall, the bound demonstrates that generic estimators can provide consistent prediction with any design matrix. From a practical point of view, the bound can help to identify the potential of specific estimators, and they can help to get a sense of the prediction accuracy in a given application.",2016,arXiv: Statistics Theory
Performance of CT-based radiomics in diagnosis of superior mesenteric vein resection margin in patients with pancreatic head cancer,"To accurately identify the relationship between a portal radiomics score (rad-score) and pathologic superior mesenteric vein (SMV) resection margin and to evaluate the diagnostic performance in patients with pancreatic head cancer. A total of 181 patients with postoperatively and pathologically confirmed pancreatic head cancer who underwent multislice computed tomography within one month of resection between January 2016 and December 2018 were retrospectively investigated. For each patient, 1029 radiomics features of the portal phase were extracted, which were reduced using the least absolute shrinkage and selection operator (LASSO) logistic regression algorithm. Multivariate logistic regression models were used to analyze the association between the portal rad-score and SMV resection margin. Patients with negative (R0) and positive (R1) margins accounted for 70.17% (127) and 29.83% (54) of the cohort, respectively. The rad-score was significantly associated with the SMV resection margin status (pâ€‰<â€‰0.05). Multivariate analyses confirmed a significant and independent association between the portal rad-score and SMV resection margin (OR 4.62; 95% CI 2.19â€“9.76; pâ€‰<â€‰0.0001). The portal rad-score had high accuracy (area under the curveâ€‰=â€‰0.750). The best cut point based on maximizing the sum of sensitivity and specificity was âˆ’ 0.741 (sensitivityâ€‰=â€‰64.8%; specificityâ€‰=â€‰74.0%; accuracyâ€‰=â€‰71.3%). Decision curve analysis indicated the clinical usefulness of radiomics score. The portal rad-score is significantly associated with the pathologic SMV resection margin, and it can accurately and noninvasively predict the SMV resection margin in patients with pancreatic cancer.",2020,Abdominal Radiology
"Binary Classification, Logistic and Nominal Regression: Application to Bank Customer Loyalty Data","Customers are the foundation of any businessâ€™s success, and a business can never be too grateful for loyal customers. Customer insight is therefore an important key to help sustain loyal and active customers. In this thesis we are going to detect significant differences between different customer types, as well as indicating future inactive customers. Doing so, we will get useful insight about the inactive customers, and perhaps understand why they choose to go from being active to being inactive. The analyses are done on a bank customer database, provided by the bank itself. The bank customers are divided into six groups, or categories, based on customer activity and the number products used. The categories are denoted by Aâ€“F, where A contains the most active customers and F contains the least active customers with no products used. We perform nominal regression in order to detect differences between these groups. We experienced that customers that have applied for loan/credit card are more likely to be customers from the categories A and B. Furthermore, we experienced that probability for being in category F is strongly decreased if the customer is a member or a nonmember with member benefits. Also, the probability for being in category A is significantly increased if the customers have activated electronical billing. Let the categories Aâ€“D relate to the active customers, and the categories Eâ€“F relate to the inactive customers. To indicate customers that are going to be inactive in the future, we create an indicator model. We perform statistical modelling and learning methods, such as binary logistic regression, random forests and XGBoost, in order to create this model. We use model selection methods, such as the Akaike Information Criterion (AIC), lasso regularization and variable importance. The performance of a model is evaluated on the AUC value on test data. The model that performed best was the XGBoost model with all the variables included. Thus, this will be used as the indicator for detecting bank customers that are going to be inactive within the next year. We experienced that balance of the customer was clearly the most significant variable, when it comes to being active or inactive in the future. The binary logistic regression coefficient for this variable is negative. Hence, the higher the balance on the deposit account of a customer, the lower is the probability for being inactive in the future. The number of transactions of the customer and if the customer has a loan, are both also very important factors when it comes to being active/inactive in the future.",2019,
HW 2 : Regression,"i |y âˆ’ Î¸x| + Î»||Î¸||q In our situation, each y is the rating of a review (one of {1, 2, 4, 5}) and x is a bag-of-words of the review. When p = q = 2, we recover ridge regression, and when p = 2 and q = 1 we recover the â€œlasso,â€ which can obtain sparse solutions. While exact solutions are possible for most of some of these conditions, we instead focus on gradient-based optimizations. Specifically, we consider two algorithms. First, we consider the widely-used quasi-Newton method LBFGS (?), and the OWL-QN variant for `1 regularization (?). Second, we consider the Adaptive Gradient (AdaGrad) variant of Stochastic Gradient Descent, using forward `2 and `1 regularization. (?). This latter algorithm is much like gradient descent, except that step sizes are determined on a per-component basis, where the step size at time t for component i is defined to be:",2012,
Imbalanced Binary Classification On Hospital Readmission Data With Missing Values,"Hospital readmission is a costly, undesirable, and often preventable patient outcome of inpatient care. Early readmission prediction can effectively prevent life-threatening events and reduce healthcare costs. However, imbalanced class distribution and high missing value rates are usually associated with readmission data and need to be handled carefully before building classification models. In this paper, we investigate the prediction of hospital readmission on a dataset with high percentage of missing values and class imbalance problem. Different methods are applied to impute missing values in the categorical variables and numerical variables. In addition, SMOTE (Synthetic Minority Over-sampling Technique) and cost-sensitive learning are combined with different classification methods (LASSO logistic regression, random forest, and gradient boosting) to explore which one will yield the best classification performance on the readmission data. Total misclassification cost and area under ROC curve are used as evaluation metrics for model comparison. Our results show that the SMOTE method causes overfitting on our readmission data and cost-sensitive learning outperforms SMOTE in terms of total misclassification cost.",2018,
A tutorial on the Lasso approach to sparse modeling,"abstract Article history:Received 13 December 2011Received in revised form 3 October 2012Accepted 5 October 2012Available online 13 October 2012Keywords:SparsityL 1 norm(Bi)convex optimizationLasso In applied research data are often collected from sources w ith a high dimensional multi variate output. Analysis ofsuchdataiscomposedofe.g.extractionandcharacterizationof underlyingpatterns,andoftenwiththeaim of ï¬nd-ingasmall subsetofsigni ï¬cant variablesorfeatures.Variable andfeatureselectioniswell-established inthe area ofregression,whereasforothertypesofmodelsthisseemsmoredifï¬cult. Penalization of the L 1 norm provides an in-teresting avenue for such a problem, as it produces a spar se solution and hence embeds variable selection. In thispaper a brief introduction to the mathematical properties of using the L 1 norm as a penalty is given. Examples ofmodelsextendedwith L 1 normpenalties/constraintsarepresented.TheexamplesincludePCAmodelingwithsparseloadings which enhance interpretability of single componen ts. Sparse inverse covariance matrix estimation is usedtounravelwhichvariablesareaffectingeachother,andamodi ï¬edPCAtomodeldatawith(piecewise)constantre-sponses in e.g. process monitoring is shown. All example s are demonstrated on real or synthetic data. The resultsindicate that sparse solutions, when approp riate, can enhance model interpretability.Â© 2012 Elsevier B.V. All rights reserved.",2012,Chemometrics and Intelligent Laboratory Systems
1-30-2012 A significance test for the lasso,"In the sparse linear regression setting, we consider testing the significance of the predictor variable that enters the current lasso model, in the sequence of models visited along the lasso solution path. We propose a simple test statistic based on lasso fitted values, called the covariance test statistic, and show that when the true model is linear, this statistic has an Exp(1) asymptotic distribution under the null hypothesis (the null being that all truly active variables are contained in the current lasso model). Our proof of this result assumes some (reasonable) regularity conditions on the predictor matrix X, and covers the important high-dimensional case p > n. Of course, for testing the significance of an additional variable between two nested linear models, one may use the usual chi-squared test, comparing the drop in residual sum of squares (RSS) to a Ï‡21 distribution. But when this additional variable is not fixed, but has been chosen adaptively or greedily, this test is no longer appropriate: adaptivity makes the drop in RSS stochastically much larger than Ï‡21 under the null hypothesis. Our analysis explicitly accounts for adaptivity, as it must, since the lasso builds an adaptive sequence of linear models as the tuning parameter Î» decreases. In this analysis, shrinkage plays a key role: though additional variables are chosen adaptively, the coefficients of lasso active variables are shrunken due to the l1 penalty. Therefore the test statistic (which is based on lasso fitted values) is in a sense balanced by these two opposing propertiesâ€”adaptivity and shrinkageâ€”and its null distribution is tractable and asymptotically Exp(1).",2015,
An Adaptive SSVEP-Based Brain-Computer Interface to Compensate Fatigue-Induced Decline of Performance in Practical Application,"Brain-computer interfaces based on steady-state visual evoked potentials are promising communication systems for people with speech and motor disabilities. However, reliable SSVEP response requires userâ€™s attention, which degrades over time due to significant eye-fatigue when low-frequency visual stimuli (5â€“15 Hz) are used. Previous studies have shown that eye-fatigue can be reduced using high-frequency flickering stimuli (>25 Hz). Here, it is quantitatively demonstrated that the performance of a high-frequency SSVEP BCI decreases over time, but this amount of decrease can be compensated effectively by using two proposed adaptive algorithms. This leaded to a robust alternative communication system for practical applications. The asynchronous spelling system implemented in this study uses a threshold-based version of LASSO algorithm for frequency recognition. In long online experiments, when participants typed a sentence with the BCI system for 16 times, accuracy of the system was close to its maximum along the experiment. However, regression analysis on typing speed of each sentence demonstrated a significant decrease in all 7 subjects ( ${p} < {0.05}$ ) when thresholds obtained from a calibration test were kept fixed over the experiment. In comparison, no significant change in typing speed was observed when the proposed adaptive algorithms were used. The analysis of variances revealed that the average typing speed of the last four sentences when using adaptive relational algorithm (8.7 char/min) was significantly higher than the tolerance-based algorithm (8.1 char/min) and significantly above 6 char/min when the fixed thresholds were used. Therefore, the relational algorithm proposed in this paper could successfully compensate for the effect of fatigue on performance of the SSVEP BCI system.",2018,IEEE Transactions on Neural Systems and Rehabilitation Engineering
Forecasting completed cost of highway construction projects using LASSO regularized regression,"Finishing highway projects within budget is critical for state highway agencies (SHAs) because budget overruns can result in severe damage to their reputation and credibility. Cost overruns in highway projects have plagued public agencies globally. Hence, this research aims to develop a parametric cost estimation model for SHAs to forecast the completed project cost prior to project execution to take necessary measures to prevent cost escalation. Ordinary least square (OLS) regression has been a commonly used parametric estimation method in the literature. However, OLS regression has certain limitations. It, for instance, requires strict statistical assumptions. This paper proposes an alternative approachâ€” least absolute shrinkage and selection operator (LASSO)â€”that has proved in other fields of research to be significantly better than the OLS method in many respects, including automatic feature selection, the ability to handle highly correlated data, ease of interpretability, and numerical stability of the model predictions. Another contribution to the body of knowledge is that this study simultaneously explores project-related variables with some economic factors that have not been used in previous research, but economic conditions are widely considered to be influential on highway construction costs. The data were separated into two groups: one for training the model and the other for validation purposes. Using the same dataset, both LASSO and OLS were used to build models, and then their performance was evaluated based on the mean absolute error, mean absolute percentage error, and root mean square error. The results showed that the LASSO regression model outperformed the OLS regression model based on the criteria.",2017,
Performance Analysis Of Regularized Linear Regression Models For Oxazolines And Oxazoles Derivitive Descriptor Dataset,"Regularized regression techniques for linear regression have been created the last few ten years to reduce the flaws of ordinary least squares regression with regard to prediction accuracy. In this paper, new methods for using regularized regression in model choice are introduced, and we distinguish the conditions in which regularized regression develops our ability to discriminate models. We applied all the five methods that use penalty-based (regularization) shrinkage to handle Oxazolines and Oxazoles derivatives descriptor dataset with far more predictors than observations. The lasso, ridge, elasticnet, lars and relaxed lasso further possess the desirable property that they simultaneously select relevant predictive descriptors and optimally estimate their effects. Here, we comparatively evaluate the performance of five regularized linear regression methods The assessment of the performance of each model by means of benchmark experiments is an established exercise. Cross-validation and resampling methods are generally used to arrive point evaluates the efficiencies which are compared to recognize methods with acceptable features. Predictive accuracy was evaluated using the root mean squared error (RMSE) and Square of usual correlation between predictors and observed mean inhibitory concentration of antitubercular activity (R square). We found that all five regularized regression models were able to produce feasible models and efficient capturing the linearity in the data. The elastic net and lars had similar accuracies as well as lasso and relaxed lasso had similar accuracies but outperformed ridge regression in terms of the RMSE and R square metrics.",2013,ArXiv
RLS-weighted Lasso for adaptive estimation of sparse signals,"The batch least-absolute shrinkage and selection operator (Lasso) has well-documented merits for estimating sparse signals of interest emerging in various applications, where observations adhere to parsimonious linear regression models. To cope with linearly growing complexity and memory requirements that batch Lasso estimators face when processing observations sequentially, the present paper develops a recursive Lasso algorithm that can also track slowly-varying sparse signals of interest. Performance analysis reveals that recursive Lasso can either estimate consistently the sparse signal's support or its nonzero entries, but not both. This motivates the development of a weighted version of the recursive Lasso scheme with weights obtained from the recursive least-squares (RLS) algorithm. The resultant RLS-weighted Lasso algorithm provably estimates sparse signals consistently. Simulated tests compare competing alternatives and corroborate the performance of the novel algorithms in estimating time-invariant and tracking slow-varying signals under sparsity constraints.",2009,"2009 IEEE International Conference on Acoustics, Speech and Signal Processing"
Agriculture as a success factor for municipalities.,"Prospective changes in agricultural policies will set more emphasis on targets and customer concerns. The analysis of a unique data base stemming from 18,000 citizensâ€™ responses in 60 communes shows interference between the performance of farming and the cognition of quality of life. Agriculture is â€“ among other factors â€“ one of the most significant predictors of quality of life in a municipality. INTRODUCTION AND MOTIVATION Austrian agricultural policy has been striving for years to reward services performed by a multifunctional agriculture. Currently, with the reform of agricultural and regional policy of the EU the targets are discussed anew. In this situation, arguments and evidence in the multi-functionality are important. To what extent does agriculture promote common goods and achieve objectives, perceived and recognised by the population? The analysis of a record on population surveys in citizen participation processes helps to answer this question. The research project â€œErfolgsVisionâ€ (engl. â€œVision of Successâ€) analyzed the results from population surveys in a cross-comparable way. The research project aimed at giving better support to citizen participation processes. The general idea was that an eagleâ€™s eye view of the recent citizen participation processes may yield new information, valuable for regional development consultants as well as municipality management and policies. Three partners joined for that project: SPES Academy, the data owner and regional developer; STUDIA-Schlierbach, an applied social researcher, so far responsible for municipal survey evaluations; and the Department of Statistics and Probability Theory at the Vienna University of Technology. PARTICIPATION PROCESSES AS A DATA SOURCE Over the past few years, the SPES Academy supervised and controlled numerous citizen participation processes in Austrian and German municipalities, mostly within the framework of Local Agenda 21, LEADER or communal business development programmes. These processes established innovative ideas, alliances and problem solutions in the municiW.E. Baaske is head of STUDIA-Schlierbach, Studienzentrum fur inter-nationale Analysen (baaske@studia-schlierbach.com) P. Filzmoser teaches at the Vienna University of Technology, Dep. of Statistics and Probability Theory (filzmoser@statistik.tuwien.ac.at) W. Mader works at SPES Academy, Panoramaweg 1, AT-4553 Schlierbach (mader@spes.co.at). R. Wieser works at the Vienna University of Technology, Dep. of Statistics and Probability Theory (wieser@statistik.tuwien.ac.at) palities. For many of them it was the first time that broad levels of the population were actively involved in local development. Interviews were conducted in order to understand citizensâ€™ demands, preferences and dispositions. They covered major issues concerning the habitation environment, infrastructure and services. They also assessed social capital in communities and positions on strategic fields of action. The results of the surveys have been used to provide a basis for local decision making. They have been reflected and discussed in communal committees and â€“ most often â€“ presented in public events or published in the local news. Each municipality received its own evaluation, consisting of tables, texts and graphics. This data gathered from local polls resulted in a unique record when summarised over the regions and years,. The SPES â€œGemeindepanoramaâ€ (engl. â€œpanorama of the municipalityâ€) is a screening of the local mind-set. Between 2000 and 2006, 60 communities participated, 45 of them from Austria (Upper and Lower Austria, Tyrol and Vorarlberg) and 15 from Germany: Baden-Wurttemberg and Bavaria. In total, 18,748 questionnaires have been collected, on average 312 per municipality. The survey has been subject to individual adaptations towards the municipal needs. It usually comprised about 250 questions, most of them multiple choice. 134 questions have been posed identically in 30 or more municipalities and yield comparable results. Some 25 questions concerned the local agriculture. In the course of the research project, those data have been merged with statistics on demography and economy. 40 mayors gave feed-backs on recent performance of their commune. ANALYZING SUCCESS FACTORS Hypotheses. Starting the research project, SPES developed a list of 27 hypotheses to be tested and questions to be answered, e.g. â€“ What makes communities successful? â€“ What makes quality of life (satisfaction)? What success factors are conditions for a sustained positive trend in quality of life? â€“ What factors encourage optimism? E.g. a strong mayor, successful projects, citizens interior binding and commitment, youth on board, good climate in coexistence and cooperation, high social capital, good communal information and public relations. â€“ Sector thinking in the communities disturbs the development of a positive quality of life. The closer to the habitat, the more important networked, holistic thinking becomes. If merchants win against farmers, then all lose in the final analysis. Data preparation included indicator building, handling of missing values, selection of variables and communes: Variables with less than 20% missing observations and observations with less than 50% missing variables were erased. Missing values were replaced by a nearest neighbour estimate. From the questions and hypotheses, we derived target variables. Sets of explanatory variables were not derived from the hypotheses, but with an automatic search procedure (Lasso regression). An allsubset regression has been carried out to find the essential set of variables explaining the target variable. Robust regression procedures have been applied to attain results that are not susceptible to singularities. ANALYSIS RESULTS FOR QUALITY OF LIFE Quality of Life (QoL) is â€“ on the one hand â€“ a subjective and personal measure of oneâ€™s own satisfaction with life. On the other hand the term is used to characterise regions or cities, reflecting the objective opportunities the location provides to the individual. QoL models reflect key areas as e.g. Being, Belonging and Becoming (Tichbon, Newton 2002) and include access to public services, nutrition and health, knowledge and the physical environment. QoL is represented graphically in order to test it on normal distribution, see Fig. 1. Lasso regression then identified the most influential 26 variables. The all-subset regression further selects variables for a regression model, see Tab. 1. The significant factors for a communeâ€™s quality of life are thus linked to supply structures, merchantsâ€™ activity and inventiveness, to social climate factors like â€œyouth-friendlinessâ€. It is important to mention, that model variables have been selected by the methods described above. Other model calculations show an influence of health servicesâ€™ supply and education and vocational training opportunities. Figure 1. Distribution and quantiles plot of â€œQuality of Lifeâ€. The variable reflects results of the question â€œPlease assess the current state of quality of life in your municipality, on a scale of 1 to 5 (1 ... very good, 5 ... very bad).â€ (n = 56) Table 1. Regression model for â€œQuality of Lifeâ€. Explaining variables: A ... state of the agriculture (question posed similar to Quality of Life, see above), Y ... state of municipalityâ€™s youth friendliness, V ... state of the municipalityâ€™s view, MC ... merchants activate the municipalityâ€™s centre, MI ... merchants are active and come up with ideas. QoL = 8.73 + 0.28 A + 0.21 Y + 0.14 V + 0.20 MC + 0.20 MI (5.9) (4.0) (3.6) (3.8) (3.2) adj. R2 = 0.93, dF = 27 Figure 2. Scatterplot of agricultural density (farms per population), x-axis, versus state of agriculture (SPES pollâ€™s result), y-axis; Loess-regression line; objects=municipalities The most important influential predictors have been displayed in scatterplots. They depict not only strength of interference, but also non-linear behaviour and the position of individual municipalities. The state of agriculture is important, but not sufficient itself to explain quality of life. The state of agriculture does not relate in a negative manner to the state of jobs in the region. The state of agriculture corresponds in part to the share of the agricultural population, especially when this density is low. At higher agricultural densities interference becomes zero, see Fig. 2.",2009,
What role for EUPHA,"G Damiani*, R Galasso, L Sommella, L Pinnarelli, SC Colosimo, R Almiento, L Sicuro, W Ricciardi Department of Public Health-Universita Cattolica S.Cuore-Rome, Italy Referral Oncological Center of Basilicata Rionero in Vulture (PZ), Italy â€˜San Filippo Neriâ€™ Hospital Trust-Rome, Italy *Contact details: gdamiani@rm.unicatt.it Background Reducing undesirable practice variations in care has long been promoted for its potential to improve the quality of health care. Besides, Computerized Clinical Decision Support Systems, designed to help physicians in clinical decision making and based on patient-specific recommendations, are widely spreading. Due to these two issues, computerized guideline (CG) implementation strategies have been considered as useful means of improving health outcomes, optimizing resource utilization and patient care quality. This study carried out a systematic review of scientific articles to evaluate the effectiveness of CG on cliniciansâ€™ behaviour, patients and organization outcomes. Methods Our literature search was carried out through electronic databases, using keywords, hand searching and analysis of further references from the bibliographic citations for each article meeting selected criteria. Analytical and experimental studies published in five languages, in adult population without time restriction. All studies were scored for methodological quality (MQ) on a previously validated scale. Proportions of studies reporting an improvement on physician, patient, and organization outcomes and their 95% confidence intervals (CI) were calculated. A logistic regression model, adjusted for study MQ, was used to investigate association between the outcomes of interest and study-specific covariates (i.e. degree of automation, user training, type of electronic suggestion). Results Out of 3672 articles found according to chosen keywords, 39 matched our criteria. Thirty were set in the USA, 6 in Europe, 2 in Asia and 1 in Australia. CG improved physician performance in 23 (67%), 95% CI 1â„4 51.9â€“83.4, of the 34 studies assessing this outcome. Out of 17 articles assessing patient and organization outcomes 8 trials (47%), 95% CI 1â„4 23.3â€“70.8, reported improvements for both issues. Studied covariates showed no association with outcomes. Conclusions Our initial results suggest that CG improve physician performance. The effects on patient and organization outcomes remain understudied and, when studied, inconsistent, so that further researches are needed.",2006,European Journal of Public Health
Histologic Factors Associated With Need for Surgery in Patients With Pedunculated T1 Colorectal Carcinomas.,"BACKGROUND & AIMS
Most patients with pedunculated T1 colorectal tumors referred for surgery are not found to have lymph node metastases, and were therefore unnecessarily placed at risk for surgery-associated complications. We aimedÂ to identify histologic factors associated with need for surgery in patients with pedunculated T1 colorectal tumors.


METHODS
We performed a cohort-nested matched case-control study of 708 patients diagnosed with pedunculated T1 colorectal tumors at 13 hospitals in The Netherlands, from January 1, 2000 through December 31, 2014, followed for a median of 44 months (interquartile range, 20-80 months). We identified 37 patients (5.2%) who required surgery (due to lymph node, intramural, or distant metastases). These patients were matched with patients with pedunculated T1 colorectal tumors without a need for surgery (no metastases, controls, nÂ = 111). Blinded pathologists analyzed specimens from eachÂ tumor, stained with H&E. We evaluated associations between histologic factors and patient need for surgery using univariable conditional logistic regression analysis. We used multivariable least absolute shrinkage and selection operator (LASSO; an online version of the LASSO model is available at:Â http://t1crc.com/calculator/) regression to develop models for identification of patients with tumors requiring surgery, and tested the accuracy of our model by projecting our case-control data toward the entire cohort (708 patients). We compared our model with previously developed strategies to identify high-risk tumors: conventional model 1 (based onÂ poor differentiation, lymphovascular invasion, or Haggitt level 4) and conventional model 2 (based on poor differentiation, lymphovascular invasion, Haggitt level 4, or tumor budding).


RESULTS
We identified 5 histologic factors that differentiated cases from controls: lymphovascular invasion, Haggitt level 4 invasion, muscularis mucosae type B (incompletely or completely disrupted), poorly differentiated clusters and tumor budding, which identified patients who required surgery with an area under the curve (AUC) value of 0.83 (95% confidence interval, 0.76-0.90). When we used a clinically plausible predicted probability threshold of â‰¥4.0%, 67.5% (478 of 708) of patients were predicted to not need surgery. This threshold identified patients who required surgery with 83.8% sensitivity (95% confidence interval, 68.0%-93.8%) and 70.3% specificity (95% confidence interval, 60.9%-78.6%). Conventional models 1 and 2 identified patients who required surgery with lower AUC values (AUC, 0.67; 95% CI, 0.60-0.74; PÂ = .002 and AUC, 0.64; 95% CI, 0.58-0.70; P < .001, respectively) than our LASSO model. When we applied our LASSO model with a predicted probability threshold of â‰¥4.0%, the percentage of missed cases (tumors mistakenly assigned as low risk) was comparable (6Â of 478 [1.3%]) to that of conventional model 1 (4 of 307 [1.3%]) and conventional model 2 (3 of 244 [1.2%]). However, the percentage of patients referred for surgery based on our LASSO model was much lower (32.5%, nÂ = 230) than that for conventional model 1 (56.6%, nÂ = 401) or conventional modelÂ 2 (65.5%, nÂ = 464).


CONCLUSIONS
In a cohort-nested matched case-control study of 708 patients with pedunculated T1 colorectal carcinomas, we developed a modelÂ based on histologic features of tumors that identifies patients who require surgery (due to high risk of metastasis) withÂ greater accuracy than previous models. Our model might be used to identify patients most likely to benefit from adjuvant surgery.",2018,Gastroenterology
Estudos sobre a aplicaÃ§Ã£o de autoencoder para construÃ§Ã£o de inferÃªncias na indÃºstria quÃ­mica,"In the modern industry, processes are constantly being optimized to seeki safer, cleaner and more energy-efficient production. To this end, advanced monitoring and control systems have been gaining prominence in chemical factories and refineries. However, industrial processes face problems in the measurement of some variables, such as product quality and component concentration. The use of in-line analyzers or laboratory measurements does not allow direct control, due to the sampling time and uncertainty of the analyzer measurements. To circumvent this problem and finally generate frequent and reliable information, this work studies the autoencoder, a tool based on neural networks not yet applied on the development and maintenance of inferences. Some techniques of machine learning will be presented and used, as pretreatment of data following the training of a neural network of unsupervised learning through the compression and decompression of the input information, called autoencoder, produces a latent space with lower dimension concentrating the information winthin the data and therefore being capable of developing soft-sensors. The proposed methodology for the development of the sofsensors is to use the reduced space by the autoencoder to predict the desired variables o the system. The latent space of the autoencoder can be seen as a generalization of the principal components of the PCA methodology. Then, a comparison of the results obtained with PCA, the current standard and the autoencoder is made to evaluate the object of this work. The first step is the pre-processing of the data. Subsequently the data are separated into calibration and test sets using the k-rank methodology. Then the models are constructed through linear regression with methods (Ridge and Lasso-Lars) that perform the selection of variables, discarding unnecessary variables to the models, which will be validated using several evaluation metrics. This methodology is tested with two case studies. One with artificial data generated with known relation between its variables. And one with data from an Aspen simulation of a propene/propane separation unit. This unit has the objective of producing propene with purity of 99.6%, from a load of GLP. It is composed of three columns of distillation. In the first column, the heavy compounds are withdrawn from the bottom and the top stream goes to the second column which has the purpose of removing the ethane from the stream. Thus, the bottom stream of this column goes to a third column, which separates the propene from the propane. The useful information to help control the unit are: concentration of heavy at the top of the first column so that the amount of this impurity can be reduced; in the second column it is important to reduce propene that escapes along with ethane at the top of the column to increase the final production of the plant; and in the third column it is important to maintain the top current within the specification, whereby it is necessary to estimate the impurity of propane at the top of the column. The results show that at this point there is no clear superiority in the quality of the predictions when comparing to the ones of the PCA method for the case-studies and the proposed methodology.",2019,
On Regression Methods for Virtual Metrology in Semiconductor Manufacturing,"Virtual metrology (VM) aims to predict metrology values using sensor data from production equipment and physical metrology values of preceding samples. VM is a promising technology for the semiconductor manufacturing industry as it can reduce the frequency of in-line metrology operations and provide supportive information for other operations such as fault detection, predictive maintenance and run-to-run control. The prediction models for VM can be from a large variety of linear and nonlinear regression methods and the selection of a proper regression method for a specific VM problem is not straightforward, especially when the candidate predictor set is of high dimension, correlated and noisy. Using process data from a benchmark semiconductor manufacturing process, this paper evaluates the performance of four typical regression methods for VM: multiple linear regression (MLR), least absolute shrinkage and selection operator (LASSO), neural networks (NN) and Gaussian process regression (GPR). It is observed that GPR performs the best among the four methods and that, remarkably, the performance of linear regression approaches that of GPR as the subset of selected input variables is increased. The observed competitiveness of high-dimensional linear regression models, which does not hold true in general, is explained in the context of extreme learning machines and functional link neural networks.",2014,
Bayesian Lasso Tobit regression,"In the present research, we have proposed a new approach for model selection in Tobit regression. The new technique uses Bayesian Lasso in Tobit regression (BLTR). It has many features that give optimum estimation and variable selection property. Specifically, we introduced a new hierarchal model. Then, a new Gibbs sampler is introduced. We also extend the new approach by adding the ridge parameter inside the variance covariance matrix to avoid the singularity in the case of multicollinearity or in case the number of predictors greater than the number of observations. A comparison was made with other previous techniques applying the simulation examples and real data. It is worth mentioning, that the obtained results were promising and encouraging, giving better results compared to the previous methods.",2019,
Genomic evaluation using machine learning algorithms in the Spanish Holstein population,"The aim of this study was to validate the recorded data and genome-assisted evaluation models for the Spanish Holstein population as an initial step towards the first official national genomic evaluation. Preliminary national genomic evaluation for production and type traits in Holstein Friesian bulls in Spain were tested using both the Spanish reference population (ESP), composed by 2,115 progeny tested bulls, and the Eurogenomics population (EG), composed by 22,247 progeny tested bulls. Four different traits currently included in the Spanish genetic evaluation were used: milk yield (MY), fat yield (FY), protein yield (PY), and udder depth (UD). Two different genomic evaluation methodologies, Bayesian-Lasso (B-Lasso) and a machine learning algorithm: Random-Boosting (R-Boost) were compared to traditional pedigree index (PI). The predictive ability was measured in terms of correlations, mean square error (MSE) and regression coefficients between progeny proofs and direct genomic values (DGV) in the validation set. Genomic evaluations were more accurate than the traditional pedigree index. The increment in Pearson correlation between observed and predicted response depended on the trait, but the EG population provided greater accuracy than ESP at predicting future progeny performance, as expected. The methodologies implemented showed similar results. B-Lasso showed higher Pearson correlations for MY (0.590 vs 0.572), FY (0.655vs 0.649) and PY (0.583vs 0.545), whereas R-Boost showed larger values for UD (0.584 vs 0.562). Genomic predictions from R-Boost resulted in 4.03% lower predictive mean square errors than B-Lasso. R-Boost showed smaller MSE for MY, PY and UD, whereas B-Lasso was preferred for FY in terms of MSE. R-Boost showed regression coefficients more close to 1 than B-Lasso. The response to different methodologies of genomic evaluation was within the range of values expected for a population of a similar size. The methods that presented higher Pearson correlation also showed larger MSE. This should be considered in model comparison study deciding the method with better predictive ability.",2012,Interbull Bulletin
"Lasso, fractional norm and structured sparse estimation using a Hadamard product parametrization","Using a multiplicative reparametrization, it is shown that a subclass of Lq penalties with q less than or equal to one can be expressed as sums of L2 penalties. It follows that the lasso and other norm-penalized regression estimates may be obtained using a very simple and intuitive alternating ridge regression algorithm. As compared to a similarly intuitive EM algorithm for Lq optimization, the proposed algorithm avoids some numerical instability issues and is also competitive in terms of speed. Furthermore, the proposed algorithm can be extended to accommodate sparse high-dimensional scenarios, generalized linear models, and can be used to create structured sparsity via penalties derived from covariance models for the parameters. Such model-based penalties may be useful for sparse estimation of spatially or temporally structured parameters.",2017,Comput. Stat. Data Anal.
Editorial,"In the past 10 years, the development of statistical methods has been greatly shaped by the demand of analyzing high-dimensional data generated in the fields of biological and medical sciences. Variable selection is a fundamental task in analyzing modern high-dimensional data. Many papers have been devoted to this important topic. Methods such as LASSO and SCAD are now household names in the literature. This special issue comprises five invited contributions from some leading experts in this area, with emphasis on solving interesting application problems. BÃ¼hlmann, RÃ¼timann and Kalisch provided a very nice review article on two widely applicable variable selection techniques: stability selection and aggregated multiple p-values. Both techniques are based on sub-sampling and aim to control false positive selections in observational data analysis. Lu, Zhang and Zeng presented a new variable selection method that is designed to identify important variables involved in optimal treatment decision-making. Their method is based on a penalized regression framework and can be easily implemented by existing software packages. The new method is illustrated with an application to an AIDS clinical trial. Jiang, Huang and Zhang studied a new cross-validated area under the ROC curve (CV-AUC) criterion for tuning parameter selection for sparely penalized logistic regression. The CV-AUC criterion is specifically designed for optimizing the classification performance for binary outcome data. It is shown that CV-AUC outperforms other popular competitors such as AIC, BIC or E-BIC. CV-AUC is used to select genes related to cancer based on microarray data. The survey paper by Zhang and Lin focuses on six important properties for high-dimension-lowsample-size classification problems: predictability, consistency, generality, stochastic stability, robustness and interpretability/sparsity. The authors reviewed several popular classifiers and compared their performance on simulated and real data. Li and Tibshirani proposed a new method for the identification of features that are associated with an outcome variable in RNA-Seq data. Their method is non-parametric and hence more robust than those parametric competitors that are based on Poisson or negative-binomial models. The new method is general enough to be applied to data with quantitative, survival, two-class or multipleclass outcomes. We hope that the readers find this special issue interesting and inspiring. Finally, we want to thank all authors and referees for their enthusiastic support.",2013,Statistical Methods in Medical Research
Penalized Weighted Least Squares for Outlier Detection and Robust Regression,"To conduct regression analysis for data contaminated with outliers, many approaches have been proposed for simultaneous outlier detection and robust regression, so is the approach proposed in this manuscript. This new approach is called ""penalized weighted least squares"" (PWLS). By assigning each observation an individual weight and incorporating a lasso-type penalty on the log-transformation of the weight vector, the PWLS is able to perform outlier detection and robust regression simultaneously. A Bayesian point-of-view of the PWLS is provided, and it is showed that the PWLS can be seen as an example of M-estimation. Two methods are developed for selecting the tuning parameter in the PWLS. The performance of the PWLS is demonstrated via simulations and real applications.",2016,arXiv: Methodology
A resilient domain decomposition polynomial chaos solver for uncertain elliptic PDEs,"Abstract A resilient method is developed for the solution of uncertain elliptic PDEs on extreme scale platforms. The method is based on a hybrid domain decomposition, polynomial chaos (PC) framework that is designed to address soft faults. Specifically, parallel and independent solves of multiple deterministic local problems are used to define PC representations of local Dirichlet boundary-to-boundary maps that are used to reconstruct the global solution. A LAD-lasso type regression is developed for this purpose. The performance of the resulting algorithm is tested on an elliptic equation with an uncertain diffusivity field. Different test cases are considered in order to analyze the impacts of correlation structure of the uncertain diffusivity field, the stochastic resolution, as well as the probability of soft faults. In particular, the computations demonstrate that, provided sufficiently many samples are generated, the method effectively overcomes the occurrence of soft faults.",2017,Comput. Phys. Commun.
