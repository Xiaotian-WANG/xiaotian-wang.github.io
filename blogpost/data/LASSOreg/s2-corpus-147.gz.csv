title,abstract,year,journal
Fractal dimension of cerebral white matter: A consistent feature for prediction of the cognitive performance in patients with small vessel disease and mild cognitive impairment,"Patients with cerebral small vessel disease (SVD) frequently show decline in cognitive performance. However, neuroimaging in SVD patients discloses a wide range of brain lesions and alterations so that it is often difficult to understand which of these changes are the most relevant for cognitive decline. It has also become evident that visually-rated alterations do not fully explain the neuroimaging correlates of cognitive decline in SVD. Fractal dimension (FD), a unitless feature of structural complexity that can be computed from high-resolution T1-weighted images, has been recently applied to the neuroimaging evaluation of the human brain. Indeed, white matter (WM) and cortical gray matter (GM) exhibit an inherent structural complexity that can be measured through the FD. In our study, we included 64 patients (mean ageâ€¯Â±â€¯standard deviation, 74.6â€¯Â±â€¯6.9, education 7.9â€¯Â±â€¯4.2â€¯years, 53% males) with SVD and mild cognitive impairment (MCI), and a control group of 24 healthy subjects (mean ageâ€¯Â±â€¯standard deviation, 72.3â€¯Â±â€¯4.4â€¯years, 50% males). With the aim of assessing whether the FD values of cerebral WM (WM FD) and cortical GM (GM FD) could be valuable structural predictors of cognitive performance in patients with SVD and MCI, we employed a machine learning strategy based on LASSO (least absolute shrinkage and selection operator) regression applied on a set of standard and advanced neuroimaging features in a nested cross-validation (CV) loop. This approach was aimed at 1) choosing the best predictive models, able to reliably predict the individual neuropsychological scores sensitive to attention and executive dysfunctions (prominent features of subcortical vascular cognitive impairment) and 2) identifying a features ranking according to their importance in the model through the assessment of the out-of-sample error. For each neuropsychological test, using 1000 repetitions of LASSO regression and 5000 random permutations, we found that the statistically significant models were those for the Montreal Cognitive Assessment scores (p-valueâ€¯=â€¯.039), Symbol Digit Modalities Test scores (p-valueâ€¯=â€¯.039), and Trail Making Test Part A scores (p-valueâ€¯=â€¯.025). Significant prediction of these scores was obtained using different sets of neuroimaging features in which the WM FD was the most frequently selected feature. In conclusion, we showed that a machine learning approach could be useful in SVD research field using standard and advanced neuroimaging features. Our study results raise the possibility that FD may represent a consistent feature in predicting cognitive decline in SVD that can complement standard imaging.",2019,NeuroImage : Clinical
New Bayesian Lasso Composite Quantile Regression,"In this paper, we propose a new Bayesian lasso inference scheme for variable selection in composite quantile regression model (C Quantile Reg). The suggested approach is to construct a hierarchical structure within the Gibbs sampling under the assumption that the residual term comes from skew Laplace distribution (asymmetric Laplace distribution) andÂ  assign scale mixture uniform (SMU) as prior distributions on the coefficients of composite quantile regression model. Our proposed method was compared to some other existing methods by testing the performance of these methods through simulation studies and real data examples.",2017,"American Scientific Research Journal for Engineering, Technology, and Sciences"
Estimating the Penalty Level of $\ell_{1}$-minimization via Two Gaussian Approximation Methods,"In this paper, we aim to give a theoretical approximation for the penalty level of $\ell_{1}$-regularization problems. This can save much time in practice compared with the traditional methods, such as cross-validation. To achieve this goal, we develop two Gaussian approximation methods, which are based on a moderate deviation theorem and Stein's method respectively. Both of them give efficient approximations and have good performances in simulations. We apply the two Gaussian approximation methods into three types of ultra-high dimensional $\ell_{1}$ penalized regressions: lasso, square-root lasso, and weighted $\ell_{1}$ penalized Poisson regression. The numerical results indicate that our two ways to estimate the penalty levels achieve high computational efficiency. Besides, our prediction errors outperform that based on the 10-fold cross-validation.",2020,arXiv: Statistics Theory
Proactive advising: a machine learning driven approach to vaccine hesitancy,"Despite once being nearly eradicated, Measles cases in Europe have surged to a 20-year high with more than 60,000 cases in 2018, due to a dramatic decrease in vaccination rates. The decrease in Measles, Mumps, and Rubella (MMR) vaccination rates can be attributed to an increase in â€˜vaccine hesitancyâ€™, or the delay in acceptance or refusal of vaccines despite their availability. Vaccine hesitancy is a relatively new global problem for which effective interventions are not yet established. In this paper, a novel machine learning approach to identify children at risk of not being vaccinated against MMR is proposed, with the objective of facilitating proactive action by healthcare workers and policymakers. A use case of the approach is the provision of individualized informative guidance to families that may otherwise become or are already vaccine hesitant. Using a LASSO logistic regression model trained on 44,000 child Electronic Health Records (EHRs), vaccine hesitant families can be identified with a higher precision (0.72) than predicting vaccine uptake based on a childâ€™s infant vaccination record alone (0.63). The model uses a low number of attributes of the child and his or her family and community to produce a prediction, making it readily interpretable by healthcare professionals. The implementation of the machine learning model into an open source dashboard for use by healthcare providers and policymakers as an Early Warning and Monitoring System (EWS) against vaccine hesitancy is proposed. The EWS would facilitate a wide variety of proactive, anticipatory and therefore potentially more effective public health interventions, compared to reactive interventions taken after vaccine rejections.",2019,2019 IEEE International Conference on Healthcare Informatics (ICHI)
Genome-Wide Association Mapping and Genomic Selection for Alfalfa (Medicago sativa) Forage Quality Traits,"Genetic progress for forage quality has been poor in alfalfa (Medicago sativa L.), the most-grown forage legume worldwide. This study aimed at exploring opportunities for marker-assisted selection (MAS) and genomic selection of forage quality traits based on breeding values of parent plants. Some 154 genotypes from a broadly-based reference population were genotyped by genotyping-by-sequencing (GBS), and phenotyped for leaf-to-stem ratio, leaf and stem contents of protein, neutral detergent fiber (NDF) and acid detergent lignin (ADL), and leaf and stem NDF digestibility after 24 hours (NDFD), of their dense-planted half-sib progenies in three growing conditions (summer harvest, full irrigation; summer harvest, suspended irrigation; autumn harvest). Trait-marker analyses were performed on progeny values averaged over conditions, owing to modest germplasm Ã— condition interaction. Genomic selection exploited 11,450 polymorphic SNP markers, whereas a subset of 8,494 M. truncatula-aligned markers were used for a genome-wide association study (GWAS). GWAS confirmed the polygenic control of quality traits and, in agreement with phenotypic correlations, indicated substantially different genetic control of a given trait in stems and leaves. It detected several SNPs in different annotated genes that were highly linked to stem protein content. Also, it identified a small genomic region on chromosome 8 with high concentration of annotated genes associated with leaf ADL, including one gene probably involved in the lignin pathway. Three genomic selection models, i.e., Ridge-regression BLUP, Bayes B and Bayesian Lasso, displayed similar prediction accuracy, whereas SVR-lin was less accurate. Accuracy values were moderate (0.3-0.4) for stem NDFD and leaf protein content, modest for leaf ADL and NDFD, and low to very low for the other traits. Along with previous results for the same germplasm set, this study indicates that GBS data can be exploited to improve both quality traits (by genomic selection or MAS) and forage yield.",2017,PLoS ONE
Estimating stellar atmospheric parameters based on LASSO and support-vector regression,"A scheme for estimating atmospheric parameters T$_{eff}$, log$~g$, and [Fe/H] is proposed on the basis of Least Absolute Shrinkage and Selection Operator (LASSO) algorithm and Haar wavelet. The proposed scheme consists of three processes. A spectrum is decomposed using the Haar wavelet transform and low-frequency components at the fourth level are considered as candidate features. Then, spectral features from the candidate features are detected using the LASSO algorithm to estimate the atmospheric parameters. Finally, atmospheric parameters are estimated from the extracted spectral features using the support-vector regression (SVR) method. The proposed scheme was evaluated using three sets of stellar spectra respectively from Sloan Digital Sky Survey (SDSS), Large Sky Area Multi-object Fiber Spectroscopic Telescope (LAMOST), and Kurucz's model, respectively. The mean absolute errors are as follows: for 40~000 SDSS spectra, 0.0062 dex for log~T$_{eff}$ (85.83 K for T$_{eff}$), 0.2035 dex for log$~g$ and 0.1512 dex for [Fe/H]; for 23963 LAMOST spectra, 0.0074 dex for log~T$_{eff}$ (95.37 K for T$_{eff}$), 0.1528 dex for log~$g$, and 0.1146 dex for [Fe/H]; and for 10469 synthetic spectra, 0.0010 dex for log T$_{eff}$(14.42K for T$_{eff}$), 0.0123 dex for log~$g$, and 0.0125 dex for [Fe/H].",2015,Monthly Notices of the Royal Astronomical Society
Quantile Regression with Elastic-net in Statistical Downscaling to Predict Extreme Rainfall Tri,"Rainfall prediction is necessary since extreme rainfall has a big impact to the environment. A method commonly used to predict rainfall is statistical downscaling. This technique develops a functional relation between local scale rainfall data and global scale General Circulation Model (GCM) output data. The multicollinearity problem in GCM output data can be overcome by the elastic-net regularization which is the combination of ridge and lasso regularizations. The elastic-net is better than only lasso or ridge regularization especially if among predictors are highly correlated. Quantile regression can be used to predict the extreme rainfall. This paper discusses quantile regression in statistical downscaling with elastic-net regularization to predict extreme rainfall. The results show that the extreme rainfall which occur in January and December 2013 was predicted properly at Q(0.75) and Q(0.90) respectively, and elastic-net penalized quantile regression model was consistent. AMS subject classification:",2016,
Feature grouping and selection over an undirected graph,"High-dimensional regression/classification continues to be an important and challenging problem, especially when features are highly correlated. Feature selection, combined with additional structure information on the features has been considered to be promising in promoting regression/classification performance. Graph-guided fused lasso (GFlasso) has recently been proposed to facilitate feature selection and graph structure exploitation, when features exhibit certain graph structures. However, the formulation in GFlasso relies on pairwise sample correlations to perform feature grouping, which could introduce additional estimation bias. In this paper, we propose three new feature grouping and selection methods to resolve this issue. The first method employs a convex function to penalize the pairwise lâˆž norm of connected regression/classification coefficients, achieving simultaneous feature grouping and selection. The second method improves the first one by utilizing a non-convex function to reduce the estimation bias. The third one is the extension of the second method using a truncated l1 regularization to further reduce the estimation bias. The proposed methods combine feature grouping and feature selection to enhance estimation accuracy. We employ the alternating direction method of multipliers (ADMM) and difference of convex functions (DC) programming to solve the proposed formulations. Our experimental results on synthetic data and two real datasets demonstrate the effectiveness of the proposed methods.",2012,KDD : proceedings. International Conference on Knowledge Discovery & Data Mining
AB1380 The impact of osteoarthritis on the functioning of individuals: A statistical validation of the brief ICF core set for osteoarthritis based on a large international sample of patients with osteoarthritis,"Background The International Classification of Functioning, Disability and Health (ICF) 1 is a reference framework to describe the impact of health conditions on functioning and disability. A Comprehensive and Brief ICF Core Set for osteoarthritis (OA) has been developed 2 and must be statistically validated. Objectives Our specific aims were (1) to identify ICF categories that best explain patientsâ€™ functioning, (2) to determine the content validity of the Brief ICF Core Set, and (3) to propose a statistically-validated version. Methods Psychometric study using Group Lasso regression on data from a convenience sample of 879 OA patients from 20 countries. The subscale on general health of the SF-36 was used as dependent variable and all ICF categories of the Comprehensive ICF Core Set for OA and some socio-demographics and disease-specific characteristics as independent variables. The most relevant ICF categories were identified as those showing a significant effect based on the pointwise 90% confidence intervals resulting from 1000 bootstrap trials. Results Based on our results, there were 15 ICF categories included in the statistically-validated version of the Brief ICF Core Set for OA: five body functions (sensation of pain, mobility of joint, muscle power, energy and drive, and emotional functions), three body structures (structure of upper extremity, structure of lower extremity, and additional musculoskeletal structures related to movement), three activities and participation (hand and arm use, walking, and dressing), and four environmental factors (products and technology for personal use in daily living, immediate family, health services, systems and policies, and societal attitudes). Twelve of the original 13 Brief ICF Core Set categories were confirmed, and three additional ICF categories were identified. Conclusions Our findings reassure the validity of the Brief ICF Core Set for OA. The statistically validated Core Set (12 categories out of 13) with the additional three statistically-derived ICF categories will likely perform better with regard to discrimination and sensitivity to change in studies and trials, and should be further explored in the future. References World Health Organization. International Classification of Functioning, Disability, and Health. WHO: Geneva, 2001. Dreinhofer K, Stucki G, Ewert T, et al. ICF Core Sets for osteoarthritis. J Rehabil Med. 2004;(44 Suppl):75-80. Disclosure of Interest None Declared",2013,Annals of the Rheumatic Diseases
A combination of variable selection and data mining techniques for high-dimensional statistical modelling,"Variable selection is fundamental to statistical modelling in diverse fields of sciences. This paper deals with the problem of high-dimensional statistical modelling through the analysis of seismological data in Greece acquired during the years 1962-2003. The dataset consists of 10,333 observations and 11 factors, used to detect possible risk factors of large earthquakes. In our study, different statistical variable selection techniques are applied, while data mining techniques enable us to discover associations, meaningful patterns and rules. The statistical methods employed in this work were the non-concave penalised likelihood methods, SCAD, LASSO and Hard, the generalised linear logistic regression and the best subset variable selection. The applied data mining methods were three decision trees algorithms, the classification and regression tree (C%RT), the chi-square automatic interaction detection (CHAID) and the C5.0 algorithm. The way of identifying the significant variables in large datasets along with the performance of used techniques are also discussed.",2013,IJIDS
Learning an Eddy Viscosity Model with Shrinkage- A Case Study with Jet-in-Crossflow Configuration,"We demonstrate a statistical method for learning a high-order eddy viscosity model from experimental data and using it to improve the predictive skill of a Reynolds-averaged Navier-Stokes (RANS) simulator. The method is tested in a supersonic-jet-in-transoniccrossflow configuration. The process starts with a cubic eddy viscosity model (CEVM), calibrated for incompressible flows. It is fitted to measurements of turbulent stresses from a compressible flow experiment using shrinkage regression, specifically LASSO. LASSO retains the terms in the CEVM that are strongly supported by the data i.e., the most important terms, while removing the rest. For our particular case, LASSO removes all the terms except one that is quadratic in vorticity. The second step involves calibrating three parameters of the RANS model (one being the coefficient of the vorticity term in the truncated CEVM) using measurements of mean flow from a jet-in-crossflow experiment. The predictions of the calibrated RANS model with (truncated) CEVM is compared with experimental data as well as a calibrated RANS model using a linear eddy viscosity model (LEVM). Preliminary results show that high-order eddy viscosity model provides better predictions of turbulent stresses vis-a-vis RANS simulations with LEVM.",2015,
Development of an Immune-Related Prognostic Signature in Breast Cancer,"Background Although increased early detection, diagnosis and treatment have improved the outcome of breast cancer patients, prognosis estimation still poses challenges due to the disease heterogeneity. Accumulating data indicated an evident correlation between tumor immune microenvironment and clinical outcomes. Objective To construct an immune-related signature that can estimate disease prognosis and patient survival in breast cancer. Methods Gene expression profiles and clinical data of breast cancer patients were collected from The Cancer Genome Atlas (TCGA) and Gene Expression Omnibus (GEO) databases, which were further divided into a training set (n = 499), a testing set (n = 234) and a Meta-validation set (n = 519). In the training set, immune-related genes were recognized using combination of gene expression data and ESTIMATE algorithm-derived immune scores. An immune-related prognostic signature was generated with LASSO Cox regression analysis. The prognostic value of the signature was validated in the testing set and the Meta-validation set. Results A total of 991 immune-related genes were identified. Twelve genes with non-zero coefficients in LASSO analysis were used to construct an immune-related prognostic signature. The 12-gene signature significantly stratified patients into high and low immune risk groups in terms of overall survival independent of clinical and pathologic factors. The signature also significantly stratified overall survival in clinical defined groups, including stage I/II disease. Several biological processes, such as immune response, were enriched among genes in the immune-related signature. The percentage of M2 macrophage infiltration was significantly different between low and high immune risk groups. Time-dependent ROC curves indicated good performance of our signature in predicting the 1-, 3- and 5-year overall survival for patients from the full TCGA cohort. Furthermore, the composite signature derived by integrating immune-related signature with clinical factors, provided a more accurate estimation of survival relative to molecular signature alone. Conclusion We developed a 12-gene prognostic signature, providing novel insights into the identification of breast cancer with a high risk of death and assessment of the possibility of immunotherapy incorporation in personalized breast cancer management.",2019,Frontiers in Genetics
Balancing Statistical and Computational Precision and Applications to Penalized Linear Regression with Group Sparsity.,"Due to technological advances, large and high-dimensional data have become the rule rather than the exception. Methods that allow for feature selection with such data are thus highly sought after, in particular, since standard methods, such as cross-validated lasso and group-lasso, can be challenging both computationally and mathematically. In this paper, we propose a novel approach to feature selection and group feature selection in linear regression. It consists of simple optimization steps and tests, which makes it computationally more efficient than standard approaches and suitable even for very large data sets. Moreover, it satisfies sharp guarantees for estimation and feature selection in terms of oracle inequalities. We thus expect that our contribution can help to leverage the increasing volume of data in Biology, Public Health, Astronomy, Economics, and other fields.",2019,arXiv: Methodology
Prognostic value of low-dose dobutamine testing in asymptomatic patients with moderate or severe aortic stenosis and preserved ejection fraction,"Purpose: In Aortic Stenosis (AS) most Doppler-echocardiographic indices that are used for assessing AS severity are flow dependent. The aim of this study was to assess prognostic value of low-dose Dobutamine Testing (DT) in patients with moderate or severe AS and preserved Ejection Fraction (EF).

Method: A total of 126 asymptomatic patients with aortic valve area (AVA) â‰¤1.5 cm2 and EF >50% were enrolled in this prospective study. The follow-up period was 14Â±2 months. Mean age was 66.47Â±10.53; (58.73% males), mean EF was 72,03Â±6,69%, mean pressure gradient (Pmean) 41.94Â±11.22 mmHg and mean AVA 0.82Â±0.22 cm2. Patients with â‰¥2+ valvular regurgitation or more than mild mitral stenosis were excluded. The dobutamine infusion protocol was begun at 5 Î¼g/kg/min body weight up to 20 Î¼g/kg/min, titrated upwards at steps of 5 Î¼g/kg/min every 3 min. The composite outcome endpoint (MACE) was defined as cardiac death, aortic valve replacement and hospitalization caused by AS symptoms according to patient's medical record.

Results: No patient experienced a serious adverse event during or after DT. A total of 70 patients had MACE (55.55%), of which 9 patients (7.14%) have died during follow-up. Out of 70 patients, 56 patients (80%) had an Aortic Valve Replacement (RAV). Patients who had an increase in AVA during DT â‰¤0.2 cm2 and/or final AVA â‰¤1 cm2 had more often RAV (hi=9.5311; df=1; p=0.002). The lasso penalized Cox regression, conducted solely on the variables at rest, showed that the greatest predictive capacity has the aortic valve resistance (AVR). At the same time, the AUC for the all analyzed pre-dobutamine variables combined, evaluated at time = 12 months, was 0.76. On the other hand, the L1 procedure, when applied on all variables (pre and during DT), chooses only dobutamine variables as the most valuable in predictive sense, improving AUC by 6% (AUC =0.82, at time = 12 months). The value of the AVR obtained during the DT was the strongest independent one-year MACE predictor (according to bootstrapped p values) of all pre and during DT varaibles, with the value of 195.12 dynes s cm-5 having the highest sensitivity ans specificity in predicting MACE (0.78 and 0.73 respectively). In addition, patients who have experienced symptoms (11/126, 8.73%) during DT had more often MACE comparing to asymptomatic patients (hi=6,7408; p<0,001; df=1).

Conclusion: The present study demonstrates that AVR, as well as flow-mediated changes during DT, can provide new, clinically relevant information in terms of outcome and timing of valve replacement in asymptomatic patients with moderate and severe AS and preserved EF.",2013,European Heart Journal
Alternating direction method of multipliers for nonconvex fused regression problems,"It is well-known that the fused least absolute shrinkage and selection operator (FLASSO) has been playing an important role in signal and image processing. Recently, the nonconvex penalty is extensively investigated due to its success in sparse learning. In this paper, a novel nonconvex fused regression model, which integrates FLASSO and the nonconvex penalty nicely, is proposed. The developed alternating direction method of multipliers (ADMM) approach is shown to be very efficient owing to the fact that each derived subproblem has a closed-form solution. In addition, the convergence is discussed and proved mathematically. This leads to a fast and convergent algorithm. Extensive numerical experiments show that our proposed nonconvex fused regression outperforms the state-of-the-art approach FLASSO.",2019,Comput. Stat. Data Anal.
Decomposition of the Gender Wage Gap using the LASSO Estimator,We use the LASSO estimator to select among a large number of explanatory variables in wage regressions for a decomposition of the gender wage gap. The LASSO selection with a one standard error rule removes about a quarter of the regressors. We use the LASSO-selected regressors for OLSbased gender wage decompositions. This approach results in a smaller error variance than in OLS without LASSO-selection. The explained gender wage gap is 1%-point greater than in the conventional OLS model.,2020,
Classification Analysis of Chronological Age Using Brief Resting Electroencephalographic (EEG) Recordings,"The present study aims to build a classification model that discriminates between chronological ages of subjects based on resting-state electroencephalography (EEG) data collected from a community sample of 269 children aged 7 to 11. Specifically, spectral power densities in four classical frequency bands: Delta (0.5â€“3 Hz), Theta (4â€“7 Hz), Alpha (8â€“12 Hz) and Beta (14â€“25 Hz) were extracted for each electrode as features, and fed to three classification algorithms including logistic regression (LR), support vector machine (SVM), and least absolute shrinkage and selection operator (Lasso). In addition, principal component analysis (PCA) was used to reduce the dimensions of the feature space. The results demonstrated that SVM and Lasso evidenced better performance (maximal accuracy = 80.68 Â± 2.01% by SVM and 77.82 Â± 2.11% by Lasso) when applied to original feature space, but LR yielded the best performance with PCA (80.72 Â± 1.73%). The accuracy of binary classification exhibited a decreasing trend with diminishing chronological gaps between the groups.",2015,
Lasso-Based Linear Regression for Interval Valued Data,"In regression analysis the relationship between one response and a set of explanatory variables is investigated. The (response and explanatory) variables are usually single-valued. However, in several real-life situations, the available information may be formalized in terms of intervals. An interval-valued datum can be described by the midpoint (its center) and the radius (its half width). Here, limiting our attention to the linear case, regression analysis for interval-valued data is studied. This is done by considering two linear regression models. One model investigates the relationship between the midpoints of the response variable and of the explanatory variables, whereas the other one analyzes the relationship between the radii. The two models are related by considering the same regression coecients, i.e. the same linear relationship is assumed for the midpoints and the radii. However, in some cases, this assumption may be too restrictive. To overcome this drawback, additive coecients for the model of the radii are introduced and their magnitude is tuned according to the Lasso technique allowing us to set to zero some of these additive coecients. In order to show how the proposed method works in practice the results of an application to real-life data are discussed.",2011,
A Model Selection Criterion for High-Dimensional Linear Regression,"Statistical model selection is a great challenge when the number of accessible measurements is much smaller than the dimension of the parameter space. We study the problem of model selection in the context of subset selection for high-dimensional linear regressions. Accordingly, we propose a new model selection criterion with the Fisher information that leads to the selection of a parsimonious model from all the combinatorial models up to some maximum level of sparsity. We analyze the performance of our criterion as the number of measurements grows to infinity, as well as when the noise variance tends to zero. In each case, we prove that our proposed criterion gives the true model with a probability approaching one. Additionally, we devise a computationally affordable algorithm to conduct model selection with the proposed criterion in practice. Interestingly, as a side product, our algorithm can provide the ideal regularization parameter for the Lasso estimator such that Lasso selects the true variables. Finally, numerical simulations are included to support our theoretical findings.",2018,IEEE Transactions on Signal Processing
Radiomic analysis of planning computed tomograms for predicting radiation-induced lung injury and outcome in lung cancer patients treated with robotic stereotactic body radiation therapy,"ObjectivesTo predict radiation-induced lung injury and outcome in non-small cell lung cancer (NSCLC) patients treated with robotic stereotactic body radiation therapy (SBRT) from radiomic features of the primary tumor.MethodsIn all, 110 patients with primary stageÂ I/IIa NSCLC were analyzed for local control (LC), disease-free survival (DFS), overall survival (OS) and development of local lung injury up to fibrosis (LF). First-order (histogram), second-order (GLCM, Gray Level Co-occurrence Matrix) and shape-related radiomic features were determined from the unprocessed or filtered planning CT images of the gross tumor volume (GTV), subjected to LASSO (Least Absolute Shrinkage and Selection Operator) regularization and used to construct continuous and dichotomous risk scores for each endpoint.ResultsContinuous scores comprising 1â€“5 histogram or GLCM features had aÂ significant (pâ€¯=â€‰0.0001â€“0.032) impact on all endpoints that was preserved in aÂ multifactorial Cox regression analysis comprising additional clinical and dosimetric factors. At 36Â months, LC did not differ between the dichotomous risk groups (93% vs. 85%, HR 0.892, 95%CI 0.222â€“3.590), while DFS (45% vs. 17%, pâ€¯<â€‰0.05, HR 0.457, 95%CI 0.240â€“0.868) and OS (80% vs. 37%, pâ€¯<â€‰0.001, HR 0.190, 95%CI 0.065â€“0.556) were significantly lower in the high-risk groups. Also, the frequency of LF differed significantly between the two risk groups (63% vs. 20% at 24Â months, pâ€¯<â€‰0.001, HR 0.158, 95%CI 0.054â€“0.458).ConclusionRadiomic analysis of the gross tumor volume may help to predict DFS and OS and the development of local lung fibrosis in early stage NSCLC patients treated with stereotactic radiotherapy.ZusammenfassungZielVorhersage von pulmonaler ToxizitÃ¤t und onkologischem Ergebnis aus Radiomics-Merkmalen des PrimÃ¤rtumors bei Patienten mit nicht-kleinzelligem Bronchialkarzinom (NSCLC), die mittels robotischer stereotaktischer Strahlentherapie (SBRT) behandelt wurden.MethodenInsgesamt 110Â Patienten mit NSCLC im StadiumÂ I/IIa wurden bzgl. lokaler Kontrolle (LC), krankheitsfreiem Ãœberleben (DFS), GesamtÃ¼berleben (OS) und Entwicklung einer lokalen Lungenfibrose (LF) untersucht. Merkmale erster Ordnung (Histogramm), zweiter Ordnung (GLCM, Gray-Level Co-Occurence Matrix) und formbezogene Merkmale wurden aus den unverarbeiteten oder gefilterten Planungs-CT-Bildern des makroskopischen Tumorvolumens (GTV) bestimmt, mittels LASSO (Least Absolute Shrinkage and Selection Operator) regularisiert und fÃ¼r die Konstruktion von kontinuierlichen und dichotomen Risikoscores fÃ¼r jeden Endpunkt verwendet.ErgebnisseKontinuierliche Scores aus 1â€“5Â Histogramm- oder GLCM-Merkmalen hatten einen signifikanten Einfluss auf alle Endpunkte (pâ€¯=â€‰0,0001â€“0,032), der in einer multifaktoriellen Cox-Regressionsanalyse mit zusÃ¤tzlichen klinischen und dosimetrischen Faktoren erhalten blieb. Nach 36Â Monaten unterschied sich die LC nicht zwischen den dichotomen Risikogruppen (93% vs. 85%; HR 0,892; 95%-KI 0,222â€“3,590), wÃ¤hrend das DFS (45% vs. 17%; pâ€¯<â€‰0,05; HR 0,457; 95%-KI 0,240â€“0,868) und das OS (80% vs. 37%; pâ€¯<â€‰0,001; HR 0,190; 95%-KI 0,065â€“0,556) in den Hochrisikogruppen signifikant schlechter waren. Auch die HÃ¤ufigkeit von LF unterschied sich signifikant zwischen den beiden Risikogruppen (63% gegenÃ¼ber 20% nach 24Â Monaten, pâ€¯<â€‰0,001; HR 0,158; 95%-KI 0,054â€“0,458).SchlussfolgerungDie Radiomics-Analyse des GTV aus dem Planungs-CT kann zur Vorhersage der Prognose und zur EinschÃ¤tzung des Risikos der Entwicklung einer lokalen Lungenfibrose nach stereotaktischer Bestrahlung von Bronchialkarzinomen beitragen.",2019,Strahlentherapie und Onkologie
Prediction of preeclampsia utilizing the first trimester screening examination.,"OBJECTIVE
To derive a prediction rule for preeclampsia and early onset preeclampsia requiring delivery <34 weeks using first trimester maternal, ultrasound, and serum markers.


STUDY DESIGN
Prospective cohort study of women enrolled at first trimester screening. Maternal history, demographics, anthropometry, ultrasound parameters, and serum analytes were compared between women with preeclampsia and normal outcome. The prediction rule was derived by Lasso logistic regression analysis.


RESULTS
In 2441 women, 108 (4.4%) women developed preeclampsia, and 18 (0.7%) early preeclampsia. Nulliparity, prior hypertension, diabetes, prior preeclampsia, mean arterial pressure, and the log pregnancy-associate pregnancy protein-A multiples of the median were primary risk factors. Prediction rules for preeclampsia/early preeclampsia had an area under the curve of 0.82/0.83 respectively. Preeclampsia was predicted with 49% sensitivity and early preeclampsia with 55% sensitivity for a 10% false positive rate.


CONCLUSION
First trimester prediction rules using parameters currently available at first trimester screening identify a significant proportion of women with subsequent preeclampsia.",2014,American journal of obstetrics and gynecology
Reassessing the relationship between landscape alteration and aquatic ecosystem degradation from a hydrologically sensitive area perspective.,"This study applies a novel landscape approach to empirically assess the linkage between terrestrial landscape alteration such as urbanization and aquatic ecosystem degradation from a hydrological sensitive area (HSA) perspective in 141 selected northern New Jersey watersheds. HSAs are hydrological ""hotspots"" in a watershed that actively contribute to runoff generation and were delineated using a soil topographic index. Land use metrics captured landscape alterations in terms of percentages of varying land uses in these watersheds and their HSAs. Aquatic ecosystem integrity was represented by a High Gradient Macroinvertebrate Index (HGMI) specifically developed for the stream types assessed in this study. Multiple linear regression (MLR) analysis was used to understand the relationships between land use metrics and HGMI score at the watershed- and HSA-scales and a data fitting procedure called Least Absolute Shrinkage and Selection Operator (LASSO) was used to identify the most statistically significant land use attributes to be retained in the MLR models. The modeling results at the HSA-scale showed more parsimonious and robust relationships between landscape alteration and aquatic integrity than at the watershed-scale in terms of both variable selection and statistical inference. While high intensity urbanization is a known stressor that can significantly degrade aquatic ecosystem integrity, the results indicate that landscapes developed more strategically by way of low intensity urbanization (e.g., rural residential) or on less hydrologically sensitive areas may lessen the detrimental effects of urbanization on aquatic ecosystem integrity. These findings support the premise that it is not just the extent of urbanization in a watershed that matters, but also the intensity and location of the disturbance on the landscape that affects aquatic ecosystem integrity. Such findings may encourage more flexible landscape planning and management practices that better protect HSAs from urban development in support of long-term aquatic ecosystem protection and restoration.",2019,The Science of the total environment
Detecting managerâ€™s fraud risk using text analysis: evidence from Iran,"In accounting and finance, researchers have used many ways to detect managerâ€™s fraud risk. Until now, many researchers have used some data mining methods in these two fields to detect this risk. The purpose of this paper is to compare the precision of two data mining methods in detecting such a risk.,For this purpose, this paper analyzed the texts of boardâ€™s reports and used two methods including the convex optimization (CVX) method and least absolute shrinkage and selection operator (LASSO) regression method. In this way, the words of these reports, which have the greatest power in explaining the managerâ€™s high fraud risk index, were identified. Using these words, this paper presented a model that could detect managerâ€™s high fraud risk index in companies.,The results indicated that both methods can detect the managerâ€™s high fraud risk index with a precision between 82.55 and 91.25 percent. The LASSO method was significantly more precise than the CVX method.,The lack of access to an official and reliable list of firms suspected to fraud and the lack of access to the Microsoft Word (MS Word) file of boardâ€™s reports were two of the most important limitations of this study.,Regulatory bodies and independent auditors can consider the proposed methods in this study for assessing the fraud risk for a firm or other legal parties.,This paper avoided using merely financial statements data to detect the managerâ€™s fraud risk index and focused on texts of boardâ€™s reports for the detection process. The capabilities of data mining and text mining methods for detecting the managerâ€™s fraud risk index using boardâ€™s reports were tested in this paper. By comparing CVX and LASSO results, this paper indicated that methods with a binary-dependent variable have more power and are more precise than methods with continuous-dependent variables for detecting fraud.",2019,Journal of Applied Accounting Research
Ensembling methods for countrywide short term forecasting of gas demand.,"Gas demand is made of three components: Residential, Industrial, and Thermoelectric Gas Demand. Herein, the one-day-ahead prediction of each component is studied, using Italian data as a case study. Statistical properties and relationships with temperature are discussed, as a preliminary step for an effective feature selection. Nine ""base forecasters"" are implemented and compared: Ridge Regression, Gaussian Processes, Nearest Neighbours, Artificial Neural Networks, Torus Model, LASSO, Elastic Net, Random Forest, and Support Vector Regression (SVR). Based on them, four ensemble predictors are crafted: simple average, weighted average, subset average, and SVR aggregation. We found that ensemble predictors perform consistently better than base ones. Moreover, our models outperformed Transmission System Operator (TSO) predictions in a two-year out-of-sample validation. Such results suggest that combining predictors may lead to significant performance improvements in gas demand forecasting.",2020,arXiv: Learning
Marketing Mix Modelling from multiple regression perspective,"The optimal allocation of the marketing budget has become a di cult issue that each company is facing. With the appearance of new marketing techniques, such as online advertising and social media advertising, the complexity of data has increased, making this problem even more challenging. Statistical tools for explanatory and predictive modelling have commonly been used to tackle the problem of budget allocation. Marketing Mix Modelling involves the use of a range of statistical methods which are suitable for modelling the variable of interest (in this thesis it is sales) in terms of advertising strategies and external variables, with the aim to construct an optimal combination of marketing strategies that would maximize the pro t. The purpose of this thesis is to investigate a number of regression-based model building strategies, with the focus on advanced regularization methods of linear regression, with the analysis of advantages and disadvantages of each method. Several crucial problems that modern marketing mix modelling is facing are discussed in the thesis. These include the choice of the most appropriate functional form that describes the relationship between the set of explanatory variables and the response, modelling the dynamical structure of marketing environment by choosing the optimal decays for each marketing advertising strategy, evaluating the seasonality e ects and collinearity of marketing instruments. To e ciently tackle two common challenges when dealing with marketing data, which are multicollinearity and selection of informative variables, regularization methods are exploited. In particular, the performance accuracy of ridge regression, the lasso, the naive elastic net and elastic net is compared using cross-validation approach for the selection of tuning parameters. Speci c practical recommendations for modelling and analyzing Nepa marketing data are provided. Sammanfattning Att fÃ¶rdela marknadsfÃ¶ringsbudgeten optimalt Ã¤r en svÃ¥r uppgift som alla fÃ¶retag stÃ¤lls infÃ¶r. Med uppkomsten av nya marknadsfÃ¶ringstekniker, som reklam pÃ¥ nÃ¤tet och sociala media, har komplexiteten av data Ã¶kat, vilket gÃ¶r detta problem Ã¤nnu mer utmanande. Statistiska verktyg fÃ¶r fÃ¶rklarande och prediktiv modellering har vanligtvis anvÃ¤nts fÃ¶r att hantera problemet med budgetallokering. MarknadsfÃ¶ringsmix Modellering Ã¤r en term som omfattar klassen av statistiska metoder som Ã¤r lÃ¤mpliga fÃ¶r modellering av den intressanta variabeln (i denna uppsats Ã¤r det fÃ¶rsÃ¤ljning) nÃ¤r det gÃ¤ller reklamstrategier och externa variaber, med mÃ¥let att maximera vinsten genom att konstruera en optimal kombination av marknadsstrategier. Syftet med denna uppsats Ã¤r att konstruera ett antal modellbyggnadsstrategier, som Ã¤ven inkluderar avancerade regulariseringsmetoder fÃ¶r linjÃ¤r regression, med en analys av fÃ¶rdelar och nackdelar fÃ¶r varje metod. Flera stora problem som den moderna marknadsfÃ¶ringsmix modellering stÃ¥r infÃ¶r har beaktats, som till exempel: att vÃ¤lja en passande funktionsformel som bÃ¤st beskriver relationen mellan den oberoende variabeln och de beroende variablerna, att hantera marknadsfÃ¶ringens dynamiska omgivningar genom att vÃ¤lja det optimala fÃ¶rfallet hos varje marknadsfÃ¶ringsstrategi, utvÃ¤rdera sÃ¤songsmÃ¤ssiga e ekten och marknadsfÃ¶ringsverktygens kollinjÃ¤ritet. FÃ¶r att Ã¶verkomma de tvÃ¥ vanligaste problemen inom marknadsfÃ¶ringsekonometri, som Ã¤r multikollinearitet och val av variabler, har regulariseringsmetoder anvÃ¤nts. I synnerhet har prestationsnoggrannheten av ridge regression, lasso, naive elastic net och elastic net jÃ¤mfÃ¶rts fÃ¶r att ge speci ka rekommendationer fÃ¶r Nepa data. Parametrarna fÃ¶r de regulariserade regressionsmetoderna har valts genom korsvalidering. Modellens resultat visar en hÃ¶g nivÃ¥ av fÃ¶rutsÃ¤gelse noggrannhet. Skillnaden mellan nÃ¤mnda metoder Ã¤r inte signi kanta fÃ¶r det givna datasetet.",2017,
Stochastic primal dual fixed point method for composite optimization,"In this paper we propose a stochastic primal dual fixed point method (SPDFP) for solving the sum of two proper lower semi-continuous convex function and one of which is composite. The method is based on the primal dual fixed point method (PDFP) proposed in [7] that does not require subproblem solving. Under some mild condition, the convergence is established based on two sets of assumptions: bounded and unbounded gradients and the convergence rate of the expected error of iterate is of the order O(kâˆ’Î±) where k is iteration number and Î± âˆˆ (0, 1]. Finally, numerical examples on graphic Lasso and logistic regressions are given to demonstrate the effectiveness of the proposed algorithm.",2020,
Local Q-linear convergence and finite-time active set identification of ADMM on a class of penalized regression problems,"We study the convergence of the ADMM (Alternating Direction Method of Multipliers) algorithm on a broad range of penalized regression problems including the Lasso, Group-Lasso and Graph-Lasso,(isotropic) TV-L1, Sparse Variation, and others. First, we establish a fixed-point iteration via a nonlinear operator-which is equivalent to the ADMM iterates. We then show that this nonlinear operator is FreÌchet-differentiable almost everywhere and that around each fixed point, Q-linear convergence is guaranteed, provided the spectral radius of the Jacobian of the operator at the fixed point is less than 1 (a classical result on stability). Moreover, this spectral radius is then a rate of convergence for the ADMM algorithm. Also, we show that the support of the split variable can be identified after finitely many iterations. In the anisotropic cases, we show that for sufficiently large values of the tuning parameter, we recover the optimal rates in terms of Friedrichs angles, that have appeared recently in the literature. Empirical results on various problems are also presented and discussed.",2016,"2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
Variable selection for zero-inflated and overdispersed data with application to health care demand in Germany.,"In health services and outcome research, count outcomes are frequently encountered and often have a large proportion of zeros. The zero-inflated negative binomial (ZINB) regression model has important applications for this type of data. With many possible candidate risk factors, this paper proposes new variable selection methods for the ZINB model. We consider maximum likelihood function plus a penalty including the least absolute shrinkage and selection operator (LASSO), smoothly clipped absolute deviation (SCAD), and minimax concave penalty (MCP). An EM (expectation-maximization) algorithm is proposed for estimating the model parameters and conducting variable selection simultaneously. This algorithm consists of estimating penalized weighted negative binomial models and penalized logistic models via the coordinated descent algorithm. Furthermore, statistical properties including the standard error formulae are provided. A simulation study shows that the new algorithm not only has more accurate or at least comparable estimation, but also is more robust than the traditional stepwise variable selection. The proposed methods are applied to analyze the health care demand in Germany using the open-source R package mpath.",2015,Biometrical journal. Biometrische Zeitschrift
Multinomial functional regression with wavelets and LASSO penalization,"A classification problem with a functional predictor is studied, and it is suggested to use a multinomial functional regression (MFR) model for the analysis. The discrete wavelet transform and LASSO penalization are combined for estimation, and the fitted model is used for classification of new curves with unknown class membership. The MFR approach is applied to two datasets, one regarding lameness detection for horses and another regarding speech recognition. In the applications, as well as in a simulation study, the performance of the MFR approach is compared to that of other methods for supervised classification of functional data, and MFR performs as well or better than the other methods.",2017,Econometrics and Statistics
An Algorithm for Iterative Selection of Blocks of Features,"We focus on the problem of linear regression estimation in high dimension, when the parameter Î² is ""sparse"" (most of its coordinates are 0) and ""blocky"" (Î²i and Î²i+1 are likely to be equal). Recently, some authors defined estimators taking into account this information, such as the Fused-LASSO [19] or the S-LASSO [10] among others. However, there are no theoretical results about the obtained estimators in the general design matrix case. Here, we propose an alternative point of view, based on the Iterative Feature Selection method [1]. We propose an iterative algorithm that takes into account the fact that Î² is sparse and blocky, with no prior knowledge on the position of the blocks. Moreover, we give a theoretical result that ensures that every step of our algorithm actually improves the statistical performance of the obtained estimator. We provide some simulations, where our method outperforms LASSO-type methods in the cases where the parameter is sparse and blocky. Moreover, we give an application to real data (CGH arrays), that shows that our estimator can be used on large datasets.",2010,
Image Coding using Generalized Predictors based on Sparsity and Geometric Transformations LuÄ±Ìs,"Directional intra prediction plays an important role in current state-of-the-art video coding standards. In directional prediction, neighbouring samples are projected along a specific direction to predict a block of samples. Ultimately, each prediction mode can be regarded as a set of very simple linear predictors, a different one for each pixel of a block. Therefore, a natural question that arises is whether one could use the theory of linear prediction in order to generate intra prediction modes that provide increased coding efficiency. However, such an interpretation of each directional mode as a set of linear predictors is too poor to provide useful insights for their design. In this paper we introduce an interpretation of directional prediction as a particular case of linear prediction, that uses first-order linear filters and a set of geometric transformations. This interpretation motivated the proposal of a generalized intra prediction framework, whereby the first-order linear filters are replaced by adaptive linear filters with sparsity constraints. In this context, we investigate the use of efficient sparse linear models, adaptively estimated for each block through the use of different algorithms, such as Matching Pursuit, Least Angle Regression, Lasso or Elastic Net. The proposed intra prediction framework was implemented and evaluated within the state-of-the-art high efficiency video coding standard. Experiments demonstrated the advantage of this predictive solution, mainly in the presence of images with complex features and textured areas, achieving higher average bitrate savings than other related sparse representation methods proposed in the literature.",2016,
Prognostic factors and predictions of survival data,"Survival outcome has been one of the major endpoints for clinical trials; it gives information on the probability of a time-to-event of interest. There has been increasing interest in survival analysis tools over the recent years, especially for high dimensional survival data. Common statistical approaches include nonparametric, semi-parametric and complete parametric analysis, several of which are widely used and readily available from major commercial software applications. However most of these approaches have limitations. Typical nonparametric approaches, such as the log-rank (or Cox-Mantel) test, are not concerned about model assumptions, but can only deal with a limited number of categorical predictors. Typical semi-parametric approaches, such as Cox proportional hazard model, depend very much on the model assumptions, such as linearity, interactions and proportionality; also these approaches can only deal with survival data when the number of predictors is less than the total number of events. Complete parametric models, such as accelerate failure time models, are similar to semi-parametric models except that they make further assumptions about the baseline hazard function. In this research paper, we studied several techniques for evaluating survival data, the typical Cox PH models including the generalized Cox linear model and the multivariate Cox regression models with nonlinear transformations, the nonparametric random survival forest approaches, penalized Cox regression models including lasso, ridge and elastic-net Cox regression models, derived-input Cox regression models including principal component Cox regression and partial least squares Cox regression models. These models were implemented and evaluated with one simulation study and one real world case study. The typical Cox models including the generalized Cox linear model and the multivariate Cox regression models with nonlinear transformations should always provide unbiased estimates, and the models are flexible for handling recurrent-event survival response; but they are incapable of making inferences for cases when there are more predictors than the actual number of events; and since they are semi-parametric",2014,
On the oracle property of adaptive group Lasso in high-dimensional linear models,"In this paper, we consider the adaptive group Lasso in high-dimensional linear regression. Some extensions have been done with other fitting procedures, such as adaptive Lasso, nonconcave penalized likelihood and adaptive elastic-net. Under appropriate conditions, we establish the consistency and asymptotic normality, which means that the adaptive group Lasso shares the oracle property in high-dimensional linear regression when the number of group variables diverges with the sample size.",2016,Statistical Papers
"Extended Comparisons of Best Subset Selection, Forward Stepwise Selection, and the Lasso","In exciting new work, Bertsimas et al. (2016) showed that the classical best subset selection problem in regression modeling can be formulated as a mixed integer optimization (MIO) problem. Using recent advances in MIO algorithms, they demonstrated that best subset selection can now be solved at much larger problem sizes that what was thought possible in the statistics community. They presented empirical comparisons of best subset selection with other popular variable selection procedures, in particular, the lasso and forward stepwise selection. Surprisingly (to us), their simulations suggested that best subset selection consistently outperformed both methods in terms of prediction accuracy. Here we present an expanded set of simulations to shed more light on these comparisons. 
The summary is roughly as follows: (a) neither best subset selection nor the lasso uniformly dominate the other, with best subset selection generally performing better in high signal-to-noise (SNR) ratio regimes, and the lasso better in low SNR regimes; (b) best subset selection and forward stepwise perform quite similarly throughout; (c) the relaxed lasso (actually, a simplified version of the original relaxed estimator defined in Meinshausen, 2007) is the overall winner, performing just about as well as the lasso in low SNR scenarios, and as well as best subset selection in high SNR scenarios.",2017,arXiv: Methodology
Variable Selection and Prediction in High Dimensional Problems,"The aim of this thesis is to develop methods for variable selection and statistical prediction for high dimensional statistical problems. Along with proposing new and innovative procedures, this thesis also focuses on the theoretical properties of the proposed methods and establishes bounds on the statistical error of resulting estimators. The main body of the thesis is divided into three parts. In Chapter 1, a variable screening method for generalized linear models is discussed. The emphasis of the chapter is to provide a procedure to reduce the number of variables in a reliable and fast manner. Then, Chapter 2 considers the linear regression problem in high dimensions when the noise has heavy tails. To perform robust variable selection, a new method, called adaptive robust Lasso, is introduced. Finally, in Chapter 3, the subject is high dimensional classification problems. In this chapter, a robust approach for this problem is proposed and theoretical properties for this approach are established. Overall, the methods proposed in this thesis collectively attempt to solve many of the issues arising in high dimensional statistics, from screening to variable selection. In Chapter 1, we study the variable screening problem for generalized linear models. In many applications, researchers often have some prior knowledge that a certain set of variables is related to the response. In such a situation, a natural assessment on the relative importance of the other predictors is the conditional contributions of the individual predictors in presence of the known set of variables. This results in conditional sure independence screening (CSIS). We propose and study CSIS in the context of generalized linear models. For ultrahigh-dimensional statistical problems, we give conditions under which sure screening is possible and derive an upper bound on the number of selected variables. We also spell out the situation under which CSIS yields model selection consistency. In Chapter 2, we consider the heavy-tailed high dimensional linear regression problem. In the ultra-high dimensional setting, where the dimensionality can grow iii exponentially with the sample size, we investigate the model selection oracle property and establish the asymptotic normality of a quantile regression based method called WR-Lasso. We show that only mild conditions on the model error distribution are needed. Our theoretical results also reveal that adaptive choice of the weight vector is essential for the WR-Lasso to enjoy these nice asymptotic properties. To make the WR-Lasso practically feasible, we propose a two-step procedure, called adaptive robust Lasso (AR-Lasso), in which the weight vector in the second step is constructed based on the L1-penalized quantile regression estimate from the first step. In Chapter 3, we provide an analysis about the issue of measurement errors in high dimensional linear classification problems. For such settings, we propose a new estimator called the robust sparse linear discriminant, that recovers the sparsity signal and adapts to the unknown noise level simultaneously. In contrast to the existing methods, we show that this new method has low risk properties even in the case of measurement errors. Moreover, we propose a new algorithm that recovers the solution paths for a continuum of regularization parameter values.",2013,
On the impact of model selection on predictor identification and parameter inference,"We assessed the ability of several penalized regression methods for linear and logistic models to identify outcome-associated predictors and the impact of predictor selection on parameter inference for practical sample sizes. We studied effect estimates obtained directly from penalized methods (Algorithm 1), or by refitting selected predictors with standard regression (Algorithm 2). For linear models, penalized linear regression, elastic net, smoothly clipped absolute deviation (SCAD), least angle regression and LASSO had a low false negative (FN) predictor selection rates but false positive (FP) rates above 20Â % for all sample and effect sizes. Partial least squares regression had few FPs but many FNs. Only relaxo had low FP and FN rates. For logistic models, LASSO and penalized logistic regression had many FPs and few FNs for all sample and effect sizes. SCAD and adaptive logistic regression had low or moderate FP rates but many FNs. 95Â % confidence interval coverage of predictors with null effects was approximately 100Â % for Algorithm 1 for all methods, and 95Â % for Algorithm 2 for large sample and effect sizes. Coverage was low only for penalized partial least squares (linear regression). For outcome-associated predictors, coverage was close to 95Â % for Algorithm 2 for large sample and effect sizes for all methods except penalized partial least squares and penalized logistic regression. Coverage was sub-nominal for Algorithm 1. In conclusion, many methods performed comparably, and while Algorithm 2 is preferred to Algorithm 1 for estimation, it yields valid inference only for large effect and sample sizes.",2017,Computational Statistics
P1050: Neurophysiological testing of the upper airway in obstructive sleep apnea showing peripheral nervous lesions,"s of Poster Presentations / Clinical Neurophysiology 125, Supplement 1 (2014) S1â€“S339 S327 P1046 Data-driven modeling of sleep EEG and EOG reveals stages indicative of pre-Parkinson and Parkinsonâ€™s disease J.A.E. Christensen1,2, H. Koch1,2, R. Frandsen1, L. Arvastson3, S. Christensen4, H.B.D. Sorensen2, P. Jennum1,5 1Glostrup University Hospital, Danish Center for Sleep Medicine, Glostrup, Denmark; 2Technical University of Denmark, DTU Electrical Engineering, Kgs. Lyngby, Denmark; 3H. Lundbeck A/S, Biostatistics, Valby, Denmark; 4H. Lundbeck A/S, Clinical Pharmacology, Valby, Denmark; 5University of Copenhagen, Center for Healthy Ageing, Copenhagen, Denmark Question: Will unsupervised modelling of sleep EOG and sleep EEG reveal sleep characteristica indicative of pre-Parkinson and Parkinsonâ€™s disease (PD)? Methods: Polysomnographic (PSG) data from 10 age-matched control subjects were used to develop an EOG sleep model and an EEG sleep model. The sleep models were applied on full-night PSG data from 24 additional control subjects, 26 patients with periodic leg movements (PLM), 31 patients with idiopathic REM sleep behavior disorder (iRBD) and 36 PD patients. Based on the data-driven sleep models, features reflecting sleep characteristics in the EEG and EOG activity were computed. The total amount of data was devided in a training dataset containing 16 subjects from each of the four groups, and a validation dataset containing the remaining subjects. The features derived were evaluated and the most discriminative ones between iRBD/PD (neurodegenerative diseases (NDD) cases) and PLM/controls (motor, but non-NDD cases) were found by training a Lasso regularized Regression model using eight-fold cross-validation on the training dataset. Results: The best classification model included four features derived from the EEG data, and the NDD patients in the validation dataset could be classified with a sensitivity of 91.4% and a specificity of 61.1%. The most discriminative features were found to be thresholded amounts and stability of EEG topics reflected to REM, N3 and N1/N2, respectively. Conclusions: This study suggests that the amount of N3 and R and the ability to maintain NREM and REM sleep determined by a data-driven sleep model can be used as potentially early PD biomarkers. P1048 Increased amount of reported total sleep time during 24 hours is related to the severity of sleep disturbances in ischemic stroke patients A. Holm1, M. Vaelimaeki1, T.-M. Haula2, S. Leskelae2, E. Rauhala1 1Satakunta Central Hospital, Clinical Neurophysiology, Pori, Finland; 2Satakunta Central Hospital, Neurology, Pori, Finland Question: Are self-reported short and long term sleep quality related to the severity of sleep related breathing disturbances (SRBD) in ischemic stroke patients? Methods: In this pilot study, 23 patients with acute first-time stroke, admitted to the stroke unit, underwent overnight polygraphy during the first 72 h after admission. Nasal airflow, respiratory movements, snoring sound, oxygenation level, and ECG were recorded. The incidence of SRBD before the stroke was studied with a questionnaire regarding their sleep three months prior the stroke. The reported quality of sleep during the treatment period in stroke unit was examined with the visual scale questionnaire containing five questions. The severity of stroke was assessed with the National Institutes of Health Stroke Scale (NIHSS) and the ability to function with the Barthel Index. Results: Increased apnea-hypopnea index (AHI) was related to the increased amount of total sleep time during 24 hours (p=0.006). No association was found between AHI and the amount of sleep time during night. Also, no correlations between AHI and VAS questions were found. Increased AHI was related with lower Barthel Index (p=0.017) but not with NIHSS. Conclusions: SRBD as well as excessive daytime sleepiness are common findings in patients after stroke. In our study, stroke patients with increased AHI reported increased amount of sleep time during 24 h but not during night time regarding three months pre-stroke, suggesting that they are napping. SRBD negatively affect to the functional outcome after stroke. In addition, SRBD increases the risk of developing cardiovascular diseases, which in turn increases risk for stroke. It is suggested that early diagnosis and treatment of underlying SRBD improve both recovery from stroke in short term and reduce the risk of relapses in long term. Our preliminary results suggest that reported napping may be a marker for a potential SRBD in stroke patients. P1049 Non-REM sleep microstructures and phasic REM sleep events are associated with intracranial oxygenation changes measured with NIRS J. Toppila1, T. Naesi2, J. Virtanen2, T. Salmi1, R. Ilmoniemi2 1Helsinki University Hospital, Medical Imaging/Clin. Neurophys., Helsinki, Finland; 2Aalto University, BECS, Espoo, Finland Question: Disordered and fragmented sleep due to sleep apnea, restless legs syndrome, and insomnia are associated with an increased risk of cardioand cerebrovascular morbidity. Autonomic activation and repetitive hypoxia are examples of possible mechanisms mediating this risk. In addition to sleep apnea episodes, non-apneic disruptions in sleep continuity are associated with transitory intracranial hypoxia measured with nearinfrared spectroscopy (NIRS). Some of the K-complexes are triggered by non-waking external auditory stimuli. Phasic rapid-eye-movement (REM) sleep is associated with the activation of the autonomic nervous system and surges in systemic blood pressure. Are these sleep microstructural phenomena associated with changes in intracranial blood oxygenation? Methods: Six healthy volunteers were measured overnight with polysomnography and NIRS for extraand intracranial blood oxygenation changes. Sleep spindles, K-complexes, and bursts of eye movements in REM sleep were detected. Average temporal changes in oxyand deoxyhemoglobin triggered by these sleep events were measured. Results: K-complexes triggered short intracranial hemodynamic responses and spindels delayed and longer responses. During and after phasic events in REM sleep, there were transient changes in intracranial blood oxygenation. Conclusions: K-complexes are associated with transient hemodynamic changes in intracranial circulation, which can mediate harmful vascular effects during poor sleep due to noisy environments. Systemic blood pressure surges during phasic REM sleep are counteracted with vasoconstriction in cerebral arteries, leading to transient cerebral hypoperfusion and hypoxia episodes during REM sleep. This could be associated with an increased risk of stroke during early mornings. P1050 Neurophysiological testing of the upper airway in obstructive sleep apnea showing peripheral nervous lesions E. Svanborg, L. Hagander, O. Sunnergren University Hospital/Linkoeping University, Clinical Neurophysiology,",2014,Clinical Neurophysiology
An immune infiltration signature to predict the overall survival of patients with colon cancer.,"Immune infiltration of tumors has been increasingly accepted as a prognostic factor in colon cancer. Here, we aim to develop a novel immune signature, based on estimated immune landscape from tumor transcriptomes, to predict the overall survival of patients with colon cancer. The compositions of 22 immune cell subtypes from three microarray datasets were characterized with the CIBERSORT deconvolution algorithm. A prognostic immunoscore (PIS) model for overall survival prediction was established by using least absolute shrinkage and selection operator (LASSO) penalized regression analysis. A total of 17 immune cell markers were screened out in the LASSO model and were then aggregated to generate the PIS. In the training cohort (n =â€‰490), patients with high PIS exhibited a remarkably poorer overall survival than those with low PIS. Similar results were obtained in patients with different TNM stages and in patients receiving adjunctive chemotherapy or not. Multivariate Cox regression indicated that the PIS was an independent predictor for overall survival in colon cancer (hazard ratio: 2.734, 95% confidence interval: 2.052-3.643, p <â€‰.001). The prognostic capability of PIS was also confirmed in the testing cohort (n =â€‰245) and the entire cohort (n =â€‰735). As for biological implications, the PIS was significantly associated with some immune checkpoints, inflammatory factors, epithelial-mesenchymal transformation regulators, and many known signaling pathways in cancer. The results of our study provide a novel and promising immune signature for overall survival prediction of patients with colon cancer.",2019,IUBMB life
Avoiding pitfalls in L1-regularised inference of gene networks.,"Statistical regularisation methods such as LASSO and related L1 regularised regression methods are commonly used to construct models of gene regulatory networks. Although they can theoretically infer the correct network structure, they have been shown in practice to make errors, i.e. leave out existing links and include non-existing links. We show that L1 regularisation methods typically produce a poor network model when the analysed data are ill-conditioned, i.e. the gene expression data matrix has a high condition number, even if it contains enough information for correct network inference. However, the correct structure of network models can be obtained for informative data, data with such a signal to noise ratio that existing links can be proven to exist, when these methods fail, by using least-squares regression and setting small parameters to zero, or by using robust network inference, a recent method taking the intersection of all non-rejectable models. Since available experimental data sets are generally ill-conditioned, we recommend to check the condition number of the data matrix to avoid this pitfall of L1 regularised inference, and to also consider alternative methods.",2015,Molecular bioSystems
Modeling Gene Regulatory Networks from Time Series Data using Particle Filtering,"Modeling Gene Regulatory Networks from Time Series Data Using Particle Filtering. (August 2011) Amina Noor, B.E., M.S., National University of Sciences and Technology Coâ€“Chairs of Advisory Committee: Dr. Erchin Serpedin Dr. Mohamed Nounou This thesis considers the problem of learning the structure of gene regulatory networks using gene expression time series data. A more realistic scenario where the state space model representing a gene network evolves nonlinearly is considered while a linear model is assumed for the microarray data. To capture the nonlinearity, a particle filter based state estimation algorithm is studied instead of the contemporary linear approximation based approaches. The parameters signifying the regulatory relations among various genes are estimated online using a Kalman filter. Since a particular gene interacts with a few other genes only, the parameter vector is expected to be sparse. The state estimates delivered by the particle filter and the observed microarray data are then fed to a LASSO based least squares regression operation, which yields a parsimonious and efficient description of the regulatory network by setting the irrelevant coefficients to zero. The performance of the aforementioned algorithm is compared with extended Kalman filtering (EKF), employing Mean Square Error as the fidelity criterion using synthetic data and real biological data. Extensive computer simulations illustrate that the particle filter based gene network inference algorithm outperforms EKF and therefore, it can serve as a natural framework for modeling gene regulatory networks.",2012,
