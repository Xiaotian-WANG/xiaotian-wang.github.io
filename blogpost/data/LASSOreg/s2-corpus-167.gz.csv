title,abstract,year,journal
Integration of Network-Based Biological Knowledge With White Matter Features in Preterm Infants Using the Graph-Guided Group Lasso,"Abstract The effect of prematurity on normal developmental programs of white and gray matter as evaluated with magnetic resonance imaging indicates global changes in white and gray matter with functional implications. We have previously identified an association between lipids and diffusion tensor imaging features in preterm infants, both through a candidate gene approach and a data-driven statistical genetics method. Here we apply a penalized linear regression model, the graph-guided group lasso (GGGL), that can utilize prior knowledge and select single nucleotide polymorphisms (SNPs) within functionally related genes associated with the trait. GGGL incorporates prior information from SNP-gene mapping as well as from the gene functional interaction network to guide variable selection.",2018,
Forecasting Temperature in a Smart Home with Segmented Linear Regression,"Abstract The efficiency of heating, ventilation and cooling operations in a home are improved when they are controlled by a system that takes into account an accurate forecast of temperature in the house. Temperature forecasts are informed by data from sensors that report on activities and conditions in and around the home. Using publicly available data, we apply linear models based on LASSO regression and our recently developled MIDFEL LASSO regression. These models take into account the past 24 hours of the sensorsâ€™ data. We have previously identified the most influential sensors in a forecast over the next 48 hours. In this paper, we compute 48 separate one-hour forecast and for each hour we identify the sensors that are most influential. This improves forecast accuracy and reveals which sensors are most valuable to install.",2019,Procedia Computer Science
Penalized and Shrinkage Estimation in the Cox Proportional Hazards Model,"This article considers the shrinkage estimation procedure in the Cox's proportional hazards regression model when it is suspected that some of the parameters may be restricted to a subspace. We have developed the statistical properties of the shrinkage estimators including asymptotic distributional biases and risks. The shrinkage estimators have much higher relative efficiency than the classical estimator, furthermore, we consider two penalty estimatorsâ€”the LASSO and adaptive LASSOâ€”and compare their relative performance with that of the shrinkage estimators numerically. A Monte Carlo simulation experiment is conducted for different combinations of irrelevant predictors and the performance of each estimator is evaluated in terms of simulated mean squared error. Simulation study shows that the shrinkage estimators are comparable to the penalty estimators when the number of irrelevant predictors in the model is relatively large. The shrinkage and penalty methods are applied to two real data sets to illustrate the usefulness of the procedures in practice.",2014,Communications in Statistics - Theory and Methods
High-dimensional Linear Regression for Dependent Observations with Application to Nowcasting,"In the last few years, an extensive literature has been focused on the $\ell_1$ penalized least squares (Lasso) estimators of high dimensional linear regression when the number of covariates $p$ is considerably larger than the sample size $n$. However, there is limited attention paid to the properties of the estimators when the errors or/and the covariates are serially dependent. In this study, we investigate the theoretical properties of the Lasso estimators for linear regression with random design under serially dependent and/or non-sub-Gaussian errors and covariates. In contrast to the traditional case in which the errors are i.i.d and have finite exponential moments, we show that $p$ can at most be a power of $n$ if the errors have only polynomial moments. In addition, the rate of convergence becomes slower due to the serial dependencies in errors and the covariates. We also consider sign consistency for model selection via Lasso when there are serial correlations in the errors or the covariates or both. Adopting the framework of functional dependence measure, we provide a detailed description on how the rates of convergence and the selection consistencies of the estimators depend on the dependence measures and moment conditions of the errors and the covariates. Simulation results show that Lasso regression can be substantially more powerful than the mixed frequency data sampling regression (MIDAS) in the presence of irrelevant variables. We apply the results obtained for the Lasso method to nowcasting mixing frequency data in which serially correlated errors and a large number of covariates are common. In real examples, the Lasso procedure outperforms the MIDAS in both forecasting and nowcasting.",2017,arXiv: Statistics Theory
"Habitat models for juvenile pleuronectids around Kodiak Island, A1aska*","-Juveniles offour species of pleuronectid flatfishes were abundant in bays and nearshore areas around Kodiak Island, Alaska, during August 1991 and 1992. The four most abundant species of juvenile (age-O or age-1) flatfishes were rock sole (Pleuronectes bilineatus). flathead sole (Hippoglossoides elassodonJ, Pacific halibut IHippoglossus stenolepisJ. and yellowfin sole (Pleuronectes asper). These species appeared to share nursery areas; however, physical characteristics ofthe nursery areas occupied by each species limited the amount of true overlap among species. Tree-based regression of catch-per-unit-of-effort data on physical parameters was used to refine conceptual models of species distribution, which were originally based only on 1991 data. Threshold values of the physical parameters were specified that best discriminated among stations with different abundances. Highest abundances of age-O rock sole were found on sand or muddy sand at temperatures greater than 8.7Â°C. as well as on other mixed sand stations less than 28 m deep. Ageoflathead sole were most abundant at temperatures less than 8.9Â°C and on mixed mud substrates. At warmer temperatures, abundances were high only if the depth was greater than 48 m, regardless ofsediment type. Age-O Pacific halibut were most abundant in depths less than 40 m at sites more than 2.9 km outside the mouths of bays. Inside bays, halibut were found in lower abundances in water over 9.0Â°C and on sediments containing both sand and mud. Age-1 yellowfin sole were always found in depths less than 28 m on mixed mud substrates. They were usually found within bays, with highest abundances at heads oflarge bays more than 32 km from the bay mouth. These four most abundant flatfishes therefore appeared to partition the available habitat in ways that minimized resource competition. Manuscript accepted 26 February 1997. Fishery Bulletin 95:504-520 (1997). Brenda L. Norcross* *",1997,
"Discussion: "" a Significance Test for the Lasso "" 1","Professors Lockhart, Taylor, Tibshirani and Tibshirani are to be congratulated for their innovative and valuable contribution to the important and timely problem of testing the significance of covariates for the Lasso. Since the invention of the Lasso in Tibshirani (1996) for variable selection, there has been a huge growing literature devoted to its theory and implementation, its extensions to various model settings and different variants and developing more general regularization methods. Most of existing studies have focused on the prediction, estimation and variable selection properties ranging from consistency in prediction and estimation to consistency in model selection in terms of recovery of the true underlying sparse model. The problem of deriving the asymptotic distributions for regularized estimators, as the global or computable solutions, in high dimensions is relatively less well studied. How to develop efficient significance testing procedures for the regularization methods is particularly important since in real applications one would like to assess the significance of selected covariates with their p-values. Such p-values are also crucial for multiple comparisons in testing the significance of a large number of covariates simultaneously. In contrast to the use of some resampling or data splitting techniques for evaluating the significance, in the present paper Lockhart, Taylor, Tibshirani and Tibshirani propose a novel powerful yet simple covariance test statistic Tk for testing the significance of the covariate Xj that enters the model at the kth step of the piecewise linear Lasso solution path in the linear regression model setting. Such a test statistic is shown to have an exact Exp(1) asymptotic null distribution in the case of orthonormal design matrix and the case of k = 1 (i.e., the global null with zero true regression coefficient vector) for general design matrix. In the general case, the Exp(1) distribution provides a conservative asymptotic null distribution. The significance test for the Lasso proposed in the paper is elegant thanks to its simplicity and theoretical guarantees in high dimensions. We appreciate the opportunity to comment on several aspects of this paper. In particular, our discussion will focus on four issues: (1) alternative test statistics, (2) the event B and generalized irrepresentable conditions, (3) model misspecification, and (4) more general regularization methods.",2014,
Prediction Model of Cardiac Risk for Dental Extraction in Elderly Patients with Cardiovascular Diseases,"Background: With the rapidly increasing population of elderly people, dental extraction in elderly individuals with cardiovascular diseases (CVDs) has become quite common. The issue of how to assure the safety of elderly patients with CVDs undergoing dental extraction has perplexed dentists and internists for many years. And it is important to derive an appropriate risk prediction tool for this population. Objectives: The aim of this retrospective, observational study was to establish and validate a prediction model based on the random forest (RF) algorithm for the risk of cardiac complications of dental extraction in elderly patients with CVDs. Methods: Between August 2017 and May 2018, a total of 603 patients who fulfilled the inclusion criteria were used to create a training set. An independent test set contained 230 patients between June 2018 and July 2018. Data regarding clinical parameters, laboratory tests, clinical examinations before dental extraction, and 1-week follow-up were retrieved. Predictors were identified by using logistic regression (LR) with penalized LASSO (least absolute shrinkage and selection operator) variable selection. Then, a prediction model was constructed based on the RF algorithm by using a 5-fold cross-validation method. Results: The training set, based on 603 participants, including 282 men and 321 women, had an average participant age of 72.38 Â± 8.31 years. Using feature selection methods, 11 predictors for risk of cardiac complications were screened out. When the RF model was constructed, its overall classification accuracy was 0.82 at the optimal cutoff value of 18.5%. In comparison to the LR model, the RF model showed a superior predictive performance. The AUROC (area under the receiver operating characteristic curve) scores of the RF and LR models were 0.83 and 0.80, respectively, in the independent test set. The AUPRC (area under the precision-recall curve) scores of the RF and LR models were 0.56 and 0.35, respectively, in the independent test set. Conclusion: The RF-based prediction model is expected to be applicable for preoperative clinical assessment for preventing cardiac complications in elderly patients with CVDs undergoing dental extraction. The findings may aid physicians and dentists in making more informed recommendations to prevent cardiac complications in this patient population.",2019,Gerontology
Inferencia de edad y sexo de usuarios de facebook basado en las actualizaciones de Estado,"The use of Social Networks Sites has grown exponentially in the last decade. People
are sharing online (publicly or in private) their social and romantic interactions, expressing their likes and dislikes, etc. Among many Social Networks Sites, Facebook
is the most popular one. Since active and more connected users tend to adopt a private profile, some demographic information is not available (at least in a
public way). Knowing hidden demographic attributes is useful in many fields, e.g., marketing campaigns. In this study we predict age and gender attributes of Facebook
users relying on their status updates only. As baseline we replicated the predictive model based on language from the Open Vocabulary Approach which uses a set of
features based on the actual communication among users. In addition, based on such a set of linguistic features, we analyzed the performance of a new document
representation called Second Order Representation in the domain of Facebook. Second Order Representation has been proposed to deal with two problems of the Bag Of
Terms representation (used in the Open Vocabulary Approach): terms considered independents of the classes and its high dimensionality and sparsity. Second Order
Representation has been introduced in the field of the Author Profiling in the domain of microblogging and social networks. In order to investigate the effect of reducing
the feature dimension, we experimented with 2-test as term selection method for both the predictive model of the Open Vocabulary Approach and the predictive
model of the Second Order Representation. Our results show that it is possible to infer gender with an accuracy of 0.908 combining the Open Vocabulary Approach to
extract linguistic features and 2-test as term selection method which reduces the time of processing and the feature dimension to only 10, 000 highly discriminative
terms. On age inferring, our results (R = 0.792) show that we can beat the baseline (R = 0.791) by using only 10,000 highly discriminative terms for representing users
with Bag Of Terms and using lasso regression, which is a kind of feature reduction technique to reduce the number of terms used to perform regression. Finally, Second
Order Representation did not beat the base line model giving an accuracy of 0.816 in gender prediction and a square-root of the coefficient of determination of 0.782
in age prediction by using a subset of features of only 15,000 highly discriminative terms.",2015,
When Does the First Spurious Variable Get Selected by Sequential Regression Procedures,"Applied statisticians use sequential regression procedures to produce a ranking of explanatory variables and, in settings of low correlations between variables and strong true effect sizes, expect that variables at the very top of this ranking are truly relevant to the response. In a regime of certain sparsity levels, however, three examples of sequential procedures--forward stepwise, the lasso, and least angle regression--are shown to include the first spurious variable unexpectedly early. We derive a rigorous, sharp prediction of the rank of the first spurious variable for these three procedures, demonstrating that the first spurious variable occurs earlier and earlier as the regression coefficients become denser. This counterintuitive phenomenon persists for statistically independent Gaussian random designs and an arbitrarily large magnitude of the true effects. We gain a better understanding of the phenomenon by identifying the underlying cause and then leverage the insights to introduce a simple visualization tool termed the double-ranking diagram to improve on sequential methods. As a byproduct of these findings, we obtain the first provable result certifying the exact equivalence between the lasso and least angle regression in the early stages of solution paths beyond orthogonal designs. This equivalence can seamlessly carry over many important model selection results concerning the lasso to least angle regression.",2017,arXiv: Statistics Theory
On Identification of Distribution Grids,"Large-scale integration of distributed energy resources into distribution feeders necessitates careful control of their operation through power flow analysis. While the knowledge of the distribution system model is crucial for this analysis, it is often unavailable or outdated. The recent introduction of synchrophasor technology in low-voltage distribution grids has created ample opportunity to learn this model from high-precision, time-synchronized measurements of voltage and current phasors at various locations. This paper focuses on joint estimation of admittance parameters and topology of a polyphase distribution network from the available telemetry data via the lasso, a method for regression shrinkage and selection. We propose tractable convex programs capable of tackling the low-rank structure of the distribution system and develop an online algorithm for early detection and localization of critical events that induce a change in the admittance matrix. The efficacy of these techniques is corroborated through power flow studies on four three-phase radial distribution systems serving real and synthetic household demands.",2019,IEEE Transactions on Control of Network Systems
A Survey of Methods in Variable Selection and Penalized Regression,"Statistical learning often deals with the problem of finding a best predictive model from a set of possible models on the basis of the observed data. â€œBestâ€ often means most parsimonious; thus a sparse model that is composed of a subset of variables is usually preferable to a full model that uses all input variables because of its better interpretability and higher prediction accuracy. To this extent, systematic approaches such as variable selection methods for choosing good interpretable and predictive models have been developed. This paper reviews variable selection methods in linear regression, grouped into two categories: sequential methods, such as forward selection, backward elimination, and stepwise regression; and penalized methods, also called shrinkage or regularization methods, including the LASSO, elastic net, and so on. In addition to covering mathematical properties of the methods, the paper presents practical examples using SAS/STATÂ® software and SASÂ® ViyaÂ®.",2020,
Spline-Lasso in High-Dimensional Linear Regression,"We consider a high-dimensional linear regression problem, where the covariates (features) are ordered in some meaningful way, and the number of covariates p can be much larger than the sample size n. The fused lasso of Tibshirani et al. is designed especially to tackle this type of problems; it yields sparse coefficients and selects grouped variables, and encourages local constant coefficient profile within each group. However, in some applications, the effects of different features within a group might be different and change smoothly. In this article, we propose a new spline-lasso or more generally, spline-MCP to better capture the different effects within the group. The newly proposed method is very easy to implement since it can be easily turned into a lasso or MCP problem. Simulations show that the method works very effectively both in feature selection and prediction accuracy. A real application is also given to illustrate the benefits of the method. Supplementary materials for this article are avai...",2016,Journal of the American Statistical Association
Efficient construction and applications of higher-order force constant models,"The vibrational properties of solids are fundamental to a large number of physical phenomena, including phase stability and thermal conduction. The canonical approach to modeling these properties requires knowledge of the interatomic forces constants (FCs). The problem of extracting the parameters in the FC expansion from a set of reference forces can be cast in linear form making it amenable to linear regression techniques. Here, we consider the efficiency of various common regression methods for FC extraction and the efficacy of the resulting models for predicting various thermodynamic properties. The regression approach drastically reduces the required number of reference calculations, which constitute the computationally most demanding task in FC extraction, compared to explicit enumeration techniques. It thereby becomes possible to extract both harmonic and high-order anharmonic FCs for large systems with low symmetry, including defects and surfaces. It is shown that ordinary least-squares, especially in connection with feature elimination, often yields the best performance in terms of convergence with respect to training set size and sparsity of the solution. Regression based on the least absolute shrinkage and selection operator (LASSO) on the other hand, while useful in some cases, tends to yield a larger number of features, with a noise level that has a detrimental effect on the prediction of e.g., the thermal conductivity. Finally, we also consider methods for the prediction of the temperature dependence of vibrational spectra from high-order FC expansions via molecular dynamics simulations as well as self-consistent phonons.",2019,arXiv: Materials Science
High-dimensional variable selection in regression and classification with missing data,"Variable selection for high-dimensional data problems, including both regression and classification, has been a subject of intense research activities in recent years. Many promising solutions have been proposed. However, less attention has been given to the case when some of the data are missing. This paper proposes a general approach to high-dimensional variable selection with the presence of missing data when the missing fraction can be relatively large (e.g., 50%). Both regression and classification are considered. The proposed approach iterates between two major steps: the first step uses matrix completion to impute the missing data while the second step applies adaptive lasso to the imputed data to select the significant variables. Methods are provided for choosing all the involved tuning parameters. As fast algorithms and software are widely available for matrix completion and adaptive lasso, the proposed approach is fast and straightforward to implement. Results from numerical experiments and applications to two real data sets are presented to demonstrate the efficiency and effectiveness of the approach. HighlightsA fast method for high-dimensional regression and classification with missing data is proposed.The proposed method combines matrix completion and adaptive lasso.It provides promising empirical results.",2017,Signal Process.
"Molecular Classification Substitutes for the Prognostic Variables Stage, Age, and MYCN Status in Neuroblastoma Risk Assessment","BACKGROUND
Current risk stratification systems for neuroblastoma patients consider clinical, histopathological, and genetic variables, and additional prognostic markers have been proposed in recent years. We here sought to select highly informative covariates in a multistep strategy based on consecutive Cox regression models, resulting in a risk score that integrates hazard ratios of prognostic variables.


METHODS
A cohort of 695 neuroblastoma patients was divided into a discovery set (n=75) for multigene predictor generation, a training set (n=411) for risk score development, and a validation set (n=209). Relevant prognostic variables were identified by stepwise multivariable L1-penalized least absolute shrinkage and selection operator (LASSO) Cox regression, followed by backward selection in multivariable Cox regression, and then integrated into a novel risk score.


RESULTS
The variables stage, age, MYCN status, and two multigene predictors, NB-th24 and NB-th44, were selected as independent prognostic markers by LASSO Cox regression analysis. Following backward selection, only the multigene predictors were retained in the final model. Integration of these classifiers in a risk scoring system distinguished three patient subgroups that differed substantially in their outcome. The scoring system discriminated patients with diverging outcome in the validation cohort (5-year event-free survival, 84.9Â±3.4 vs 63.6Â±14.5 vs 31.0Â±5.4; P<.001), and its prognostic value was validated by multivariable analysis.


CONCLUSION
We here propose a translational strategy for developing risk assessment systems based on hazard ratios of relevant prognostic variables. Our final neuroblastoma risk score comprised two multigene predictors only, supporting the notion that molecular properties of the tumor cells strongly impact clinical courses of neuroblastoma patients.",2017,"Neoplasia (New York, N.Y.)"
PLS-Based and Regularization-Based Methods for the Selection of Relevant Variables in Non-targeted Metabolomics Data,"Non-targeted metabolomics constitutes a part of the systems biology and aims at determining numerous metabolites in complex biological samples. Datasets obtained in the non-targeted metabolomics studies are high-dimensional due to sensitivity of mass spectrometry-based detection methods as well as complexity of biological matrices. Therefore, a proper selection of variables which contribute into group classification is a crucial step, especially in metabolomics studies which are focused on searching for disease biomarker candidates. In the present study, three different statistical approaches were tested using two metabolomics datasets (RH and PH study). The orthogonal projections to latent structures-discriminant analysis (OPLS-DA) without and with multiple testing correction as well as the least absolute shrinkage and selection operator (LASSO) with bootstrapping, were tested and compared. For the RH study, OPLS-DA model built without multiple testing correction selected 46 and 218 variables based on the VIP criteria using Pareto and UV scaling, respectively. For the PH study, 217 and 320 variables were selected based on the VIP criteria using Pareto and UV scaling, respectively. In the RH study, OPLS-DA model built after correcting for multiple testing, selected 4 and 19 variables as in terms of Pareto and UV scaling, respectively. For the PH study, 14 and 18 variables were selected based on the VIP criteria in terms of Pareto and UV scaling, respectively. In the RH and PH study, the LASSO selected 14 and 4 variables with reproducibility between 99.3 and 100%, respectively. In the light of PLS-based models, the larger the search space the higher the probability of developing models that fit the training data well with simultaneous poor predictive performance on the validation set. The LASSO offers potential improvements over standard linear regression due to the presence of the constrain, which promotes sparse solutions. This paper is the first one to date utilizing the LASSO penalized logistic regression in untargeted metabolomics studies.",2016,Frontiers in Molecular Biosciences
Connectivity-based parcellation of functional SubROIs in putamen using a sparse spatially regularized regression model,"Abstract In this paper, we present a novel framework for parcellation of a brain region into functional subROIs (Sub-Region-of-Interest) based on their connectivity patterns with other brain regions. By utilising previously established neuroanatomy information, the proposed method aims at finding spatially continuous, functionally consistent subROIs in a given brain region. The proposed framework relies on (1) a sparse spatially-regularized fused lasso regression model for encouraging spatially and functionally adjacent voxels to share similar regression coefficients; (2) an iterative merging and adaptive parameter tuning process; (3) a Graph-Cut optimization algorithm for assigning overlapped voxels into separate subROIs. Our simulation results demonstrate that the proposed method could reliably yield spatially continuous and functionally consistent subROIs. We applied the method to resting-state fMRI data obtained from normal subjects and explored connectivity to the putamen. Two distinct functional subROIs could be parcellated out in the putamen region in all subjects. This approach provides a way to extract functional subROIs that can then be investigated for alterations in connectivity in diseases of the basal ganglia, for example in Parkinson's disease.",2016,Biomed. Signal Process. Control.
Research on Regional Water Use Economic Efficiency Coefficient Based on Gray Relational Analysis and Lasso Method,"Optimal allocation of water is an effective way to resolve the issue of regional water shortages,while identification of economic efficiency coefficient of the regional water is the basis.Use relational analysis method in gray system theory,the various factors that affect water use economic benefits based on regional historical water use data is analyzed,and the main impact factors is distinguished.Mining the relationship between impact factors and total economic output combined with Lasso regression,and the method is used to solve water use economic efficiency coefficient of industrial water and agricultural water in Zhoukou City.The result shows that the method can calculate the water use economic efficiency coefficient of study area more accurately,and provide a more accurate and quantitative methods in fixing the economic efficiency coefficient of optimal allocation of water.",2013,
Polygenic scores via penalized regression on summary statistics.,"Polygenic scores (PGS) summarize the genetic contribution of a person's genotype to a disease or phenotype. They can be used to group participants into different risk categories for diseases, and are also used as covariates in epidemiological analyses. A number of possible ways of calculating PGS have been proposed, and recently there is much interest in methods that incorporate information available in published summary statistics. As there is no inherent information on linkage disequilibrium (LD) in summary statistics, a pertinent question is how we can use LD information available elsewhere to supplement such analyses. To answer this question, we propose a method for constructing PGS using summary statistics and a reference panel in a penalized regression framework, which we call lassosum. We also propose a general method for choosing the value of the tuning parameter in the absence of validation data. In our simulations, we showed that pseudovalidation often resulted in prediction accuracy that is comparable to using a dataset with validation phenotype and was clearly superior to the conservative option of setting the tuning parameter of lassosum to its lowest value. We also showed that lassosum achieved better prediction accuracy than simple clumping and P-value thresholding in almost all scenarios. It was also substantially faster and more accurate than the recently proposed LDpred.",2017,Genetic epidemiology
Exact Post-Selection Inference for Changepoint Detection and Other Generalized Lasso Problems,"We study tools for inference conditioned on model selection events that are defined by the generalized lasso regularization path. The generalized lasso estimate is given by the solution of a penalized least squares regression problem, where the penalty is the l1 norm of a matrix D times the coefficient vector. The generalized lasso path collects these estimates for a range of penalty parameter ({\lambda}) values. Leveraging a sequential characterization of this path from Tibshirani & Taylor (2011), and recent advances in post-selection inference from Lee et al. (2016), Tibshirani et al. (2016), we develop exact hypothesis tests and confidence intervals for linear contrasts of the underlying mean vector, conditioned on any model selection event along the generalized lasso path (assuming Gaussian errors in the observations). By inspecting specific choices of D, we obtain post-selection tests and confidence intervals for specific cases of generalized lasso estimates, such as the fused lasso, trend filtering, and the graph fused lasso. In the fused lasso case, the underlying coordinates of the mean are assigned a linear ordering, and our framework allows us to test selectively chosen breakpoints or changepoints in these mean coordinates. This is an interesting and well-studied problem with broad applications, our framework applied to the trend filtering and graph fused lasso serves several applications as well. Aside from the development of selective inference tools, we describe several practical aspects of our methods such as valid post-processing of generalized estimates before performing inference in order to improve power, and problem-specific visualization aids that may be given to the data analyst for he/she to choose linear contrasts to be tested. Many examples, both from simulated and real data sources, are presented to examine the empirical properties of our inference methods.",2016,arXiv: Methodology
On feature selection for supervised learning problems involving high-dimensional analytical information,"Several computational methods were applied to feature selection for supervised learning problems that can be encountered in the field of analytical chemistry. Namely, Genetic Algorithm (GA), Firefly Algorithm (FA), Particle Swarm Optimization (PSO), Least Absolute Shrinkage and Selection Operator (LASSO), Least Angle Regression Algorithm (LARS), interval Partial Least Squares (iPLS), sparse PLS (sPLS), and Uninformative Variable Elimination-PLS (UVE-PLS). Methods were compared in two case studies which cover both supervised learning cases; (i) regression: multivariate calibration of soil carbonate content using Fourier transform mid-infrared (FT-MIR) spectral information, and (ii) classification: diagnosis of prostate cancer patients using gene expression information. Beside quantitative performance measures: error and accuracy often used in feature selection studies, a qualitative measure, the selection index (SI), was introduced to evaluate the methods in terms of quality of selected features. Robustness was evaluated introducing artificially generated noise variables to both datasets. Results of the first case study have shown that in order of decreasing predictive ability and robustness: GA > FA â‰ˆ PSO > LASSO > LARS (errors of 1.775, 4.504, 4.055 mg gâˆ’1, 10.085, and 10.510 mg gâˆ’1) are recommended for application in regression involving spectral information. In the second case study, the following trend: GA > PSO > FA â‰ˆ LASSO > LARS (accuracies of 100, 95.12 and 90.24%) has been observed. Strong robustness has been observed in the regression case with no decrease in SI for GA, and SI decreasing from 28.85 to 10.26, and 36.11 to 21.05%, for FA and PSO, respectively. In the classification case, only LARS exhibited a considerable decrease in accuracy upon introduction of noise features. Major sources of errors were identified and mostly originated from the analytical methods themselves, which confirmed strong applicability of the evaluated feature selection methods.",2016,RSC Advances
Group-Personalized Regression Models for Predicting Mental Health Scores From Objective Mobile Phone Data Streams: Observational Study,"BACKGROUND
Objective behavioral markers of mental illness, often recorded through smartphones or wearable devices, have the potential to transform how mental health services are delivered and to help users monitor their own health. Linking objective markers to illness is commonly performed using population-level models, which assume that everyone is the same. The reality is that there are large levels of natural interindividual variability, both in terms of response to illness and in usual behavioral patterns, as well as intraindividual variability that these models do not consider.


OBJECTIVE
The objective of this study was to demonstrate the utility of splitting the population into subsets of individuals that exhibit similar relationships between their objective markers and their mental states. Using these subsets, ""group-personalized"" models can be built for individuals based on other individuals to whom they are most similar.


METHODS
We collected geolocation data from 59 participants who were part of the Automated Monitoring of Symptom Severity study at the University of Oxford. This was an observational data collection study. Participants were diagnosed with bipolar disorder (n=20); borderline personality disorder (n=17); or were healthy controls (n=22). Geolocation data were collected using a custom Android app installed on participants' smartphones, and participants weekly reported their symptoms of depression using the 16-item quick inventory of depressive symptomatology questionnaire. Population-level models were built to estimate levels of depression using features derived from the geolocation data recorded from participants, and it was hypothesized that results could be improved by splitting individuals into subgroups with similar relationships between their behavioral features and depressive symptoms. We developed a new model using a Dirichlet process prior for splitting individuals into groups, with a Bayesian Lasso model in each group to link behavioral features with mental illness. The result is a model for each individual that incorporates information from other similar individuals to augment the limited training data available.


RESULTS
The new group-personalized regression model showed a significant improvement over population-level models in predicting mental health severity (P<.001). Analysis of subgroups showed that different groups were characterized by different features derived from raw geolocation data.


CONCLUSIONS
This study demonstrates the importance of handling interindividual variability when developing models of mental illness. Population-level models do not capture nuances in how different individuals respond to illness, and the group-personalized model demonstrates a potential way to overcome these limitations when estimating mental state from objective behavioral features.",2018,Journal of Medical Internet Research
Fused Group Lasso Regularized Multi-Task Feature Learning and Its Application to the Cognitive Performance Prediction of Alzheimerâ€™s Disease,"Alzheimerâ€™s disease (AD) is characterized by gradual neurodegeneration and loss of brain function, especially for memory during early stages. Regression analysis has been widely applied to AD research to relate clinical and biomarker data such as predicting cognitive outcomes from MRI measures. Recently, multi-task based feature learning (MTFL) methods with sparsity-inducing â„“2,1$ \ell _{2,1} $-norm have been widely studied to select a discriminative feature subset from MRI features by incorporating inherent correlations among multiple clinical cognitive measures. However, existing MTFL assumes the correlation among all tasks is uniform, and the task relatedness is modeled by encouraging a common subset of features via sparsity-inducing regularizations that neglect the inherent structure of tasks and MRI features. To address this issue, we proposed a fused group lasso regularization to model the underlying structures, involving 1) a graph structure within tasks and 2) a group structure among the image features. To this end, we present a multi-task feature learning framework with a mixed norm of fused group lasso and â„“2,1$ \ell _{2,1} $-norm to model these more flexible structures. For optimization, we employed the alternating direction method of multipliers (ADMM) to efficiently solve the proposed non-smooth formulation. We evaluated the performance of the proposed method using the Alzheimerâ€™s Disease Neuroimaging Initiative (ADNI) datasets. The experimental results demonstrate that incorporating the two prior structures with fused group lasso norm into the multi-task feature learning can improve prediction performance over several competing methods, with estimated correlations of cognitive functions and identification of cognition-relevant imaging markers that are clinically and biologically meaningful.",2018,Neuroinformatics
2D QSAR and Virtual Screening based on Pyridopyrimidine Analogs of Epidermal Growth Factor Receptor Tyrosine Kinase.,"BACKGROUND
Epidermal Growth Factor Receptor tyrosine kinase (EGFR) is an important anticancer drug target. Series of pyridopyrimidine analogs have been reported as EGFR inhibitors and they inhibit by binding to the ATP binding pocket of the tyrosine kinase domain.


OBJECTIVE
To identify key properties of pyridopyrimidine analogs involved in the inhibition of the EGFR protein tyrosine kinase by developing 2D QSAR model.


METHODS
Variable selection was performed by least absolute shrinkage and selection operator (LASSO) method and multiple linear regression (MLR) method was applied by using Build QSAR software to develop QSAR model. Model validation was done by Leave One Out method (LOO). Further, based on the bioactive and structural similarity, virtual screening was performed using Pubchem database. Using the developed QSAR model and Molinspiration server, PIC50 values and kinase inhibition activity were predicted for all the virtually screened compounds respectively.


RESULTS
The best QSAR model consists of two descriptors namely Basak and MOE type descriptors, and has R2 = 0.8205, F= 57.129 & S = 0.308 and the validation results show significant statistics of R2/cv = 0.655, Average standard deviation = 0.416. 140 compounds were obtained from virtual screening and the predicted PIC50 of all these compounds are in the range of 4.73 - 6.78. All the compounds produce positive scores which suggest that the compounds may have good kinase inhibitory profile.


CONCLUSION
This developed model may be useful to predict EGFR inhibition activity (PIC50) for the newly synthesized pyridopyrimidines analogs.",2016,Current computer-aided drug design
Forecasting tree mortality using change metrics derived from MODIS satellite data,"Insect-induced tree mortality can cause substantial timber and carbon losses in many regions of the world. There is a critical need to forecast tree mortality to guide forest management decisions. Moderate Resolution Imaging Spectroradiometer (MODIS) satellite imagery provides inexpensive and frequent coverage over large areas, facilitating forest health monitoring. This study examined time series of MODIS satellite images to forecast tree mortality for a Pinus radiata plantation in southern New South Wales, Australia. Dead tree density derived from ADS40 aerial imagery was used to evaluate the performance of change metrics derived from time series of MODIS-based vegetation indices. Continuous subset selection by LASSO regression and model assessment using a variant of the bootstrap were used to select the best performing change metrics out of a large amount of predictor variables to account for over-fitting. The results suggest that 250 m 16-daily MODIS images are effective for forecasting tree mortality. Seasonal change metrics derived from the Normalized Difference Vegetation Index (NDVI) outperformed the Enhanced Vegetation Index (EVI) and the Normalized Difference Infrared Index (NDII). Temporal analysis illustrated that optimal forecasting power was obtained using change metrics based on three years of satellite data for this population. The forecast could be used to optimise the scheduling of detailed forest health surveys and silvicultural operations which currently are planned based on stratified, annual assessments. This coarse-scale, spatio-temporal analysis represents a potentially cost-effective early warning approach to forecasting tree mortality in pine plantations by identifying compartments that require more detailed investigation.",2009,Forest Ecology and Management
Abstract LB-296: Tumor tissue microRNA expression in association with triple negative breast cancer recurrence and mortality,"Proceedings: AACR Annual Meeting 2014; April 5-9, 2014; San Diego, CA

Background Triple-negative breast cancer (TNBC) accounts for 15-20% of breast cancers in the United States1. Compared with other breast cancer patients, TNBC displays high rates of metastasis, has poorer prognosis, and has no targeted therapies. microRNAs are small non-coding RNAs that function in transcriptional and post-transcriptional regulation of gene expression. Recent studies, primarily based on cell-line and animal studies, suggest that miRNA expression may be related to cancer metastasis and prognosis1-5.

Purpose To systematically investigate associations of tumor expression of 38 miRNAs that have been previously implicated in breast cancer prognosis with TNBC recurrence and cancer-specific mortality.

Method Included in the study were 456 TNBC cases recruited by the Shanghai Breast Cancer Survival Study between March 2002 and April 2006 and aged 20 to 75 years at diagnosis. Information on breast cancer diagnosis, treatment, demographics, lifestyle factors, and disease progression was collected approximately 6 months after diagnosis and reassessed at 18, 36, and 60 months after diagnosis in follow-up interviews. Information on disease recurrence and mortality was collected via in-person follow-up surveys and linkages with population-based cancer registry and vital statistics databases. miRNA expression levels in formalin-fixed, paraffin-embedded breast cancer tissue sections were measured using the NanoString nCounter assay. The association of miRNA expression with breast cancer recurrence and mortality was evaluated by Cox regression analysis with adjustment for age at diagnosis and TNM stage (I-IV).

Results Of the 38 miRNAs evaluated, expression levels of miR-374b (P=0.0022), miR-148a (P=0.0029), miR-126 (P=0.0059), and miR-218 (P=0.0087) were significantly and inversely associated with recurrence and breast cancer mortality among TNBC patients independent of age and TNM stage. The directions of association were consistent with those previously reported in the literature. A composite score derived from the expression levels of these four miRNAs was more significantly associated with breast cancer recurrence and mortality (P=0.0001), with hazard ratios (95% confidence interval) of 1.2 (0.77-2.0), 0.53 (0.30-0.94), and 0.23 (0.11-0.48) for the second to fourth quartiles compared with the lowest quartile of scores.

Conclusion In this, the largest study to date of tumor miRNA expression and TNBC outcomes, we found that miR-374b, miR-148a, miR-126, and miR-218, were individually and jointly associated with TNBC prognosis.

1. Cascione L, Gasparini P, Lovat F et al., Integrated MicroRNA and mRNA Signatures Associated with Survival in Triple Negative Breast Cancer. PLoS ONE 2013 8(2): e55910

2. Voliniaa S, Galassoa M, Sanaa M, et al., Breast cancer signatures for invasiveness and prognosis defined by deep sequencing of microRNA, PNAS 2012 109, 3024-3029

3. Png K, Halberg N, Yoshida M et al. A microRNA regulon that mediates endothelial recruitment and metastasis by cancer cells, Nature 2012 481:190-194

4. Zhang Y, Yang P, Sun T et al., miR-126 and miR-126* repress recruitment of mesenchymal stem cells and inflammatory monocytes to inhibit breast cancer metastasis. Nature Cell Biology 2013 15, 284-294

5. Pencheva N & Tavazoie S Control of metastatic progression by microRNA regulatory networks. Nature Cell Biology 2013 15, 546-554

Citation Format: Yan Liu, Qiuyin Cai, Fei Ye, Ying Zheng, Jie Wu, Yinghao Su, Hui Cai, Ping-Ping Bao, Wei Zheng, Wei Lu, Xiao-Ou Shu. Tumor tissue microRNA expression in association with triple negative breast cancer recurrence and mortality. [abstract]. In: Proceedings of the 105th Annual Meeting of the American Association for Cancer Research; 2014 Apr 5-9; San Diego, CA. Philadelphia (PA): AACR; Cancer Res 2014;74(19 Suppl):Abstract nr LB-296. doi:10.1158/1538-7445.AM2014-LB-296",2014,Cancer Research
Phillips-Inspired Machine Learning for Band Gap and Exciton Binding Energy Prediction.,"Here in this work, inspired by Phillips' ionicity theory in solid-state physics, we directly sort out the critical factors of the band gap's feature correlations in the machine learning architected with the Lasso algorithm. Even based on a small 2D materials dataset, we can fundamentally approach an accurate and rational model about the band gap and exciton binding energy with robust transferability to other databases. Our machine learning outputs can reveal the exact physics pictures behind the predicted quantity as well as the ""secondary understanding"" of the correlation between the approximated physics models in exciton. This work stressed the significant value of physics endorsement on the ML algorithm and provided a symbolic-regression solution for the ""Few-Shot"" training scheme for the ML technology in materials science. Moreover, physics-inspired secondary understanding could be an essential supplement for machine learning in scientific research fields.",2019,The journal of physical chemistry letters
Toward creating simpler hydrological models: A LASSO subset selection approach,"A formalised means of simplifying hydrological models concurrent with calibration is proposed for use when nonlinear models can be initially formulated as over-parameterised constrained absolute deviation regressions of nonlinear expressions. This provides a flexible modelling framework for approximation of nonlinear situations, while allowing the models to be amenable to algorithmic simplification. The degree of simplification is controlled by a user-specified forcing parameter Î». That is, an original over-parameterised linear model is reduced to a simpler working model which is no more complex than required for a given application. The degree of simplification is a compromise between two factors. With weak simplification most parameters will remain, risking calibration overfitting. On the other hand, a high degree of simplification generates inflexible models. The linear LASSO (Least Absolute Shrinkage and Selection Operator) is utilised for the simplification process because of its ability to deal with linear constraints in the over-parameterised initial model. Models are first formulated as linearly-constrained linear functions.The linear functions can be linear combinations of pre-calculated nonlinear functions.The degree of simplification is controlled by a user-specified forcing parameter.Simplification and calibration are carried out by linear programming minimisation.",2015,Environ. Model. Softw.
A Multilevel Framework for Sparse Optimization with Application to Inverse Covariance Estimation and Logistic Regression,"Solving l1 regularized optimization problems is common in the fields of computational biology, signal processing and machine learning. Such l1 regularization is utilized to find sparse minimizers of convex functions. A well-known example is the LASSO problem, where the l1 norm regularizes a quadratic function. A multilevel framework is presented for solving such l1 regularized sparse optimization problems efficiently. We take advantage of the expected sparseness of the solution, and create a hierarchy of problems of similar type, which is traversed in order to accelerate the optimization process. This framework is applied for solving two problems: (1) the sparse inverse covariance estimation problem, and (2) l1-regularized logistic regression. In the first problem, the inverse of an unknown covariance matrix of a multivariate normal distribution is estimated, under the assumption that it is sparse. To this end, an l1 regularized log-determinant optimization problem needs to be solved. This task is challenging especially for large-scale datasets, due to time and memory limitations. In the second problem, the l1-regularization is added to the logistic regression classification objective to reduce overfitting to the data and obtain a sparse model. Numerical experiments demonstrate the efficiency of the multilevel framework in accelerating existing iterative solvers for both of these problems.",2016,SIAM J. Scientific Computing
Did the genomic data flood overrun the statistical levee ? Statistical approaches to analyse genomic data ( without drowning ),"Some practical aspects in design and analysis of biomarker studies Stephan Lehr Baxter Innovations GmbH, Vienna, Austria The aim of this presentation is to set the scene for the workshop from a practical perspective. In particular, some definitions and guidelines will be sketched, possible pitfalls in the development of a prediction rule based on highdimensional data will be demonstrated and clinical trial designs for predictive biomarker validation will be discussed. Test statistics for two-group comparisons of zero-inflated intensity values Andreas Gleiss, Mohammed Dakna , Harald Mischak and Georg Heinze 1 Section for Clinical Biometrics, Center for Medical Statistics, Informatics and Intelligent Systems, Medical University Vienna, Austria 2 Mosaiques Diagnostics GmbH, Hannover, Germany Many experiments conducted in molecular biology compare intensity values obtained by micro-RNA transcriptomics, proteomics or metabolomics ('Omics') procedures between two groups of independent biological samples differing in an experimental condition or in the health status of the subjects. A special characteristic of such data is the frequent occurrence of zero intensity values caused by compounds (e.g., micro RNAs, peptides or metabolites) that cannot be detected in some samples. Zero intensities can arise either by true absence of a compound or by a signal that is below a technical limit of detection. In the literature the distribution of observed signals is viewed either as a mixture of a binomial distribution (absence or presence of a detectable signal) and a continuous distribution (intensity if signal is present) or as a left-censored continuous distribution. While so-called two-part tests compare mixture distributions between groups, one-part tests treat the zero-inflated distributions as left-censored. We propose to employ a Left-Inflated Mixture model to combine these two approaches. We perform a comparative simulation study using the setting of a typical omics experiment with the aim to test differential expression in several hundreds of compounds simultaneously. Both types of distributional assumptions as well as combinations of both are considered when comparing power and effect estimation across the various methods considered. We discuss issues of application using an example from proteomics. We conclude that the considered tests generally show their strengths in scenarios satisfying their respective distributional assumptions. If it is a priori unclear which distributional assumptions best fit the data at hand then a two-part Wilcoxon test can be recommended. Assuming a log-normal distribution the Left-Inflated Mixture model provides direct estimates for the proportions of the two considered types of zero intensities. Stopping rules for sequential trials in high-dimensional data Sonja Zehetmayer Section for Medical Statistics, Center for Medical Statistics, Informatics and Intelligent Systems, Medical University Vienna, Austria Sequential trials have been proposed that allow for an early stopping of the trial in studies involving large scale hypothesis testing as in microarray experiments. To control the False Discovery Rate, multiplicity adjustment is required only for the number of hypotheses but not for the number of interim looks (under suitable assumptions asymptotically for an increasing number of hypotheses). In this talk we introduce novel stopping rules that stop a trial early if a certain success criterion is fulfilled based on the proportion of rejected hypotheses. Next Generation Sequencing Data Analysis: From ChIP-Seq read islands to epigenomics information Markus Jaritz Research Intitute of Molecular Pathology (IMP), Campus Vienna Biocenter Within the Epigenomics Gen-Au project, several hundred samples have been sequenced using ChIP-Seq technology since 2008. Despite powerful open source tools, many scripts, workflows and analysis strategies had to be developed. We present an overview of our analysis pipeline, addressing issues such as sequencing quality measurements, sample similarity, peak calling, peak overlaps and motif finding. The Relevance of Next Generation Sequencing for Personalized Medicine Christoph Bock CeMM Research Center for Molecular Medicine of the Austrian Academy of Sciences, Vienna, Austria Department of Laboratory Medicine, Medical University of Vienna, Vienna, Austria Max Planck Institute for Informatics, SaarbrÃ¼cken, Germany In my presentation, I will summarize the role of next generation sequencing for personalized medicine and highlight the relevance of bioinformatic and biostatistical methods for interpreting the vast amount of genome, epigenome and transcriptome data that are being generated at CeMM and at many genomics institutes worldwide. The talk will also discuss our ongoing work with the European BLUEPRINT project consortium (http://blueprint-epigenome.eu/) aimed at establishing comprehensive epigenome maps of hematopoietic cell types and various types of leukemia cells. I will conclude by outlining an integrated computational/experimental approach toward rational design of epigenetic combination therapies (Bock and Lengauer 2012 Nature Reviews Cancer), which we pursue in collaboration between the CeMM Research Center for Molecular Medicine and the Medical University of Vienna. Contact: cbock@cemm.oeaw.ac.at and http://epigenomics.cemm.oeaw.ac.at. Class-imbalanced class prediction for high-dimensional data Lara Lusa and Rok Blagus Institute for Biostatistics and Medical Informatics, Faculty of Medicine , University of Ljubljana, Slovenia The goal of class prediction studies is to develop rules to accurately predict the class membership of new samples. The rules are derived using the values of the variables available for each subject: the main characteristic of highdimensional data is that the number of variables greatly exceeds the number of samples. Frequently the classifiers are developed using class-imbalanced data, i.e., data sets where the number of samples in each class is not equal. Standard classification methods used on class-imbalanced data often produce classifiers that do not accurately predict the minority class; the prediction is biased towards the majority class. High-dimensionality poses additional challenges when dealing with class-imbalanced prediction. We show how class-imbalance impacts six types of classifiers, using simulated data and a publicly available data set from a breast cancer geneexpression microarray study. We also investigate the effectiveness of some strategies that are available to overcome the effect of class imbalance. We devote special attention to SMOTE, a popular oversampling technique and to boosting methods. Gene selection in microarray survival studies under non-proportional hazards Daniela Dunkler and Georg Heinze Section for Clinical Biometrics, Center for Medical Statistics, Informatics and Intelligent Systems, Medical University Vienna, Austria Many studies aim at finding, among a huge number of candidate genes, those which are possibly linked to survival. With non-proportional hazards Cox regression (CR) could lead to underor overestimation of effects. Given the large number of genes it is not feasible to determine the functional form of the time dependency for each gene. Moreover, researchers are often interested in a risk score, which could be used to identify high and low risk groups at the start of follow-up. Such a score should be interpretable irrespective of time-dependent effects. We consider the odds of concordance, which are defined by OC=c/(1-c) where c is the concordance probability P(T1<T0), the probability that a person randomly selected from group G1 dies earlier than a person randomly selected from G0. Since gene expressions are not binary, we generalize OC to continuous data. Under proportional hazards, OC is estimated by the CR hazard ratio. Under non-proportional hazards OC can be obtained from weighted Cox regression (WCR) or a novel method based on conditional logistic regression, called concordance regression (CCR). The latter has the additional advantage that it has an attractive nonparametric analogue. We demonstrate properties and aspects of application of these novel methods in the analysis of real and simulated studies. We also briefly discuss the possibility of their application in the context of regularized regression models with a high-dimensional predictor space. Concluding, OC and c are concise single numbers useful for clear decisions at time zero. They can be utilized to select genes irrespective of proportional hazards and are obtained by WCR or CCR. WCR and CCR are available as R packages coxphw and concreg, respectiveley, on CRAN. Regularized regression for omics data: Why one size doesnâ€™t fit all, but you nevertheless should try Harald Binder Institute of Medical Biostatistics, Epidemiology and Informatics, University Medical Center Johannes Gutenberg University Mainz, Germany Modern â€œomicsâ€ techniques for molecular measurement, such as SNP microarrays or RNA-Seq, promise improved prognosis for patients or even personalized medicine. For statisticians this means the challenge of linking highdimensional covariate vectors to clinical endpoints, such as survival, for identifying important molecular entities. While univariate techniques are popular for controlling false discovery rates, they do not directly provide risk prediction models. Regularized regression techniques, such as the lasso or componentwise boosting, allow to fit regression models with high-dimensional covariate vectors, providing automatic selection of a small number of potentially important molecular entities, e.g. SNP or gene signatures. Unfortunately, application to different types of molecular platforms is not straightforward. Potential pitfalls will exemplarily be illustrated for componentwise boosting in applications to SNP data and to RNA-Seq data. For both types of molecular measurements, the variance of c",2013,
