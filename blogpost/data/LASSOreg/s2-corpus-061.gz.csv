title,abstract,year,journal
Penalized least squares regression methods and applications to neuroimaging,"The goals of this paper are to review the most popular methods of predictor selection in regression models, to explain why some fail when the number P of explanatory variables exceeds the number N of participants, and to discuss alternative statistical methods that can be employed in this case. We focus on penalized least squares methods in regression models, and discuss in detail two such methods that are well established in the statistical literature, the LASSO and Elastic Net. We introduce bootstrap enhancements of these methods, the BE-LASSO and BE-Enet, that allow the user to attach a measure of uncertainty to each variable selected. Our work is motivated by a multimodal neuroimaging dataset that consists of morphometric measures (volumes at several anatomical regions of interest), white matter integrity measures from diffusion weighted data (fractional anisotropy, mean diffusivity, axial diffusivity and radial diffusivity) and clinical and demographic variables (age, education, alcohol and drug history). In this dataset, the number P of explanatory variables exceeds the number N of participants. We use the BE-LASSO and BE-Enet to provide the first statistical analysis that allows the assessment of neurocognitive performance from high dimensional neuroimaging and clinical predictors, including their interactions. The major novelty of this analysis is that biomarker selection and dimension reduction are accomplished with a view towards obtaining good predictions for the outcome of interest (i.e., the neurocognitive indices), unlike principal component analysis that are performed only on the predictors' space independently of the outcome of interest.",2011,NeuroImage
A Horse Race in High Dimensional Space,"In this paper, we study the predictive power of dense and sparse estimators in a high dimensional space. We propose a new forecasting method, called Elastically Weighted Principal Components Analysis (EWPCA) that selects the variables, with respect to the target variable, taking into account the collinearity among the data using the Elastic Net soft thresholding. Then, we weight the selected predictors using the Elastic Net regression coefficient, and we finally apply the principal component analysis to the new â€œelasticallyâ€ weighted data matrix. We compare this method to common benchmark and other methods to forecast macroeconomic variables in a data-rich environment, dived into dense representation, such as Dynamic Factor Models and Ridge regressions and sparse representations, such as LASSO regression. All these models are adapted to take into account the linear dependency of the macroeconomic time series. Moreover, to estimate the hyperparameters of these models, including the EWPCA, we propose a new procedure called â€œbrute forceâ€. This method allows us to treat all the hyperparameters of the model uniformly and to take the longitudinal feature of the time-series data into account. Our findings can be summarized as follows. First, the â€œbrute forceâ€ method to estimate the hyperparameters is more stable and gives better forecasting performances, in terms of MSFE, than the traditional criteria used in the literature to tune the hyperparameters. This result holds for all samples sizes and forecasting horizons. Secondly, our two-step forecasting procedure enhances the forecastsâ€™ interpretability. Lastly, the EWPCA leads to better forecasting performances, in terms of mean square forecast error (MSFE), than the other sparse and dense methods or naive benchmark, at different forecasts horizons and sample sizes.",2019,
"Cross-Validation, Shrinkage and Variable Selection in Linear Regression Revisited","In deriving a regression model analysts often have to use variable 
selection, despite of problems introduced by data- dependent model 
building. Resampling approaches are proposed to handle some of the critical 
issues. In order to assess and compare several strategies, we will conduct a 
simulation study with 15 predictors and a complex correlation structure in 
the linear regression model. Using sample sizes of 100 and 400 and estimates of 
the residual variance corresponding to R2 of 0.50 and 0.71, we consider 4 scenarios with varying amount of information. 
We also consider two examples with 24 and 13 predictors, respectively. We will 
discuss the value of cross-validation, shrinkage and backward 
elimination (BE) with varying significance level. We will assess whether 2-step 
approaches using global or parameterwise shrinkage (PWSF) can improve selected models and will compare results to 
models derived with the LASSO procedure. Beside of MSE we will use model 
sparsity and further criteria for model assessment. The amount of information 
in the data has an influence on the selected models and the comparison of the 
procedures. None of the approaches was best in all scenarios. The 
performance of backward elimination with a suitably chosen significance level 
was not worse compared to the LASSO and BE models selected were much sparser, 
an important advantage for interpretation and transportability. Compared to 
global shrinkage, PWSF had better performance. Provided that the amount of 
information is not too small, we conclude that BE followed by PWSF is a suitable 
approach when variable selection is a key part of data analysis.",2013,Open Journal of Statistics
Sparse Regression Incorporating Graphical Structure among Predictors.,"With the abundance of high dimensional data in various disciplines, sparse regularized techniques are very popular these days. In this paper, we make use of the structure information among predictors to improve sparse regression models. Typically, such structure information can be modeled by the connectivity of an undirected graph using all predictors as nodes of the graph. Most existing methods use this undirected graph edge-by-edge to encourage the regression coefficients of corresponding connected predictors to be similar. However, such methods do not directly utilize the neighborhood information of the graph. Furthermore, if there are more edges in the predictor graph, the corresponding regularization term will be more complicate. In this paper, we incorporate the graph information node-by-node, instead of edge-by-edge as used in most existing methods. Our proposed method is very general and it includes adaptive Lasso, group Lasso, and ridge regression as special cases. Both theoretical and numerical studies demonstrate the effectiveness of the proposed method for simultaneous estimation, prediction and model selection.",2016,Journal of the American Statistical Association
Reverse engineering gene regulatory networks related to quorum sensing in the plant pathogen Pectobacterium atrosepticum.,"The objective of the project reported in the present chapter was the reverse engineering of gene regulatory networks related to quorum sensing in the plant pathogen Pectobacterium atrosepticum from micorarray gene expression profiles, obtained from the wild-type and eight knockout strains. To this end, we have applied various recent methods from multivariate statistics and machine learning: graphical Gaussian models, sparse Bayesian regression, LASSO (least absolute shrinkage and selection operator), Bayesian networks, and nested effects models. We have investigated the degree of similarity between the predictions obtained with the different approaches, and we have assessed the consistency of the reconstructed networks in terms of global topological network properties, based on the node degree distribution. The chapter concludes with a biological evaluation of the predicted network structures.",2010,Methods in molecular biology
On regularized estimation methods for precision and covariance matrix and statistical network inference,"Estimation of the covariance matrix is an important problem in statistics in general because the covariance matrix is an essential part of principal component analysis, statistical pattern recognition, multivariate regression and network exploration, just to mention but a few applications. Penalized likelihood methods are used when standard estimates cannot be computed. This is a common case when the number of explanatory variables is much larger compared to the sample size (high-dimensional case). An alternative ridge-type estimator for the precision matrix estimation is introduced in Article I. This estimate is derived using a penalized likelihood estimation method. Undirected networks, which are connected to penalized covariance and precision matrix estimation and some applications related to networks are also explored in this dissertation. In Article II novel statistical methods are used to infer population networks from discrete measurements of genetic data. More precisely, Least Absolute Shrinkage and Selection Operator, LASSO for short, is applied in neighborhood selection. This inferred network is used for more detailed inference of population structures. We illustrate how community detection can be a promising tool in population structure and admixture exploration of genetic data. In addition, in Article IV it is shown how the precision matrix estimator introduced in Article I can be used in graphical model selection via a multiple hypothesis testing procedure. Article III in this dissertation contains a review of current tools for practical graphical model selection and precision/covariance matrix estimation. The other three publications have detailed descriptions of the fundamental computational and mathematical results which create a basis for the methods presented in these articles. Each publication contains a collection of practical research questions where the novel methods can be applied. We hope that these applications will help readers to better understand the possible applications of the methods presented in this dissertation.",2018,
Probabilistic Hourly Load Forecasting Using Additive Quantile Regression Models,"Short-term hourly load forecasting in South Africa using additive quantile regression (AQR) models is discussed in this study. The modelling approach allows for easy interpretability and accounting for residual autocorrelation in the joint modelling of hourly electricity data. A comparative analysis is done using generalised additive models (GAMs). In both modelling frameworks, variable selection is done using least absolute shrinkage and selection operator (Lasso) via hierarchical interactions. Four models considered are GAMs and AQR models with and without interactions, respectively. The AQR model with pairwise interactions was found to be the best fitting model. The forecasts from the four models were then combined using an algorithm based on the pinball loss (convex combination model) and also using quantile regression averaging (QRA). The AQR model with interactions was then compared with the convex combination and QRA models and the QRA model gave the most accurate forecasts. Except for the AQR model with interactions, the other two models (convex combination model and QRA model) gave prediction interval coverage probabilities that were valid for the 90 % , 95 % and the 99 % prediction intervals. The QRA model had the smallest prediction interval normalised average width and prediction interval normalised average deviation. The modelling framework discussed in this paper has established that going beyond summary performance statistics in forecasting has merit as it gives more insight into the developed forecasting models.",2018,Energies
A Path Algorithm for Constrained Estimation.,"Many least-square problems involve affine equality and inequality constraints. Although there are a variety of methods for solving such problems, most statisticians find constrained estimation challenging. The current article proposes a new path-following algorithm for quadratic programming that replaces hard constraints by what are called exact penalties. Similar penalties arise in l1 regularization in model selection. In the regularization setting, penalties encapsulate prior knowledge, and penalized parameter estimates represent a trade-off between the observed data and the prior knowledge. Classical penalty methods of optimization, such as the quadratic penalty method, solve a sequence of unconstrained problems that put greater and greater stress on meeting the constraints. In the limit as the penalty constant tends to âˆž, one recovers the constrained solution. In the exact penalty method, squared penalties!are replaced by absolute value penalties, and the solution is recovered for a finite value of the penalty constant. The exact path-following method starts at the unconstrained solution and follows the solution path as the penalty constant increases. In the process, the solution path hits, slides along, and exits from the various constraints. Path following in Lasso penalized regression, in contrast, starts with a large value of the penalty constant and works its way downward. In both settings, inspection of the entire solution path is revealing. Just as with the Lasso and generalized Lasso, it is possible to plot the effective degrees of freedom along the solution path. For a strictly convex quadratic program, the exact penalty algorithm can be framed entirely in terms of the sweep operator of regression analysis. A few well-chosen examples illustrate the mechanics and potential of path following. This article has supplementary materials available online.",2013,"Journal of computational and graphical statistics : a joint publication of American Statistical Association, Institute of Mathematical Statistics, Interface Foundation of North America"
Indirect Comparison of Interaction Graphs,Astrategy for testing differential conditional independence structures (CIS) between two graphs is introduced. The graphs have the same set of nodes and are estimated from data sampled under two different conditions. The test uses the entire pathplot in a Lasso regression as the information on how a node connects with the remaining nodes in the graph.,2010,
The diagnostic value of power spectra analysis of the sleep electroencephalography in narcoleptic patients.,"OBJECTIVE
Manifestations of narcolepsy with cataplexy (NC) include disturbed nocturnal sleep - hereunder sleep-wake instability, decreased latency to rapid eye movement (REM) sleep, and dissociated REM sleep events. In this study, we characterized the electroencephalography (EEG) of various sleep stages in NC versus controls.


METHODS
EEG power spectral density (PSD) was computed in 136 NC patients and 510 sex- and age-matched controls. Features reflecting differences in PSD curves were computed. A Lasso-regularized regression model was used to find an optimal feature subset, which was validated on 19 NC patients and 708 non-NC patients from a sleep clinic. Reproducible features were analyzed using receiver operating characteristic (ROC) curves.


RESULTS
Thirteen features were selected based on the training dataset. Three were applicable in the validation dataset, indicating that NC patients show (1) increased alpha power in REM sleep, (2) decreased sigma power in wakefulness, and (3) decreased delta power in stage N1 versus wakefulness. Sensitivity of these features ranged from 4% to 10% with specificity around 98%, and it did not vary substantially with and without treatment.


CONCLUSIONS
EEG spectral analysis of REM sleep, wake, and differences between N1 and wakefulness contain diagnostic features of NC. These traits may represent sleepiness and dissociated REM sleep in patients with NC. However, the features are not sufficient for differentiating NC from controls, and further analysis is needed to completely evaluate the diagnostic potential of these features.",2015,Sleep medicine
Scaled and square-root elastic net,"In scaled lasso, the unknown regression coefficients and the scale parameter of the error distribution are estimated jointly. In lasso, the optimal penalty parameter is well-known to depend on the error scale, and it is therefore typically chosen using cross-validation. The main benefit of scaled lasso is that the penalty parameter is scale-free and can be predetermined from pure theoretical considerations. Nevertheless, scaled lasso performs poorly when there exist strong correlations between the predictors. As a remedy, we propose two different scaled elastic net (EN) formulations and derive convergent algorithms for their computation. The first formulation uses a conventional EN penalty whereas the second formulation differs from the former in that the â„“2-loss is not squared. The former approach is referred to as the scaled EN estimator and the latter as the square-root EN estimator. We illustrate via numerical examples and simulations that the proposed methods outperform the scaled lasso, especially in the presence of high mutual coherence in the feature space.",2017,"2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
Optimal segmentation of classification and prediction maps for monitoring forest condition with spectral and spatial information from hyperspectral data,"Fusion of spectral and spatial information has good potential for building highly accurate classification model for land cover and prediction model for biomass estimation. In this study, a new method with spectral-spatial this fusion and object-based segmentation for monitoring peat swamp forest condition is proposed. Peatland is a major CO2 emission source by peat burn, peat decomposition and forest fire. Remote sensing is effective tool for monitoring environmental condition of peatland and forest ecosystem. For the monitoring, forest type classification map and biomass distribution map are useful for understanding about the forest condition and to estimate mass volume of CO2 storage. In order to have enough accurate maps without overfitting problem, sparse discrimination analysis (SDA) was applied to spectral and spatial information from hyperspectral data for the classification model, and LASSO regression was applied for the biomass prediction model. Furthermore, to obtain the well-segmented maps, mean shift clustering as object-based segmentation was applied to those maps for identifying suitable class and biomass with majority voting in each segmentation. These proposed scheme improved classification and prediction accuracies and provides accurate segmented maps.",2014,2014 IEEE Geoscience and Remote Sensing Symposium
A Power and Prediction Analysis for Knockoffs with Lasso Statistics,"Knockoffs is a new framework for controlling the false discovery rate (FDR) in multiple hypothesis testing problems involving complex statistical models. While there has been great emphasis on Type-I error control, Type-II errors have been far less studied. In this paper we analyze the false negative rate or, equivalently, the power of a knockoff procedure associated with the Lasso solution path under an i.i.d. Gaussian design, and find that knockoffs asymptotically achieve close to optimal power with respect to an omniscient oracle. Furthermore, we demonstrate that for sparse signals, performing model selection via knockoff filtering achieves nearly ideal prediction errors as compared to a Lasso oracle equipped with full knowledge of the distribution of the unknown regression coefficients. The i.i.d. Gaussian design is adopted to leverage results concerning the empirical distribution of the Lasso estimates, which makes power calculation possible for both knockoff and oracle procedures.",2017,arXiv: Methodology
Understanding Hydrologic Processes and Correlations using Modeling and Machine Learning with Remote Sensing and In-Situ Wireless Sensor Network Data,"This work addresses three challenging issues about the overall applicability of hydrologic modelling. The first challenge is improving the collection of sub-surface data. Our approach uses a long-term deployment of wireless sensor network with environmental sensors. This approach is cost-effective when compared with the use of data-loggers and more flexible as it allows real-time monitoring of environmental variables. The plot scale environmental data is collected from our own WSN, deployed in western Pennsylvania, currently composed by 104 nodes and over 240 sensors including commercially available soil moisture, water potential and temperature sensors along with lab-made xylem sap flow sensors. 
The second challenge is improving the availability and accuracy of continuous streamflow time-series estimates. The hydrometric network is modelled as a sparse Gaussian graphical model where each site represents a node in a graph. The graph model will have an edge between two sites only when their streamflow time-series are conditionally dependent given the other sites. A novel algorithm is presented, estimating a sparse graph by imposing sparsity to the precision (covariance inverse) matrix via the Graphical Lasso algorithm. The resulting graph is used for inference and a second algorithm determines which gauges can be removed with the least loss of information. The estimated streamflow time-series have better accuracy that other methods based on geographic proximity (least distance) or marginal correlation. 
The third challenge is estimating the soil-water characteristics from biased and noisy observations of soil moisture. A novel method is presented for the simultaneous estimation of soil moisture and soil-related parameters. The simulation of soil moisture is performed using the Noah and the VIC models. The simulated site is a well-documented testbed in the state of Oklahoma. The calibration of the soil-related parameters uses Machine Learning techniques such as clustering, regression and classification, and soil-water correlations, providing physical and statistical constrains in the parameter space. Thus, the search is made within a reduced parameter space which makes the parameter calibration approach more effective and realistic. The performance of 
v 
the calibration algorithm is assessed regarding the quality of the soil moisture estimations while keeping the parameters in a feasible range",2019,
Tensor Decomposition With Generalized Lasso Penalties,"ABSTRACTWe present an approach for penalized tensor decomposition (PTD) that estimates smoothly varying latent factors in multiway data. This generalizes existing work on sparse tensor decomposition and penalized matrix decompositions, in a manner parallel to the generalized lasso for regression and smoothing problems. Our approach presents many nontrivial challenges at the intersection of modeling and computation, which are studied in detail. An efficient coordinate-wise optimization algorithm for PTD is presented, and its convergence properties are characterized. The method is applied both to simulated data and real data on flu hospitalizations in Texas and motion-capture data from video cameras. These results show that our penalized tensor decomposition can offer major improvements on existing methods for analyzing multiway data that exhibit smooth spatial or temporal features.",2015,Journal of Computational and Graphical Statistics
A group bridge approach for variable selection.,"In multiple regression problems when covariates can be naturally grouped, it is important to carry out feature selection at the group and within-group individual variable levels simultaneously. The existing methods, including the lasso and group lasso, are designed for either variable selection or group selection, but not for both. We propose a group bridge approach that is capable of simultaneous selection at both the group and within-group individual variable levels. The proposed approach is a penalized regularization method that uses a specially designed group bridge penalty. It has the oracle group selection property, in that it can correctly select important groups with probability converging to one. In contrast, the group lasso and group least angle regression methods in general do not possess such an oracle property in group selection. Simulation studies indicate that the group bridge has superior performance in group and individual variable selection relative to several existing methods.",2009,Biometrika
Asymptotic properties of Lasso in high-dimensional partially linear models,"We study the properties of the Lasso in the high-dimensional partially linear model where the number of variables in the linear part can be greater than the sample size. We use truncated series expansion based on polynomial splines to approximate the nonparametric component in this model. Under a sparsity assumption on the regression coefficients of the linear component and some regularity conditions, we derive the oracle inequalities for the prediction risk and the estimation error. We also provide sufficient conditions under which the Lasso estimator is selection consistent for the variables in the linear part of the model. In addition, we derive the rate of convergence of the estimator of the nonparametric function. We conduct simulation studies to evaluate the finite sample performance of variable selection and nonparametric function estimation.",2016,Science China Mathematics
A predictive model of revenue per available room in the Eastern Mediterranean using bilateral exchange rates,"In an era of exchange rate volatility, performance of hospitality industry assets may be highly sensitive to currency shocks which affect international demand, such as the fall in the value of the Russian Ruble in late 2014. We aim to create a systematic and meaningful predictive model to give insight for managers and investors in the hospitality industry in this region regarding which bilateral exchange rates to watch most closely and particularly the timing of anticipated effects. This study will examine the relationship between Revenue per Available Room (RevPAR), the key measure of operational performance in the hospitality industry, and bilateral exchange rates in eight countries of the Balkans and Eastern Mediterranean. The study methodology follows a LASSO penalised regression model for simultaneous and systematic selection of variables and lag lengths and estimation of model parameters.",2015,
Restricted LASSO and Double Shrinking,"In the context of multiple regression model, suppose that the vector parameter of interest \beta is subjected to lie in the subspace hypothesis H\beta = h, where this restriction is based on either additional information or prior knowledge. Then, the restricted estimator performs fairly well than the ordinary least squares one. In addition, when the number of variables is relatively large with respect to observations, the use of least absolute shrinkage and selection operator (LASSO) estimator is suggested for variable selection purposes. In this paper, we deffine a restricted LASSO estimator and configure three classes of LASSO-type estimators to fulfill both variable selection and restricted estimation. Asymptotic performance of the proposed estimators are studied and a simulation is conducted to analyze asymptotic relative efficiencies. The application of our result is considered for the prostate dataset where the expected prediction errors and risks are compared. It has been shown that the proposed shrunken LASSO estimators, resulted from double shrinking methodology, perform better than the classical LASSO.",2015,arXiv: Statistics Theory
Building Energy Use Surrogate Model Feature Selection â€“ A Methodology Using Forward Stepwise Selection and LASSO Regression Methods,"Statistical regression models were developed to permit the rapid modelling of large commercial office buildings within a single climate zone. The regression models are developed using a large number of building parameters and their hourly energy model simulated results. In previous building energy regression modelling, there is a research gap in selecting building parameters using statistical approaches. This paper investigates a feature selection method, including forward stepwise selection and LASSO regression, to identify building parameters that, together, have the most significant impact on total building energy load. The regression model, with 25 features selected through this methodology, predicts total energy load at 93.5% accuracy, on average.",2019,
Statistically modeling I - V characteristics of CNT-FET with LASSO,"With the advent of internet of things (IOT), the need for studying new material and devices for various applications is increasing. Traditionally we build compact models for transistors on the basis of physics. But physical models are expensive and need a very long time to adjust for non-ideal effects. As the vision for the application of many novel devices is not certain or the manufacture process is not mature, deriving generalized accurate physical models for such devices is very strenuous, whereas statistical modeling is becoming a potential method because of its data oriented property and fast implementation. In this paper, one classical statistical regression method, LASSO, is used to model the I - V characteristics of CNT-FET and a pseudo-PMOS inverter simulation based on the trained model is implemented in Cadence. The normalized relative mean square prediction error of the trained model versus experiment sample data and the simulation results show that the model is acceptable for digital circuit static simulation. And such modeling methodology can extend to general devices.",2017,Journal of Semiconductors
Head-to-Head Comparison and Evaluation of 92 Plasma Protein Biomarkers for Early Detection of Colorectal Cancer in a True Screening Setting.,"PURPOSE
Novel noninvasive blood-based screening tests are strongly desirable for early detection of colorectal cancer. We aimed to conduct a head-to-head comparison of the diagnostic performance of 92 plasma-based tumor-associated protein biomarkers for early detection of colorectal cancer in a true screening setting.


EXPERIMENTAL DESIGN
Among all available 35 carriers of colorectal cancer and a representative sample of 54 men and women free of colorectal neoplasms recruited in a cohort of screening colonoscopy participants in 2005-2012 (N = 5,516), the plasma levels of 92 protein biomarkers were measured. ROC analyses were conducted to evaluate the diagnostic performance. A multimarker algorithm was developed through the Lasso logistic regression model and validated in an independent validation set. The .632+ bootstrap method was used to adjust for the potential overestimation of diagnostic performance.


RESULTS
Seventeen protein markers were identified to show statistically significant differences in plasma levels between colorectal cancer cases and controls. The adjusted area under the ROC curves (AUC) of these 17 individual markers ranged from 0.55 to 0.70. An eight-marker classifier was constructed that increased the adjusted AUC to 0.77 [95% confidence interval (CI), 0.59-0.91]. When validating this algorithm in an independent validation set, the AUC was 0.76 (95% CI, 0.65-0.85), and sensitivities at cutoff levels yielding 80% and 90% specificities were 65% (95% CI, 41-80%) and 44% (95% CI, 24-72%), respectively.


CONCLUSIONS
The identified profile of protein biomarkers could contribute to the development of a powerful multimarker blood-based test for early detection of colorectal cancer.",2015,Clinical cancer research : an official journal of the American Association for Cancer Research
Identifying the white matter impairments among ART-naÃ¯ve HIV patients: a multivariate pattern analysis of DTI data,"AbstractObjectiveTo identify the white matter (WM) impairments of the antiretroviral therapy (ART)-naÃ¯ve HIV patients by conducting a multivariate pattern analysis (MVPA) of Diffusion Tensor Imaging (DTI) dataMethodsWe enrolled 33 ART-naÃ¯ve HIV patients and 32 Normal controls in the current study. Firstly, the DTI metrics in whole brain WM tracts were extracted for each subject and feed into the Least Absolute Shrinkage and Selection Operators procedure (LASSO)-Logistic regression model to identify the impaired WM tracts. Then, Support Vector Machines (SVM) model was constructed based on the DTI metrics in the impaired WM tracts to make HIV-control group classification. Pearson correlations between the WM impairments and HIV clinical statics were also investigated.ResultsExtensive HIV-related impairments were observed in the WM tracts associated with motor function, the corpus callosum (CC) and the frontal WM. With leave-one-out cross validation, accuracy of 83.08% (P=0.002) and the area under the Receiver Operating Characteristic curve of 0.9110 were obtained in the SVM classification model. The impairments of the CC were significantly correlated with the HIV clinic statics.ConclusionThe MVPA was sensitive to detect the HIV-related WM changes. Our findings indicated that the MVPA had considerable potential in exploring the HIV-related WM impairments.Key pointsâ€¢ WM impairments along motor pathway were detected among the ART-naÃ¯ve HIV patients
 â€¢ Prominent HIV-related WM impairments were observed in CC and frontal WM
 â€¢ The impairments of CC were significantly related to the HIV clinic statics
 â€¢ The CC might be susceptible to immune dysfunction and HIV replication
 â€¢ Multivariate pattern analysis had potential for studying the HIV-related white matter impairments",2017,European Radiology
Valid simultaneous inference in high-dimensional settings (with the HDM package for R),"Due to the increasing availability of high-dimensional empirical applications in many research disciplines, valid simultaneous inference becomes more and more important. For instance, high-dimensional settings might arise in economic studies due to very rich data sets with many potential covariates or in the analysis of treatment heterogeneities. Also the evaluation of potentially more complicated (non-linear) functional forms of the regression relationship leads to many potential variables for which simultaneous inferential statements might be of interest. Here we provide a review of classical and modern methods for simultaneous inference in (high-dimensional) settings and illustrate their use by a case study using the R package hdm. The R package hdm implements valid joint powerful and efficient hypothesis tests for a potentially large number of coeffcients as well as the construction of simultaneous confidence intervals and, therefore, provides useful methods to perform valid post-selection inference based on the LASSO.",2019,arXiv: Econometrics
Factors Associated With Nutritional Risk Among Homebound Older Adults With Depressive Symptoms.,"OBJECTIVES
This study used the Evans model of public health determinants to identify factors associated with nutritional risk in older adults.


DESIGN
The Evans model domains (physical and mental well-being, social/environmental statuses, individual choice, and economic security) were measured in a sample of homebound older adults. Regularized logistic regression analysis with LASSO penalty function was used to determine the strongest domain of the Evans model. Using traditional logistic regression, individual variables across all domains were compared to identify the significant predictors.


SETTING
Older adults receiving home meal services were referred to the study by community program staff.


PARTICIPANTS
Participants included 164 homebound older adults (age > 60) who endorsed at least one gateway symptom of depression.


MEASUREMENTS
Measurements: Nutritional risk was determined using the Mini Nutritional Assessment. Domains of the Evans model were measured using the MAI Medical Condition Checklist, items from the IADL scale, the Structured Clinical Interview for DSM-IV Axis I Disorders, the Duke Social Support Index, living arrangements, marital status, the Alcohol Use Disorders Identification Test, items from the SCID Screening Module, and a self-report of perceived financial security.


RESULTS
Poor mental well-being, defined by a diagnosis of major depressive disorder, was identified as the strongest Evans model domain in the prediction of nutritional risk. When each variable was independently evaluated across domains, instrumental support (Waldâ€™s Z=-2.24, p=0.03) and a history of drug use (Waldâ€™s Z=-2.40, p=0.02) were significant predictors.


CONCLUSIONS
The Evans model is a useful conceptual framework for understanding nutritional health, with the mental domain found to be the strongest domain predictor of nutritional risk. Among individual variables across domains, having someone to help with shopping and food preparation and a history of drug use were associated with lower nutritional risk. These analyses highlight potential targets of intervention for nutritional risk among older adults.",2016,The Journal of frailty & aging
Robust Generalized Fuzzy Systems Training from High-Dimensional Time-Series Data using Local Structure Preserving PLS,"Establishing fuzzy models from time-series data with predictive capabilities for numerical targets typically requires dimension reduction techniques to overcome red the severe curse of dimensionality effects. Linear projection methods are promising candidates in this context as they â€” unlike non-linear dimension reduction techniques â€” preserve interpretability of the resulting models. However, linear projections do not reveal the inherent (non-linear, local) cluster structure of the data and are thus not ideally suited for the identification of fuzzy rule bases. To overcome this limitation, we here present a new fuzzy modeling approach that combines generalized fuzzy systems modeling with a local structure preserving variant of partial least squares (PLS). In contrast to ordinary PLS, our approach maps a weighted (adjacency) graph on the directions associated with high co-variance with the response in order to emphasizes local data structures when constructing the latent variable (LV) space. This operates into the direction from which the (training of the) fuzzy model benefits, as therein local regions are represented by sub-models in form of generalized TS fuzzy rules. The local structure preserving LV space is obtained by solving a new penalized objective function, which assures $global\ optimality$ of the solutions by virtue of the specific properties of the Laplacian matrix. Local regions are characterized in two ways, i.) through nearest neighbor points (assuming fixed local region sizes) and ii.) through density regions identified by clustering (achieving variable local region sizes). To establish a robust time-series based forecast model, the training of a generalized TS fuzzy model is conducted in the LV space with reduced dimensionality. It is realized by an iterative robust version of $Gen \text{-} Smart \text{-} EFS$ 
, allowing multiple passes over the complete data sets until convergence of the antecedent space. Consequent parameters are estimated by a fuzzily weighted elastic net approach, embedding a convex combination of ridge regression and Lasso to achieve robust solutions also in case of ill-posed problems and meeting the (more stable and interpretable) local learning spirit. The new approach is termed as $LS \text{-} PLS \text{-} Fuzzy$ 
, short for $local\ structure\ preserving\ partial\ least\ squares\ fuzzy\ regression$ 
). An extension of the new approach also takes into account the non-local structure of the data and combines it in a clever way with the local structure preserving aspect through a combined Laplacian matrix in the penalization term; termed as LSNLS PLS-Fuzzy. It was successfully evaluated on three real-world application scenarios including a total of 11 time-series based prediction modeling problems with different proportions between the number of training samples and original input dimensionality. Our results show significantly improved model accuracy compared to i) related SoA modeling approaches, ii) an alternative dimension reduction technique, and iii) when using conventional PLS and/or non-generalized fuzzy rules.",2019,IEEE Transactions on Fuzzy Systems
Semiparametric IV Estimation and Model Selection with Weak Instruments and Heteroskedasticity,"This paper proposes a new two stage least squares (2SLS) estimator which is consistent and asymptotically normal in the presence of many weak instruments and heteroskedasticity. The rst stage of the estimator consists of two components: rst, an adaptive absolute shrinkage and selection operator (LASSO) that selects the instruments; and second, an OLS regression with the selected regressors. The adaptive LASSO is constructed to allow the possibility of either a nonparametric or parametric model. The second stage uses an OLS regression with the tted values of the rst stage. The methodology exploits the model selection benets of the adaptive LASSO, reduces its post-selection bias, and is the rst time a nonparametric adaptive LASSO is analyzed in the presence of instrument weakness.",2011,
Predicting the Trends of Social Events on Chinese Social Media,"Growing interest in social events on social media came along with the rapid development of the Internet. Social events that occur in the ""real"" world can spread on social media (e.g., Sina Weibo) rapidly, which may trigger severe consequences and thus require the government's timely attention and responses. This article proposes to predict the trends of social events on Sina Weibo, which is currently the most popular social media in China. Based on the theories of social psychology and communication sciences, we extract an unprecedented amount of comprehensive and effective features that relate to the trends of social events on Chinese social media, and we construct the trends of prediction models by using three classical regression algorithms. We found that lasso regression performed better with the precision 0.78 and the recall 0.88. The results of our experiments demonstrated the effectiveness of our proposed approach.",2017,"Cyberpsychology, behavior and social networking"
Testing for Differences in Gaussian Graphical Models: Applications to Brain Connectivity,"Functional brain networks are well described and estimated from data with Gaussian Graphical Models (GGMs), e.g. using sparse inverse covariance estimators. Comparing functional connectivity of subjects in two population calls for comparing these estimated GGMs. We study the problem of identifying differences in Gaussian Graphical Models (GGMs) known to have similar structure. We aim to characterize the uncertainty of differences with confidence intervals obtained using a para-metric distribution on parameters of a sparse estimator. Sparse penalties enable statistical guarantees and interpretable models even in high-dimensional and low-number-of-samples settings. Quantifying the uncertainty of the parameters selected by the sparse penalty is an important question in applications such as neuroimaging or bioinformatics. Indeed, selected variables can be interpreted to build theoretical understanding or to make therapeutic decisions. Characterizing the distributions of sparse regression models is inherently challenging since the penalties produce a biased estimator. Recent work has shown how one can invoke the sparsity assumptions to effectively remove the bias from a sparse estimator such as the lasso. These distributions can be used to give us confidence intervals on edges in GGMs, and by extension their differences. However, in the case of comparing GGMs, these estimators do not make use of any assumed joint structure among the GGMs. Inspired by priors from brain functional connectivity we focus on deriving the distribution of parameter differences under a joint penalty when parameters are known to be sparse in the difference. This leads us to introduce the debiased multi-task fused lasso. We show that we can debias and characterize the distribution in an efficient manner. We then go on to show how the debiased lasso and multi-task fused lasso can be used to obtain confidence intervals on edge differences in Gaussian graphical models. We validate the techniques proposed on a set of synthetic examples as well as neuro-imaging dataset created for the study of autism.",2016,
A General Family of Penalties for Combining Differing Types of Penalties in Generalized Structured Models,"Penalized estimation has become an established tool for regularization and model selection in regression models. 
A variety of penalties with specific features are available 
and effective algorithms for specific penalties have been proposed. 
But not much is available to fit models that call for a combination of different penalties. 
When modeling rent data, which will be considered as an example, various types of predictors call for a combination of a Ridge, a grouped Lasso and a Lasso-type penalty within one model. 
Algorithms that can deal with such problems, are in demand. 
We propose to approximate penalties that are (semi-)norms of scalar linear transformations of the coefficient vector in generalized structured models. 
The penalty is very general such that the Lasso, the fused Lasso, the Ridge, the smoothly clipped absolute deviation penalty (SCAD), the elastic net and many more penalties are embedded. 
The approximation allows to combine all these penalties within one model. 
The computation is based on conventional penalized iteratively re-weighted least squares (PIRLS) algorithms and hence, easy to implement. 
Moreover, new penalties can be incorporated quickly. 
The approach is also extended to penalties with vector based arguments; that is, to penalties with norms of linear transformations of the coefficient vector. 
Some illustrative examples and the model for the Munich rent data show promising results.",2013,
Accelerated hyperfractionated radiochemotherapy with temozolomide is equivalent to normofractionated radiochemotherapy in a retrospective analysis of patients with glioblastoma,"BackgroundCurrent standard of treatment for newly diagnosed patients with glioblastoma (GBM) is surgical resection with adjuvant normofractionated radiotherapy (NFRT) combined with temozolomide (TMZ) chemotherapy. Hyperfractionated accelerated radiotherapy (HFRT) which was known as an option from randomized controlled trials before the temozolomide era has not been compared to the standard therapy in a randomized setting combined with TMZ.MethodsData of 152 patients with newly diagnosed GBM treated from 10/2004 until 7/2018 at a single tertiary care institution were extracted from a clinical database and retrospectively analyzed. Thirty-eight patients treated with NFRT of 60â€‰Gy in 30 fractions (34 with simultaneous and 2 with sequential TMZ) were compared to 114 patients treated with HFRT of 54.0â€‰Gy in 30 fraction of 1.8â€‰Gy twice daily (109 with simultaneous and 3 with sequential TMZ). The association between treatment protocol and other variables with overall survival (OS) was assessed using univariable and multivariable Cox regression analysis; the latter was performed using variables selected by the LASSO method.ResultsMedian overall survival (OS) was 20.3â€‰month for the entire cohort. For patients treated with NFRT median OS was 24.4â€‰months compared to 18.5â€‰months in patients treated with HFRT (pÂ =â€‰0.131). In univariable regression analysis the use of dexamethasone during radiotherapy had a significant negative impact on OS in both patient groups, HR 2.21 (95% CI 1.47â€“3.31, pÂ =â€‰0.0001). In multivariable analysis adjusted for O6-methylguanine-DNA methyl-transferase (MGMT) promotor methylation status, salvage treatment and secondary GBM, the use of dexamethasone was still a negative prognostic factor, HR 1.95 (95% CI 1.21â€“3.13, pÂ =â€‰0.006). Positive MGMT-methylation status and salvage treatment were highly significant positive prognostic factors. There was no strong association between treatment protocol and OS (pÂ =â€‰0.504).ConclusionsOur retrospective analysis supports the hypothesis of equivalence between HFRT and the standard protocol of treatment for GBM. For those patients who are willing to obtain the benefit of shortening the course of radiochemotherapy, HFRT may be an alternative with comparable efficacy although it was not yet tested in a large prospective randomized study against the current standard. The positive influence of salvage therapy and negative impact of concomitant use of corticosteroids should be addressed in future prospective trials. To confirm our results, we plan to perform a pooled analysis with other tertiary clinics in order to achieve better statistical reliability.",2019,"Radiation Oncology (London, England)"
Gap Safe screening rules for faster complex-valued multi-task group Lasso,"Linear regression with sparsity-inducing penalties is a popular tool for high-dimensional problems such as source localization or denoising [1, 4]. The Group Lasso is a particular choice of convex penalty that considers the `2,1 norm to promote group (or block) sparsity patterns. Since no general closed-form solution is available for this problem, iterative solvers are needed. This can lead to very slow convergence especially if the problem is ill-conditioned or if the dimension of the problem is particularly large. Safe screening rules [5] (see also [15]) and in particular dynamic ones [2, 3] speed-up the optimization process by progressively discarding regressors identified as irrelevant. In this work, we consider the case where features and observations can be complex-valued, a common case in signal processing when working with time-frequency operators. We derive Gap Safe screening rules in this context and propose a block coordinate descent (BCD) optimization strategy [14, 7]. In practice, we illustrate significant speed-ups in terms of convergence compared to classical solvers on a neuroscience problem, namely the problem of source localization using magnetoand electroencephalography (M/EEG). I. The complex-valued multi-task group Lasso In the following, n is the number of observations (or sensors), q the number of tasks (or time instants), and p the number of features (or variables). Given observations Y âˆˆ CnÃ—q, a design matrix (or forward operator) X âˆˆ CnÃ—p and a set of groups G (i.e., a partition of {1, . . . , p}) the complex-valued multitask group Lasso solves PÎ»(Î²) : Î²âˆ— âˆˆ arg min Î²âˆˆCpÃ—q 1 2 â€–Y âˆ’XÎ²â€– 2 F + Î» â€–Î²â€–F,1, where Î» > 0 is a regularization parameter, â€–Î²â€–F,1 = âˆ‘ gâˆˆG â€–Î²gâ€–F , Î²g denotes the sub-matrix of Î² composed of rows whose indices are in g and â€–Aâ€–F = ( âˆ‘ i,j |Ai,j |) 1 2 denotes the Frobenius norm of A. For z, zâ€² âˆˆ C, instead of the Hermitian inner product ã€ˆz, zã€‰H = âˆ‘d i=1 zizÌ„ â€² i we use ã€ˆz, zâ€²ã€‰ = 1 2 âˆ‘d i=1(zizÌ„ â€² i + zÌ„iz i). Contrary to ã€ˆÂ·, Â·ã€‰H, it is real-valued and enables us to define the Fenchel conjugate of a function f : C â†’ R as fâˆ— : u 7â†’ supzâˆˆCdã€ˆu, zã€‰ âˆ’ f(u). For X,Y âˆˆ CdÃ—d â€² , we consider ã€ˆX,Y ã€‰ = 1 2 tr(X HY + Y HX). Using this we can prove that â€–Â·â€–F,1 = â€–Â·â€–F,âˆž, and that 1 2 â€–Â·â€– 2 F is its self Fenchel conjugate. The dual problem of PÎ»(Î²) then reads: sup Î¸âˆˆCnÃ—q inf Î²âˆˆCpÃ—q Î¼âˆˆCnÃ—q 1 2 â€–Î¼â€– 2 F + Î» â€–Î²â€–F,1 + ã€ˆÎ¸, Î¼âˆ’ Y +XÎ²ã€‰ . With the properties of the Fenchel transform, this amounts to solving DÎ»(Î¸): max Î¸âˆˆâˆ†X â€–Y â€–F /2 âˆ’ Î» 2â€–Y/Î»âˆ’ Î¸â€–F /2, introducing the dual feasible set âˆ†X = { Î¸ âˆˆ CnÃ—q | â€–XÎ¸â€–F,âˆž â‰¤ 1 } . A popular iterative solver for the multi-task group lasso uses a BCD scheme [13]: at each iteration k, for each group g successively, PÎ»(Î²) is optimized w.r.t. Î²g. II. Gap safe screening rules for complex Lasso Let Î²âˆ— be an optimal solution of PÎ»(Î²) and Î¸âˆ— be the (unique) dual solution, both linked through Y = XÎ²âˆ— + Î»Î¸âˆ—. Fermatâ€™s rule reads: âˆ€g,X g Î¸ âˆˆ âˆ‚â€–Î² gâ€–F = { Î²âˆ— g/ âˆ¥âˆ¥Î²âˆ— gâˆ¥âˆ¥F , if Î²âˆ— g 6= 0 Bâ€–Â·â€–F , otherwise , where âˆ‚â€–Â·â€–F is defined relatively to ã€ˆÂ·, Â·ã€‰, and Bâ€–Â·â€–F is the â€–Â·â€–F unit ball. We thus have â€–XH g Î¸â€–F < 1â‡’ Î²âˆ— g = 0. Î¸âˆ— being unknown, this rule has to be relaxed: given a safe region C containing Î¸âˆ—, we can upper bound â€–XH g Î¸â€–F and the rule becomes supÎ¸âˆˆCâ€–X g Î¸â€–F < 1â‡’ Î²âˆ— g = 0. In a series of work [6, 10, 11] so called Gap Safe rule have been proposed. It uses at iteration k the safe ball centered on Î¸k = (Y âˆ’XÎ²k)/(Î»Î±k) (with Î±k s.t. Î¸k âˆˆ âˆ†X), with radius âˆš 2(PÎ»(Î²k)âˆ’DÎ»(Î¸k))/Î»2. To ensure that DÎ»(Î¸k) is increasing, we slightly modify this rule, and update Î¸k only when DÎ»(Î¸k) > DÎ»(Î¸kâˆ’1). We use Î¸0 = Y/Î»max, where Î»max = â€–XHY â€–F,âˆž. III. Numerical experiments The M/EEG inverse problem with `2,1 regularization leads to a particular case of multi-task group Lasso [12]: in this context, Y is a matrix of sensor measurements (n signals of length q), X is the composition of the forward operator (encoding the electromagnetic dependency between source amplitudes and measurements) and a time-frequency decomposition operator, and Î² is the complex-valued matrix of time-frequency coefficients [9]. We adopt the free orientation setting which leads to the estimation of a vector field: blocks of three consecutives rows of Î² represent the activity of one source, decomposed over three orthogonal spatial dimensions. Imposing a â€–Â·â€–F,1 group penalty over these blocks results in a solution where only a few sources are active (i.e., a few rows of Î²âˆ— are non zero), which is a desired property from a biological standpoint. We use data from the MNE dataset [8], with 302 MEG sensors, 7498 sources (22494 oriented dipoles), and 181 time instants which corresponds to about 300ms of event related field data following an auditory stimulation in the left ear. We decompose the signals over 1518 time-frequency atoms. We evaluate the acceleration obtained with dynamic screening for different values of Î» (screening every 10 pass over all groups with Î²0 = 0). Figure 1 shows objective convergence for Î» = Î»max/4. Screening restricts the BCD steps to a smaller and smaller set of sources, resulting in significant acceleration. As we see in Figure 2, a high Î» results in very sparse solutions, for which screening is well-suited. On the contrary, for a very low Î», few features are discarded and screening does not greatly speed up the convergence.",2017,
