title,abstract,year,journal
Temporal Matrix Factorization for Tracking Concept Drift in Individual User Preferences,"The matrix factorization (MF) technique has been widely adopted for solving the rating prediction problem in recommender systems. The MF technique utilizes the latent factor model to obtain static user preferences (user latent vectors) and item characteristics (item latent vectors) based on historical rating data. However, in the real world, user preferences are not static but full of dynamics. Though there are several previous works that addressed this time-varying issue of user preferences, it seems (to the best of our knowledge) that none of them are specifically designed for tracking concept drift in individual user preferences. Motivated by this, we develop a temporal MF approach for tracking concept drift in each individual user latent vector. There are two key innovative steps in our approach: 1) we develop a modified stochastic gradient descent method to learn an individual user latent vector at each time step and 2) by Lasso regression, we learn a linear model for the transition of the individual user latent vectors. We test our method on a synthetic data set and several real data sets. In comparison with the original MF, our experimental results show that our temporal method is able to achieve lower root mean square errors (RMSEs) for both the synthetic and real data sets. One interesting finding is that the performance gain in RMSE is mostly from those users who indeed have concept drift in their user latent vectors at the time of prediction. In particular, for the synthetic data set and the Ciao data set, there are quite a few users with that property and the performance gains for these two data sets are roughly 20% and 5%, respectively.",2018,IEEE Transactions on Computational Social Systems
An Evaluation of the PrediXcan Method for the Identification of Lipid Associated Genes,"PrediXcan, an imputed gene expression-trait association method, was compared to multiple linear regressions (MLR) of single nucleotide polymorphisms (SNPs) using the quantitative phenotypes serum total cholesterol (TC), low-density lipoprotein cholesterol (LDL), high-density lipoprotein cholesterol (HDL) and triglycerides (TG). The gene expression prediction models were trained using transcriptomeand genome-wide data from Depression Genes and Networks (DGN whole blood) and Genotype-Tissue Expression (GTEx) Project (GTEx whole blood, GTEx pancreas and GTEx liver). Linear combinations of the effect sizes derived using elastic net or least absolute shrinkage and selection operator (LASSO) with genotypes from 1304 European patients from the Diabetes Control and Complications Trial (DCCT) were used to estimate the genetically regulated expression (GReX) for genes. Different gene expression predictors were present in each training set. The 10-fold cross-validated predictive performance, estimated GReX, and p values from associations for matched genes were weakly correlated across training sets and strongly correlated for models derived using elastic net and LASSO. MLR models had more significant associations than PrediXcan models and larger inflation factors for p values. A comparison of p values for matched genes between PrediXcan and MLR models showed weak correlations but strong evidence for LDL and HDL associations with genes at locus 1p13.3 and 16q13, respectively.",2018,
Measurement error correction in the least absolute shrinkage and selection operator model when validation data are available,"Measurement of serum biomarkers by multiplex assays may be more variable as compared to single biomarker assays. Measurement error in these data may bias parameter estimates in regression analysis, which could mask true associations of serum biomarkers with an outcome. The Least Absolute Shrinkage and Selection Operator (LASSO) can be used for variable selection in these high-dimensional data. Furthermore, when the distribution of measurement error is assumed to be known or estimated with replication data, a simple measurement error correction method can be applied to the LASSO method. However, in practice the distribution of the measurement error is unknown and is expensive to estimate through replication both in monetary cost and need for greater amount of sample which is often limited in quantity. We adapt an existing bias correction approach by estimating the measurement error using validation data in which a subset of serum biomarkers are re-measured on a random subset of the study sample. We evaluate this method using simulated data and data from the Tucson Epidemiological Study of Airway Obstructive Disease (TESAOD). We show that the bias in parameter estimation is reduced and variable selection is improved.",2019,Statistical Methods in Medical Research
Comparison of analyses of the XVth QTLMAS common dataset III: Genomic Estimations of Breeding Values,"BackgroundThe QTLMAS XVth dataset consisted of pedigree, marker genotypes and quantitative trait performances of animals with a sib family structure. Pedigree and genotypes concerned 3,000 progenies among those 2,000 were phenotyped. The trait was regulated by 8 QTLs which displayed additive, imprinting or epistatic effects. The 1,000 unphenotyped progenies were considered as candidates to selection and their Genomic Estimated Breeding Values (GEBV) were evaluated by participants of the XVth QTLMAS workshop. This paper aims at comparing the GEBV estimation results obtained by seven participants to the workshop.MethodsFrom the known QTL genotypes of each candidate, two ""true"" genomic values (TV) were estimated by organizers: the genotypic value of the candidate (TGV) and the expectation of its progeny genotypic values (TBV). GEBV were computed by the participants following different statistical methods: random linear models (including BLUP and Ridge Regression), selection variable techniques (LASSO, Elastic Net) and Bayesian methods. Accuracy was evaluated by the correlation between TV (TGV or TBV) and GEBV presented by participants. Rank correlation of the best 10% of individuals and error in predictions were also evaluated. Bias was tested by regression of TV on GEBV.ResultsLarge differences between methods were found for all criteria and type of genetic values (TGV, TBV). In general, the criteria ranked consistently methods belonging to the same family.ConclusionsBayesian methods - A<B<C<CÏ€ - were the most efficient whatever the criteria and the True Value considered (with the notable exception of the MSEP of the TBV). The selection variable procedures (LASSO, Elastic Net and some adaptations) performed similarly, probably at a much lower computing cost. The TABLUP, which combines BayesB and GBLUP, generally did well. The simplest methods, GBLUP or Ridge Regression, and even worst, the fixed linear model, were much less efficient.",2012,BMC Proceedings
The linkage disequilibrium lasso for SNP selection in genetic association studies,"by SAMUEL G. YOUNKIN A rapid increase in the number of human genetic variants available for study has led genetic epidemiologists to an agnostic approach to disease-gene mapping. In recent years the field has been dominated by the genome-wide association study in which the entire genome is interrogated for single nucleotide polymorphisms (SNPs) responsible for disease. This method has resulted in the identification of only a small portion of the expected disease susceptibility sites, and it is our belief that many of the remaining SNPs contribute such a small amount to overall heritability that studies of this type cannot overcome the inherent difficulties that arise from the vast amount of multiple testing associated with the agnostic approach. Here we develop a statistical method designed for a genetic association signal that is more representative of the remaining disease susceptibility SNPs. The genetic association signal that we seek is no longer a peak, because the increase in SNP density, coupled with low effect sizes give rise to a plateau-like signal with gaps. We address this by formulating our method as a penalized least squares regression estimator based on the linkage disequilibrium present between SNPs in the same haplotype block. The method which we call the LD LASSO is an adaptation of the fused LASSO used for subset selection in a regression framework when the signal is sparse and block-like. We implement this method in the R package ldlasso, and make it available to all. We demonstrate the use of the method by examining six regions on chromosome 8 suspected to contain variants associated with Late Onset Alzheimerâ€™s Disease.",2011,
Bayesian composite quantile regression,"One advantage of quantile regression, relative to the ordinary least-square (OLS) regression, is that the quantile regression estimates are more robust against outliers and non-normal errors in the response measurements. However, the relative efficiency of the quantile regression estimator with respect to the OLS estimator can be arbitrarily small. To overcome this problem, composite quantile regression methods have been proposed in the literature which are resistant to heavy-tailed errors or outliers in the response and at the same time are more efficient than the traditional single quantile-based quantile regression method. This paper studies the composite quantile regression from a Bayesian perspective. The advantage of the Bayesian hierarchical framework is that the weight of each component in the composite model can be treated as open parameter and automatically estimated through Markov chain Monte Carlo sampling procedure. Moreover, the lasso regularization can be naturally incorporated into the model to perform variable selection. The performance of the proposed method over the single quantile-based method was demonstrated via extensive simulations and real data analysis.",2015,Journal of Statistical Computation and Simulation
TH-AB-304-07: A Two-Stage Signature-Based Data Fusion Mechanism to Predict Radiation Pneumonitis in Patients with Non-Small-Cell Lung Cancer (NSCLC),"Purpose: For NSCLC radiotherapy, toxicity outcomes such as radiation pneumonitis â‰¥G2 (RP2) may depend on patientsâ€™ physical, clinical, biological and genomic characteristics, and on biomarkers measured during the course of radiotherapy. This can include 100s of predictors. To reduce complexity, a two-step, signature-based data fusion mechanism was developed to estimate a relationship between patient specific characteristics and the probability of RP2 in terms of a modifying effect on mean lung dose (MLD). Methods: Data came from 82 NSCLC patients, 15 with RP2. Besides MLD, each had 179 predictors including 10 clinical factors (eg, age, gender, KPS), cytokines before (30) and during (30) treatment, microRNAs (49), and single-nucleotide polymorphisms (SNPs) (60). In stage1, cytokines, microRNAs, and SNPs were used to build separate â€œsignaturesâ€ via ridge regression. In stage2, a logistic regression predictive model for RP2 was determined in terms of MLD, the other clinical factors, and the signatures using the least absolute shrinkage and selection operator (LASSO). Leave-one-out cross-validation was conducted. This was all implemented via â€˜glmnetâ€™ in the R programming environment. Results: For stage1, signatures modifying the effect of MLD for cytokine_pre, cytokine_during, microRNA and SNP included 2, 19, 3, 12 important predictors, respectively. For stage2, only the cytokine_during and SNP signatures remained as important modifying effects to MLD. The cross-validated area under curve (AUC) reaches 0.81 (95% CI 0.70â€“0.89 based on 2000 stratified bootstrap replicates); significantly better than a null value of 0.50 (p<0.01). Conclusions: As implemented here, the two-stage, signature-based data fusion mechanism approach includes many patient specific measurements in generation of the signatures (a characteristic of ridge regression), then only includes important signatures and other clinical factors for RP2 prediction (a characteristic of LASSO). This potentially more intuitive approach to handling high dimensional predictors could be an important component of decision support for personalized adaptive radiation treatment.",2015,Medical Physics
Restricted Eigenvalue Properties for Correlated Gaussian Designs,"Methods based on l1-relaxation, such as basis pursuit and the Lasso, are very popular for sparse regression in high dimensions. The conditions for success of these methods are now well-understood: (1) exact recovery in the noiseless setting is possible if and only if the design matrix X satisfies the restricted nullspace property, and (2) the squared l2-error of a Lasso estimate decays at the minimax optimal rate k log p / n, where k is the sparsity of the p-dimensional regression problem with additive Gaussian noise, whenever the design satisfies a restricted eigenvalue condition. The key issue is thus to determine when the design matrix X satisfies these desirable properties. Thus far, there have been numerous results showing that the restricted isometry property, which implies both the restricted nullspace and eigenvalue conditions, is satisfied when all entries of X are independent and identically distributed (i.i.d.), or the rows are unitary. This paper proves directly that the restricted nullspace and eigenvalue conditions hold with high probability for quite general classes of Gaussian matrices for which the predictors may be highly dependent, and hence restricted isometry conditions can be violated with high probability. In this way, our results extend the attractive theoretical guarantees on l1-relaxations to a much broader class of problems than the case of completely independent or unitary designs.",2010,J. Mach. Learn. Res.
Katyusha: Accelerated Variance Reduction for Faster SGD,"We consider minimizing f(x) that is an average of n convex, smooth functions fi(x), and provide the first direct stochastic gradient method Katyusha that has the accelerated convergence rate. It converges to an Îµ-approximate minimizer using O((n + âˆš nÎº) Â· log f(x0)âˆ’f(x âˆ—) Îµ ) stochastic gradients where Îº is the condition number. Katyusha is a primal-only method, supporting proximal updates, non-Euclidean norm smoothness, mini-batch sampling, as well as non-uniform sampling. It also resolves the following open questions in machine learning â€¢ If f(x) is not strongly convex (e.g., Lasso, logistic regression), Katyusha gives the first stochastic method that achieves the optimal 1/ âˆš Îµ rate. â€¢ If f(x) is strongly convex and each fi(x) is â€œrank-oneâ€ (e.g., SVM), Katyusha gives the first stochastic method that achieves the optimal 1/ âˆš Îµ rate. â€¢ If f(x) is not strongly convex and each fi(x) is â€œrank-oneâ€ (e.g., L1SVM), Katyusha gives the first stochastic method that achieves the optimal 1/Îµ rate. The main ingredient in Katyusha is a novel â€œnegative momentum on top of momentumâ€ that can be elegantly coupled with the existing variance reduction trick for stochastic gradient descent. As a result, since variance reduction has been successfully applied to fast growing list of practical problems, our paper implies that one had better hurry up and give Katyusha a hug in each of them, in hoping for a faster running time also in practice.",2016,ArXiv
A bias-variance based heuristic for constructing a hybrid logistic regression-naÃ¯ve Bayes model for classification,"Abstract Discriminative classifiers tend to have lower asymptotic classification errors, while generative classifiers can be more accurate when the training set size is small. In this paper, we examine the construction of hybrid models from categorical data, where we use logistic regression (LR) as a discriminative component, and naive Bayes (NB) as a generative component. We adopt a bias-variance tradeoff based strategy, with the objective of minimizing the sum of these two errors. Specifically, the proposed heuristic consists of functions of training sample size and conditional dependence among features. These functions serve as proxies for model variance and model bias. We implement our method on 25 different classification datasets, and find that the hybrid model does better than pure LR and pure NB. Our proposed method is competitive with random forest. Although the hybrid model fails to beat LASSO in predictive performance, as suggested by the experimental results, the difference appears to be insignificant when the number of features is small. Also, the hybrid model requires less training time than LASSO, which makes it more attractive when the training time is a big concern.",2020,Int. J. Approx. Reason.
Proximal Algorithms in Statistics and Machine Learning,"In this paper we develop proximal methods for statistical learning. Proximal point algorithms are useful in statistics and machine learning for obtaining optimization solutions for composite functions. Our approach exploits closed-form solutions of proximal operators and envelope representations based on the Moreau, Forward-Backward, Douglas-Rachford and Half-Quadratic envelopes. Envelope representations lead to novel proximal algorithms for statistical optimisation of composite objective functions which include both non-smooth and non-convex objectives. We illustrate our methodology with regularized Logistic and Poisson regression and non-convex bridge penalties with a fused lasso norm. We provide a discussion of convergence of non-descent algorithms with acceleration and for non-convex functions. Finally, we provide directions for future research.",2015,ArXiv
Linear Regression Modelling on Epigallocatechin-3-gallate Sensor Data for Green Tea,"In this paper, linear regression machine learning techniques are applied to determine the quality of green tea samples. The data set is obtained by applying Differential Pulse Voltammetry (DPV) on green tea samples using Epigallocatechin-3-gallate (EGCG) specific sensor based on Molecular Imprinted Polymer (MIP) technique. Multiple linear regression models have been developed using this dataset that gives more hidden insight of the dataset and helps to find the input feature importance out of it. Regularization techniques are applied on linear regression like Ridge regression (L2 Penalty), Lasso regression (L1 Penalty) and ElasticNet regression (combination of L1 and L2 Penalty) considered to reduce overfitting of the model and to provide better prediction. The variation of cross validation score vs regularization parameter for different regularized techniques of linear regression are also taken under consideration and best value of the regularization parameter is calculated to develop the model for getting better prediction with high accuracy. From the result obtained from model metrics, a clear picture is portrayed how lasso regression performs better than ridge regression for this dataset and eliminates the less important features to develop the model as sparsity can be useful in practice if we have a high dimensional dataset with many features that are not effective for modelling. The beauty of ElasticNet Regression model is also highlighted how both L1 and L2 penalty go hand in hand to give prediction at a high accuracy.",2018,2018 Fourth International Conference on Research in Computational Intelligence and Communication Networks (ICRCICN)
On the Exponentially Weighted Aggregate with the Laplace Prior,"In this paper, we study the statistical behaviour of the Exponentially Weighted Aggregate (EWA) in the problem of high-dimensional regression with fixed design. Under the assumption that the underlying regression vector is sparse, it is reasonable to use the Laplace distribution as a prior. The resulting estimator and, specifically, a particular instance of it referred to as the Bayesian lasso, was already used in the statistical literature because of its computational convenience, even though no thorough mathematical analysis of its statistical properties was carried out. The present work fills this gap by establishing sharp oracle inequalities for the EWA with the Laplace prior. These inequalities show that if the temperature parameter is small, the EWA with the Laplace prior satisfies the same type of oracle inequality as the lasso estimator does, as long as the quality of estimation is measured by the prediction loss. Extensions of the proposed methodology to the problem of prediction with low-rank matrices are considered.",2016,ArXiv
1 Variable Selection in Restricted Linear Regression Models,"The use of prior information in the linear regression is well known to provide more efficient estimators of regression coefficients. The methods of non-stochastic restricted regression estimation proposed by Theil and Goldberger (1961) are preferred when prior information is available. In this study, we will consider parameter estimation and the variable selection in non-stochastic restricted linear regression model, using least absolute shrinkage and selection operator (LASSO) method introduced by Tibshirani (1996). A small simulation study and real data example are provided to illustrate the performance of the proposed method for dealing with the variable selection and the parameter estimation in restricted linear regression models.",2017,
Bayesian regularized quantile regression,"Regularization, e.g. lasso, has been shown to be effective in quantile regression in improving the prediction accuracy (Li and Zhu 2008; Wu and Liu 2009). This paper studies regularization in quantile regressions from a Bayesian perspective. By proposing a hierarchical model framework, we give a generic treatment to a set of regularization approaches, including lasso, group lasso and elastic net penalties. Gibbs samplers are derived for all cases. This is the first work to discuss regularized quantile regression with the group lasso penalty and the elastic net penalty. Both simulated and real data examples show that Bayesian regularized quantile regression methods often outperform quantile regression without regularization and their non-Bayesian counterparts with regularization.",2010,Bayesian Analysis
Learning surrogate models for simulationâ€based optimization,"A central problem in modeling, namely that of learning an algebraic model from data obtained from simulations or experiments is addressed. A methodology that uses a small number of simulations or experiments to learn models that are as accurate and as simple as possible is proposed. The approach begins by building a low-complexity surrogate model. The model is built using a best subset technique that leverages an integer programming formulation to allow for the efficient consideration of a large number of possible functional components in the model. The model is then improved systematically through the use of derivative-free optimization solvers to adaptively sample new simulation or experimental points. Automated learning of algebraic models for optimization (ALAMO), the computational implementation of the proposed methodology, along with examples and extensive computational comparisons between ALAMO and a variety of machine learning techniques, including Latin hypercube sampling, simple least-squares regression, and the lasso is described. Â© 2014 American Institute of Chemical Engineers AIChE J, 60: 2211â€“2227, 2014",2014,Aiche Journal
Modeling of Pesticide Biodegradation in Soil,"Quantitative Biodegradability-Structure Relationships (QSBRs) are valuable tools for estimating pesticide half-lives in soil. Advance in QSBR modeling is based on several prerequisites: (1) separation of (bio)degradation from formation of non-extractable soil residues (NER), (2) degradation data from a homogeneous soil source, (3) robust predictor selection, (4) proper control of model complexity and (5) validation of model generalization performance on data external to model construction. This study addresses (1) to (5) in two steps. In step one, first order rate constants of both primary degradation and NER formation were calculated from inverse model optimization by Genetic Algorithms, based on 87 Speyer 2.2 soil degradation data sets. Degradation was found to be faster than NER formation for most of the compounds, although both processes were correlated. In a second step, variation in degradation rate constants was modeled by LASSO shrinkage regression and, in addition, by a novel robust filtering technique combined with forward selection, termed Recursive Bootstrap Subsampling (RBS). RBS and shrinkage regression were applied to four predictor sets of increasing complexity and validated by (a) internal crossvalidation (CV), (b) external CV and (c) external CV under proper control of model complexity. As a result, (a) overestimated generalization performance for all models and predictor sets, whereas (b) underestimated the performance achievable with a given data set. (c), on the other hand, gave more reasonable estimates. Moreover, shrinkage regression could outperform RBS under setting (b). However, when (c) control of model complexity was assured and the number p of predictors outnumbered that of the samples n (n << p problem), RBS outperformed shrinkage regression. Therefore, only RBS was utilized to assess the importance of molecular and non-molecular predictors for explaining variation in degradation rates. Of a total of 670 candidate predictors, only few were important. The most relevant ones were: 3 collinear fragments containing sulfur, 2 collinear ester functionalities, 3 collinear N-heteroaromatic moeities, nitrogen with double or triple bonds, a methyl and a methylene descriptor, double bonded oxygen and water-organic carbon partitioning coefficients. It is concluded that a combination of robust predictor selection, external validation and control of model complexity other than internal CV is essential for reliable QSBR modeling.",2010,
"Multi-Resolution Functional ANOVA for Large-Scale, Many-Input Computer Experiments","The Gaussian process is a standard tool for building emulators for both deterministic and stochastic computer experiments. However, application of Gaussian process models is greatly limited in practice, particularly for large-scale and many-input computer experiments that have become typical. We propose a multi-resolution functional ANOVA model as a computationally feasible emulation alternative. More generally, this model can be used for large-scale and many-input non-linear regression problems. An overlapping group lasso approach is used for estimation, ensuring computational feasibility in a large-scale and many-input setting. New results on consistency and inference for the (potentially overlapping) group lasso in a high-dimensional setting are developed and applied to the proposed multi-resolution functional ANOVA model. Importantly, these results allow us to quantify the uncertainty in our predictions. Numerical examples demonstrate that the proposed model enjoys marked computational advantages. Data capabilities, both in terms of sample size and dimension, meet or exceed best available emulation tools while meeting or exceeding emulation accuracy.",2017,arXiv: Methodology
Genomic DNA Methylation-Derived Algorithm Enables Accurate Detection of Malignant Prostate Tissues,"Introduction
The current methodology involving diagnosis of prostate cancer (PCa) relies on the pathology examination of prostate needle biopsies, a method with high false negative rates partly due to temporospatial, molecular, and morphological heterogeneity of prostate adenocarcinoma. It is postulated that molecular markers have a potential to assign diagnosis to a considerable portion of undetected prostate tumors. This study examines the genome-wide DNA methylation changes in PCa in search of genomic markers for the development of a diagnostic algorithm for PCa screening.


Methods
Archival PCa and normal tissues were assessed using genomic DNA methylation arrays. Differentially methylated sites and regions (DMRs) were used for functional assessment, gene-set enrichment and protein interaction analyses, and examination of transcription factor-binding patterns. Raw signal intensity data were used for identification of recurrent copy number variations (CNVs). Non-redundant fully differentiating cytosine-phosphate-guanine sites (CpGs), which did not overlap CNV segments, were used in an L1 regularized logistic regression model (LASSO) to train a classification algorithm. Validation of this algorithm was performed using a large external cohort of benign and tumor prostate arrays.


Results
Approximately 6,000 probes and 600 genomic regions showed significant DNA methylation changes, primarily involving hypermethylation. Gene-set enrichment and protein interaction analyses found an overrepresentation of genes related to cell communications, neurogenesis, and proliferation. Motif enrichment analysis demonstrated enrichment of tumor suppressor-binding sites nearby DMRs. Several of these regions were also found to contain copy number amplifications. Using four non-redundant fully differentiating CpGs, we trained a classification model with 100% accuracy in discriminating tumors from benign samples. Validation of this algorithm using an external cohort of 234 tumors and 92 benign samples yielded 96% sensitivity and 98% specificity. The model was found to be highly sensitive to detect metastatic lesions in bone, lymph node, and soft tissue, while being specific enough to differentiate the benign hyperplasia of prostate from tumor.


Conclusion
A considerable component of PCa DNA methylation profile represent driver events potentially established/maintained by disruption of tumor suppressor activity. As few as four CpGs from this profile can be used for screening of PCa.",2018,Frontiers in Oncology
A New Sparse and Robust Adaptive Lasso Estimator for the Independent Contamination Model,"Many problems in signal processing require finding sparse solutions to under-determined, or ill-conditioned, linear systems of equations. When dealing with real-world data, the presence of outliers and impulsive noise must also be accounted for. In past decades, the vast majority of robust linear regression estimators has focused on robustness against rowwise contamination. Even so called `high breakdown' estimators rely on the assumption that a majority of rows of the regression matrix is not affected by outliers. Only very recently, the first cellwise robust regression estimation methods have been developed. In this paper, we define robust oracle properties, which an estimator must have in order to perform robust model selection for under-determined, or ill-conditioned linear regression models that are contaminated by cellwise outliers in the regression matrix. We propose and analyze a robustly weighted and adaptive Lasso type regularization term which takes into account cellwise outliers for model selection. The proposed regularization term is integrated into the objective function of the MM-estimator, which yields the proposed MM-Robust Weighted Adaptive Lasso (MM-RWAL), for which we prove that at least the weak robust oracle properties hold. A performance comparison to existing robust Lasso estimators is provided using Monte Carlo experiments. Further, the MM-RWAL is applied to determine the temporal releases of the European Tracer Experiment (ETEX) at the source location. This ill-conditioned linear inverse problem contains cellwise and rowwise outliers and is sparse both in the regression matrix and the parameter vector. The proposed RWAL penalty is not limited to the MM-estimator but can easily be integrated into the objective function of other robust estimators.",2017,arXiv: Statistics Theory
Incorporating correlations between drugs and heterogeneity of multi-omics data in structured penalized regression for drug sensitivity prediction,"Targeted cancer drugs have been developed to interfere with specific molecular targets, which are expected to affect the growth of cancer cells in a way that can be characterized by multi-omics data. The prediction of cancer drug sensitivity simultaneously for multiple drugs based on heterogeneous multi-omics data (e.g., mRNA expression, DNA copy number or DNA mutation) is an important but challenging task. We use joint penalized regression models for multiple cancer drugs rather than a separate model for each drug, thus being able to address the correlation structure between drugs. In addition, we employ integrative penalty factors (IPF) to allow penalizing data from different molecular data sources differently. By integrating IPF with tree-lasso, we create the IPF-tree-lasso method, which can capture the heterogeneity of multi-omics data and the correlation between drugs at the same time. Additionally, we generalize the IPF-lasso to the IPF-elastic-net, which combines $\ell_1$- and $\ell_2$-penalty terms and can lead to improved prediction performance. To make the computation of IPF-type methods more feasible, we present that the IPF-type methods are equivalent to the original lasso-type methods after augmenting the data matrix, and employ the Efficient Parameter Selection via Global Optimization (EPSGO) algorithm for optimizing multiple penalty factors efficiently. Simulation studies show that the new model, IPF-tree-lasso, can improve the prediction performance significantly. We demonstrate the performance of these methods on the Genomics of Drug Sensitivity in Cancer (GDSC) data.",2019,
Feature extraction of big climate data,"Abstract Variables related to climate change are divided into quantitative variables and qualitative variables. Quantitative variables, such as temperature and precipitation, take on numerical values, whereas qualitative variables, such as land types, take on values in one of different categories or classes. Regression is a process of predicting qualitative response, whereas clustering or classification is a process of predicting qualitative responses. The most widely used regression in the context of big data are Ridge and Lasso regressions. Big climatic data environment can enhance the tendency of classical linear regression model to overfit. Ridge and lasso regressions are more suitable to handle big data environment than classical regression since they can control the size of regression coefficients. The most widely-used classifiers in the context of big data are linear discriminant analysis and K-nearest neighbors. More state-of-the-art classifiers include decision trees, random forests, support vector machines, and so on. These methods provide managing an efficient exploitation of big data from Earth observation systems.",2020,
Automatic regularization of cross-entropy cost for speaker recognition fusion,"In this paper we study automatic regularization techniques for the fusion of automatic speaker recognition systems. Parameter regularization could dramatically reduce the fusion training time. In addition, there will not be any need for splitting the development set into different folds for cross- validation. We utilize majorization-minimization approach to automatic ridge regression learning and design a similar way to learn LASSO regularization parameter automatically. By experiments we show improvement in using automatic regularization.",2013,
A combined approach for predicting sparse variables such as tips ratio and daily precipitation,"Author(s): Li, Jinshu | Advisor(s): WU, YINGNIAN | Abstract: A sparse variable is a variable whose values are mostly zero. Because of its sparsity, satisfactory prediction results of a sparse variable usually cannot be obtained by either pure (i.e. single) regression or pure classification machine learning methods. Therefore, to resolve this difficulty, this thesis paper proposes a framework that combines a regression model and a classification model. Furthermore, two types of the combined regression and classification framework are discussed, and their differences are illustrated. Two sparse variables are selected as the case studies: taxi tips ratio (i.e. tips amount divided by total fare) and daily precipitation volume (i.e. total rainfall amount in one day). The author first employs Lasso regression to select relevant features for each sparse variable, with the best Lasso parameter determined by cross-validation (CV). Second, the author selects Logistic regression and the AdaBoost method as the classification methods, while the XGBoost method is chosen as the regression method. The hyperparameters are determined by fine-tuning. The author then surveys over the prediction results of the pure classification method, the pure regression method, and the combined method, using root mean square error (RMSE) as the metric. The results show that the pure regression method provides the least RMSE for both variables; however, it does not satisfy the sparsity requirement. On the contrary, the combined method, whose RMSE is close to the RMSE of the pure regression method, can also provide the sparse results, which makes it an efficient way to predict sparse variables like taxi tip ratio and daily precipitation.",2019,
Multi-step ahead time series forecasting via sparse coding and dictionary based techniques,"Abstract Sparse coding is based on the concept of having a large dictionary of candidate basis vectors. Any given vector is expressed as a sparse linear combination of the dictionary vectors. It has been developed in the signal processing field, and has many applications in data compression and image processing. In this paper we propose applying sparse coding to the time series forecasting field. Specifically, the paper investigates different dictionary based local learning techniques for building predictive models for the time series forecasting problem. The proposed methodology is based on a local learning framework whereby the query point is embedded and coded in terms of a sparse combination of the training dictionary atoms (vectors). Then this embedding is used for estimating the target value of the query point, by applying the same embedding to the target vectors of the dictionary training atoms. We present an experimental study of several sparse coding algorithms. Experiments are performed on the large monthly time series benchmark from the M3 competition, and these experiments showed that the sparse methods Lasso and Elastic-Net presented the best results among the sparse coding algorithms. Moreover, they outperformed the K-nearest neighbor (KNN) regression and most of the compared machine learning and statistical forecasting techniques, especially for higher horizons.",2018,Appl. Soft Comput.
SI-ADMM: A Stochastic Inexact ADMM Framework for Stochastic Convex Programs,"We consider the structured stochastic convex program requiring the minimization of $\mathbb{E}[\tilde f(x,\xi)]+\mathbb{E}[\tilde g(y,\xi)]$ subject to the constraint $Ax + By = b$. Motivated by the need for decentralized schemes and structure, we propose a stochastic inexact ADMM (SI-ADMM) framework where subproblems are solved inexactly via stochastic approximation schemes. Based on this framework, we prove the following: (i) under suitable assumptions on the associated batch-size of samples utilized at each iteration, the SI-ADMM scheme produces a sequence that converges to the unique solution almost surely; (ii) If the number of gradient steps (or equivalently, the number of sampled gradients) utilized for solving the subproblems in each iteration increases at a geometric rate, the mean-squared error diminishes to zero at a prescribed geometric rate; (iii) The overall iteration complexity in terms of gradient steps (or equivalently samples) is found to be consistent with the canonical level of $\mathcal{O}(1/\epsilon)$. Preliminary applications on LASSO and distributed regression suggest that the scheme performs well compared to its competitors.",2017,arXiv: Optimization and Control
The Role of Self-Efficacy on Accounting Near-Graduate Studentsâ€™ Employment Outcomes,"Advances in artificial intelligences and robotics have modernized the business environment of the 21st century. It is not enough for graduates to have only the occupation-specific knowledge and transferrable kills for graduates to meet the needs of labour markets. They are also expected to have a number of personal attributes including self-awareness, self-confidence, independence, emotional intelligence, flexibility and adaptability, creativity and initiative, willingness to be lifelong learner. The present research analyses the association between the three self-efficacy factors of the general selfefficacy scale (GSES): initiative, effort and persistence on accounting near-graduate employment outcomes. The study sample consisted of 337 near-graduate accounting students from Victoria University and Swinburne University of Technology, Melbourne, Australia. The research employed logistic regression, as well as Lasso and R-glmulti statistical techniques, to examine the main research questions. In addition, Mann-Whitney U tests and Pearson chi-square tests were conducted to examine the association between accounting studentsâ€™ individual characteristics and the three factors of GSES.",2020,
Potential application of machine learning in health outcomes research and some statistical cautions.,"Traditional analytic methods are often ill-suited to the evolving world of health care big data characterized by massive volume, complexity, and velocity. In particular, methods are needed that can estimate models efficiently using very large datasets containing healthcare utilization data, clinical data, data from personal devices, and many other sources. Although very large, such datasets can also be quite sparse (e.g., device data may only be available for a small subset of individuals), which creates problems for traditional regression models. Many machine learning methods address such limitations effectively but are still subject to the usual sources of bias that commonly arise in observational studies. Researchers using machine learning methods such as lasso or ridge regression should assess these models using conventional specification tests.",2015,Value in health : the journal of the International Society for Pharmacoeconomics and Outcomes Research
