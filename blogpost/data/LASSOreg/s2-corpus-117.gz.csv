title,abstract,year,journal
Improving LASSO performance for Grey Leaf Spot disease resistance prediction based on genotypic data by considering all possible two-way SNP interactions.,"Disease resistance prediction using genotypic data has been widely pursued in animal as well as plant research, mostly in cases where genotypic data can be readily available for a large number of subjects. With the evolution of SNP marker genotyping technology and the consequent cost reduction for genotyping thousands of SNP markers, significant research effort is being undertaken in the statistics and machine learning community to perform efficient analysis of these multidimensional datasets. For large plant breeding programs, besides identifying biomarkers associated with disease resistance, developing accurate predictive models of the phenotype based on the genotype alone is one of the most relevant scientific goals, as it allows for efficient selection without having to grow and phenotype every individual. While the importance of interactions for understanding diseases has been shown in many studies, the majority of the existing methods are limited by considering each biomarker as an independent variable, completely ignoring complex interactions among biomarkers. In this study, logistic regression p-value, Pearson correlation and mutual information were calculated for all two-way SNP interactions with respect to the Grey Leaf Spot (GLS) disease resistance phenotype. These interactions were subsequently ranked based on these measures and the performance of the LASSO algorithm for GLS disease resistance prediction was then shown to be maximized by adding the top 10â€‰000 two-way interactions from the logistic regression p-value based rank. The logistic regression p-value based rank also led to an error rate of more than 3 percentual points lower than not adding any interaction and more than 3.5 percentual points lower than adding interactions chosen at random.",2012,Integrative biology : quantitative biosciences from nano to macro
Abstract 3113: Hyperactivation of FOXM1 drives ovarian cancer growth and metastasis independent of the G2-M cell cycle checkpoint.,"Forkhead box M1 (FOXM1) is a TP53- and phosphorylation-dependent transcription factor that plays a critical role in cell cycle progression. Recent analyses by the Cancer Genome Atlas (TCGA) consortium indicate that FOXM1 overexpression may be a key, early event driving the growth of epithelial ovarian cancer (EOC). However, this hypothesis has not been critically tested. Using RT-qPCR, we examined the expression of each of the known FOXM1 splice variants. We found that levels of FOXM1c but not FOXM1b or the translationally inactive FOXM1a are significantly higher in specimens of EOC (n=5) than normal ovary (n=3) or fallopian tube (n=3). By western blot, we found that FOXM1 is highly phosphorylated in EOC, suggesting that its hyperactivation is a consistent feature of ovarian cancer. When FOXM1 expression was targeted in TP53-wt HEYA8, TP53-mutated OVCAR8, and TP53-deficient SkOViP31 ovarian cancer cells by siRNAs, we found that ovarian cancer cell lines with reduced FOXM1 expression grow much more slowly than control cultures transfected with a non-targeting siRNA control and that a loss of FOXM1 results in G2-M cell cycle arrest that can be detected by flow cytometry using propidium iodide stained cells. Reduced FOXM1 expression also inhibited migration and invasion when OVCAR8 and HeyA8 cells were evaluated using standard Boyden chamber assays. Using qPCR and Western blot, we examined the ability of FOXM1 to directly regulate >19 gene products that are highly expressed in EOC. Despite the fact that FOXM1 regulates many of these genes (Cyclin B1, Plk1, and CENPF) in other cells and tissues, levels of their expression were not observed when FOXM1 was knocked down SKOV3ip1, HEY8 and OVCAR8 cells. Therefore, we analyzed patterns of gene expression from 581 TCGA ovarian cancer specimens using serial linear regression with L1 normalization (Lasso analysis) to test for correlations with FOXM1 expression. These analyses identified multiple gene products whose expression is potentially regulated by FOXM1 in ovarian cancer. Altered levels of expression for multiple gene products identified by these analyses have been examined by qPCR and Western blot in ovarian cancer cell lines transfected with shRNA targeting FOXM1 expression. These included multiple gene products, including VEGF-B and -C known to regulate angiogenesis. To explore the in vivo impact of targeting FOXM1 expression, a xenograft model was established by inoculating Fox1Nu mice with OVCAR8 cells stably transfected with either shRNA targeting FOXM1 and a non-targeting control. Collectively, these results indicate that hyperactivation of FOX1-mediated transcriptional activity is a consistent feature of EOC and plays a critical role in regulating ovarian cancer metastasis in addition to proliferation. FOXM1 may be a valuable therapeutic target for overcoming the genetic heterogeneity that limits current ovarian cancer treatments. Citation Format: Triparna Ghosh-Choudhury, Holli A. Loomans, Ying-Wooi Wan, Zhangdon Liu, Shannon M. Hawkins, Matthew L. Anderson. Hyperactivation of FOXM1 drives ovarian cancer growth and metastasis independent of the G2-M cell cycle checkpoint. [abstract]. In: Proceedings of the 104th Annual Meeting of the American Association for Cancer Research; 2013 Apr 6-10; Washington, DC. Philadelphia (PA): AACR; Cancer Res 2013;73(8 Suppl):Abstract nr 3113. doi:10.1158/1538-7445.AM2013-3113",2013,Cancer Research
Detection and quantification in real-time polymerase chain reaction,"The estimation of the concentration of an infectious agent in the environment is a key step to trigger an alert when there is a biological threat. This concentration can be obtained trough a quantitative polymerase chain reaction (qPCR). Nevertheless, standard real-time procedure do not address detection delay which is a main concern in alert triggering. Therefore, we propose a method based on Lasso regression and CUSUM change detection to accurately estimate the concentration while minimizing the detection delay. The trade-off between accuracy and delay can be managed through a parameter. We compare our results with those found by a standard method (threshold method) and promising results are obtained.",2013,
Modified SCAD penalty for constrained variable selection problems,"Abstract Instead of using sample information only to do variable selection, in this article we also take priori information â€” linear constraints of regression coefficients â€” into account. The penalized likelihood estimation method is adopted. However under constraints, it is not guaranteed that information criteria like AIC and BIC are minimized at an oracle solution using the lasso or SCAD penalty. To overcome such difficulties, a modified SCAD penalty is proposed. The definitions of information criteria GCV, AIC and BIC for constrained variable selection problems are also proposed. Statistically, we show that if the tuning parameter is appropriately chosen, the proposed estimators enjoy the oracle properties and satisfy the linear constraints. Additionally, they also possess the robust property to outliers if the linear model with M-estimation is used.",2014,Statistical Methodology
Simultaneous Coefficient Penalization and Model Selection in Geographically Weighted Regression: The Geographically Weighted Lasso,"In the field of spatial analysis, the interest of some researchers in modeling relationships between variables locally has led to the development of regression models with spatially varying coefficients. One such model that has been widely applied is geographically weighted regression (GWR). In the application of GWR, marginal inference on the spatial pattern of regression coefficients is often of interest, as is, less typically, prediction and estimation of the response variable. Empirical research and simulation studies have demonstrated that local correlation in explanatory variables can lead to estimated regression coefficients in GWR that are strongly correlated and, hence, problematic for inference on relationships between variables. The author introduces a penalized form of GWR, called the â€˜geographically weighted lassoâ€™ (GWL) which adds a constraint on the magnitude of the estimated regression coefficients to limit the effects of explanatory-variable correlation. The GWL also performs local model selection by potentially shrinking some of the estimated regression coefficients to zero in some locations of the study area. Two versions of the GWL are introduced: one designed to improve prediction of the response variable, and one more oriented toward constraining regression coefficients for inference. The results of applying the GWL to simulated and real datasets show that this method stabilizes regression coefficients in the presence of collinearity and produces lower prediction and estimation error of the response variable than does GWR and another constrained version of GWRâ€”geographically weighted ridge regression.",2009,Environment and Planning A
Lasso ANOVA decompositions for matrix and tensor data,"Abstract Consider the problem of estimating the entries of an unknown mean matrix or tensor given a single noisy realization. In the matrix case, this problem can be addressed by decomposing the mean matrix into a component that is additive in the rows and columns, i.e. the additive ANOVA decomposition of the mean matrix, plus a matrix of elementwise effects, and assuming that the elementwise effects may be sparse. Accordingly, the mean matrix can be estimated by solving a penalized regression problem, applying a lasso penalty to the elementwise effects. Although solving this penalized regression problem is straightforward, specifying appropriate values of the penalty parameters is not. Leveraging the posterior mode interpretation of the penalized regression problem, moment-based empirical Bayes estimators of the penalty parameters can be defined. Estimation of the mean matrix using these moment-based empirical Bayes estimators can be called LANOVA penalization, and the corresponding estimate of the mean matrix can be called the LANOVA estimate. The empirical Bayes estimators are shown to be consistent. Additionally, LANOVA penalization is extended to accommodate sparsity of row and column effects and to estimate an unknown mean tensor. The behavior of the LANOVA estimate is examined under misspecification of the distribution of the elementwise effects, and LANOVA penalization is applied to several datasets, including a matrix of microarray data, a three-way tensor of fMRI data and a three-way tensor of wheat infection data.",2019,Comput. Stat. Data Anal.
Estimation of genomic breeding values using the Horseshoe prior,"BackgroundA method for estimating genomic breeding values (GEBV) based on the Horseshoe prior was introduced and used on the analysis of the 16th QTLMAS workshop dataset, which resembles three milk production traits. The method was compared with five commonly used methods: Bayes A, Bayes B, Bayes C, Bayesian Lasso and GLUP.MethodsThe main difference between the methods is the prior distribution assumed during the estimation of the SNP effects. The distribution of the Bayesian Lasso is a Laplace distribution; for Bayes A is a Student-t; for Bayes B and Bayes C is a spike and slab prior combining a proportion of SNP without effect and a proportion with effect distributed as a Student-t or Gaussian for Bayes B and C, respectively; for GBLUP is similar to a ridge regression. The distribution for the Horseshoe prior behaves like log(1+1/Î²2) (up to a constant). It has an infinite spike at zero and heavy tail that decay by Î²-2 (slower than the Laplace or the Student-t). The implementation of all methods (except GBLUP) was done using a MCMC approach, where the relevant parameters defining the prior distributions were jointly estimated from the data. The GBLUP was done using ASREML.ResultsThe accuracy for all methods ranged from 0.74 to 0.83, representing an improvement of 44% to 78% over the traditional BLUP evaluation. GEBV with the highest accuracy were obtained with Bayes A, Bayes B and the Horseshoe prior. The Horseshoe tended to select smaller number of SNP and assigning them larger effects, while strongly shrinking the remaining SNP to have an effect closer to zero.ConclusionsThe Horseshoe prior showed a different shrinkage pattern than the other methods. While for this specific dataset, this has little impact on the accuracy of the GEBV, it may prove a good property to discriminate true effect from noise, and thereby, improve overall prediction under different scenarios.",2014,BMC Proceedings
Tsunami Inversion of Fault Displacements Based on Sparse Modeling,"A study on tsunami inversion analysis was conducted using Lasso regression, which is a typical approach to sparse modeling. Characteristics of spatiotemporal distribution of the fault slips were then analyzed. Sparse modeling is characterized by yielding obtained solutions that contribute significantly to the output of the model. This feature is useful for investigating the development of prediction models. This paper applies a tsunami inversion analysis based on sparse modeling to the 2011 Tohoku Earthquake Tsunami and analyzes the characteristics of the fault slips.",2019,
Estimating Sparse Neuronal Signal from Hemodynamic Response: the Mixture Components Inference Approach,"The approximate knowledge of the hemodynamic response to neuronal activity is widely used in statistical testing of effects of external stimulation, but has also been applied to estimate the neuronal activity directly from functional magnetic resonance data without knowing the stimulus timing. To this end, sparse linear regression methods have been previously used, including the well-known LASSO and the Dantzig selector. These methods generate a parametric family of solutions with different sparsity, among which a choice is finally based using some information criteria. As an alternative we propose a novel approach that instead utilizes the whole family of sparse regression solutions. Their ensemble provides a first approximation of probability of activation at each timepoint, and together with the conditional neuronal activity distributions estimated with the theory of mixtures with varying concentrations, they serve as the inputs to a Bayes classifier ultimately deciding between the true and false activations. As we show in extensive numerical simulations, the new method performs favourably in comparison with standard approaches in a range of realistic scenarios. This is mainly due to the avoidance of overfitting and underfitting that commonly plague the solutions based on sparse regression combined with model selection methods, including the corrected Akaike Information Criterion. This advantage is finally documented on fMRI task dataset.",2019,bioRxiv
"Lasso, knockoff and Gaussian covariates: a comparison.","Given data $\mathbf{y}$ and $k$ covariates $\mathbf{x}_j$ one problem in linear regression is to decide which if any of the covariates to include when regressing the dependent variable $\mathbf{y}$ on the covariates $\mathbf{x}_j$. In this paper three such methods, lasso, knockoff and Gaussian covariates are compared using simulations and real data. The Gaussian covariate method is based on exact probabilities which are valid for all $\mathbf{y}$ and $\mathbf{x}_j$ making it model free. Moreover the probabilities agree with those based on the F-distribution for the standard linear model with i.i.d. Gaussian errors. It is conceptually, mathematically and algorithmically very simple, it is very fast and makes no use of simulations. It outperforms lasso and knockoff in all respects by a considerable margin.",2018,arXiv: Methodology
Create a predictive model for neurogenic bladder patients: upper urinary tract damage predictive nomogram.,"Objective: To create a nomogram to evaluate the risk of upper urinary tract damage (UUTD) in patients with neurogenic bladder (NGB) Methods: A retrospective analysis was conducted on 301 patients with NGB who were admitted to certain hospitals. Data collected included clinical symptoms, patients' characteristics, laboratory parameters, imaging findings and urodynamic parameters. The least absolute shrinkage and selection operatorï¼ˆLASSOï¼‰regression model was used to optimize the selection of predictors. Multivariate logistic regression analysis was performed to develop a UUTD risk predictive model. Validation was performed by bootstrap. Results: The predictors included in the nomogram included sex, duration of disease, history of UTI, bladder compliance, and fecal incontinence. The model presented good discrimination with a C-index value of 0.796 (95% confidence interval: 0.74896-0.84304) and good calibration. The C-index value of the interval validation was 0.7872112. The results of decision curve analysis (DCA) demonstrated that the UUTD-risk predictive nomogram was clinically useful. Conclusion: The nomogram incorporating the sex, duration of disease, history of UTI, bladder compliance, and fecal incontinence could be an important tool of UUTD risk prediction in NGB patients.",2019,The International journal of neuroscience
Augmenting LASSO regression with decision tree for identifying the correlation of genetic polymorphism and adverse events,"A novel algorithm that combines LASSO regression and decision tree is proposed to explore the correlation of adverse events (AE) and genetic polymorphism of CYP2D6*2, *10, *14, CYP1A2*1C, *1F in human subjects in a clinical trial. The genotypes of 30 healthy human subjects in a clinical trial for a natural herbal drug and 53 subjects in the blank group were detected by polymerase chain reaction (PCR) and DNA sequencing. The AEs occurring during the trial were recorded. The correlations of AE and genetic polymorphism are analyzed by the new combined algorithm. 53 AEs are reported in the end of the study. Five gene subtypes are selected as correlative factors to the specific AEs by the new algorithm: wild type of CYP1A2*1F and abnormal platelet counting, homozygous CYP1A2*1C and abnormal fibrinogen, heterozygous CYP1A2*1C and abnormal blood chlorine, heterozygous CYP1A2*1C and abnormal urobilinogen, wild type of CYP2D6*2 and abnormal APTT (activated partial thromboplastin time). The result indicates the novel algorithm is effective and is able to detect the correlation of AEs and genetic polymorphism in clinical trials.",2013,2013 IEEE International Conference on Bioinformatics and Biomedicine
Sparse covariance thresholding for high-dimensional variable selection,"In high-dimensions, many variable selection methods, such as the lasso, are often limited by excessive variability and rank deficiency of the sample covariance matrix. Covariance sparsity is a natural phenomenon in high-dimensional applications, such as microarray analysis, image processing, etc., in which a large number of predictors are independent or weakly correlated. In this paper, we propose the covariance-thresholded lasso, a new class of regression methods that can utilize covariance sparsity to improve variable selection. We establish theoretical results, under the random design setting, that relate covariance sparsity to variable selection. Real-data and simulation examples indicate that our method can be useful in improving variable selection performances.",2010,arXiv: Methodology
Why Geometric Progression in Selecting the LASSO Parameter: A Theoretical Explanation,"In situations when we know which inputs are relevant, the least squares method is often the best way to solve linear regression problems. However, in many practical situations, we do not know beforehand which inputs are relevant and which are not. In such situations, a 1-parameter modification of the least squares method known as LASSO leads to more adequate results. To use LASSO, we need to determine the value of the LASSO parameter that best fits the given data. In practice, this parameter is determined by trying all the values from some discrete set. It has been empirically shown that this selection works the best if we try values from a geometric progression. In this paper, we provide a theoretical explanation for this empirical fact. 1 Formulation of the Problem Need for regression. In many real-life situations, we know that the quantity y is uniquely determined by the quantities x1, . . . , xn, but we do not know the exact formula for this dependence. For example, in physics, we know that the aerodynamic resistance increases with the bodyâ€™s velocity, but we often do not know how exactly. In economics, we may know that a change in tax rate influences the economic growth, but we often do not know how exactly. In all such cases, we need to find the dependence y = f(x1, . . . , xn) between several quantities based on the available data, i.e., based on the previous observations (xk1, . . . , xkn, yk) in each of which we know both the values xki of the input quantities xi and the value yk of the output quantity y. In statistics, determining the dependence from the data is known as regression.",2020,
Adaptive penalization in high-dimensional regression and classification with external covariates using variational Bayes.,"Penalization schemes like Lasso or ridge regression are routinely used to regress a response of interest on a high-dimensional set of potential predictors. Despite being decisive, the question of the relative strength of penalization is often glossed over and only implicitly determined by the scale of individual predictors. At the same time, additional information on the predictors is available in many applications but left unused. Here, we propose to make use of such external covariates to adapt the penalization in a data-driven manner. We present a method that differentially penalizes feature groups defined by the covariates and adapts the relative strength of penalization to the information content of each group. Using techniques from the Bayesian tool-set our procedure combines shrinkage with feature selection and provides a scalable optimization scheme. We demonstrate in simulations that the method accurately recovers the true effect sizes and sparsity patterns per feature group. Furthermore, it leads to an improved prediction performance in situations where the groups have strong differences in dynamic range. In applications to data from high-throughput biology, the method enables re-weighting the importance of feature groups from different assays. Overall, using available covariates extends the range of applications of penalized regression, improves model interpretability and can improve prediction performance.",2019,Biostatistics
Analysis of High-Dimensional Regression Models Using Orthogonal Greedy Algorithms,"We begin by reviewing recent results of Ing and Lai (Stat Sin 21:1473â€“1513, 2011) on the statistical properties of the orthogonal greedy algorithm (OGA) in high-dimensional sparse regression models with independent observations. In particular, when the regression coefficients are absolutely summable, the conditional mean squared prediction error and the empirical norm of OGA derived by Ing and Lai (Stat Sin 21:1473â€“1513, 2011) are introduced. We then explore the performance of OGA under more general sparsity conditions. Finally, we obtain the convergence rate of OGA in high-dimensional time series models, and illustrate the advantage of our results compared to those established for Lasso by Basu and Michailidis (Ann Stat 43:1535â€“1567, 2015) and Wu and Wu (Electron J Stat 10:352â€“379, 2016).",2018,
Eigen-Epistasis for detecting gene-gene interactions,"BackgroundA large amount of research has been devoted to the detection and investigation of epistatic interactions in genome-wide association studies (GWASs). Most of the literature focuses on low-order interactions between single-nucleotide polymorphisms (SNPs) with significant main effects.ResultsIn this paper we propose an original approach for detecting epistasis at the gene level, without systematically filtering on significant genes. We first compute interaction variables for each gene pair by finding its Eigen-Epistasis component, defined as the linear combination of Gene SNPs having the highest correlation with the phenotype. The selection of significant effects is done using a penalized regression method based on Group Lasso controlling the False Discovery Rate.ConclusionThe method is tested against two recent alternative proposals from the literature using synthetic data, and shows good performances in different settings. We demonstrate the power of our approach by detecting new gene-gene interactions on three genome-wide association studies.",2017,BMC Bioinformatics
Robust variable selection for the varying coefficient model based on composite L 1â€“L 2 regression,"The varying coefficient model (VCM) is an important generalization of the linear regression model and many existing estimation procedures for VCM were built on L 2 loss, which is popular for its mathematical beauty but is not robust to non-normal errors and outliers. In this paper, we address the problem of both robustness and efficiency of estimation and variable selection procedure based on the convex combined loss of L 1 and L 2 instead of only quadratic loss for VCM. By using local linear modeling method, the asymptotic normality of estimation is driven and a useful selection method is proposed for the weight of composite L 1 and L 2. Then the variable selection procedure is given by combining local kernel smoothing with adaptive group LASSO. With appropriate selection of tuning parameters by Bayesian information criterion (BIC) the theoretical properties of the new procedure, including consistency in variable selection and the oracle property in estimation, are established. The finite sample performance of the new method is investigated through simulation studies and the analysis of body fat data. Numerical studies show that the new method is better than or at least as well as the least square-based method in terms of both robustness and efficiency for variable selection.",2013,Journal of Applied Statistics
Analysis on Train Stopping Accuracy based on Regression Algorithms,"Stopping accuracy is one of the most important indexes of efficiency of automatic train operation (ATO) systems. Traditional stopping control algorithms in ATO systems have some drawbacks, as many factors have not been taken into account. In the large amount of field-collected data about stopping accuracy there are many factors (e.g. system delays, stopping time, net pressure) which affecting stopping accuracy. In this paper, three popular data mining methods are proposed to analyze the train stopping accuracy. Firstly, we find fifteen factors which have impact on the stopping accuracy. Then, ridge regression, lasso regression and elastic net regression are employed to mine models to reflecting the relationship between the fifteen factors and the stopping accuracy. Then, the three models are compared by using Akaike information criterion (AIC), a model selection criterion which considering the trade-off between accuracy and complexity.Â The computational results show that elastic net regression model has a best performance on AIC value. Finally, we obtain the parameters which can make the train stop more accurately which can provide a reference to improve stopping accuracy for ATO systems.",2014,JSW
Penerapan Group Least Absolute Shrinkage and Selection Operator sebagai Metode Alternatif dalam Menangani Data Berdimensi Tinggi.,"HAIFA MARDHOTILLAH. The Application of Group Least Absolute Shrinkage and Selection Operator as an Alternative Method for Handling High Dimensional Data. Supervised by BAGUS SARTONO and YENNI ANGRAINI. Coefficients regression can be estimated by using Ordinary Least Squares (OLS) Method. However, LSM can not be applied to high-dimensional data (HDD). HDD is the data that the number of explanatory variables is more than the number of observations. This can be happened due to the singularity of matrix X'X so it does not have inverse. One of the method to handle that problem is Group of Least Absolute Shrinkage and Selection Operator (Group LASSO). Group LASSO is selecting groups of explanatory variables by shrinking the value of the coefficient to zero or close to zero to get a simple model. The data used in this study is the content of Curcuma in a simplicia measured using Fourier Transform Infrared (FTIR). Grouping was done by information about the nature of chemical properties and sequence of wave numbers. Grouping which provides the simplest models with high accuracy values were grouping that based on FTIR spectrum plot which was divided into 23 groups. This group gave 166 explanatory variables. The accuracy of the model value by 0.98.",2015,
Triangular Simultaneous Equations Models in High-Dimensional Settings with Possibly Many Endogenous Regressors,"This paper explores the validity of the two-stage estimation procedures for triangular simultaneous linear equations models when the number(s) of the rst and/or second-stage regressors grow with and exceed the sample size n. In particular, the number of endogenous regressors in the main equation can also grow with and exceed n. The analysis concerns the sparsity case, i.e., k1(= k1n), the maximum number of non-zero components in the vectors of parameters in the rst-stage equations, and k2(= k2n), the number of non-zero components in the vector of parameters in the second-stage equation, are allowed to grow with n but small compared to n. I consider the high-dimensional version of the two-stage least square estimator where one obtains the tted regressors from the rst-stage regression by a least square estimator with l1regularization (Lasso or Dantzig selector) when the rst-stage regression concerns a large number of regressors relative to n, and then apply a Lasso technique with these tted regressors in the second-stage regression. I establish su cient conditions for estimation consistency in l2âˆ’norm and variable-selection consistency (i.e., the two-stage high-dimensional estimators correctly select the non-zero coe cients in the main equation with high probability). Depending on the underlying su cient conditions that are imposed, the rates of convergence in terms of the l2âˆ’error and the smallest sample size required to obtain these consistency results di er by factors involving k1 and/or k2. Simulations are conducted to gain insight on the nite sample performance of the two-stage high-dimensional estimator. âˆ—Haas School of Business, UC Berkeley, CA, 94720",2013,
Inversion of Chromophoric Dissolved Organic Matter Using Sparse Regression,"Chromophoric dissolved organic matter (CDOM) retrieval remains to be a challenging task in water color remote sensing research due to its highly spatial and temporal variability. In this paper, we present a novel CDOM retrieval algorithm that takes advantage of the sparse learning, which can simultaneously perform feature selection and parameter estimation. More specifically, by incorporating the band interaction terms into the original spectral matrix and let it be the basis matrix, then the inversion task can be converted to a classical sparse regression problem, namely LASSO, which can be efficiently solved by the coordinate descend algorithm. Experimental results conducted on both simulated and in-situ datasets have demonstrated the efficiency and superiority of the proposed method over some conventional empirical algorithms.",2019,IGARSS 2019 - 2019 IEEE International Geoscience and Remote Sensing Symposium
A Novel Gene Signature-Based Model Predicts Biochemical Recurrence-Free Survival in Prostate Cancer Patients after Radical Prostatectomy,"Abstract: Currently, decision-making regarding biochemical recurrence (BCR) following prostatectomy relies solely on clinical parameters. We therefore attempted to develop an integrated prediction model based on a molecular signature and clinicopathological features, in order to forecast the risk for BCR and guide clinical decision-making for postoperative therapy. Using high-throughput screening and least absolute shrinkage and selection operator (LASSO) in the training set, a novel gene signature for biochemical recurrence-free survival (BCRFS) was established. Validation of the prognostic value was performed in five other independent datasets, including our patient cohort. Multivariate Cox regression analysis was performed to evaluate the importance of risk for BCR. Time-dependent receiver operating characteristic (tROC) was used to evaluate the predictive power. In combination with relevant clinicopathological features, a decision tree was built to improve the risk stratification. The gene signature exhibited a strong capacity for identifying high-risk BCR patients, and multivariate Cox regression analysis demonstrated that the gene signature consistently acted as a risk factor for BCR. The decision tree was successfully able to identify the high-risk subgroup. Overall, the gene signature established in the present study is a powerful predictor and risk factor for BCR after radical prostatectomy.",2019,Cancers
Thermochemistry of gas-phase and surface species via LASSO-assisted subgraph selection,"Graph theory-based regression techniques, such as group additivity, have widely been implemented for fast estimation of thermochemistry of large molecules. The essence of these techniques lies in graphs that molecules are decomposed to. These graphs are selected based on heuristics and as a result, they may not give optimal accuracy and are hard to choose for non-nearest-neighbor electronic effects such as ring strain, steric hindrance, and resonance structures. Here, we explore LASSO, a feature selection algorithm, to select the optimal set of graph descriptors for predicting the standard enthalpy of formation, Î”fHÂ°. We gather hydrocarbon gas-phase data from the NIST Webbook and the Burcat's databases. We find that models using LASSO-based graph descriptors from the exhaustively enumerated graph descriptor space predict Î”fHÂ° more accurately than the traditional group additivity. We compare our framework with state-of-the-art machine-learning models for the QM9 data set. The mean absolute error of 1.39 kcal molâˆ’1 is comparable to published machine learning models. To cope with the computational cost of complete enumeration, we present: (1) a semi-supervised LASSO learning method and (2) an adsorbate subgraph mining algorithm. The former prunes the graph descriptor space on-the-fly during the LASSO regression and is applied to a gas-phase hydrocarbon data set. The latter enumerates a truncated graph descriptor space from adsorbate graphs of surface science data. For lignin monomer adsorbates on Pt(111), considered here as an illustrative example, descriptors selected from the adsorbate subgraph space result in a mean absolute error and a root mean square error of 2.08 and 3.03 kcal molâˆ’1, respectively. We discuss a simple method that identifies outliers in descriptor space that result in large model errors so the accuracy can be improved with the addition of suitable data.",2018,Reaction Chemistry and Engineering
Parametric and semi-parametric models for predicting genomic breeding values of complex traits in Nelore cattle,"Animal breeding aims to improve economic productivity of future generations of domestic species through selection. Most of the traits of economic interest in livestock have a complex and quantitative expression i.e. are influenced by a large number of genes and affected by environmental factors. Statistical analysis of phenotypes and pedigree information allows estimating the breeding values of the selection candidates based on infinitesimal model. A large amount of genomic data is now available for the identification and selection of genetically superior individuals with the potential to increase the accuracy of prediction of genetic values and thus, the efficiency of animal breeding programs. Numerous studies have been conducted in order to identify appropriate methodologies to specific breeds and traits, which will result in more accurate genomic estimated breeding values (GEBVs). Therefore, the objective of this study was to verify the possibility of applying semi-parametric models for genomic selection and to compare their ability of prediction with those of parametric models for real (carcass, meat quality, growth and reproductive traits) and simulated data. The phenotypic and pedigree information used were provided by farms belonging to four animal breeding programs which represent eleven farms. For carcass and meat quality traits, the data set contained 3,643 records for rib eye area (REA), 3,619 records for backfat thickness (BFT), 3,670 records for meat tenderness (TEN) and 3,378 observations for hot carcass weight (HCW). A total of 825,364 records for yearling weight (YW) and 166,398 for age at first calving (AFC) were used as growth and reproductive traits of Nelore cattle. Genotypes of 2,710, 2,656, 2,749, 2,495, 4,455 and 1,760 animals were available for REA, BFT, TEN, HCW, YW and AFC, respectively. After quality control, approximately 450,000 single nucleotide polymorphisms (SNP) remained. Methods of analysis were genomic BLUP (GBLUP), single-step GBLUP (ssGBLUP), Bayesian LASSO (BL) and the semi-parametric approaches Reproducing Kernel Hilbert Spaces (RKHS) regression and Kernel Averaging (KA). A five-fold cross-validation with thirty random replicates was carried out and models were compared in terms of their prediction mean squared error (MSE) and accuracy of prediction (ACC). The ACC ranged from 0.39 to 0.40 (REA), 0.38 to 0.41 (BFT), 0.23 to 0.28 (TEN), 0.33 to 0.35 (HCW), 0.36 to 0.51 (YW) and 0.49 to 0.56 (AFC). For all traits, the GBLUP and BL models showed very similar prediction accuracies. For REA, BFT and HCW, models provided similar prediction accuracies, however RKHS regression had the best fit across traits considering multiple-step models and compared to KA. For traits which have a higher number of animals with phenotypes compared to the number of those with genotypes (YW and AFC), the ssGBLUP is indicated. Judged by overall performance, across all traits, the RKHS regression is particularly appealing for application in genomic selection, especially for low heritability traits. Simulated genotypes, pedigree, and phenotypes for four traits A, B, C and D were obtained using heritabilities based on real data (0.09, 0.12, 0.36 and 0.39 for each trait, respectively). The simulated genome",2017,
Least Absolute Shrinkage and Selection Operator as a Multivariate Calibration Tool for Simultaneous Determination of Diphenylamine and its Nitro Derivatives in Propellants,"Determination of diphenylamine (DPA) and its nitro derivatives received great attention for storing, deposition and on-time usage of propellants. Herein, we present a novel and simple method for simultaneous determination of DPA and its nitro derivatives in solid propellants using UV-Vis spectroscopy and chemometrics techniques. The UV-Vis spectra of the analytes revealed strong overlap and it was difficult to determine them individually in their mixture without any separation and purification. To tackle the overlapping problem in collected spectra, analysis of first-order UV-Vis data was performed using multivariate calibration techniques. In this way, principle component regression (PCR), different modes of partial least square (PLS) and least absolute shrinkage and selection operator (LASSO) have been used for correlating the collected spectra to the concentration of DPAs in synthetic and real samples. The important variables were selected by confining the L1-norm of the regression coefficients in multivariate model via the shrinkage and selection operator in LASSO approach. The results obtained by LASSO regression technique in this work were superior to those obtained by different modes of PLS algorithm. Moreover, it is shown that LASSO can be used as a reliable variable selection and modeling technique in multivariate calibration studies. Generally, the proposed strategy in this work is simple, non-destructive, low-cost and rapid and can be effectively applied for simultaneous determination of DPA and its nitro derivatives in solid propellants.",2018,"Propellants, Explosives, Pyrotechnics"
"An Algorithmic Theory of Dependent Regularizers, Part 1: Submodular Structure","We present an exploration of the rich theoretical connections between several classes of regularized models, network flows, and recent results in submodular function theory. This work unifies key aspects of these problems under a common theory, leading to novel methods for working with several important models of interest in statistics, machine learning and computer vision. 
In Part 1, we review the concepts of network flows and submodular function optimization theory foundational to our results. We then examine the connections between network flows and the minimum-norm algorithm from submodular optimization, extending and improving several current results. This leads to a concise representation of the structure of a large class of pairwise regularized models important in machine learning, statistics and computer vision. 
In Part 2, we describe the full regularization path of a class of penalized regression problems with dependent variables that includes the graph-guided LASSO and total variation constrained models. This description also motivates a practical algorithm. This allows us to efficiently find the regularization path of the discretized version of TV penalized models. Ultimately, our new algorithms scale up to high-dimensional problems with millions of variables.",2013,arXiv: Machine Learning
An empirical Bayes method for estimating epistatic effects of quantitative trait loci.,"The genetic variance of a quantitative trait is often controlled by the segregation of multiple interacting loci. Linear model regression analysis is usually applied to estimating and testing effects of these quantitative trait loci (QTL). Including all the main effects and the effects of interaction (epistatic effects), the dimension of the linear model can be extremely high. Variable selection via stepwise regression or stochastic search variable selection (SSVS) is the common procedure for epistatic effect QTL analysis. These methods are computationally intensive, yet they may not be optimal. The LASSO (least absolute shrinkage and selection operator) method is computationally more efficient than the above methods. As a result, it has been widely used in regression analysis for large models. However, LASSO has never been applied to genetic mapping for epistatic QTL, where the number of model effects is typically many times larger than the sample size. In this study, we developed an empirical Bayes method (E-BAYES) to map epistatic QTL under the mixed model framework. We also tested the feasibility of using LASSO to estimate epistatic effects, examined the fully Bayesian SSVS, and reevaluated the penalized likelihood (PENAL) methods in mapping epistatic QTL. Simulation studies showed that all the above methods performed satisfactorily well. However, E-BAYES appears to outperform all other methods in terms of minimizing the mean-squared error (MSE) with relatively short computing time. Application of the new method to real data was demonstrated using a barley dataset.",2007,Biometrics
