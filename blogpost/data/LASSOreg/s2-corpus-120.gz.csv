title,abstract,year,journal
Machine Learning Assisted MRI Characterization for Diagnosis of Neonatal Acute Bilirubin Encephalopathy,"Background: The use of magnetic resonance imaging (MRI) in diagnosis of neonatal acute bilirubin encephalopathy (ABE) in newborns has been limited by its difficulty in differentiating confounding image contrast changes associated with normal myelination. This study aims to demonstrate the feasibility of building a machine learning prediction model based on radiomics features derived from MRI to better characterize and distinguish ABE from normal myelination. Methods: In this retrospective study, we included 32 neonates with clinically confirmed ABE and 29 age-matched controls with normal myelination. Radiomics features were extracted from the manually segmented region of interest (ROI) on T1-weighted spin echo images, followed by the feature selection using two-sample independent t-test, least absolute shrinkage and selection operator (Lasso) regression, and Pearson's correlation matrix. Additional feature quantifying the relative mean intensity of ROI was defined and calculated. A prediction model based on the selected features was built to classify ABE and normal myelination using multiple machine learning classifiers and a leave-one-out cross-validation scheme. Receiver operating characteristics (ROC) analysis was used to evaluate the prediction performance with the area under the curve (AUC) and feature importance ranked based on the Fisher score. Results: Among 1319 radiomics features, one radiologist-defined intensity-based feature and 12 texture features were selected as the most discriminative features. Based on these features, decision trees had the best classification performance with the largest AUC of 0.946, followed by support vector machine (SVM), tree-bagger, logistic regression, NaÃ¯ve Bayes, discriminant analysis, and k-nearest neighborhood (KNN), which have an AUC of 0.931, 0.925, 0.905, 0.891, 0.883, and 0.817, respectively. The relative mean intensity outperformed other 12 texture features in differentiating ABE from controls. Conclusions: The results from this study demonstrated a new strategy of characterizing ABE-induced intensity and morphological changes in MRI, which are difficult to be recognized, interpreted, or quantified by the routine experience and visual-based reading strategy. With more quantitative and objective measurements, the reported machine learning assisted radiomics features-based approach can improve the diagnosis and support clinical decision-making.",2019,Frontiers in Neurology
Composite quantile regression and variable selection of the partial linear single-index models,"In this paper, we propose a composite minimizing average check loss estimation(CMACLE) method for the composite quantile regression(CQR) of the partial linear single-index model(PLSIM) by local linear method. Based on constructive approach, the estimators by CMACLE are able to achieve the best convergence rate. The asymptotical normalities of the estimators are also derived. Meanwhile, the asymptotic efficiency of the CQR estimation relative to the mean regression are investigated. Further more, we propose a variable selection method for the CQR of PLSIM by combining the CMACLE procedure with the adaptive LASSO penalized method. The oracle properties of the proposed variable selection method are also established. Simulations with various non-normal errors and a real data analysis are conducted to assess the finite sample property of the proposed estimation and variable selection methods.",2014,
Short-Term Demand Prediction Method for Online Car-Hailing Services Based on a Least Squares Support Vector Machine,"The purpose of this paper is to study the short-term demand prediction for online car-hailing services problem. Prediction of short-term network car demand can provide many benefits such as an increase in the income of network car drivers. In addition, demand prediction is an important resource for recommendation systems, carpooling systems, and network car scheduling; and therefore, predicting the demand for network cars has great significance. In the last few decades, scholars have studied various problems related to short-term demand prediction for online car-hailing services based on clustering algorithms and regression algorithms. However, these studies are still problematic because the accuracy of demand prediction is not high enough. Therefore, this paper studies a method of improving the accuracy and the efficiency of demand prediction. Due to the high prediction accuracy and the fast training efficiency of least squares support vector machine (LS-SVM), a short-term demand prediction method for online car-hailing services based on LS-SVM is proposed. The modeling process involves selecting the dependent and independent variables, the basic principles of the LS-SVM, the kernel function and superparameter, model training, and prediction. In a numerical experiment, we use network car order data as the network car demand data to test the model. We show that the experimental results with the LS-SVM method implemented in this paper and then compare the model with lasso linear regression, nearest neighbor regression, decision tree regression, and neural network. The experimental results show that the short-term demand prediction model for online car-hailing services based on LS-SVM performs better than the other methods.",2019,IEEE Access
Long intergenic non-coding RNA detection benefited from integrative modeling of (Epi)genomic data,"Prediction of long intergenic non-coding RNAs (lincRNAs) is a prerequisite to analyze sequence features of non-coding RNAs and explore their regulatory function. Genomic sequence features provide fundamental backgrounds for lincRNA predictions, due to that sequence information at least partially aids such predictions. However, genomic sequence alone seems to reach an end involving sensitivity for lincRNA prediction in eukaryotes. Chromatin factors leave marks that can be captured by high-throughput approaches such as ChIP-seq are also important features, as revealed by previous studies. We demonstrate that the performance of lincRNA predictions can be improved when incorporating both high-throughput chromatin modification and genomic sequence features by logistic regression with LASSO regularization. The discriminating features include H3K4me1, H3K27ac, H3K9me3, Open Reading Frames and several repeat elements. Importantly, chromatin information is suggested to be complementary to genomic sequence information, highlighting the importance of an integrated model. We also show that the lincRNA expression specificity can be efficiently modeled by the chromatin data with same developmental stage. The study not only supports the biological hypothesis that chromatin factors can regulate developmental-stage-specific expression of lincRNAs, also reveals the discriminating features between lincRNA and coding genes.",2013,2013 10th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)
Differentially Expressed lncRNAs in Gastric Cancer Patients: A Potential Biomarker for Gastric Cancer Prognosis,"Current studies indicate that long non-coding RNAs (lncRNAs) are frequently aberrantly expressed in cancers and implicated with prognosis in gastric cancer (GC). We intended to generate a multi-lncRNA signature to improve prognostic prediction of GC. By analyzing ten paired GC and adjacent normal mucosa tissues, 339 differentially expressed lncRNAs were identified as the candidate prognostic biomarkers in GC. Then we used LASSO Cox regression method to build a 12-lncRNA signature and validated it in another independent GEO dataset. An innovative 12-lncRNA signature was established, and it was significantly associated with the disease free survival (DFS) in the training dataset. By applying the 12-lncRNA signature, the training cohort patients could be categorized into high-risk or low-risk subgroup with significantly different DFS (HR = 4.52, 95%CI= 2.49-8.20, P < 0.0001). Similar results were obtained in another independent GEO dataset (HR=1.58, 95%CI=1.05 - 2.38, P=0.0270). Further analysis showed that the prognostic value of this 12-lncRNA signature was independent of AJCC stage and postoperative chemotherapy. Receiver operating characteristic (ROC) analysis showed that the area under receiver operating characteristic curve (AUC) of combined model reached 0.869. Additionally, a well-performed nomogram was constructed for clinicians. Moreover, single-sample gene-set enrichment analysis (ssGSEA) showed that a group of pathways related to drug resistance and cancer metastasis significantly enriched in the high risk patients. A useful innovative 12-lncRNA signature was established for prognostic evaluation of GC. It might complement clinicopathological features and facilitate personalized management of GC.",2017,Journal of Cancer
Analysis of Penalized Regression Methods in a Simple Linear Model on the High-Dimensional Data,"Shrinkage methods for linear regression were developed over the last ten years to reduce the weakness of ordinary least squares (OLS) regression with respect to prediction accuracy. And, high dimensional data are quickly growing in many areas due to the development of technological advances which helps collect data with a large number of variables. In this paper, shrinkage methods were used to evaluate regression coefficients effectively for the high-dimensional multiple regression model, where there were fewer samples than predictors. Also, regularization approaches have become the methods of choice for analyzing such high dimensional data. We used three regulation methods based on penalized regression to select the appropriate model. Lasso, Ridge and Elastic Net have desirable features; they can simultaneously perform the regulation and selection of appropriate predictor variables and estimate their effects. Here, we compared the performance of three regular linear regression methods using cross-validation method to reach the optimal point. Prediction accuracy using the least squares error (MSE) was evaluated. Through conducting a simulation study and studying real data, we found that all three methods are capable to produce appropriate models. The Elastic Net has better prediction accuracy than the rest. However, in the simulation study, the Elastic Net outperformed other two methods and showed a less value in terms of MSE.",2019,American Journal of Theoretical and Applied Statistics
Data-driven Optimal Transport Cost Selection for Distributionally Robust Optimization,"Some recent works showed that several machine learning algorithms, such as square-root Lasso, Support Vector Machines, and regularized logistic regression, among many others, can be represented exactly as distributionally robust optimization (DRO) problems. The distributional uncertainty set is defined as a neighborhood centered at the empirical distribution, and the neighborhood is measured by optimal transport distance. In this paper, we propose a methodology which learns such neighborhood in a natural data-driven way. We show rigorously that our framework encompasses adaptive regularization as a particular case. Moreover, we demonstrate empirically that our proposed methodology is able to improve upon a wide range of popular machine learning estimators.",2019,
Ultrahigh Dimensional Variable Selection for Interpolation of Point Referenced Spatial Data: A Digital Soil Mapping Case Study,"Modern soil mapping is characterised by the need to interpolate point referenced (geostatistical) observations and the availability of large numbers of environmental characteristics for consideration as covariates to aid this interpolation. Modelling tasks of this nature also occur in other fields such as biogeography and environmental science. This analysis employs the Least Angle Regression (LAR) algorithm for fitting Least Absolute Shrinkage and Selection Operator (LASSO) penalized Multiple Linear Regressions models. This analysis demonstrates the efficiency of the LAR algorithm at selecting covariates to aid the interpolation of geostatistical soil carbon observations. Where an exhaustive search of the models that could be constructed from 800 potential covariate terms and 60 observations would be prohibitively demanding, LASSO variable selection is accomplished with trivial computational investment.",2016,PLoS ONE
Preconditioning to comply with the Irrepresentable Condition,"Preconditioning is a technique from numerical linear algebra that can accelerate algorithms to solve systems of equations. In this paper, we demonstrate how preconditioning can circumvent a stringent assumption for sign consistency in sparse linear regression. Given $X \in R^{n \times p}$ and $Y \in R^n$ that satisfy the standard regression equation, this paper demonstrates that even if the design matrix $X$ does not satisfy the irrepresentable condition for the Lasso, the design matrix $F X$ often does, where $F \in R^{n\times n}$ is a preconditioning matrix defined in this paper. By computing the Lasso on $(F X, F Y)$, instead of on $(X, Y)$, the necessary assumptions on $X$ become much less stringent. 
Our preconditioner $F$ ensures that the singular values of the design matrix are either zero or one. When $n\ge p$, the columns of $F X$ are orthogonal and the preconditioner always circumvents the stringent assumptions. When $p\ge n$, $F$ projects the design matrix onto the Stiefel manifold; the rows of $F X$ are orthogonal. We give both theoretical results and simulation results to show that, in the high dimensional case, the preconditioner helps to circumvent the stringent assumptions, improving the statistical performance of a broad class of model selection techniques in linear regression. Simulation results are particularly promising.",2012,arXiv: Statistics Theory
Gamma Response Regression with Percentile Lasso and Ridge to Estimate Extreme Rainfall,"An extreme rainfall can cause flood or drought and so it is necessary to develop a prediction model to anticipate the impact of extreme rainfall phenomenon. The statistical downscaling is one of the existing methods to analyze the rainfall data based on Global Circulation Model (GCM) output. The rainfall data can be Gamma distribution because the data can be zero or more than zero. The extreme rainfall data could be in the tail of Gamma distribution and so General Linier Model (GLM) with Gamma response variable.The GCM output is usually high dimensional data and multicollinearity. Least absolute shrinkage and selection operator (lasso) and ridge penalty included in a model can overcome such problems. The value of optimum penalty can be estimated using cross validation. However, the method is very sensitive tothe k-fold size and the results are unstable. This weakness is overcome by repeating cross validation and finding some percentiles. This paper discusses a regression model with the response of Gamma distribution to predict extreme rainfall based on the25th,50th,75th,90th,and95thpercentiles. The result shows that,based on the value of root mean square error of prediction (RMSEP), the model with lasso (RMSEP=48.42 mm/month) on percentile 75th is better than that with ridge (RMSEP=50.44 mm/month) on percentile 25th.",2016,
Fine mapping and subphenotyping implicates ADRA1B gene variants in psoriasis susceptibility in a Chinese population.,"AIM
A genomic region on 5q33.3 lies between and encompasses the IL12B and PTTG1 genes, and contains many potential psoriasis causal variants. We aimed to further examine the influence of variants in and around this region.


MATERIALS & METHODS
We used least absolute shrinkage and selection operator (LASSO)-based regression analysis to assess independent contributions of 2171 variants to psoriasis susceptibility and tested them for association with different clinical psoriasis subtypes.


RESULTS
We found that ADRA1B gene variants contribute to psoriasis in Chinese population. ADRA1B gene variants have a stronger association with moderate-to-severe disease group and an earlier age at onset of psoriasis than IL-12B and PTTG1 variants.


CONCLUSION
The association of variants in the ADRA1B gene with psoriasis could explain why variants in the IL-12B, ADRA1B and PTTG1 gene regions are associated with psoriasis.",2019,Epigenomics
Development and validation of a simple-to-use nomogram for predicting refractory Mycoplasma pneumoniae pneumonia in children.,"OBJECTIVE
This study aimed to develop and validate a simple-to-use nomogram for predicting refractory Mycoplasma pneumoniae pneumonia (RMPP) in children.


METHODS
A total of 73 children with RMPP and 146 children with general Mycoplasma pneumoniae pneumonia were included. Clinical, laboratory, and radiological data were obtained. A least absolute shrinkage and selection operator (LASSO) regression model was used to determine optimal predictors. The nomogram was plotted by multivariable logistic regression. The performance of the nomogram was assessed by calibration, discrimination, and clinical utility.


RESULTS
The LASSO regression analysis identified lactate dehydrogenase, albumin, neutrophil ratio, and high fever as significant predictors of RMPP. This nomogram-illustrated model showed good discrimination, calibration, and clinical value. The area under the receiver operating characteristic curve of the nomogram was 0.884 (95% CI, 0.823-0.945) in the training set and 0.881 (95% CI, 0.807-0.955) in the validating set. Calibration curve and Hosmer-Lemeshow test showed good consistency between the predictions of the nomogram and the actual observations, and decision curve analysis showed that the nomogram was clinically useful.


CONCLUSION
A simple-to-use nomogram for predicting RMPP in early stage was developed and validated. This may help physicians recognize RMPP earlier.",2020,Pediatric pulmonology
Adaptive multinomial regression with overlapping groups for multi-class classification of lung cancer,"Multi-class classification has attracted much attention in cancer diagnosis and treatment and many machine learning methods have emerged for addressing this issue recently. However, class imbalance and gene selection problems occur in classifying lung cancer data. In this paper, an adaptive multinomial regression with a sparse overlapping group lasso penalty is proposed to perform classification and grouped gene selection for lung cancer gene expression data. An overlapped grouping strategy with biological interpretability is proposed, which highlights the importance of gene groups from the minority classes. By using the conditional mutual information, the gene significance within each group is evaluated and the data-driven weights are constructed. Based on the grouping strategy and constructed weights, a regularized adaptive multinomial regression is presented and the solving algorithm is developed, which can not only select the important gene groups for each class in performing multi-class classification, but also adaptively select important genes within each group. The experiment results show that the proposed method significantly outperforms the other 6 methods on classification accuracy, and the selected genes are disease-causing genes for lung cancer.",2018,Computers in biology and medicine
Partial least squares method based on least absolute shrinkage and selection operator,"In many multivariate statistical techniques, a set of linear functions of the original variables is produced. But this kind of model derived is difficult to interpret, Such as principle component regression (PCR) and partial least squares regression (PLSR), they cannot select variables. The approach least absolute shrinkage and selection operator (LASSO) can easily produce sparse solutions and select variables during estimate parameters. This article proposes a new technique for interpretation based on these properties, it's a combination of partial least squares (PLS) and LASSO and can easily interpret regression models. This method will be more favorable for large number of variables compared to PLS.",2010,2010 3rd International Conference on Advanced Computer Theory and Engineering(ICACTE)
A hybrid regression technique for house prices prediction,"Usually, House price index represents the summarized price changes of residential housing. While for a single family house price prediction, it needs more accurate method based on location, house type, size, build year, local amenities, and some other factors which could affect house demand and supply. With limited dataset and data features, a practical and composite data pre-processing, creative feature engineering method is examined in this paper. The paper also proposes a hybrid Lasso and Gradient boosting regression model to predict individual house price. The proposed approach has recently been deployed as the key kernel for Kaggle Challenge â€œHouse Prices: Advanced Regression Techniquesâ€. The performance is promising as our latest score was ranked top 1% out of all competition teams and individuals.",2017,2017 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM)
Integrative Bayesian variable selection with gene-based informative priors for genome-wide association studies,"BackgroundGenome-wide Association Studies (GWAS) are typically designed to identify phenotype-associated single nucleotide polymorphisms (SNPs) individually using univariate analysis methods. Though providing valuable insights into genetic risks of common diseases, the genetic variants identified by GWAS generally account for only a small proportion of the total heritability for complex diseases. To solve this â€œmissing heritabilityâ€ problem, we implemented a strategy called integrative Bayesian Variable Selection (iBVS), which is based on a hierarchical model that incorporates an informative prior by considering the gene interrelationship as a network. It was applied here to both simulated and real data sets.ResultsSimulation studies indicated that the iBVS method was advantageous in its performance with highest AUC in both variable selection and outcome prediction, when compared to Stepwise and LASSO based strategies. In an analysis of a leprosy caseâ€“control study, iBVS selected 94 SNPs as predictors, while LASSO selected 100 SNPs. The Stepwise regression yielded a more parsimonious model with only 3 SNPs. The prediction results demonstrated that the iBVS method had comparable performance with that of LASSO, but better than Stepwise strategies.ConclusionsThe proposed iBVS strategy is a novel and valid method for Genome-wide Association Studies, with the additional advantage in that it produces more interpretable posterior probabilities for each variable unlike LASSO and other penalized regression methods.",2014,BMC Genetics
Randomized pick-freeze for sparse Sobol indices estimation in high dimension,"This article investigates selection of variables in high-dimension from a non-parametric regression model. In many concrete situations, we are concerned with estimating a non-parametric regression function f that may depend on a large number p of inputs variables. Unlike standard procedures, we do not assume that f belongs to a class of regular functions (Holder, Sobolev, ...), yet we assume that f is a square-integrable function with respect to a known product measure. Furthermore, observe that, in some situations, only a small number s of the coordinates actually affects f in an additive manner. In this context, we prove that, with only ð’ª(s logâ€‰p ) random evaluations of f , one can find which are the relevant input variables with overwhelming probability. Our proposed method is an unconstrained l 1 -minimization procedure based on the Sobolâ€™s method. One step of this procedure relies on support recovery using l 1 -minimization and thresholding. More precisely, we use a thresholded-LASSO to faithfully uncover the significant input variables. In this frame, we prove that one can relax the mutual incoherence property (known to require ð’ª(s 2 logâ€‰p ) observations) and still ensure faithful recovery from ð’ª(s Î± logâ€‰p ) observations for any 1 â‰¤ Î± â‰¤ 2.",2014,Esaim: Probability and Statistics
Aggregation of supports along the Lasso path,"In linear regression with fixed design, we propose two procedures that aggregate a data-driven collection of supports. The collection is a subset of the $2^p$ possible supports and both its cardinality and its elements can depend on the data. The procedures satisfy oracle inequalities with no assumption on the design matrix. Then we use these procedures to aggregate the supports that appear on the regularization path of the Lasso in order to construct an estimator that mimics the best Lasso estimator. If the restricted eigenvalue condition on the design matrix is satisfied, then this estimator achieves optimal prediction bounds. Finally, we discuss the computational cost of these procedures.",2016,
A Mathematical Analysis Of Mathematical Salaries and More,"In this final project of Experimental Mathematics Class (Spring 2019), we use the data of the tenured and tenure-track faculty in the Rutgers math department as a case study to demonstrate the statistical and mathematical relationships among several variables, e.g., the number of publications and citations, the rank of professorship and of course, the salaries. Different statistical tools, including simple and multi-variable regression, logistic regression, lasso and ridge regression, neural network and unsupervised learning, are exploited so that the results obtained from various methods can be easily compared.",2019,
Weighted composite quantile regression for single-index models,"In this paper we propose a weighted composite quantile regression (WCQR) estimation for single-index models. For parametric part, the WCQR is augmented using a data-driven weighting scheme. With the error distribution unspecified, the proposed estimators share robustness from quantile regression and achieve nearly the same efficiency as the semiparametric maximum likelihood estimator for a variety of error distributions including the Normal, Studentâ€™s t, Cauchy distributions, etc. Furthermore, based on the proposed WCQR, we use the adaptive-LASSO to study variable selection for parametric part in the single-index models. For nonparametric part, the WCQR is augmented combining the equal weighted estimators with possibly different weights. Because of the use of weights, the estimation bias is eliminated asymptotically. By comparing asymptotic relative efficiency theoretically and numerically, WCQR estimation all outperforms the CQR estimation and some other estimate methods. Under regularity conditions, the asymptotic properties of the proposed estimations are established. The simulation studies and two real data applications are conducted to illustrate the finite sample performance of the proposed methods.",2016,J. Multivar. Anal.
AcSel: selecting variables with accuracy in correlated datasets,"With the emergence of high-throughput technologies, it is possible to measure large amounts of data relatively at low cost. Such situations arise in many fields from sciences to humanities, and variable selection may be of great help to answer challenges that are specific to each of them. Variable selection may allow to know, among all measured variables, which are of interest and which are not. A lot of methods have been proposed to handle this issue, with the Lasso and other penalized regression as special cases. These methods fail in some cases and linear correlation between explanatory variables is the most common of these, especially in big datasets. In this article, we introduce AcSel, a wrapping algorithm able to enhance the accuracy of any variable selection method. To achieve this result, we use intensive computational simulations.",2015,arXiv: Computation
Quantile regression and variable selection of single-index coefficient model,"In this paper, a minimizing average check loss estimation (MACLE) procedure is proposed for the single-index coefficient model (SICM) in the framework of quantile regression (QR). The resulting estimators have the asymptotic normality and achieve the best convergence rate. Furthermore, a variable selection method is investigated for the QRSICM by combining MACLE method with the adaptive LASSO penalty, and we also established the oracle property of the proposed variable selection method. Extensive simulations are conducted to assess the finite sample performance of the proposed estimation and variable selection procedure under various error settings. Finally, we present a real-data application of the proposed approach.",2017,Annals of the Institute of Statistical Mathematics
Evaluation of Risk Factors for Parametrization of Cancer Models,"Cancer is the second most cause of death in Austria and around 38000 people are diagnosed with cancer each year [1]. The goal of this paper is to analyze methods for evaluation of risk factors in order to parametrize a micro simulation model for cancer prevalence. The focus of this paper is on modeling the survival time. This is done by the methods of survival analysis and model selection. Firstly, the survival function is estimated by the Kaplan-Meier estimate. Afterwards, a Cox proportional hazards regression is performed with all possible sets of parameters. These models are tested by twos with the likelihood ratio test in order to compare them. Another approach is the so-called Lasso method. This method puts a constraint on the sum of the absolute values of the regression coefficients and in most cases forces some of the coefficients to go to zero. The Akaike Information Criterion is also applied. All three methods are compared and the parameters which are supported, at least to a certain extent, by all of them are included in the estimation of the survival time of the prevalence model.",2015,Simul. Notes Eur.
SCAD-penalized quantile regression for high-dimensional data analysis and variable selection,"type=""main""> The present penalized quantile variable selection methods are only applicable to finite number of predictors or do not have oracle property associated with estimator. This technique is considered as an alternative to ordinary least squares regression in case of the outliers and the heavy-tailed errors existing in linear models. The variable selection through quantile regression with diverging number of parameters is investigated in this paper. The convergence rate of estimator with smoothly clipped absolute deviation penalty function is also studied. Moreover, the oracle property with proper selection of tuning parameter for quantile regression under certain regularity conditions is also established. In addition, the rank correlation screening method is used to accommodate ultra-high dimensional data settings. Monte Carlo simulations demonstrate finite performance of the proposed estimator. The results of real data reveal that this approach provides substantially more information as compared with ordinary least squares, conventional quantile regression, and quantile lasso.",2015,Statistica Neerlandica
Geographically weighted elastic net logistic regression,"AbstractThis paper develops a localized approach to elastic net logistic regression, extending previous research describing a localized elastic net as an extension to a localized ridge regression or a localized lasso. All such models have the objective to capture data relationships that vary across space. Geographically weighted elastic net logistic regression is first evaluated through a simulation experiment and shown to provide a robust approach for local model selection and alleviating local collinearity, before application to two case studies: county-level voting patterns in the 2016 USA presidential election, examining the spatial structure of socio-economic factors associated with voting for Trump, and a species presenceâ€“absence data set linked to explanatory environmental and climatic factors at gridded locations covering mainland USA.
 The approach is compared with other logistic regressions. It improves prediction for the election case study only which exhibits much greater spatial heterogeneity in the binary response than the species case study. Model comparisons show that standard geographically weighted logistic regression over-estimated relationship non-stationarity because it fails to adequately deal with collinearity and model selection. Results are discussed in the context of predictor variable collinearity and selection and the heterogeneities that were observed. Ongoing work is investigating locally derived elastic net parameters.
",2018,Journal of Geographical Systems
Recognizing Human Interactions Using Group Feature Relevance in Multinomial Kernel Logistic Regression,"We propose a supervised approach incorporating group feature sparsity in multi-class kernel logistic regression (GFR-MKLR). The need for group sparsity arises in several practical situations where a subset of a set of factors can explain a predicted variable and each factor consists of a group of variables. We apply our approach for predicting human interactions based on body parts motion (e.g., hands, legs, head, etc.) where image features are organised in groups corresponding to body parts. Our approach, leads to sparse models by assigning weights to groups of features having the highest discrimination between different types of interactions. Experiments conducted on the UT-Interaction dataset have demonstrated the performance of our method with regard to stat-of-art methods. Introduction Models with sparsity constraint on solutions plays a central role in many high-dimensional classification problems (Hastie et al. 2015; Tibshirani 1996). In some cases, explanatory variables can be grouped together into separate factors influencing prediction of classes (Rao et al. 2016). This is the case, for example, in real world human activities captured in videos where a single activity can be decomposed into co-occurring actions performed by different persons (e.g., hand shaking, hugging, meeting, etc.) (Noceti et al. 2014). Each action, performed by the person, incurs the motion of different body parts depending on the gestures performed in the action (Aggarwal et al. 2011). Therefore, having sparse classification models selecting features at the gesture level is an important issue for better activity recognition. Sparse models usually prevent over-fitting and lead to more interpretable solutions in high-dimensional machine learning problems (Hastie et al. 2015; Rao et al. 2016). Group sparsity has been proposed in the past mainly as an extension to the least absolute shrinkage and selection operator (LASSO) method (Tibshirani et al. 1996; Yuan et al. 2006). Contrarily to LASSO which performs feature selection for individual features, group LASSO performs selection for entire groups of variables, where each group constitutes a separate explanatory factor (Hastie et al. 2015). Copyright c âƒ 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. In particular, it can be assumed that the optimal sparsity will tend to involve clusters or groups of coefficients, corresponding to preexisting groups of features (Yuan et al. 2006). While the form of the groups can be a priori known (e.g., in activity recognition, a group can correspond to all features associated with a part of the body performing a gesture), the subset of groups that is relevant to the classification task at hand can be unknown. Recently, group LASSO methods have enjoyed a tremendous success in high dimensional classification problems (Vincent et al. 2014; Wang et al. 2008; Wang et al. 2013). It remains, however, that most proposed methods are limited to binary classification based on linear models. One of the earliest work on two person interaction recognition using motion trajectories was proposed by Datta et al. (Datta et al. 2002). This method tracks the trajectories of different parts of the body, then tries to dissociate violent from non-violent actions. In (Park et la. 2003), a hierarchical Bayesian network (BN) is proposed for interaction recognition. In this method, low-level nodes of the network are used to represent the pose of body parts, whereas high-level nodes estimate the overall body pose. (Ryoo et al. 2009) designed a method to measure structural similarity between sets of spatiotemporal features extracted from two videos. The authors then derived a kernel for action classification based on support vector machines (SVM). In the same vein, (Slimani et al. 2014) designed a correlation matrix between spatiotemporal features used to represent and classify interactions between persons using SVM. In (Meng et al. 2012), location and appearance of human joints are combined for interaction representation, which are then classified using SVM. Similarly to our approach, (Yuan et al. 2012) groups trajectories of densely-sampled key points in videos to form interaction components. Then, different interactions are compared using a spatiotemporal context kernel plugged in an SVM classifier. In (Do and Pustejovsky 2017), authors propose a compact representation for human-object interactions by comparing quantitative and qualitative features at two levels: frame level features (by visualizing human and objet trajectories) and event level (by summarizing the change between first, middle and last frames across event duration). These methods, however, incorporate sparsity at the level of interaction representation, whereas our method focuses on integrating sparsity in the classification stage of interactions. In this paper, we are interested in extending group sparsity in a multi-class setting for recognizing activities involving interactions between two individuals. Building on the success of kernel-based classification methods applied to single action recognition in videos (e.g., walking, sitting, etc.) (e.g., walking, sitting, etc.) (Aggarwal et al.2011; Schuldt et al. 2004), we propose a sparse model based on multinomial kernel logistic regression for recognition of activities involving interactions between persons. We represent motion of each moving person by tracking trajectories of key joints over the video frames. A group of features is defined for each trajectory and the concatenation of the group of features gives the final representation of each interaction. The direction of the trajectories generated by the persons are different among different types of interactions. To emphasize these differences and select the most discriminative trajectories, group of features weighting is integrated in kernel logistic regression instead of weighting each feature as in the case of LASSO. We show that our algorithm yields better results in comparison with several recent methods. The rest of the paper is organized as follows. Section 2 provides details about interaction representation. Section 3 presents the proposed model for interaction classification. Section 4 presents some experimental results. We conclude the paper with a conclusion and future work perspectives. Interaction representation and classification In our method, we represent interactions using features extracted from the motion of human joints. On the constructed feature space, interaction recognition is performed using sparse multinomial kernel logistic regression. In the following section, we first describe human interaction representation. Then, we give the details of the proposed classification model based on group sparsity. The outline of the steps of our method is shown in Figure 1. Interaction representation For an input video with T frames, we extract the trajectory for all key points corresponding to a set of human joints J. For this purpose, we track each joint over video frames Ft, t âˆˆ {1, ..., T} using the algorithm proposed in (Yang et al. 2011). There are a total of 7 joints in the following order: head (H), right shoulder (RS), left shoulder (LS), right hand (RH), left hand (LH), right foot (RF) and left foot (LF), which are tracked over the video frames. The concatenation of joint locations (l1, l2, ..., lT ) with lt = (xt, yt) form the interaction trajectories trJi , i âˆˆ {1, ..., 14} (7 trajectories per person). In order to eliminate false joints detections over frames, we use a median filter to smooth the resulting trajectories. The static joints which give points or small trajectories are retained since the goal is to prove the implication or not of a joint movement to discriminate between different interactions. Figure 2 shows examples of extracted trajectories for punch, kick and point interactions. From each trajectory trJi , two features are computed to describe joints shape and motion. Given a trajectory of length T, its shape is described by : (âˆ†l1,âˆ†l2, ...,âˆ†lTâˆ’1), with âˆ†lt = (âˆ†lxt ,âˆ†lyt) = (xt+1âˆ’xt, yt+1âˆ’yt). The final displacement vectors according to the coordinates x and y are normalized as follow: Dx,y = (âˆ†l1,âˆ†l2, ...,âˆ†lTâˆ’1) âˆ‘Tâˆ’1 j=1 âˆ¥âˆ†ljâˆ¥ , (1) The normalized histogram of displacement HOD is then obtained by concatenating the histograms of Dx and Dy as follow HOD = [HODx,HODy].The motion of each trajectory is described by a local curvature in space and time coordinates, respectively, x,y and t (Rao et al. 2002). The curvature Ct at each frame t is defined in Eq. (2) Ct = x â€² ty â€²â€² t âˆ’ y â€² tx â€²â€² t (x â€²2 t + y â€²2 t + 1) 3/2 , (2) with, x â€² t, y â€² t, x â€²â€² t andy â€²â€² t are the first and second order temporal derivatives of the trajectory position, with: x â€² t = âˆ†lxt , y â€² t = âˆ†lyt , x â€²â€² t = âˆ†x â€² t, y â€²â€² t = âˆ†y â€² t and âˆ†t = 1 knowing that the trajectories are extracted over successive frames. The shape and the motion of the given trajectory is then described by a concatenation of normalized histogram of displacement and curvature to form a group of features: [HOD,HOC]. Finally, for each video two descriptors per trajectory are concatenated following a certain spatial order starting from right to left and from up to bottom as follow: [(HODtrH , HOCtrH )(p1), (HODtrRS ,HOCtrRS )(p1), ..., (HODtrLF ,HOCtrLF )(p1),HOD(trH ,HOCtrH )(p2), ..., (HODtrLF ,HOCtrLF )(p2)], p1 and p2 refers to person 1 and person 2 in a current video frame. Interaction classification Since each interaction is represented by a group of features of each trajectory, we aim in this section to discriminate between activities by weighting a discriminant group of features according to their contribution. This work is an extension of our previous work (Ouyed et Allili. 2014), instead of weighting individual features in a multinomial kernel logistic regression the weights are attributed for a group of features which refers to a trajectory descriptors. details of",2018,
Predicting EMG Data from M1 Neurons with Variational Bayesian Least Squares,"An increasing number of projects in neuroscience requires the statistical analysis of high dimensional data sets, as, for instance, in predicting behavior from neural firing or in operating artificial devices from brain recordings in brain-machine interfaces. Linear analysis techniques remain prevalent in such cases, but classical linear regression approaches are often numerically too fragile in high dimensions. In this paper, we address the question of whether EMG data collected from arm movements of monkeys can be faithfully reconstructed with linear approaches from neural activity in primary motor cortex (M1). To achieve robust data analysis, we develop a full Bayesian approach to linear regression that automatically detects and excludes irrelevant features in the data, regularizing against overfitting. In comparison with ordinary least squares, stepwise regression, partial least squares, LASSO regression and a brute force combinatorial search for the most predictive input features in the data, we demonstrate that the new Bayesian method offers a superior mixture of characteristics in terms of regularization against overfitting, computational efficiency and ease of use, demonstrating its potential as a drop-in replacement for other linear regression techniques. As neuroscientific results, our analyses demonstrate that EMG data can be well predicted from M1 neurons, further opening the path for possible real-time interfaces between brains and machines.",2005,
Novel Multiple miRNA-Based Signatures for Predicting Overall Survival and Recurrence-Free Survival of Colorectal Cancer Patients,"BACKGROUND Colorectal cancer (CRC) has become a heavy health burden around the world, accounting for about 10% of newly diagnosed cancer cases. In the present study, we aimed to establish the miRNA-based prediction signature to assess the prognosis of CRC patients. MATERIAL AND METHODS A total of 451 CRC patients' expression profiles and clinical information were download from the TCGA database. LASSO Cox regression was conducted to construct the overall survival (OS)- and recurrence-free survival (RFS)-associated prediction signatures, by which CRC patients were divided into low- and high-risk groups. Kaplan-Meier (K-M) curve and receiver operating characteristic (ROC) curves were used to explore the discriminatory ability and stability of the signatures. Functional enrichment analyses were performed to identify the probable mechanisms. RESULTS miRNA-216a, miRNA-887, miRNA-376b, and miRNA-891a were used to build the prediction formula associated with OS, while miR-1343, miR-149, miR-181a-1, miR-217, miR-3130-1, miR-378a, miR-542, miR-6716, miR-7-3, miR-7702, miR-677, and miR-891a were obtained to construct the formula related to RFS. K-M curve and ROC curve revealed the good discrimination and efficiency of OS in the training (P<0.001, AUC=0.712) and validation cohorts (P=0.019, AUC=0.657), as well as the results of RFS in the training (P<0.001, AUC=0.714) and validation cohorts (P=0.042, AUC=0.651). The function annotations for the targeted genes of these miRNAs show the potential mechanisms of CRC. CONCLUSIONS We established 2 novel miRNA-based prediction signatures of OS and RFS, which are reliable tools to assess the prognosis of CRC patients.",2019,Medical Science Monitor : International Medical Journal of Experimental and Clinical Research
"SuRF: a New Method for Sparse Variable Selection, with Application in Microbiome Data Analysis","In this paper, we present a new variable selection method for regression and classification purposes. Our method, called Subsampling Ranking Forward selection (SuRF), is based on LASSO penalised regression, subsampling and forward-selection methods. SuRF offers major advantages over existing variable selection methods in terms of both sparsity of selected models and model inference. We provide an R package that can implement our method for generalized linear models. We apply our method to classification problems from microbiome data, using a novel agglomeration approach to deal with the special tree-like correlation structure of the variables. Existing methods arbitrarily choose a taxonomic level a priori before performing the analysis, whereas by combining SuRF with these aggregated variables, we are able to identify the key biomarkers at the appropriate taxonomic level, as suggested by the data. We present simulations in multiple sparse settings to demonstrate that our approach performs better than several other popularly used existing approaches in recovering the true variables. We apply SuRF to two microbiome data sets: one about prediction of pouchitis and another for identifying samples from two healthy individuals. We find that SuRF can provide a better or comparable prediction with other methods while controlling the false positive rate of variable selection.",2019,arXiv: Methodology
Identification of optimal prediction models using multi-omic data for selecting hybrid rice,"Genomic prediction benefits hybrid rice breeding by increasing selection intensity and accelerating breeding cycles. With the rapid advancement of technology, other omic data, such as metabolomic data and transcriptomic data, are readily available for predicting breeding values for agronomically important traits. In this study, the best prediction strategies were determined for yield, 1000 grain weight, number of grains per panicle, and number of tillers per plant of hybrid rice (derived from recombinant inbred lines) by comprehensively evaluating all possible combinations of omic datasets with different prediction methods. It was demonstrated that, in rice, the predictions using a combination of genomic and metabolomic data generally produce better results than single-omics predictions or predictions based on other combined omic data. Best linear unbiased prediction (BLUP) appears to be the most efficient prediction method compared to the other commonly used approaches, including least absolute shrinkage and selection operator (LASSO), stochastic search variable selection (SSVS), support vector machines with radial basis function and epsilon regression (SVM-R(EPS)), support vector machines with radial basis function and nu regression (SVM-R(NU)), support vector machines with polynomial kernel and epsilon regression (SVM-P(EPS)), support vector machines with polynomial kernel and nu regression (SVM-P(NU)) and partial least squares regression (PLS). This study has provided guidelines for selection of hybrid rice in terms of which types of omic datasets and which method should be used to achieve higher trait predictability. The answer to these questions will benefit academic research and will also greatly reduce the operative cost for the industry which specializes in breeding and selection.",2019,Heredity
What explains health in persons with low vision,"Background 
Visual impairment is associated with important limitations in functioning. The international Classification of Functioning, Disability and Health (ICF) adopted by the World Health Organization (WHO) relies on a globally accepted framework for classifying problems in functioning and contextual factors such as environmental factors and personal factors, that might influence functioning. Its comprehensive perspective, including biological, individual and social aspects of health, enables the ICF to describe the whole health experience of persons with visual impairment (PVI). The objectives of this study are (1) to analyze whether ICF can be used to comprehensively describe the problems in functioning of PVI and the environmental factors that influence their lives and (2) to select the ICF categories that best capture self-perceived health of PVI. 
 
Methods 
Data from persons with visual impairment (VI) (Visual acuity â‰¤ 20/63) were collected, including socio-demographic data, vision-related data, the Extended ICF Checklist and the visual analogue scale of the EuroQol-5D, to assess self-perceived health. Patients included in the study furthermore had to be at least 18 years old, had to be informed about the study and understand its purpose as well as sign the informed consent form. Data were collected by two medical researchers in the Eye Clinic of the Ludwig-Maximilian-University Munich, Germany. Descriptive statistics and a group Lasso regression were performed. The main outcome measures were functioning defined as impairments in Body functions, Body structures, limitations in Activities and restrictions in Participation, influencing Environmental factors and self-perceived health. 
 
Results 
66 females and 39 males with VI with a mean age of 63 years at interview were included in the study. The mean time since diagnosis was 17 years. In total, 120 ICF categories covering a broad range of Body functions, Body structures, aspects of Activities and Participation and Environmental factors were identified. Thirteen ICF categories that best capture self-perceived health were selected based on the Group Lasso regression. While Activities-and-Participation categories were selected most frequently, the greatest impact on self-perceived health was found in Body-functions categories. 
 
Conclusion 
The ICF can be used as a framework to comprehensively describe the problems of persons with VI and the Environmental factors which influence their lives. 
There are plenty of ICF categories, Environmental-factors categories in particular, which are relevant to persons with VI, but have hardly ever been taken into consideration in literature and VI-specific, patient-reported outcome measures.",2016,
Sparse Estimation and Inference for Censored Median Regression.,"Censored median regression has proved useful for analyzing survival data in complicated situations, say, when the variance is heteroscedastic or the data contain outliers. In this paper, we study the sparse estimation for censored median regression models, which is an important problem for high dimensional survival data analysis. In particular, a new procedure is proposed to minimize an inverse-censoring-probability weighted least absolute deviation loss subject to the adaptive LASSO penalty and result in a sparse and robust median estimator. We show that, with a proper choice of the tuning parameter, the procedure can identify the underlying sparse model consistently and has desired large-sample properties including root-n consistency and the asymptotic normality. The procedure also enjoys great advantages in computation, since its entire solution path can be obtained efficiently. Furthermore, we propose a resampling method to estimate the variance of the estimator. The performance of the procedure is illustrated by extensive simulations and two real data applications including one microarray gene expression survival data.",2010,Journal of statistical planning and inference
Non-Gaussian spatial modeling in index flood estimation,"Index flood estimation is important in regionalization procedure to solve an issue of ungauged catchment that has received great attention among hydrologists. The UK index flood estimation model known as the FEH-QMED model is a well established one with nonlinear effect of explanatory variables identified. However the shortcomings of current research in literature such as not taking into account a spatial dependency and non-Gausianity that exist in the flooding data motivated us to investigate further. This thesis aims to improve the existing methodology in index flood estimation model, where FEH-QMED model is chosen as the benchmark. Three objectives that have been developed in this thesis are: (i) to explore the shortcomings of the current research with a possibly improved model in estimating the UK index flood, (ii) to develop a more efficient statistical model in estimating the index flood that better fits the UK flooding data, and (iii) to discover more relevant predictive catchment characteristics that may improve the index flood estimation model. To answer the objectives, statistical methods have been proposed and applied into the UK flooding data analysis to establish new index flood regression models detailedly discussed in chapters of the thesis respectively. In Chapter 2 we apply the spatial additive and spatial error analyses into the UK flooding data to explore possibly improved models for the UK index flood estimation. Chapter 3 proposes a new spatial error model with skewed normal distribution for residuals and develops a maximum likelihood computational algorithm to apply into the UK flooding data for the purpose of establishing a new index flood estimation model. Chapter 4 is focused on model selection of index flood estimation model by proposing a panelized likelihood estimation method that utilizes adaptive Lasso as a regularization tool in variable selection of all available catchment characteristics in the UK flooding data source. We also present the simulations to investigate the finite sample performance of the proposed statistical methods. In comparison study, AIC scores have been used as model selection criteria, while to measure the performance of different models, the percentage improvement in mean square prediction error relative to the updated FEH-QMED model in Kjeldsen and Jones (2010) is applied. The obtained results demonstrate that the skewed spatial error flood model that is established by using statistical method suggested in Chapter 4 outperforms the others and can significantly improve the FEH-QMED model in estimating the UK index flood.",2018,
Alternative strategies for selecting subsets of predicting SNPs by LASSO-LARS procedure,"BackgroundThe least absolute shrinkage and selection operator (LASSO) can be used to predict SNP effects. This operator has the desirable feature of including in the model only a subset of explanatory SNPs, which can be useful both in QTL detection and GWS studies. LASSO solutions can be obtained by the least angle regression (LARS) algorithm. The big issue with this procedure is to define the best constraint (t), i.e. the upper bound of the sum of absolute value of the SNP effects which roughly corresponds to the number of SNPs to be selected. Usai et al. (2009) dealt with this problem by a cross-validation approach and defined t as the average number of selected SNPs overall replications. Nevertheless, in small size populations, such estimator could give underestimated values of t. Here we propose two alternative ways to define t and compared them with the ""classical"" one.MethodsThe first (strategy 1), was based on 1,000 cross-validations carried out by randomly splitting the reference population (2,000 individuals with performance) into two halves. The value of t was the number of SNPs which occurred in more than 5% of replications. The second (strategy 2), which did not use cross-validations, was based on the minimization of the Cp-type selection criterion which depends on the number of selected SNPs and the expected residual variance.ResultsThe size of the subset of selected SNPs was 46, 189 and 64 for the classical approach, strategy 1 and 2 respectively. Classical and strategy 2 gave similar results and indicated quite clearly the regions were QTL with additive effects were located. Strategy 1 confirmed such regions and added further positions which gave a less clear scenario. Correlation between GEBVs estimated with the three strategies and TBVs in progenies without phenotypes were 0.9237, 0.9000 and 0.9240 for classical, strategy 1 and 2 respectively.ConclusionsThis suggests that the Cp-type selection criterion is a valid alternative to the cross-validations to define the best constraint for selecting subsets of predicting SNPs by LASSO-LARS procedure.",2012,BMC Proceedings
Flexible Collaborative Estimation of the Average Causal Effect of a Treatment using the Outcome-Highly-Adaptive Lasso,"Many estimators of the average causal effect of an intervention require estimation of the propensity score, the outcome regression, or both. For these estimators, we must carefully con- sider how to estimate the relevant regressions. It is often beneficial to utilize flexible techniques such as semiparametric regression or machine learning. However, optimal estimation of the regression function does not necessarily lead to optimal estimation of the average causal effect. Therefore, it is important to consider criteria for evaluating regression estimators and selecting hyper-parameters. A recent proposal addressed these issues via the outcome-adaptive lasso, a penalized regression technique for estimating the propensity score. We build on this proposal and offer a method that is simultaneously more flexible and more efficient than the previous pro- posal. We propose the outcome-highly-adaptive LASSO, a semi-parametric regression estimator designed to down-weight regions of the confounder space that do not contribute variation to the outcome regression. We show that tuning this method using collaborative targeted learning leads to superior finite-sample performance relative to competing estimators.",2018,arXiv: Methodology
A 6-gene leukemic stem cell score identifies high risk pediatric acute myeloid leukemia,"Recently, mRNA-expression signature enriched in LSCs was used to create a 17-gene leukemic stem cell (LSC17) score predictive of prognosis in adult AML. By fitting a Cox-LASSO regression model to the clinical outcome and gene-expression levels of LSC enriched genes in 163 pediatric participants of the AML02 multi-center clinical trial (NCT00136084), we developed a six-gene LSC score of prognostic value in pediatric AML (pLSC6). In the AML02 cohort, the 5-year event-free survival (EFS) of patients within low-pLSC6 group (nâ€‰=â€‰97) was 78.3 (95% CIâ€‰=â€‰70.5â€“86.9%) as compared with 34.5(95% CIâ€‰=â€‰24.7â€“48.2 %) in patients within high-pLSC6 group (nâ€‰=â€‰66 subjects), pâ€‰<â€‰0.00001. pLSC6 remained significantly associated with EFS and overall survival (OS) after adjusting for induction 1-MRD status, risk-group, FLT3-status, WBC-count at diagnosis and age. pLSC6 formula developed in the AML02 cohort was validated in the pediatric AML-TARGET project data (nâ€‰=â€‰205), confirming its prognostic value in both single-predictor and multiple-predictor Cox regression models. In both cohorts, pLSC6 predicted outcome of transplant patients, suggesting it as a useful criterion for transplant referrals. Our results suggest that pLSC6 score holds promise in redefining initial risk-stratification and identifying poor risk AML thereby providing guidance for developing novel treatment strategies.",2019,Leukemia
