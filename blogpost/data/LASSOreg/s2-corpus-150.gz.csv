title,abstract,year,journal
Optimal vision system design for characterization of apples using US/VIS/NIR spectroscopy data,"Quality monitoring of the food items by spectroscopy provides information in a large number of wavelengths including highly correlated and redundant information. Although increasing the information, the increase in the number of wavelengths causes the vision set-up to be more complex and expensive. In this paper, three sparse regression methods; lasso, elastic-net and fused lasso are employed for estimation of the chemical and physical characteristics of one apple cultivar using their high dimensional spectroscopic measurements. The use of sparse regression reduces the number of required wavelengths for prediction and thus, simplifies the required vision set-up. It is shown that, considering a tradeoff between the number of selected bands and the corresponding validation performance during the training step can result in a significant reduction in the number of bands at a small price in the test performance. Furthermore, appropriate regression methods for different number of bands and spectrophotometer design are determined.",2013,"2013 20th International Conference on Systems, Signals and Image Processing (IWSSIP)"
B Selected variables in KPI data analysis,"Regularization plays a critical role in modern statistical research, especially in high dimensional variable selection problems. Existing Bayesian methods usually assume independence between variables a priori. In this article, we propose a novel Bayesian approach, which explicitly models the dependence structure through a graph Laplacian matrix. We also generalize the graph Laplacian to allow both positively and negatively correlated variables. A prior distribution for the graph Laplacian is then proposed, which allows conjugacy and thereby greatly simplifies the computation. We show that the proposed Bayesian model leads to proper posterior distribution. We also connect our method with some existing regularization methods, such as Elastic Net, Lasso, OSCAR and Ridge regression. For posterior computation, we develop an efficient MCMC method based on parameter augmentation. Finally, we demonstrate the method through simulation studies and real data analysis.",2010,
Serum levels of fibroblast growth factor-2 distinguish Takayasu arteritis from giant cell arteritis independent of age at diagnosis,"Takayasu arteritis (TAK) and giant cell arteritis (GCA) are two major variants of large vessel vasculitis, and age is a major factor in their differential diagnosis. We sought to determine whether the two diseases exist on the same spectrum. We compared the serum levels of multiple cytokines and chemokines in 25 patients with TAK, 20 patients with GCA, and sex- and age-matched healthy donors for either condition (HD-TAK and HD-GCA). To evaluate the effects of age on the levels of cytokines and chemokines, we performed multiple logistic regression analysis using the least absolute shrinkage and selection operator (LASSO) method. The levels of IL-1RA, IL-10, GM-CSF, G-CSF, FGF-2, eotaxin, and IP-10 were significantly different between TAK and GCA, but no differences were found in the levels of IL-6, IL-12(p40), IL-17, IFN-Î³, and TNF-Î±. Significant differences in the levels of IL-1RA, IL-10, GM-CSF, eotaxin, and IP-10 were observed between the HD-TAK and HD-GCA groups. Multiple logistic regression analysis demonstrated that only FGF-2 and IP-10 could significantly distinguish the diseases when added to age. Multiple logistic analysis using factors selected by the LASSO method revealed that FGF-2 was the only significant factor to distinguish the diseases when added to age. Among numerous cytokines and chemokines analyzed, only FGF-2 could be used together with age at diagnosis to differentiate TAK and GCA. Our results suggested the importance of considering the effects of age on serum cytokines.",2019,Scientific Reports
Radiomics analysis of multiparametric MRI for the preoperative evaluation of pathological grade in bladder cancer tumors,"ObjectivesTo develop and validate an MRI-based radiomics strategy for the preoperative estimation of pathological grade in bladder cancer (BCa) tumors.MethodsA primary cohort of 70 patients (31 high-grade BCa and 39 low-grade BCa) with BCa were retrospectively enrolled. Three sets of radiomics features were separately extracted from tumor volumes on T2-weighted imaging (T2WI), diffusion-weighted imaging (DWI), and apparent diffusion coefficient (ADC) maps. Two sets of multimodal features were separately generated by the maxout and concatenation of the aboveÂ mentioned single-modality features. Each feature set was subjected to a two-sample t test and the least absolute shrinkage and selection operator (LASSO) algorithm for feature selection. Multivariable logistic regression (LR) analysis was used to obtain five corresponding radiomics models. The diagnostic abilities of the radiomics models were evaluated using receiver operating characteristic (ROC) curve analysis and compared using the DeLong test. Validation was performed on a time-independent cohort containing 30 consecutive patients.ResultsThe areas under the ROC curves (AUCs) of single-modality T2WI, DWI, and ADC models in the training cohort were 0.7933 (95% confidence interval [CI] 0.7471â€“0.8396), 0.8083 (95% CI 0.7565â€“0.8601), and 0.8350 (95% CI 0.7924â€“0.8776), respectively. Both multimodality models achieved higher AUCs (maxout 0.9233, 95% CI 0.9001â€“0.9466; concatenation 0.9233, 95% CI 0.9001â€“0.9466) than single-modality models. The AUCs of the maxout and concatenation models in the validation cohort were 0.9186 and 0.9276, respectively.ConclusionsThe MRI-based multiparametric radiomics approach has the potential to be used as a noninvasive imaging tool for preoperative grading of BCa tumors. Multicenter validation is needed to acquire high-level evidence for its clinical application.Key Pointsâ€¢ Multiparametric MRI may help in the preoperative grading of BCa tumors.â€¢ The Joint_Model established from T2WI, DWI, and ADC feature subsets demonstrated a high diagnostic accuracy for preoperative prediction of pathological grade in BCa tumors.â€¢ The radiomics approach has the potential to preoperatively assess tumor grades in BCa and avoid subjectivity.",2019,European Radiology
Bayesian shrinkage,"Penalized regression methods, such as L1 regularization, are routinely used in high-dimensional applications, and there is a rich literature on optimality p roperties under sparsity assumptions. In the Bayesian paradigm, sparsity is routinely induced throu gh two-component mixture priors having a probability mass at zero, but such priors encounter daunti ng computational problems in high dimensions. This has motivated an amazing variety of contin uous shrinkage priors, which can be expressed as global-local scale mixtures of Gaussians, fac ilitating computation. In sharp contrast to the corresponding frequentist literature, very little i s known about the properties of such priors. Focusing on a broad class of shrinkage priors, we provide pre cise results on prior and posterior concentration. Interestingly, we demonstrate that most co mm nly used shrinkage priors, including the Bayesian Lasso, are suboptimal in high-dimensional set tings. A new class of Dirichlet Laplace (DL) priors are proposed, which are optimal and lead to effici nt posterior computation exploiting results from normalized random measure theory. Finite samp le performance of Dirichlet Laplace priors relative to alternatives is assessed in simulations .",2012,
"Using in situ ultravioletâ€visual spectroscopy to measure nitrogen, carbon, phosphorus, and suspended solids concentrations at a high frequency in a brackish tidal marsh","The collection of high frequency water quality data are key to making the next leap in hydrological and biogeochemical sciences. Commercially available in situ ultraviolet-visual (UV-Vis) spectrometers make possible the long-term collection of absorption spectra multiple times per hour. This technology has proven useful for measuring nitrate, dissolved organic carbon, and total suspended solids in many environments, but has not been tested in tidal marsh conditions where upstream freshwater mixes with estuarine waters, resulting in rapid changes in concentrations and salinity. These three parameters encompass only a portion of the nutrients that are of interest in these systems. To test the potential of spectroscopy to measure these and other nutrient concentrations, spectrometers were installed in a constructed brackish tidal marsh and absorbance spectra were compared to lab analyses for coinciding discrete samples. Variable selection techniques, including partial least squares regression, lasso regression, and stepwise regression, were used to develop models with which nitrate, total kjeldahl nitrogen, dissolved organic carbon, phosphate, total phosphorus, total suspended solids, and salinity in brackish marsh waters can be predicted from UV-Vis spectrometer measurements. Significant relationships between the absorption spectra and the laboratory measured concentrations were observed for all of the parameters. Phosphate and total phosphorus were the only nutrients which had R2 values less than 0.86 for their best calibrations. This study shows the potential to collect multiple water quality parameters at a high frequency in brackish waters using in situ spectrometers and gives the tools to replicate this analysis in all environments.",2014,Limnology and Oceanography-methods
Boosting part-sense multi-feature learners toward effective object detection,"AdaBoost has been applied to object detection to construct the detectors with high performance of discrimination and generalization by single-feature learner. However, the poor discriminative power of extremely weak single-feature learners limits its application for general object detection. In this paper, we propose a novel comprehensive learner design mechanism toward effective object detection in terms of both discrimination and generalization abilities. Firstly, the part-sense multi-feature learners are designed to linearly combine the multiple local features to improve the descriptive and discriminative capacity of the learner. Secondly, we formulate the feature selection in part-sense multi-feature learner as a weighted LASSO regression. Using Least Angle Regression (LARS) method, our approach can choose features adaptively, efficiently and as few as possible to guarantee generalization performance. Finally, a robust L1-regularized gradient boosting is proposed to integrate our part-sense sparse features learner into an object classifier. Extensive experiments and comparisons on the face dataset and the human dataset show the proposed approach outperforms the traditional single-feature learner and other multi-feature learners in discriminative and generalization abilities.",2011,Comput. Vis. Image Underst.
A Data Mining Approach Identified Salivary Biomarkers That Discriminate between Two Obesity Measures,"Background
A key mechanism of obesity involves dysregulation of metabolic and inflammatory markers. This study aimed to identify salivary biomarkers and other factors associated with obesity using an ensemble data mining approach.


Methods
For a random cohort of over 700 subjects from 8137 Kuwait children (10.00â€‰Â±â€‰0.67â€‰years), four data mining methods were applied to identify important variables associated with obesity, including logistic regression by lasso regularization (Lasso), multivariate adaptive regression spline (MARS), random forests (RF), and boosting classification trees (BT). Each algorithm generated a variable importance rank list, based on an internal cross-validation procedure. An aggregated importance ranking was constructed by averaging the rank ordering of variables from individual list, weighted by the classification performance of respective models. Subsequently, the subset of top-ranking variables that were identified with at least three algorithms was evaluated by classification performance using receiver operating characteristic (ROC) analysis with bootstrap percentile resampling.


Results
Obesity was defined either by the waist circumference (OBW) or by the body mass index (BMI) (OBWHO). We identified C-reactive protein (CRP), insulin, leptin, adiponectin, as salivary biomarkers associated with OBW, plus a clinical feature fitness level. A similar set of biomarkers was identified for OBWHO, but not including leptin. Tree-based clustering analysis revealed patterns that were significantly different between the OBW and OBWHO subjects.


Conclusion
A data mining approach based on multiple algorithms is useful for identifying factors associated with phenotypes, especially in cases where relationships are not salient, and a consensus from multiple methods can help produce a more generalizable subset of features. In this case, we have demonstrated that evaluation using the waist circumference includes association with high levels of salivary leptin, which is not seen with evaluation by BMI.",2019,Journal of Obesity
A GA-Based Approach for Building Regularized Sparse Polynomial Models for Wind Turbine Power Curves,"In this paper, the classical polynomial model for wind turbines power curve estimation is revisited aiming at an automatic and parsimonious design. In this regard, using genetic algorithms we introduce a methodoloy for estimating a suitable order for the polynomial as well its relevant terms. The proposed methodology is compared with the state of the art in estimating the power curve of wind turbines, such as logistic models (with 4 and 5 parameters), artificial neural networks and weighted polynomial regression. We also show that the proposed approach performs better than the standard LASSO approach for building regularized sparse models. The results indicate that the proposed methodology consistently outperforms all the evaluated alternative methods. 1. IntroduÃ§Ã£o Com o crescimento da indÃºstria eÃ³lica, aerogeradores estÃ£o sendo instalados em diversos tipos de ambientes que se diferem, por exemplo, em relaÃ§Ã£o Ã  umidade relativa do ar, densidade do ar, altitude, condiÃ§Ãµes climÃ¡ticas sazonais e complexidade do terreno. Estas caracterÄ±Ìsticas, alÃ©m de outras nÃ£o mencionadas e outras nem conhecidas, influenciam significativamente a produÃ§Ã£o energÃ©tica local. Em face de tais dificuldades, faz-se necessÃ¡rio uma modelagem adequada da relaÃ§Ã£o velocidade do vento versus potÃªncia gerada (conhecida genericamente como curva de potÃªncia) pelos aerogeradores em certo parque eÃ³lico, com o intuito de prever corretamente sua produÃ§Ã£o energÃ©tica, bem como o monitorar o seu desempenho. Dada sua importÃ¢ncia, a modelagem (ou estimaÃ§Ã£o) de curvas de potÃªncia Ã© um tema que sempre despertou (e ainda desperta) o interesse da comunidade de energias renovÃ¡veis haja visto o grande nÃºmero de artigos disponÄ±Ìveis na litertura especializada, principalmente na forma de surveys [Marciukaitis et al. 2017, Wang et al. 2018, Sohoni et al. 2016, Lee et al. 2015, Shokrzadeh et al. 2014, Lydia et al. 2014, Clifton et al. 2013, Lydia et al. 2013, Li et al. 2001]. Dentre os mÃ©todos encontrados na literatura, destacam-se o modelo polinomial como um dos mais utilizados, o polinomial ponderado, o modelo logÄ±Ìstico de 4 e 5 parÃ¢metros e redes neurais artificiais. Em particular, o modelo polinomial tem seu uso disseminado entre os praticantes da Ã¡rea devido nÃ£o somente Ã  sua formulaÃ§Ã£o mais simples quando comparado a modelos mais complexos, tais como redes neurais, mas principalmente porque encontra-se disponÄ±Ìvel em planilhas de cÃ¡lculo populares (e.g. Excel e LibreOffice Calc). Mesmo sendo de uso mais simples, o modelo polinomial requer a especificaÃ§Ã£o a priori da ordem do polinÃ´mio. A especificaÃ§Ã£o correta da ordem do polinÃ´mio Ã© essencial para um bom desempenho preditivo do modelo, mas tambÃ©m para evitar problemas",2018,
Accuracy of genomic breeding values for meat tenderness in Polled Nellore cattle.,"Zebu () cattle, mostly of the Nellore breed, comprise more than 80% of the beef cattle in Brazil, given their tolerance of the tropical climate and high resistance to ectoparasites. Despite their advantages for production in tropical environments, zebu cattle tend to produce tougher meat than Bos taurus breeds. Traditional genetic selection to improve meat tenderness is constrained by the difficulty and cost of phenotypic evaluation for meat quality. Therefore, genomic selection may be the best strategy to improve meat quality traits. This study was performed to compare the accuracies of different Bayesian regression models in predicting molecular breeding values for meat tenderness in Polled Nellore cattle. The data set was composed of Warner-Bratzler shear force (WBSF) of longissimus muscle from 205, 141, and 81 animals slaughtered in 2005, 2010, and 2012, respectively, which were selected and mated so as to create extreme segregation for WBSF. The animals were genotyped with either the Illumina BovineHD (HD; 777,000 from 90 samples) chip or the GeneSeek Genomic Profiler (GGP Indicus HD; 77,000 from 337 samples). The quality controls of SNP were Hard-Weinberg Proportion -value â‰¥ 0.1%, minor allele frequency > 1%, and call rate > 90%. The FImpute program was used for imputation from the GGP Indicus HD chip to the HD chip. The effect of each SNP was estimated using ridge regression, least absolute shrinkage and selection operator (LASSO), Bayes A, Bayes B, and Bayes CÏ€ methods. Different numbers of SNP were used, with 1, 2, 3, 4, 5, 7, 10, 20, 40, 60, 80, or 100% of the markers preselected based on their significance test (-value from genomewide association studies [GWAS]) or randomly sampled. The prediction accuracy was assessed by the correlation between genomic breeding value and the observed WBSF phenotype, using a leave-one-out cross-validation methodology. The prediction accuracies using all markers were all very similar for all models, ranging from 0.22 (Bayes CÏ€) to 0.25 (Bayes B). When preselecting SNP based on GWAS results, the highest correlation (0.27) between WBSF and the genomic breeding value was achieved using the Bayesian LASSO model with 15,030 (3%) markers. Although this study used relatively few animals, the design of the segregating population ensured wide genetic variability for meat tenderness, which was important to achieve acceptable accuracy of genomic prediction. Although all models showed similar levels of prediction accuracy, some small advantages were observed with the Bayes B approach when higher numbers of markers were preselected based on their -values resulting from a GWAS analysis.",2016,Journal of animal science
Relating household characteristics to urban sheep keeping in West Africa,"Abstract Urban sheep production is widespread in Bobo-Dioulasso despite its formal illegality. This study was aimed at the identification of socioeconomic characteristics influencing the decisions of households to take up this activity. One hundred and thirty-six households (half of them keeping sheep, half not keeping small ruminants) were interviewed to collect data on their socioeconomic situation. Three techniques of multivariate analyses were compared. Cluster analysis and logistic regression revealed the following socioeconomic differences between the two groups: the probability of keeping sheep increases with the size of the household and the rate of illiteracy. Households are also more likely to keep sheep if urban cattle husbandry is practised, if there is only one household in the compound and if the keeper has already changed his/her trade at least once. Correspondence analysis provided visual confirmation of these results. Cluster analysis allowed a more profound understanding of the situation by drawing attention to a â€˜transitional differentiationâ€™: non-keepers in a group of keepers and vice versa tell us something about potential future keepers or non-keepers.",2001,Agricultural Systems
Adaptive restart of accelerated gradient methods under local quadratic growth condition,"By analyzing accelerated proximal gradient methods under a local quadratic growth condition, we show that restarting these algorithms at any frequency gives a globally linearly convergent algorithm. This result was previously known only for long enough frequencies. Then, as the rate of convergence depends on the match between the frequency and the quadratic error bound, we design a scheme to automatically adapt the frequency of restart from the observed decrease of the norm of the gradient mapping. Our algorithm has a better theoretical bound than previously proposed methods for the adaptation to the quadratic error bound of the objective. We illustrate the efficiency of the algorithm on a Lasso problem and on a regularized logistic regression problem.",2019,Ima Journal of Numerical Analysis
A Statistical Framework for Predictive Model Evaluation in MOOCs,"Feature extraction and model selection are two essential processes when building predictive models of student success. In this work we describe and demonstrate a statistical approach to both tasks, comparing five modeling techniques (a lasso penalized logistic regression model, naÃ¯ve Bayes, random forest, SVM, and classification tree) across three sets of features (week-only, summed, and appended). We conduct this comparison on a dataset compiled from 30 total offerings of five different MOOCs run on the Coursera platform. Through the use of the Friedman test with a corresponding post-hoc Nemenyi test, we present comparative performance results for several classifiers across the three different feature extraction methods, demonstrating a rigorous inferential process intended to guide future analyses of student success systems.",2017,Proceedings of the Fourth (2017) ACM Conference on Learning @ Scale
"Stability selection for lasso, ridge and elastic net implemented with AFT models","Abstract The instability in the selection of models is a major concern with data sets containing a large number of covariates. We focus on stability selection which is used as a technique to improve variable selection performance for a range of selection methods, based on aggregating the results of applying a selection procedure to sub-samples of the data where the observations are subject to right censoring. The accelerated failure time (AFT) models have proved useful in many contexts including the heavy censoring (as for example in cancer survival) and the high dimensionality (as for example in micro-array data). We implement the stability selection approach using three variable selection techniquesâ€”Lasso, ridge regression, and elastic net applied to censored data using AFT models. We compare the performances of these regularized techniques with and without stability selection approaches with simulation studies and two real data examplesâ€“a breast cancer data and a diffuse large B-cell lymphoma data. The results suggest that stability selection gives always stable scenario about the selection of variables and that as the dimension of data increases the performance of methods with stability selection also improves compared to methods without stability selection irrespective of the collinearity between the covariates.",2019,Statistical Applications in Genetics and Molecular Biology
"LASSO-type penalization in the framework of generalized additive models for location, scale and shape","For numerous applications it is of interest to provide full probabilistic forecasts, which are able to assign probabilities to each predicted outcome. Therefore, attention is shifting constantly from conditional mean models to probabilistic distributional models capturing location, scale, shape (and other aspects) of the response distribution. One of the most established models for distributional regression is the generalized additive model for location, scale and shape (GAMLSS). In high dimensional data set-ups classical fitting procedures for the GAMLSS often become rather unstable and methods for variable selection are desirable. Therefore, we propose a regularization approach for high dimensional data set-ups in the framework for GAMLSS. It is designed for linear covariate effects and is based on L1 -type penalties. The following three penalization options are provided: the conventional least absolute shrinkage and selection operator (LASSO) for metric covariates, and both group and fused LASSO for categorical predictors. The methods are investigated both for simulated data and for two real data examples, namely Munich rent data and data on extreme operational losses from the Italian bank UniCredit.",2019,Comput. Stat. Data Anal.
Enriched taxa were found among the gut microbiota of centenarians in East China,"BACKGROUND
Gut microbiota is closely related to age. Studies from Europe and the U.S. identified featured microbiota in different age groups for the elderly. Asian studies mainly focused on people living in longevity areas. Featured microbiota for the elderly people of different age groups, especially in the centenarian in the general population, has not been well investigated in China.


METHOD
We conducted a comparative study by including 198 subjects of three age groups (65-70, 90-99, and 100+ years) in East China. Information regarding age, sex, height, weight, waist circumference, hip circumference, food preference, smoking status and alcohol consumption were collected by using a structured questionnaire. Fecal samples for each participant were collected as well. 16S rRNA gene sequencing were employed to analyze the gut microbiota composition. Logistic regression with LASSO feature selection was used to identify featured taxa in different age groups and to assess their potential interactions with other factors such as lifestyle.


RESULT
The gut microbiota of the 90-99 year and 100+ year age groups showed more diversity, robustness, and richness compared with the 65-70 year age group. PCoA analysis showed a clear separation between the 65-70 and 100+ year age groups. At the species level, Bacteroides fragilis, Parabacteroides merdae, Ruminococcus gnavus, Coprococcus and Clostridium perfringens increased, but Bacteroides vulgatus, Ruminococcus sp.5139BFAA and Clostridium sp.AT5 decreased in the 90-99 year age group. The age differences in gut microbiota were similar across the strata of smoking, alcohol consumption status and food preference.


CONCLUSION
Our study demonstrated age differences in many aspects of gut microbiota, such as overall diversity, microbiota structure, and relative abundance of key taxa. Moreover, the gut microbiota of centenarian was significantly different from those of younger age groups of the elderly.",2019,PLoS ONE
Predicting Psychiatric Hospitalizations among Elderly Veterans with a History of Mental Health Disease,"Introduction
Patient Aligned Care Team (PACT) care managers are tasked with identifying aging Veterans with psychiatric disease in attempt to prevent psychiatric crises. However, few resources exist that use real-time information on patient risk to prioritize coordinating appropriate care amongst a complex aging population.


Objective
To develop and validate a model to predict psychiatric hospital admission, during a 90-day risk window, in Veterans ages 65 or older with a history of mental health disease.


Methods
This study applied a cohort design to historical data available in the Veterans Affairs (VA) Corporate Data Warehouse (CDW). The Least Absolute Shrinkage and Selection Operator (LASSO) regularization regression technique was used for model development and variable selection. Individual predicted probabilities were estimated using logistic regression. A split-sample approach was used in performing external validation of the fitted model. The concordance statistic (C-statistic) was calculated to assess model performance.


Results
Prior to modeling, 61 potential candidate predictors were identified and 27 variables remained after applying the LASSO method. The final model's predictive accuracy is represented by a C-statistic of 0.903. The model's predictive accuracy during external validation is represented by a C-statistic of 0.935. Having a previous psychiatric hospitalization, psychosis, bipolar disorder, and the number of mental-health related social work encounters were strong predictors of a geriatric psychiatric hospitalization.


Conclusion
This predictive model is capable of quantifying the risk of a geriatric psychiatric hospitalization with acceptable performance and allows for the development of interventions that could potentially reduce such risk.",2018,eGEMs
Prediction of pKa Using Machine Learning Methods with Rooted Topological Torsion Fingerprints: Application to Aliphatic Amines.,"The acid-base dissociation constant, pKa, is a key parameter to define the ionization state of a compound and directly affects its biopharmaceutical profile. In this study, we developed a novel approach for pKa prediction using rooted topological torsion fingerprints in combination with five machine learning methods: random forest, partial least squares, extreme gradient boosting, Lasso regression and support vector regression. With a large and diverse set of 14,999 experimental pKa values, pKa models were developed for aliphatic amines. The models demonstrated consistently good prediction statistics and were able to generate accurate prospective predictions as validated with an external test set of 726 pKa values (RMSE 0.45, MAE 0.33, R2 0.84 by the top model). The factors that may affect prediction accuracy and model applicability were carefully assessed. The results demonstrated that rooted topological torsion fingerprints coupled with machine learning methods provide a promising approach for developing accurate pKa prediction models.",2019,Journal of chemical information and modeling
An alpha-defensin gene single nucleotide polymorphism modulates the gut microbiota and may alter the risk of acute graft-versus-host disease.,"We previously reported a protective association between single nucleotide polymorphisms (SNPs; rs4415345G and rs4610776A alleles) of Paneth cell Î±-defensin-5 against acute graft-versus-host disease (aGVHD). Because dysbiosis has been associated with aGVHD, we hypothesized that these SNPs may have a gut microbiota signature. In Lasso regression analysis of 248 healthy individuals, rs4415345G was associated with a higher abundance of Odoribacter splanchnicus, an anaerobic butyrogenic commensal. In multivariable analysis of data from 613 allogeneic haematopoietic cell transplant recipients, peri-engraftment presence of O.Â splanchnicus was associated with ~50% lower risk for grade II-IV aGVHD (hazard ratio 0Â·53, 95% confidence interval 0Â·28-1Â·00, PÂ =Â 0Â·05). O.Â splanchnicus may protect rs4415345G individuals against aGVHD.",2020,British journal of haematology
Spherical Hamiltonian Monte Carlo for Constrained Target Distributions,"Statistical models with constrained probability distributions are abundant in machine learning. Some examples include regression models with norm constraints (e.g., Lasso), probit models, many copula models, and Latent Dirichlet Allocation (LDA) models. Bayesian inference involving probability distributions confined to constrained domains could be quite challenging for commonly used sampling algorithms. For such problems, we propose a novel Markov Chain Monte Carlo (MCMC) method that provides a general and computationally efficient framework for handling boundary conditions. Our method first maps the D-dimensional constrained domain of parameters to the unit ball [Formula: see text], then augments it to a D-dimensional sphere SD such that the original boundary corresponds to the equator of SD . This way, our method handles the constraints implicitly by moving freely on the sphere generating proposals that remain within boundaries when mapped back to the original space. To improve the computational efficiency of our algorithm, we divide the dynamics into several parts such that the resulting split dynamics has a partial analytical solution as a geodesic flow on the sphere. We apply our method to several examples including truncated Gaussian, Bayesian Lasso, Bayesian bridge regression, and a copula model for identifying synchrony among multiple neurons. Our results show that the proposed method can provide a natural and efficient framework for handling several types of constraints on target distributions.",2014,JMLR workshop and conference proceedings
Development of a frailty score based on hospital discharge data linked to cohort data,"IntroductionFrailty is strongly associated with adverse health outcomes and health care costs in elders. However, we have almost no idea of the prevalence of frail older inpatients in Swiss hospitals. Hospital discharge data could contribute to predicting frailty in these patients, and eventually improving SwissDRGs system or casemix-adjustment. 
Objectives and ApproachThe HFrailty project aimed to develop a predictive model of Friedâ€™s Frailty Phenotype (FFP) based on hospital discharge data. We linked Lausanne University Hospital (CHUV) discharge data to clinical data from the Lausanne cohort study (Lc65+) over the period 2004-2015. The Lc65+ is a longitudinal population-based cohort comprising three random samples of approximately 1500 Lausanne residents aged 65 to 70, born respectively before, during and after World War II. With stepwise and lasso penalized logistic regression, random forest and neural networks, we identified the best-performing model for predicting FFP using CHUVâ€™s data recorded within 12 months prior to frailty assessments. 
ResultsAmong Lc65+ participants, 1649 were assessed for frailty and hospitalized at least once during the follow-up period, resulting in 3499 FFP assessments of which 544 were preceded by at least one hospitalization within 12 months. Â In total, 45.7% of the participants were men and 9.4% were frail (FFP score â‰¥ 3). As expected, prevalence of frailty increased with age from 4.1% in the 66-70 age group, to 5.3% and 10.5% in the 71-75 and 76-80 groups, respectively. Logistic regression with lasso penalty was finally the best model regarding both performance and complexity. It had an area under receiver operating curve of 0.67 to predict FFP based on detailed diagnosis and procedure codes. 
Conclusion/ImplicationsHospital discharge data may be used to identify frail and non-frail individuals and estimate their prevalence in the Swiss non-institutionalized population. Our predictive model showed limited performance and could be improved. We are currently testing groups of diagnosis and procedure codes, as predictors, instead of detailed ones.",2018,International Journal for Population Data Science
A New Sparse LSSVM Method Based the Revised LARS,"Least squares support vector machine (LSSVM) has comparable performance with support vector machine (SVM) and it has been widely used for classification and regression problems. The solutions of LSSVM are obtained by solving linear equations, but it lack of sparseness, which result in that it is unable to handle large-scale data sets. The state-of-art method least angle regression (LARS) can obtain a sparse solution by solving the Least Absolute Shrinkage and Selection Operator (LASSO) problem. So we use the idea of the LARS to obtain the sparse solution of the LSSVM, i.e., RLARS-LSSVM is proposed, which is an efficient method. The feature of the method is to select the most important samples as support vectors iteratively and to remove the samples that are similar to the selected support vectors simultaneously. Experimental results show that the proposed method can obtain much higher test accuracy compared with other sparse LSSVM methods at the same number of support vectors.",2017,2017 International Conference on Machine Vision and Information Technology (CMVIT)
The Smooth-Lasso and other $\ell_1+\ell_2$-penalized methods,"We consider a linear regression problem in a high dimensional setting where the number of covariates $p$ can be much larger than the sample size $n$. In such a situation, one often assumes sparsity of the regression vector, \textit{i.e.}, the regression vector contains many zero components. We propose a Lasso-type estimator $\hat{\beta}^{Quad}$ (where '$Quad$' stands for quadratic) which is based on two penalty terms. The first one is the $\ell_1$ norm of the regression coefficients used to exploit the sparsity of the regression as done by the Lasso estimator, whereas the second is a quadratic penalty term introduced to capture some additional information on the setting of the problem. We detail two special cases: the Elastic-Net $\hat{\beta}^{EN}$, which deals with sparse problems where correlations between variables may exist; and the Smooth-Lasso $\hat{\beta}^{SL}$, which responds to sparse problems where successive regression coefficients are known to vary slowly (in some situations, this can also be interpreted in terms of correlations between successive variables). From a theoretical point of view, we establish variable selection consistency results and show that $\hat{\beta}^{Quad}$ achieves a Sparsity Inequality, \textit{i.e.}, a bound in terms of the number of non-zero components of the 'true' regression vector. These results are provided under a weaker assumption on the Gram matrix than the one used by the Lasso. In some situations this guarantees a significant improvement over the Lasso. Furthermore, a simulation study is conducted and shows that the S-Lasso $\hat{\beta}^{SL}$ performs better than known methods as the Lasso, the Elastic-Net $\hat{\beta}^{EN}$, and the Fused-Lasso with respect to the estimation accuracy. This is especially the case when the regression vector is 'smooth', \textit{i.e.}, when the variations between successive coefficients of the unknown parameter of the regression are small. The study also reveals that the theoretical calibration of the tuning parameters and the one based on $10$ fold cross validation imply two S-Lasso solutions with close performance.",2011,arXiv: Statistics Theory
Ozone-precursor relationships from EKMA diagrams. Reply to comments.,"S IR The regression and rollback equations presented in my note ( I ) were stated to be approximate and valid only within specified constraints of concentrations and NMHC/NO, ratios. These will be called â€œtransition ratiosâ€ in this response to the comments by Kumar. Kumar acknowledged that the rollback equation had been stated to be applicable only within the specified constraints. Yet in his calculations he disregards the constraints and therefore incorrectly concludes that the equations are suspect and always overestimate the NMHC reductions. Of the 27 examples in Table I of Kumarâ€™s comments, only two were in compliance with the constraints. For the two valid cases, the hydrocarbon reductions calculated by the rollback equation were within 10 and 15% of those determined by Kumar by using the standard EKMA diagram. The 15% difference of the hydrocarbon reduction corresponds to an equivalent difference of about 0.01 ppm ozone at the O,, level. This amount is within the realm of the term â€œapproximateâ€. As stated by Kumar the equations do not allow for NO, inhibition of ozone production under any condition. The equations apply only within the transition ratios where ozone is a function of P1f2. They do not apply below the lower transition ratio where NO2 inhibition occurs. Nor do the equations apply above the upper transition ratio where ozone depends mainly on the NO, concentration. None of the examples or comments by Kumar addressed the area near or above the upper transition ratio, which is about 20 based on the standard EKMA diagram. Kumar also stated that when NMHC and NO, reductions are introduced, the resulting NMHC/NO, ratios generally are below the lower limit of the range of validity (below the lower transition ratio) of the regression equation. Obviously this was true for 25 of his 27 examples. However, the lower transition ratio will not be reached when the hydrocarbon control required is moderate and the initial ratio is high. It will not be reached when the control requirements are substantial, but the reductions of hydrocarbons and NO, are commensurate or nearly so. As stated in the note, the transition ratios differ depending on environmental chamber conditions and other factors, but to my knowledge the reason or cause of the differences has not been satisfactorily explained. Subsequent to publication of the note, a recent article by Spicer was reviewed to determine the transition ratios based on his environmental chamber study results (2). Within an ozone concentration range of 0.23-0.35 ppm the transition ratios appeared to be about 10 and 25. These are higher than those reported in the note. Glassonâ€™s environmental chamber data were reviewed for the same purpose (3). On the basis of his data, the transition ratios appeared to be about 25 and 50 within an ozone concentration range of from 0.10 to 0.25 ppm. It was not possible to determine the ratios accurately because the number of data points from Glassonâ€™s studies were limited, as were those from Spicerâ€™s studies. Thus the transition ratios determined from results of various chamber studies differ considerably, and these differences raise questions as to which if any transition ratio values are applicable to urban atmospheric situations. The lack of reliable ambient air NMHC data has hindered the determination of ozone-precursor relationships in the ambient air. However during a number of summer days in 1976 and 1977, reliable speciated hydrocarbon measurements were made in the ambient air a t Los Angeles from 6:OO to 9:00 a.m. PDT (4, 5) . Los Angeles precusor-Pasadena ozone relationships were developed by using this data and indicate the following: (1) The daily maximum ozone concentration at Pasadena correlates well with the 6:OO-9:00 a.m. P112 concentrations at Los Angeles. With a sample number of 36, the ozone-precursor correlation coefficient was 0.70 and is highly significant. (2) The mean NMHC/NO, ratio was 8.7, and the range was 5.3-14.3. The correlation coefficient of ozone as a function of the ratios was -0.02, and the ozone scatter was reasonably uniform throughout the range of the NMHC/NO, ratios. Thus, there is no indication of NO2 inhibition on ozone formation in the ambient air of the Los Angeles area. If inhibition occurs in urban atmospheres, it does so at ratios below 5.3.",1983,Environmental science & technology
Network Reconstruction of Dynamic Biological Systems,"Inference of network topology from experimental data is a central endeavor in biology, since knowledge of the underlying signaling mechanisms a requirement for understanding biological phenomena. As one of the most important tools in bioinformatics area, development of methods to reconstruct biological networks has attracted remarkable attention in the current decade. Integration of different data types can lead to remarkable improvements in our ability to identify the connectivity of different segments of networks and to predict events within a cellular system. Several recent studies used data integration to reconstruct biochemical networks and to build predictive models from large-scale datasets. In this dissertation we first prescribe directions to reconstruct biological networks based on data properties and priorities in terms of network reconstruction performance. We use experimentally measured and synthetic data sets to compare three popular methods--principal component regression (PCR), linear matrix inequalities (LMI), and Least Absolute Shrinkage and Selection Operator (LASSO)-- in terms of root-mean-squared-error (RMSE), average fractional error in the value of the coefficients, accuracy, sensitivity, specificity and the geometric mean of sensitivity and specificity. This comparison enables us to establish criteria for selection of an appropriate approach for network reconstruction based on a priori properties of experimental data. Reconstruction of biological and biochemical networks from large biological datasets is challenging when the data in question are dynamic. To contribute to this challenge, we also developed a new method, called Doubly Penalized Linear Absolute Shrinkage and Selection Operator (DPLASSO), for reconstruction of dynamic biological networks. DPLASSO consists of two components, statistical significance testing of model coefficients and penalized/constrained optimization. A partial least squares with statistical significance testing acts as a supervisory-level filter to extract the most informative components of the network from a dataset (Layer 1). Then, LASSO with extra weights on the smaller parameters identified in the first layer is employed to retain the main predictors and to set the smallest coefficients to zero (Layer 2). We illustrate that DPLASSO outperforms LASSO in terms of sensitivity, specificity and accuracy. Most of biological systems are nonlinear, therefore, expressing the network model in linear form may not be able to appropriately represent the real structure of the network or to predict the response of the network as accurately as a proper nonlinear model does. Accordingly, as another contribution we have introduced a novel method to reconstruct nonlinear biological networks. In this method, we use a quadratic nonlinear model as the representation of second-order Taylor series expansion of a nonlinear system around an arbitrary point of interest. We apply LASSO to shrink some of the small coefficients to zero. A statistical significance testing (t-test) will complete the parameter (network link) selection. We demonstrate that our proposed approach will lead to considerable improvements in predicting the response of the system and fair improvement in accuracy and sensitivity of the network identified",2013,
Model Selection for Zhengzhou Sugar Futures Price,"The liner model for price of 1011 sugar futures,traded at Zhengzhou Futures market,with other sugar futures was analyzed by stepwise regression and Lasso methods to select significant variables.Real data were analyzed and compared,which showed that the Lasso method out-performed in prediction,resulting in a more accurate and reliable model for prediction.",2011,Journal of Beijing Normal University
On lasso for censored data,"In this paper, we propose a new lasso-type estimator for cen- sored data after one-step em update. While several penalized likelihood estimators have been proposed for censored data variable selection through hazards regression, many such estimators require parametric or propor- tional hazards assumptions. The proposed estimator, on the other hand, is based on the linear model and least-squares principles. Penalized Buckley- James estimators are also popular in this setting but have been shown to be unstable and unreliable. Unlike path-based learning based on least- squares approximation, our method requires no covariance assumption and the method is valid for even modest sample sizes. Our calibration estima- tor is equivalent to the minimizer of a well-defined convex loss function and, thus, yields an exact regularized solution path. Thus, the numerical algorithms are fast, reliable, and readily available because they build on existing software for complete, uncensored data. We examine the large and small sample properties of our estimator and illustrate the method through simulation studies and application to two real data sets.",2009,Electronic Journal of Statistics
Variable selection for functional regression models via the L1 regularization,"In regression analysis, L1 regularizations such as the lasso or the SCAD provide sparse solutions, which leads to variable selection. We consider the variable selection problem where variables are given as functional forms, using L1 regularization. In order to select functional variables each of which is controlled by multiple parameters, we treat parameters as grouped parameters and then apply the group SCAD. A crucial issue in the regularization method is the choice of regularization parameters. We derive a model selection criterion for evaluating the model estimated by the regularization method via the group SCAD penalty. Results of simulation and real data analysis show the effectiveness of the proposed modeling strategy.",2011,Comput. Stat. Data Anal.
Optimal Feature Selection for Pedestrian Detection Based on Logistic Regression Analysis,"This paper describes a pedestrian detection method using feature selection based on logistic regression analysis. As the parent features, Haar-like and Histograms of Oriented Gradients (HOG) features are used manually. For the statistical analysis, stepwise forward selection, backward elimination, and Least Absolute Shrinkage and Selection Operator (LASSO) methods are applied to our Logistic Regression Model for Pedestrian Detection (LRMPD). The experimental results shows that the average of 48.5% of a full model are selected for LRMPD and this classifier shows performance of up to 95% for detection rate with an approximately 10% false positive rate. Processing time for one test image is about 1.22ms.",2013,"2013 IEEE International Conference on Systems, Man, and Cybernetics"
Cmar_a_233595 10445..10453,"*These authors contributed equally to this work Purpose: The objective of this research was to validate the diagnostic value of threedimensional texture parameters and clinical characteristics in the differentiation of colorectal signet-ring cell carcinoma (SRCC) and adenocarcinoma (AC). Methods: We retrospectively analyzed data from 102 patients with SRCC or AC confirmed by pathology, including 51 SRCC (from January 2015 to July 2019) and 51 AC patients (from January 2019 to July 2019). CT findings and clinical data, including age, gender, clinical symptoms, serological biomarkers, tumor size, and tumor location, were compared between SRCC and AC. CT texture features were quantified on portal phase images using three-dimensional analysis. A list of texture parameters was generated with MaZda software for the classification of tumors. The texture features, clinical data and CT findings were statistically analyzed for the discrimination ability of SRCC and AC, and the potential predictive parameters that may be used to differentiate the two groups were subsequently tested using the least absolute shrinkage and selection operator (LASSO) and logistic regression analyses. The receiver operating characteristic curve (ROC) provided a range of values for establishing the cutoff value, as well as the sensitivity and specificity of prediction for each significant variable. Results: SRCC occurred more often in men than AC did (80.39% vs 49.02%, P < 0.01). The patients were younger in the SRCC group than in the AC group, without a statistically significant difference (55.84 vs 59.20 years, P = 0.216). There were no significant differences in the clinical symptoms, tumor size, or tumor location between the two groups (P=0.505, P=0.19, P=0.843, respectively). The elevation of serological biomarker CA724 was more common in SRCC than in AC (P< 0.001). Perc.01%3D, Perc.10%3D and s(1,0,0) SumAverg were lower in the SRCC group than in the AC group during the portal phase, with the areas under curve (AUCs) of 0.892â€“0.929, sensitivity of 76.5â€“84.3% and specificity of 88.2â€“96.1%. In the differentiation between SRCC and AC, the 1-NN minimal classification error (MCR) was 29.41%. Conclusion: Three-dimensional texture parameters, including Perc.01%3D, Perc.10%3D and s(1,0,0) SumAverg, exhibited a favorable discriminatory ability to distinguish SRCC from AC.",2019,
Pulse Diagnosis Signals Analysis of Fatty Liver Disease and Cirrhosis Patients by Using Machine Learning,"OBJECTIVE
. To compare the signals of pulse diagnosis of fatty liver disease (FLD) patients and cirrhosis patients.


METHODS
After collecting the pulse waves of patients with fatty liver disease, cirrhosis patients, and healthy volunteers, we do pretreatment and parameters extracting based on harmonic fitting, modeling, and identification by unsupervised learning Principal Component Analysis (PCA) and supervised learning Least squares Regression (LS) and Least Absolute Shrinkage and Selection Operator (LASSO) with cross-validation step by step for analysis.


RESULTS
There is significant difference between the pulse diagnosis signals of healthy volunteers and patients with FLD and cirrhosis, and the result was confirmed by 3 analysis methods. The identification accuracy of the 1st principal component is about 75% without any classification formation by PCA, and supervised learning's accuracy (LS and LASSO) was even more than 93% when 7 parameters were used and was 84% when only 2 parameters were used.


CONCLUSION
The method we built in this study based on the combination of unsupervised learning PCA and supervised learning LS and LASSO might offer some confidence for the realization of computer-aided diagnosis by pulse diagnosis in TCM. In addition, this study might offer some important evidence for the science of pulse diagnosis in TCM clinical diagnosis.",2015,The Scientific World Journal
CoCoA: A General Framework for Communication-Efficient Distributed Optimization,"The scale of modern datasets necessitates the development of efficient distributed optimization methods for machine learning. We present a general-purpose framework for distributed computing environments, CoCoA, that has an efficient communication scheme and is applicable to a wide variety of problems in machine learning and signal processing. We extend the framework to cover general non-strongly-convex regularizers, including L1-regularized problems like lasso, sparse logistic regression, and elastic net regularization, and show how earlier work can be derived as a special case. We provide convergence guarantees for the class of convex regularized loss minimization objectives, leveraging a novel approach in handling non-strongly-convex regularizers and non-smooth loss functions. The resulting framework has markedly improved performance over state-of-the-art methods, as we illustrate with an extensive set of experiments on real distributed datasets.",2017,J. Mach. Learn. Res.
