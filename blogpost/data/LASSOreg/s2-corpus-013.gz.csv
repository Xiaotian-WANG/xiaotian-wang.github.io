title,abstract,year,journal
Hybrid Single Node Genetic Programming for Symbolic Regression,"This paper presents a first step of our research on designing an effective and efficient GP-based method for symbolic regression. First, we propose three extensions of the standard Single Node GP, namely 1 a selection strategy for choosing nodes to be mutated based on depth and performance of the nodes, 2 operators for placing a compact version of the best-performing graph to the beginning and to the end of the population, respectively, and 3 a local search strategy with multiple mutations applied in each iteration. All the proposed modifications have been experimentally evaluated on five symbolic regression benchmarks and compared with standard GP and SNGP. The achieved results are promising showing the potential of the proposed modifications to improve the performance of the SNGP algorithm. We then propose two variants of hybrid SNGP utilizing a linear regression technique, LASSO, to improve its performance. The proposed algorithms have been compared to the state-of-the-art symbolic regression methods that also make use of the linear regression techniques on four real-world benchmarks. The results show the hybrid SNGP algorithms are at least competitive with or better than the compared methods.",2016,Trans. Comput. Collect. Intell.
Low sample size and regression: A Monte Carlo approach,"This article performs simulations with different small samples considering the regression techniques of OLS, Jackknife, Bootstrap, Lasso and Robust Regression in order to stablish the best approach in terms of lower bias and statistical significance with a pre-specified data generating process -DGP-. The methodology consists of a DGP with 5 variables and 1 constant parameter which was regressed among the simulations with a set of random normally distributed variables considering samples sizes of 6, 10, 20 and 500. Using the expected values discriminated by each sample size, the accuracy of the estimators was calculated in terms of the relative bias for each technique. The results indicate that Jackknife approach is more suitable for lower sample sizes as it was stated by Speed (1994), Bootstrap approach reported to be sensitive to a lower sample size indicating that it might not be suitable for stablish significant relationships in the regressions. The Monte Carlo simulations also reflected that when a significant relationship is found in small samples, this relationship will also tend to remain significant when the sample size is increased.",2019,
Bayesian regularization in regression models for survival data,"This thesis is concerned with the development of flexible continuous-time survival models based on the accelerated failure time (AFT) model for the survival time and the Cox relative risk (CRR) model for the hazard rate. The flexibility concerns on the one hand the extension of the predictor to take into account simultaneously for a variety of different forms of covariate effects. On the other hand, the often too restrictive parametric assumptions about the survival distribution are replaced by semiparametric approaches that allow very flexible shapes of survival distribution. We use the Bayesian methodology for inference. The arising problems, like e. g. the penalization of high-dimensional linear covariate effects, the smoothing of nonlinear effects as well as the smoothing of the baseline survival distribution, are solved with the application of regularization priors tailored for the respective demand. 
The considered expansion of the two survival model classes enables to deal with various challenges arising in practical analysis of survival data. For example the models can deal with high-dimensional feature spaces (e. g. gene expression data), they facilitate feature selection from the whole set or a subset of the available covariates and enable the simultaneous modeling of any type of nonlinear covariate effects for covariates that should always be included in the model. The option of the nonlinear modeling of covariate effects as well as the semiparametric modeling of the survival time distribution enables furthermore also a visual inspection of the linearity assumptions about the covariate effects or accordingly parametric assumptions about the survival time distribution. 
In this thesis it is shown, how the p>n paradigm, feature relevance, semiparametric inference for functional effect forms and the semiparametric inference for the survival distribution can be treated within a unified Bayesian framework. Due the option to control the amount of regularization of the considered priors for the linear regression coefficients, there is no need to distinguish conceptionally between the cases p n. To accomplish the desired regularization, the regression coefficients are associated with shrinkage, selection or smoothing priors. Since the utilized regularization priors all facilitate a hierarchical representation, the resulting modular prior structure, in combination with adequate independence assumptions for the prior parameters, enables to establish a unified framework and the possibility to construct efficient MCMC sampling schemes for joint shrinkage, selection and smoothing in flexible classes of survival models. The Bayesian formulation enables therefore the simultaneous estimation of all parameters involved in the models as well as prediction and uncertainty statements about model specification. 
The presented methods are inspired from the flexible and general approach for structured additive regression (STAR) for responses from an exponential family and CRR-type survival models. Such systematic and flexible extensions are in general not available for AFT models. An aim of this work is to extend the class of AFT models in order to provide such a rich class of models as resulting from the STAR approach, where the main focus relies on the shrinkage of linear effects, the selection of covariates with linear effects together with the smoothing of nonlinear effects of continuous covariates as representative of a nonlinear modeling. Combined are in particular the Bayesian lasso, the Bayesian ridge and the Bayesian NMIG (a kind of spike-and-slab prior) approach to regularize the linear effects and the P-spline approach to regularize the smoothness of the nonlinear effects and the baseline survival time distribution. To model a flexible error distribution for the AFT model, the parametric assumption for the baseline error distribution is replaced by the assumption of a finite Gaussian mixture distribution. For the special case of specifying one basis mixture component the estimation problem essentially boils down to estimation of log-normal AFT model with STAR predictor. In addition, the existing class of CRR survival models with STAR predictor, where also baseline hazard rate is approximated by a P-spline, is expanded to enable the regularization of the linear effects with the mentioned priors, which broadens further the area of application of this rich class of CRR models. Finally, the combined shrinkage, selection and smoothing approach is also introduced to the semiparametric version of the CRR model, where the baseline hazard is unspecified and inference is based on the partial likelihood. 
Besides the extension of the two survival model classes the different regularization properties of the considered shrinkage and selection priors are examined. The developed methods and algorithms are implemented in the public available software BayesX and in R-functions and the performance of the methods and algorithms is extensively tested by simulation studies and illustrated through three real world data sets.",2013,
Fusion of Hyperspectral and LiDAR Data for Landscape Visual Quality Assessment,"Landscape visual quality is an important factor associated with daily experiences and influences our quality of life. In this work, the authors present a method of fusing airborne hyperspectral and mapping light detection and ranging (LiDAR) data for landscape visual quality assessment. From the fused hyperspectral and LiDAR data, classification and depth images at any location can be obtained, enabling physical features such as land-cover properties and openness to be quantified. The relationship between physical features and human landscape preferences is learned using least absolute shrinkage and selection operator (LASSO) regression. The proposed method is applied to the hyperspectral and LiDAR datasets provided for the 2013 IEEE GRSS Data Fusion Contest. The results showed that the proposed method successfully learned a human perception model that enables the prediction of landscape visual quality at any viewpoint for a given demographic used for training. This work is expected to contribute to automatic landscape assessment and optimal spatial planning using remote sensing data.",2014,IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing
Variable Selection in Logistic Regression Model,"Variable selection is one of the most important problems in pattern recognition. In linear regression model, there are many methods can solve this problem, such as Least absolute shrinkage and selection operator (LASSO) and many improved LASSO methods, but there are few variable selection methods in generalized linear models. We study the variable selection problem in logistic regression model. We propose a new variable selection method-the logistic elastic net, prove that it has grouping effect which means that the strongly correlated predictors tend to be in or out of the model together. The logistic elastic net is particularly useful when the number of predictors (p) is much bigger than the number of observations (n). By contrast, the LASSO is not a very satisfactory variable selection method in the case when p is more larger than n. The advantage and effectiveness of this method are demonstrated by real leukemia data and a simulation study.",2015,Chinese Journal of Electronics
Using Exploratory Data Mining to Identify Important Correlates of Nonsuicidal Self-Injury Frequency,"Objective: Nonsuicidal self-injury (NSSI) has been linked to many adverse outcomes, with more frequent NSSI increasing the likelihood of impairment, severity, and more serious self-harming behavior (e.g., suicidality). Despite the determined importance of NSSI frequency in understanding the severity of oneâ€™s behavior, there is still a need to identify which constructs may be influential in predicting frequency. The current study aimed to fill this gap by identifying which correlates are most important in relation to NSSI frequency through 2 exploratory data mining methods. Method: Seven hundred twelve undergraduate students with a history of NSSI completed self-report measures of NSSI behavior, suicidality, cognitiveâ€“affective deficits, and psychopathology symptomology. Results: Both exploratory data mining methodsâ€”lasso regression and random forestsâ€”demonstrated number of NSSI methods to be the factor with the most importance in relation to lifetime NSSI frequency. Once this variable was removed, suicide plan and depressive symptomology were significant correlates across methods. Conclusions: The current findings support the literature concerning the relationship between NSSI frequency and NSSI methods but also implicate suicide plans, an often-overlooked factor, and depression in NSSI severity.",2018,Psychology of Violence
Fast local linear regression with anchor regularization,"Regression is an important task in machine learning and data mining. It has several applications in various domains, including finance, biomedical, and computer vision. Recently, network Lasso, which estimates local models by making clusters using the network information, was proposed and its superior performance was demonstrated. In this study, we propose a simple yet effective local model training algorithm called the fast anchor regularized local linear method (FALL). More specifically, we train a local model for each sample by regularizing it with precomputed anchor models. The key advantage of the proposed algorithm is that we can obtain a closed-form solution with only matrix multiplication; additionally, the proposed algorithm is easily interpretable, fast to compute and parallelizable. Through experiments on synthetic and real-world datasets, we demonstrate that FALL compares favorably in terms of accuracy with the state-of-the-art network Lasso algorithm with significantly smaller training time (two orders of magnitude).",2020,ArXiv
Graph structure inference for high-throughput genomic data,"Graph Structured Inference For High-Throughput Genomic Data Hui Zhou Recent advances in high-throughput sequencing technologies enable us to study a large number of biomarkers and use their information collectively. Based on highthroughput experiments, there are many genome-wide networks constructed to characterize the complex physical or functional interactions between the biomarkers. To identify outcome-related biomarkers, it is often advantageous to make use of the known relational structure, because graph structured inference introduces smoothness and reduces complexity in modelling. In this dissertation, we propose models for high-dimensional epigenetic and genomic data that incorporate the network structure and update the network structure based on empirical evidence. In the first part of this dissertation, we propose a penalized conditional logistic regression model for high dimensional DNA methylation data. DNA methylation of CpG sites within genes are often correlated and the number of CpG sites typically far outnumbers the sample size. The new penalty function combines the truncated lasso penalty and a graph fuse-lasso penalty to induce parsimonious and consistent models, and to incorporate the CpG sites network structure without introducing extra bias. An efficient minorization-maximization algorithm that utilizes difference of convex programming and alternating direction method of multipliers is presented. Extensive simulations demonstrated superior performance of the proposed method compared to several existing methods in both model selection consistency and parameter estimation accuracy. We also applied the proposed method to a matched case-control breast invasive carcinoma methylation data from the Cancer Genome Atlas (TCGA), generated from both Illumina Infinium HumanMethylation27 (HM27) and HumanMethylation450 (HM450) Beadchip. The proposed method identified several outcome-related CpG sites that have been missed by the existing methods. In the latter part of this dissertation, we propose a Bayesian hierarchical graphstructured model that integrates a priori network information with empirical evidence. Empirical data may suggest modifications to the given network structure, which could lead to new and interesting biological findings when the prior knowledge on the graphical structure among the variables is limited or partial. We present the full hierarchical model along with the Markov Chain Monte Carlo sampling inference procedure. Using both simulations and brain aging gene pathway data, we showed that the new method can identify discrepancy between data and a prior known graph structure and suggest modifications and updates. Motivated by methylation and gene expression data, the two models we propose in this thesis make use of the available structure in the data and produce better inferential results. The proposed methods can be applied to a wider range of problems.",2014,
Multivariate statistical methods for estimating grassland leaf area index and chlorophyll content using hyperspectral measurements,"Grassland habitat covers about one-quarter of the Earthâ€™s land surface, providing significant contribution to the worldâ€™s total agricultural production, plant biodiversity, and carbon sequestration. The advent of hyperspectral remote sensing and the future launch of planned spaceborne hyperspectral missions will open up new possibilities over conventional multispectral RS to better quantify grassland characteristics. Hyperspectral data, while rich in information, presents a challenge for analysis due to its high dimensionality and multicollinearity. This present study investigated three promising high dimensional multivariate regression models namely partial least squares regression (PLSR), regularization and shrinkage method Lasso, and nonparametric Random Forest (RF) regression, to estimate grassland leaf area index (LAI) and chlorophyll using field canopy hyperspectral measurements (n=185). For each regression model, three spectral transformations namely continuum-removal, firstderivative, and pseudo-absorbance were evaluated.",2015,
Online Hyperparameter Search Interleaved with Proximal Parameter Updates,"There is a clear need for efficient algorithms to tune hyperparameters for statistical learning schemes, since the commonly applied search methods (such as grid search with N-fold cross-validation) are inefficient and/or approximate. Previously existing algorithms that efficiently search for hyperparameters relying on the smoothness of the cost function cannot be applied in problems such as Lasso regression. 
In this contribution, we develop a hyperparameter optimization method that relies on the structure of proximal gradient methods and does not require a smooth cost function. Such a method is applied to Leave-one-out (LOO)-validated Lasso and Group Lasso to yield efficient, data-driven, hyperparameter optimization algorithms. 
Numerical experiments corroborate the convergence of the proposed method to a local optimum of the LOO validation error curve, and the efficiency of its approximations.",2020,ArXiv
Bayesian lasso binary quantile regression,"In this paper, a Bayesian hierarchical model for variable selection and estimation in the context of binary quantile regression is proposed. Existing approaches to variable selection in a binary classification context are sensitive to outliers, heteroskedasticity or other anomalies of the latent response. The method proposed in this study overcomes these problems in an attractive and straightforward way. A Laplace likelihood and Laplace priors for the regression parameters are proposed and estimated with Bayesian Markov Chain Monte Carlo. The resulting model is equivalent to the frequentist lasso procedure. A conceptional result is that by doing so, the binary regression model is moved from a Gaussian to a full Laplacian framework without sacrificing much computational efficiency. In addition, an efficient Gibbs sampler to estimate the model parameters is proposed that is superior to the Metropolis algorithm that is used in previous studies on Bayesian binary quantile regression. Both the simulation studies and the real data analysis indicate that the proposed method performs well in comparison to the other methods. Moreover, as the base model is binary quantile regression, a much more detailed insight in the effects of the covariates is provided by the approach. An implementation of the lasso procedure for binary quantile regression models is available in the R-package bayesQR.",2013,Computational Statistics
Development of novel prognostic models for predicting complications of urethroplasty,"Introduction and objectiveTo identify predictors of thirty-day perioperative complications after urethroplasty and create a model to predict patients at increased risk.MethodsWe selected all patients recorded in the National Surgery Quality Improvement Program (NSQIP) from 2005 to 2015 who underwent urethroplasty, determined by Current Procedural Terminology (CPT) codes. The primary outcome of interest was a composite 30-day complication rate. To develop predictive models of urethroplasty complications we used random forest and logistic regression with tenfold cross-validation employing demographic, comorbidity, laboratory, and wound characteristics as candidate predictors. Models were selected based on the receiver operating characteristic (ROC) curve, with the primary measure of performance being the area under curve (AUC).ResultsWe identified 1135 patients who underwent urethroplasty and met inclusion criteria. The mean age was 53Â years with 84% being male. The overall incidence of complications was 8.6% (nâ€‰=â€‰98). Patients who experienced a complication more commonly had diabetes, a preoperative blood transfusion, preoperative sepsis, lower hematocrit and albumin, as well as a longer operative time (pâ€‰<â€‰0.05). LASSO logistic and random forest logistic models for predicting urethroplasty complications had an AUC (95% CI) 0.73 (0.58â€“0.87), and 0.48 (0.33â€“0.68), respectively. The variables that were determined to be most important and included in the predictive models were operative time, age, American Society of Anesthesiologists (ASA) classification and preoperative laboratory values (white blood cell count, hematocrit, creatinine, platelets).ConclusionOur predictive models of complications perform well and may allow for improved preoperative counseling and risk stratification in the surgical management of urethral stricture.",2018,World Journal of Urology
Estimation de l'effacement de consommation Ã©lectrique d'un groupe de clients rÃ©sidentiels,"Dans cette these, nous developpons une methode dâ€™estimation de lâ€™effacement de consommation electrique dâ€™un groupe de clients residentiels. Lâ€™effacement, correspondant a une reduction de la puissance electrique sur une certaine duree, est desormais valorise sur les marches electriques et contribue a equilibrer le systeme electrique. Pour le quantifier, il faut estimer quâ€™elle aurait ete la puissance appelee, i.e. la baseline, en lâ€™absence de lâ€™effacement. Ce dernier sâ€™obtient alors par difference de la baseline et de la puissance realisee. Les methodes dâ€™estimation de la baseline reposent sur des profils de consommation, des modeles de regression et des methodes fondees sur un groupe de controle. Ces dernieres offrent les resultats les plus precis mais deployer un groupe de controle aleatoire pour un usage operationnel nâ€™est pas envisageable.On sâ€™interesse donc a selectionner un groupe de controle non-experimental selon deux approches : la premiere emploie les caracteristiques observables des clients controles et la seconde leurs courbes de charge individuelles. Cette derniere idee consiste a selectionner ces individus tels que la distance entre leur courbe de charge moyenne et celle du groupe recevant les effacements soit minimale. A cette fin, nous proposons un algorithme de selection et adaptons les methodes de regression sous contrainte, ridge et Lasso. Ces nouvelles methodes procurent les meilleurs resultats. Enfin, pour estimer lâ€™effacement en ligne, nous mettons en place un outil innovant qui associe un systeme de gestion de flux de donnees a un logiciel statistique",2015,
On Performance of Shrinkage Methods â€“ A Monte Carlo Study,"Multicollinearity has been a serious problem in regression analysis, Ordinary Least Squares (OLS) regression may result in high variability in the estimates of the regression coefficients in the presence of multicollinearity. Least Absolute Shrinkage and Selection Operator (LASSO) methods is a well established method that reduces the variability of the estimates by shrinking the coefficients and at the same time produces interpretable models by shrinking some coefficients to exactly zero. We present the performance of LASSO -type estimators in the presence of multicollinearity using Monte Carlo approach. The performance of LASSO, Adaptive LASSO, Elastic Net, Fused LASSO and Ridge Regression (RR) in the presence of multicollinearity in simulated data sets are compared Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) criteria. A Monte Carlo experiment of 1000 trials was carried out at different sample sizes n (50, 100 and 150) with different levels of multicollinearity among the exogenous variables (Ï = 0.3, 0.6, and 0.9). The overall performance of Lasso appears to be the best but Elastic net tends to be more accurate when the sample size is large.",2015,International journal of statistics and applications
Bayesian Iterative Adaptive Lasso Quantile Regression,"Based on the Bayesian adaptive Lasso quantile regression (Alhamzawi et al., 2012), we propose the iterative adaptive Lasso quantile regression, which is an extension to the Expectation Conditional Maximization (ECM) algorithm (Sun et al., 2010). The proposed method is demonstrated via simulation studies and a real data set. Results indicate that the new algorithm performs quite good in comparison to the other existing methods.",2017,IOSR Journal of Mathematics
Skull Retrieval for Craniosynostosis Using Sparse Logistic Regression Models,"Craniosynostosis is the premature fusion of the bones of the calvaria resulting in abnormal skull shapes that can be associated with increased intracranial pressure. While craniosynostoses of multiple different types can be easily diagnosed, quantifying the severity of the abnormality is much more subjective and not a standard part of clinical practice. For this purpose we have developed a severity-based retrieval system that uses a logistic regression approach to quantify the severity of the abnormality of each of three types of craniosynostoses. We compare several different sparse feature selection techniques: L1 regularized logistic regression, fused lasso, and clustering lasso (cLasso). We evaluate our methodology in three ways: 1) for classification of normal vs. abnormal skulls, 2) for comparing pre-operative to post-operative skulls, and 3) for retrieving skulls in order of abnormality severity as compared with the ordering of a craniofacial expert.",2012,Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention
A novel variable-separation method based on sparse representation for stochastic partial differential equations,"In this paper, we propose a novel variable-separation (NVS) method for generic multivariate functions. The idea of NVS is extended to to obtain the solution in tensor product structure for stochastic partial differential equations (SPDEs). 
Compared with many widely used variation-separation methods, NVS shares their merits but has less computation complexity and better efficiency. 
NVS can be used to get the separated representation of the solution for SPDE in a systematic enrichment manner. 
No iteration is performed at each enrichment step. This is a significant improvement compared with proper generalized decomposition. Because the stochastic functions of the separated representations obtained by NVS depend on the previous terms, this impacts on the computation efficiency and brings great challenge for numerical simulation for the problems in high stochastic dimensional spaces. 
In order to overcome the difficulty, we propose an improved least angle regression algorithm (ILARS) and a hierarchical sparse low rank tensor approximation (HSLRTA) method based on sparse regularization. For ILARS, we explicitly give the selection of the optimal regularization parameters at each step based on least angle regression algorithm (LARS) for lasso problems such that ILARS is much more efficient. 
HSLRTA hierarchically decomposes a high dimensional problem into some low dimensional problems and brings an accurate approximation for the solution to SPDEs in high dimensional stochastic spaces using limited computer resource. 
A few numerical examples are presented to illustrate the efficacy of the proposed methods.",2016,arXiv: Numerical Analysis
The Double Dantzig G,"The Dantzig selector (Candes and Tao, 2007) is a new approach th t as been proposed for performing variable selection and model fitting on linear regression models. It uses anL1 penalty to shrink the regression coefficients towards zero, in a similar fashion to the Lasso. While both the Lasso and Dantzig select or potentially do a good job of selecting the correct variables, several researcher s ave noted that they tend to over shrink the final coefficients. This results in an unfortu nate tradeoff. One can either select a high shrinkage tuning parameter that produces an accurate model but poor coefficient estimates or a low shrinkage tuning parameter th at produces more accurate coefficients but includes many irrelevant variables. W e propose a new approach called the â€œDouble Dantzigâ€. The Double Dantzig has two key a dvantages over both the Dantzig selector and the Lasso. First, we demonstrate th it can select the correct model without over shrinking the coefficient estimates. Sec ond, it can be applied to all standard generalized linear model response distributi ons. In addition we develop a path algorithm to simultaneously compute the coefficient e stimates for all values of the tuning parameter, making our approach computationally efficient. A detailed simulation study is performed which illustrates the advantage s of the Double Dantzig in relation to other possible methods. Finally, we demonstrat e the Double Dantzig on several real world data sets. Some key words : Dantzig Selector; Double Dantzig; Generalized Linear Mod els; Lasso; Variable",2007,
"Explaining domestic energy consumption - The comparative contribution of building factors, socio-demographics, behaviours and attitudes","This paper tests to what extent different types of variables (building factors, socio-demographics, attitudes and self-reported behaviours) explain annualized energy consumption in residential buildings, and goes on to show which individual variables have the highest explanatory power. In contrast to many other studies, the problem of multicollinearity between predictors is recognised, and addressed using Lasso regression to perform variable selection.",2015,Applied Energy
Machine Learning to Predict In-hospital Morbidity and Mortality after Traumatic Brain Injury.,"Recently, successful predictions using machine learning (ML) algorithms have been reported in various fields. However, in traumatic brain injury (TBI) cohorts, few studies have examined modern ML algorithms. To develop a simple ML model for TBI outcome prediction, we conducted a performance comparison of nine algorithms: ridge regression, LASSO regression, random forest, gradient boosting, extra trees, decision tree, Gaussian naÃ¯ve Bayes, multinomial naÃ¯ve Bayes, and support vector machine. Fourteen feasible parameters were introduced in the ML models, including age, Glasgow coma scale, systolic blood pressure, abnormal pupillary response, major extracranial injury, computed tomography findings, and routinely collected laboratory values (glucose, C-reactive protein, and fibrin/fibrinogen degradation products). Data from 232 TBI patients were randomly divided into a training sample (80%) for hyperparameter tuning and validation sample (20%). The bootstrap method was used for validation. Random forest demonstrated the best performance for in-hospital poor outcome prediction and ridge regression for in-hospital mortality prediction: the mean statistical measures were 100% sensitivity, 72.3% specificity, 91.7% accuracy, and 0.895 area under the receiver operating characteristic curve (AUC); and 88.4% sensitivity, 88.2% specificity, 88.6% accuracy, and 0.875 AUC, respectively. Based on the feature selection method using the tree-based ensemble algorithm, age, Glasgow coma scale, fibrin/fibrinogen degradation products, and glucose were identified as the most important prognostic factors for poor outcome and mortality. Our results indicated the relatively good predictive performance of modern ML for TBI outcome. Further external validation is required for more heterogeneous samples to confirm our results.",2019,Journal of neurotrauma
Modelling Recovery Rates for Non-Performing Loans,"Based on a rich dataset of recoveries donated by a debt collection business, recovery rates for non-performing loans taken from a single European country are modelled using linear regression, linear regression with Lasso, beta regression and inflated beta regression. We also propose a two-stage model: beta mixture model combined with a logistic regression model. The proposed model allowed us to model the multimodal distribution we found for these recovery rates. All models were built using loan characteristics, default data and collections data prior to purchase by the debt collection business. The intended use of the models was to estimate future recovery rates for improved risk assessment, capital requirement calculations and bad debt management. They were compared using a range of quantitative performance measures under K-fold cross validation. Among all the models, we found that the proposed two-stage beta mixture model performs best.",2019,
Short-term travel time prediction by deep learning: A comparison of different LSTM-DNN models,"Predicting short-term travel time with considerable accuracy and reliability is critically important for advanced traffic management and route planning in Intelligent Transportation Systems (ITS). Short-term travel time prediction uses real travel time values within a sliding time window to predict travel time one or several time step(s) in future. However, the nonstationary properties and abrupt changes of travel time series make challenges in obtaining accurate and reliable predictions. Recent achievements of deep learning approaches in classification and regression shed a light on innovations of time series prediction. This study establishes a series of long short-term memory neural networks with deep neural layers (LSTM-DNN) using 16 settings of hyperparameters and investigates their performance on a 90-day travel time dataset from Caltrans Performance Measurement System (PeMS). Then competitive LSTM-DNN models are tested along with linear models such as linear regression, Ridge and Lasso regression, ARIMA and DNN models under 10 sets of sliding windows and predicting horizons via the same dataset. The results demonstrate the advantage of LSTM-DNN models while showing different characteristics of these deep learning models with different settings of hyperparameters, providing insights for optimizing the structures.",2017,2017 IEEE 20th International Conference on Intelligent Transportation Systems (ITSC)
Group subset selection for linear regression,"Two fast group subset selection (GSS) algorithms for the linear regression model are proposed in this paper. GSS finds the best combinations of groups up to a specified size minimising the residual sum of squares. This imposes an l0 constraint on the regression coefficients in a group context. It is a combinatorial optimisation problem with NP complexity. To make the exhaustive search very efficient, the GSS algorithms are built on QR decomposition and branch-and-bound techniques. They are suitable for middle scale problems where finding the most accurate solution is essential. In the application motivating this research, it is natural to require that the coefficients of some of the variables within groups satisfy some constraints (e.g. non-negativity). Therefore the GSS algorithms (optionally) calculate the model coefficient estimates during the exhaustive search in order to screen combinations that do not meet the constraints. The faster of the two GSS algorithms is compared to an extension to the original group Lasso, called the constrained group Lasso (CGL), which is proposed to handle convex constraints and to remove orthogonality requirements on the variables within each group. CGL is a convex relaxation of the GSS problem and hence more straightforward to solve. Although CGL is inferior to GSS in terms of group selection accuracy, it is a fast approximation to GSS if the optimal regularisation parameter can be determined efficiently and, in some cases, it may serve as a screening procedure to reduce the number of groups.",2014,Comput. Stat. Data Anal.
Adaptive Robust Methodology for Parameter Estimation and Variable Selection,"The dissertation consists of three distinct but related projects. We consider regression model fitting, variable selection in regression, and autocorrelation estimation in time series. In each procedure we formulate the problem in terms of minimizing an objective function which adapts to the given data. First we propose a robust M-estimation procedure for regression. The main purpose of the proposed methodology is to develop a procedure that adapts to light/heavy tailed, symmetric/asymmetric distributions with/without outliers. We focus on studying the properties of the maximum likelihood estimator of the asymmetric exponential power distribution, a broad distribution class that holds both Normal and asymmetric Laplace distributions as special cases. The proposed methodology unifies least squares and quantile regression in a data driven manner to capture both tail decay and asymmetry of the underlying distributions. Finite sample performance of the method is exhibited via extensive Monte Carlo simulation and real data applications. Second, we capitalize on the success of the proposed method and extend it to a variable selection procedure that selects the important predictors under a sparse setting. Quantile regression Lasso, i.e., quantile regession with L1 norm on the regression coefficients for regularization is a robust technique to perform variable selection. However which quantile should be adopted is unclear. The proposed methodology introduces a way to choose the most â€œinformativeâ€ quantile of interest that is used in the adaptive quantile regression Lasso. A modified BIC criterion is used to select the optimal tuning parameter. The proposed procedure selects the quantile based on the log-likelihood of the asymmetric Laplace distribution, and aims to perform the best quantile regression Lasso which is confirmed in both simulation study and a real data analysis. Third, we focus on alleviating the underestimation issue of the sample autocorrelation in linear stationary time series. We first formulate autocorrelation estimation into a least squares prob-",2017,
Too many covariates and too few cases? - a comparative study.,"Prior research indicates that 10-15 cases or controls, whichever fewer, are required per parameter to reliably estimate regression coefficients in multivariable logistic regression models. This condition may be difficult to meet even in a well-designed study when the number of potential confounders is large, the outcome is rare, and/or interactions are of interest. Various propensity score approaches have been implemented when the exposure is binary. Recent work on shrinkage approaches like lasso were motivated by the critical need to develop methods for the p >> n situation, where p is the number of parameters and n is the sample size. Those methods, however, have been less frequently used when pâ‰ˆn, and in this situation, there is no guidance on choosing among regular logistic regression models, propensity score methods, and shrinkage approaches. To fill this gap, we conducted extensive simulations mimicking our motivating clinical data, estimating vaccine effectiveness for preventing influenza hospitalizations in the 2011-2012 influenza season. Ridge regression and penalized logistic regression models that penalize all but the coefficient of the exposure may be considered in these types of studies. Copyright Â© 2016 John Wiley & Sons, Ltd.",2016,Statistics in medicine
Hyperspectral data compression using lasso algorithm for spectral decorrelation,"Among discrete orthogonal transforms, Karhunen-Loeve transform (KLT) achieves the most optimal spectral decorrelation for hyperspectral data compression with minimum mean square error. A common approach for those spectral decorrelation transform techniques such as KLT is to select m coefficient using some threshold value and then treating the rest of the coefficients as zero, this will result in loss of information. In order to preserve more information on small target data, this paper focused on a new technique called joint KLT-Lasso. The Lasso was applied to KLT coefficient. Sparse loadings were obtained using the Lasso constraint on KLT regression coefficients and more coefficients were shrunk to exact zero. The goal of our new method is to introduce a limit on the sum of the absolute values of the KLT coefficients and in which some coefficients consequently become zero without using any threshold value. A simulation on different hyperspectral data showed encouraging results.",2014,
Systematic Comparison of Machine Learning Methods for Identification of miRNA Species as Disease Biomarkers,"Micro RNA (miRNA) plays important roles in a variety of biological processes and can act as disease biomarkers. Thus, establishment of discovery methods to detect disease-related miRNAs is warranted. Human omics data including miRNA expression profiles have orders of magnitude with much more number of descriptors (p) than that of samples (n), which is so called â€œp > > n problemâ€. Since traditional statistical methods mislead to localized solutions, application of machine learning (ML) methods that handle sparse selection of the variables are expected to solve this problem. Among many ML methods, least absolute shrinkage and selection operator (LASSO) and multivariate adaptive regression splines (MARS) give a few variables from the result of supervised learning with endpoints such as human disease statuses. Here, we performed systematic comparison of LASSO and MARS to discover biomarkers, using six miRNA expression data sets of human disease samples, which were obtained from NCBI Gene Expression Omnibus (GEO). We additionally conducted partial least square method discriminant analysis (PLS-DA), as a control traditional method to evaluate baseline performance of discriminant methods. We observed that LASSO and MARS showed relatively higher performance compared to that of PLS-DA, as the number of the samples increases. Also, some of the identified miRNA species by ML methods have already been reported as candidate disease biomarkers in the previous biological studies. These findings should contribute to the extension of our knowledge on ML method performances in empirical utilization of clinical data.",2015,
