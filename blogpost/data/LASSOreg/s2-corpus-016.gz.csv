title,abstract,year,journal
"InfÃ©rence statistique en grande dimension pour des modÃ¨les structurels. ModÃ¨les linÃ©aires gÃ©nÃ©ralisÃ©s parcimonieux, mÃ©thode PLS et polynÃ´mes orthogonaux et dÃ©tection de communautÃ©s dans des graphes.","Cette these s'inscrit dans le cadre de l'analyse statistique de donnees en grande dimension. Nous avons en effet aujourd'hui acces a un nombre toujours plus important d'information. L'enjeu majeur repose alors sur notre capacite a explorer de vastes quantites de donnees et a en inferer notamment les structures de dependance. L'objet de cette these est d'etudier et d'apporter des garanties theoriques a certaines methodes d'estimation de structures de dependance de donnees en grande dimension.La premiere partie de la these est consacree a l'etude de modeles parcimonieux et aux methodes de type Lasso. Apres avoir presente les resultats importants sur ce sujet dans le chapitre 1, nous generalisons le cas gaussien a des modeles exponentiels generaux. La contribution majeure a cette partie est presentee dans le chapitre 2 et consiste en l'etablissement d'inegalites oracles pour une procedure Group Lasso appliquee aux modeles lineaires generalises. Ces resultats montrent les bonnes performances de cet estimateur sous certaines conditions sur le modele et sont illustres dans le cas du modele Poissonien. Dans la deuxieme partie de la these, nous revenons au modele de regression lineaire, toujours en grande dimension mais l'hypothese de parcimonie est cette fois remplacee par l'existence d'une structure de faible dimension sous-jacente aux donnees. Nous nous penchons dans cette partie plus particulierement sur la methode PLS qui cherche a trouver une decomposition optimale des predicteurs etant donne un vecteur reponse. Nous rappelons les fondements de la methode dans le chapitre 3. La contribution majeure a cette partie consiste en l'etablissement pour la PLS d'une expression analytique explicite de la structure de dependance liant les predicteurs a la reponse. Les deux chapitres suivants illustrent la puissance de cette formule aux travers de nouveaux resultats theoriques sur la PLS . Dans une troisieme et derniere partie, nous nous interessons a la modelisation de structures au travers de graphes et plus particulierement a la detection de communautes. Apres avoir dresse un etat de l'art du sujet, nous portons notre attention sur une methode en particulier connue sous le nom de spectral clustering et qui permet de partitionner les noeuds d'un graphe en se basant sur une matrice de similarite. Nous proposons dans cette these une adaptation de cette methode basee sur l'utilisation d'une penalite de type l1. Nous illustrons notre methode sur des simulations.",2015,
Higher Order Refinements by Bootstrap in Lasso and other Penalized Regression Methods,"Selection of important covariates and to drop the unimportant ones from a high-dimensional regression model is a long standing problem and hence have received lots of attention in the last two decades. After selecting the correct model, it is also important to properly estimate the existing parameters corresponding to important covariates. In this spirit, Fan and Li (2001) proposed Oracle property as a desired feature of a variable selection method. Oracle property has two parts; one is the variable selection consistency (VSC) and the other one is the asymptotic normality. Keeping VSC fixed and making the other part stronger, Fan and Lv (2008) introduced the strong oracle property. In this paper, we consider different penalized regression techniques which are VSC and classify those based on oracle and strong oracle property. We show that both the residual and the perturbation bootstrap methods are second order correct for any penalized estimator irrespective of its class. Most interesting of all is the Lasso, introduced by Tibshirani (1996). Although Lasso is VSC, it is not asymptotically normal and hence fails to satisfy the oracle property.",2019,arXiv: Statistics Theory
Developing and validating a multivariable prediction model for in-hospital mortality of pneumonia with advanced chronic kidney disease patients: a retrospective analysis using a nationwide database in Japan,"The prognosis of pneumonia in patients with advanced stage chronic kidney disease (CKD) remains unimproved for years. We attempt to develop a simple and more useful scoring system for predicting in-hospital mortality for advanced CKD patients with pneumonia. Using the Diagnosis Procedure Combination database, we identified the in-hospital adult patients both with a record of pneumonia and stage 5 or 5D CKD as a comorbidity on admission between April 1, 2012 and March 31, 2016. Predictive variable selection was analyzed by multivariable logistic regression analysis, stepwise method, LASSO method and random forest method, and then develop a new simple scoring system seeking for highest c-statistics combination of variables in one sample data set for model development. Finally, we compared c-statistics of univariate logistic regression about new scoring system with c-statistics about â€œA-DROPâ€ in the other sample data set. We identified 8402 patients in 707 hospitals, and the total in-hospital mortality was 11.0% (437 patients) in development data set. Seven variables were selected, which includes age (maleâ€‰â‰¥â€‰70 years, femaleâ€‰â‰¥â€‰75 years), respiratory failure, orientation disturbance, low blood pressure, the need of assistance in feeding or bowel control, severe or moderate thinness and CRP 200 mg/L or extent of consolidation on chest X-rayâ€‰â‰¥â€‰2/3 of one lung. The c-statistics of univariate logistic regression was 0.8017 using seven variables, while that was 0.7372 using â€œA-DROPâ€ In advanced CKD patients, if we select appropriate variables for predicting in-hospital mortality, simple scoring system may have better discrimination than â€œA-DROPâ€.",2020,Clinical and Experimental Nephrology
A six-gene prognostic model predicts overall survival in bladder cancer patients,"BackgroundThe fatality and recurrence rates of bladder cancer (BC) have progressively increased. DNA methylation is an influential regulator associated with gene transcription in the pathogenesis of BC. We describe a comprehensive epigenetic study performed to analyse DNA methylation-driven genes in BC.MethodsData related to DNA methylation, the gene transcriptome and survival in BC were downloaded from The Cancer Genome Atlas (TCGA). MethylMix was used to detect BC-specific hyper-/hypo-methylated genes. Metascape was used to carry out gene ontology (GO) enrichment and Kyoto Encyclopedia of Genes and Genomes (KEGG) pathway analyses. A least absolute shrinkage and selection operator (LASSO)-penalized Cox regression was conducted to identify the characteristic dimension decrease and distinguish prognosis-related methylation-driven genes. Subsequently, we developed a six-gene risk evaluation model and a novel prognosis-related nomogram to predict overall survival (OS). A survival analysis was carried out to explore the individual prognostic significance of the six genes.ResultsIn total, 167 methylation-driven genes were identified. Based on the LASSO Cox regression, six genes, i.e., ARHGDIB, LINC00526, IDH2, ARL14, GSTM2, and LURAP1, were selected for the development of a risk evaluation model. The Kaplanâ€“Meier curve indicated that patients in the low-risk group had considerably better OS (Pâ€‰=â€‰1.679eâˆ’05). The area under the curve (AUC) of this model was 0.698 at 3Â years of OS. The verification performed in subgroups demonstrated the validity of the model. Then, we designed an OS-associated nomogram that included the risk score and clinical factors. The concordance index of the nomogram was 0.694. The methylation levels of IDH2 and ARL14 were appreciably related to the survival results. In addition, the methylation and gene expression-matched survival analysis revealed that ARHGDIB and ARL14 could be used as independent prognostic indicators. Among the six genes, 6 methylation sites in ARHGDIB, 3 in GSTM2, 1 in ARL14, 2 in LINC00526 and 2 in LURAP1 were meaningfully associated with BC prognosis. In addition, several abnormal methylated sites were identified as linked to gene expression.ConclusionWe discovered differential methylation in BC patients with better and worse survival and provided a risk evaluation model by merging six gene markers with clinical characteristics.",2019,Cancer Cell International
"Measuring social media influencer index- insights from facebook, Twitter and Instagram","Abstract The growth of social media has completely revamped the way people interact, communicate and engage. These platforms play a key role in facilitating greater outreach and influence. This study proposes a mechanism for measuring the influencer index across popular social media platforms including Facebook, Twitter, and Instagram. A set of features that determine the impact on the consumers are modelled using a regression approach. The underlying machine learning algorithms including Ordinary Least Squares (OLS), K-NN Regression (KNN), Support Vector Regression (SVR), and Lasso Regression models are adapted to compute a cumulative score in terms of influencer index. Findings indicate that engagement, outreach, sentiment, and growth play a key role in determining the influencers. Further, the ensemble of the four models resulted in the highest accuracy of 93.7% followed by the KNN regression with 93.6%. The study has implications across various domains of e-commerce, viral marketing, social media marketing and brand management wherein identification of key information propagators is essential. These influencer indices may further be utilized by e-commerce portals and brands for the purpose of social media promotion and engagement for larger outreach.",2019,Journal of Retailing and Consumer Services
Experimental Validation of Genomic Selection in Sugarcane,"Sugarcane cultivars (Saccharum spp.) are interspecific hybrids characterized by highly heterozygous and polyploid genome. Genomic selection (GS) approach is believed to be well suited for complex traits by including all markers in prediction models. Our objective was to test the GS approach in a complex polyploid crop. Predictions of genetic values were carried out on two independent panels, each composed of 167 cultivars and breeding materials covering the worldwide diversity. Accessions were genotyped using 1499 DArT. Phenotyping was carried out in Reunion for one panel and in Guadeloupe for the other one. We considered ten traits relative to sugar and fiber contents, digestibility and composition of the bagasse, plant morphology and disease resistances. We used seven predictive models: Bayesian Regression, Bayesian LASSO, Ridge-regression, BayesA, BayesB, Reproducing Kernel Hilbert Space and Partial Least Square Regression. Accuracies of predictions were assessed through correlations between observed and predicted genetic values, firstly by using a cross-validation within each panel, and secondly by using a cross-validation between panels. Accuracies of predictions were of similar value between the seven GS models for a given trait, while differences were observed among traits. Depending on the trait considered, the average GS accuracy values related to within-panel prediction ranged from 0.29 to 0.61 in the Reunion panel and from 0.13 to 0.5 in the Guadeloupe panel. GS accuracy values based on cross-validations between the two independent panels ranged from 0.13 (smut resistance) to 0.55 (brix). This study represents the first validation of GS approach in sugarcane with experimental data. (Resume d'auteur)",2013,
Contribution Ã  la sÃ©lection de modÃ¨le via pÃ©nalisation Lasso en Ã‰pidÃ©miologie,"Mes travaux portent principalement sur le developpement, lâ€™adaptation, lâ€™implementation et lâ€™application de methodes statistiques de selection de modele. Ma principale contribution consiste a adapter des methodes de l'apprentissage statistique supervise qui sont devenues tres populaires lors de la derniere decennie, les regressions penalisees de type Lasso, a l'analyse de donnees issues d'etudes epidemiologiques. L'enjeu est de s'attaquer aux problemes des donnees volumineuses (\textit{Big Data}) tout en respectant les objectifs et specificites de la discipline. Le volume important se refere ici au fait que le nombre d'observations et/ou le nombre de variables est bien plus important que celui qui etait classique dans le domaine, sans exclure le cas ou le nombre de variables est superieur au nombre d'observations (donnees de grande dimension). 
 
Le contexte de la pratique epidemiologique est en plein changement avec les evolutions technologiques et la consequente disponibilite croissante des Big Data. Le Systeme National des Donnees de Sante (SNDS), regroupant les principales bases de donnees de sante publique existantes en France, constitue un exemple de Big Data en sante. Le donnees ``omiques'' (genomiques, transcriptomiques, proteomiques, metabolomiques, microbiomiques, mycobiomiques, viromiques,$\ldots$) issues des avancees des techniques de sequencage a haut debit constituent un autre exemple de Big Data en sante. Enfin, les mesures de l'\textit{exposome} (par opposition aux facteurs genetiques), qui designe en epidemiologie lâ€™ensemble des expositions environnementales que subit un individu au long de sa vie peut egalement constituer une source de Big Data. 
 
Ce document s'articule autour de trois chapitres. Il resume mon activite de recherche depuis 2005, soit depuis mon recrutement a lâ€™Universite de Bordeaux apres ma these. 
Le premier chapitre est une introduction generale dans laquelle je contextualise, motive et enonce la problematique abordee tout au long de mes recherches. Le deuxieme chapitre est consacre a mes travaux en lien avec les etudes sur les traumatismes accidentels et expositions medicamenteuses a partir des donnees du SNDS. Le troisieme chapitre est consacre a mes travaux en lien avec des etudes biomedicales: la prediction de la charge virale censuree par un seuil de detection a partir des mutations du VIH, d'une part, et l'automatisation de la detection des seuils d'anomalie des hemogrammes en population generale, d'autre part.",2018,
Profit or Loss: A Long Short Term Memory based model for the Prediction of share price of DLF group in India,"Presently, the prediction of share is a challenging issue for the research community as share market is a chaotic place. The reason behind it, there are several factors such as government policies, international market, weather, performance of company. In this article, a model has been developed using long short term memory (LSTM) to predict the share price of DLF group. Moreover, for the experimental purpose the data of DLF group has been taken from yahoo financial services in the time duration of 2008 to 2018 and the recurrent neural network (RNN) model has been trained using data ranging from 2008 to 2017. This RNN based model has been tested on the data of year 2018. For the performance comparison purpose, other linear regression algorithms i.e. k-nn regression, lasso regression, XGboost etc has been executed and the proposed algorithm outperforms with 2.6% root mean square error.",2019,2019 IEEE 9th International Conference on Advanced Computing (IACC)
Analysis of Energy Consumption Influencing Factors in China Based on the Lasso Method,"éšç€ç»æµŽå‘å±•çš„åŠ å¿«å’Œèµ„æºéœ€æ±‚é‡çš„åŠ å¤§ï¼Œèƒ½æºæ¶ˆè´¹å‘ˆçŽ°å‡ºè¿žå¹´æ”€å‡çš„æ€åŠ¿ï¼Œèƒ½æºæ¶ˆè´¹å½±å“å› ç´ çš„ç ”ç©¶åŠèƒ½æºæ¶ˆè´¹éœ€æ±‚çš„åˆç†é¢„æµ‹ï¼Œå¯¹ä¿è¯æˆ‘å›½ç»æµŽå¹³ç¨³æŒç»­å¥åº·å‘å±•æ˜¯ååˆ†å¿…è¦çš„ã€‚ç›®å‰å­¦è€…ä»¬åˆ†åˆ«ç”¨è¿‡ç®€å•çº¿æ€§å›žå½’æ³•ã€ä¸»æˆåˆ†å›žå½’æ³•åŠå²­å›žå½’æ³•å¯¹æˆ‘å›½èƒ½æºæ¶ˆè´¹å½±å“å› ç´ è¿›è¡Œåˆ†æžï¼Œä½†è¿™äº›ç ”ç©¶å¾—åˆ°çš„æ¨¡åž‹å¯èƒ½å¤ªè¿‡ç²¾ç®€è€Œæœªèƒ½è¾ƒä¸ºå…¨é¢åœ°æ‰¾å‡ºèƒ½æºæ¶ˆè´¹çš„ä¸»è¦å½±å“å› ç´ ã€‚è€Œæœ¬æ–‡ä¾æ®2000å¹´~2012å¹´æˆ‘å›½èƒ½æºæ¶ˆè´¹æ€»é‡çš„ç›¸å…³æ•°æ®ï¼Œé’ˆå¯¹å˜é‡åå¤šï¼Œè§‚æµ‹æ•°æ®å°‘çš„ç‰¹ç‚¹é€‰ç”¨äº†Lassoæ–¹æ³•å¯¹æˆ‘å›½èƒ½æºæ¶ˆè´¹å½±å“å› ç´ å»ºç«‹äº†å›žå½’æ¨¡åž‹ï¼Œå¾—åˆ°äº†å½±å“æˆ‘å›½èƒ½æºæ¶ˆè´¹çš„ä¸»è¦å› ç´ æœ‰ç»æµŽå¢žé•¿å› ç´ ã€äººå£å¢žé•¿å› ç´ ã€äº§ä¸šç»“æž„å› ç´ ã€æŠ€æœ¯è¿›æ­¥å› ç´ ã€èƒ½æºåˆ©ç”¨æ•ˆçŽ‡å› ç´ ä»¥åŠèƒ½æºä»·æ ¼å› ç´ ï¼Œå› æ­¤ï¼Œæˆ‘ä»¬å¯ä¸»è¦ä»Žè¿™äº›å› ç´ å…¥æ‰‹ï¼Œå¯¹èƒ½æºæ¶ˆè´¹åŠ ä»¥ç®¡ç†å’ŒæŽ§åˆ¶ã€‚åŒæ—¶æˆ‘ä»¬è¿˜ç”¨é€æ­¥å›žå½’æ³•å’Œå²­å›žå½’æ³•åˆ†åˆ«å»ºç«‹äº†å›žå½’æ¨¡åž‹ï¼Œå¹¶å°†Lassoæ–¹æ³•å¾—åˆ°çš„ç»“æžœä¸Žå…¶è¿›è¡Œæ¯”è¾ƒï¼Œç»“æžœè¡¨æ˜ŽLassoæ–¹æ³•åœ¨èƒ½æºæ¶ˆè´¹å½±å“å› ç´ çš„é€‰æ‹©æ–¹é¢ï¼Œæ¯”å…¶ä»–ä¸¤ç§æ–¹æ³•æ›´ä¸ºå…¨é¢åœ°æ‰¾å‡ºèƒ½æºæ¶ˆè´¹çš„ä¸»è¦å½±å“å› ç´ ï¼Œåœ¨å¯¹2013å¹´åŠ2014å¹´èƒ½æºæ¶ˆè´¹æ€»é‡é¢„æµ‹æ–¹é¢ï¼ŒLassoæ–¹æ³•æ¯”å…¶ä»–ä¸¤ç§æ–¹æ³•æ›´ä¸ºç²¾ç¡®ã€‚ With the acceleration of economic development and the increasing demand for resources, energy consumption shows a rising trend in recent years. To ensure the stable, sustainable and healthy development of Chinaâ€™s economy, it is necessary to study on consumption factors and to forecast energy consumption demand reasonably. As so far, scholars have used simple linear regression, principal component regression and ridge regression method for analyzing Chinaâ€™s energy consumption factors, but models achieved from these studies may be too lean to find more comprehensive energy consumption factors. While according to the related data of domestic energy consumption during 2000-2012, this paper chooses a new methodâ€”Lasso method to make regression model for domestic energy consumption, and then we get the main energy consumption effecting factors: economic development, demographic factor, industrial structure, technological progress, energy consumption efficiency and energy price factor, so we can control energy consumption through these main factors. Additionally, we use stepwise regression and ridge regression to make regression models, the results got from the Lasso, stepwise regression and ridge regression are compared, the study shows the Lasso method is better than the other methods in terms of variable selection, because it could find more comprehensive energy consumption factors; for predictions of 2013 and 2014, Lasso method is more accurate than the other two methods.",2017,
Performance-Based Prediction of Chronic Kidney Disease Using Machine Learning for High-Risk Cardiovascular Disease Patients,"People at high-risk of cardiovascular disease are most likely vulnerable to chronic kidney diseases, and historical medical records can help avert complicated kidney problems. In this paper, 12 supervised machine learning algorithms were used to analyses a retrospective electronic medical data on chronic kidney disease. The study targeted 544 outpatients although 48 failed to meet the inclusion criteria and some other 21 cases had missing values and were excluded from the study. The profiling and the preliminaries result established that 88.5% of the cases were labeled as advance CKD while 11.5% were labelled as early-stage CKD cases. The classification task and the subsequent evaluation of the models were based on the correct classification of the two groups. Of the evaluated algorithms, decision tree boosted decision tree, and CN2 rule induction was the least accurate ones. However, logistic regression (Ridge and Lasso), neural network (logistic and stochastic gradient descent), and support vector machine (Radial Basis Function and Polynomial) had very high accuracies and efficiency. With an efficiency of 93.4% and a classification accuracy of 91.7%, Polynomial Support Vector Machine algorithm was the most efficient and accurate. The model suggested 253 2-dimensional combinations of factors with a history of vascular diseases and smoking as the most influential factors. The other combinations can provide information that can be used to predict or detect chronic kidney disease based on historical records. Future research prospects should consider using discretized Glomerular Filtration Rate to ensure that the classification integrates the five stages of the CKD.",2020,
Meta_LASH Tree: Bagging at Meta Level Using LASSO Regression Hoeffding Tree for Streaming Data,"Building predictive model for streaming data is a major challenge as it involves high speed and huge amount of data stream which is impossible to store and process the entire data. Since the distribution of streaming data changes over time, the traditional static model is not suitable for streaming data. Nowadays most of the streaming data solutions have been centered around ensembles, which integrates predictive responses from multiple homogeneous or heterogeneous base learning algorithms. In this paper a novel, memory efficient and practically useful Ensemble Bagging at Meta level Framework denoted as Meta_LASH Tree is proposed which comprises of two phases. In the first phase a memory efficient base learner named as LASSO Regression Hoeffding Tree (LASH Tree) is constructed, which incorporates Hoeffding Tree and LASSO Regression, that produces better predictions and better insights than using both the models separately. This hybrid model is highly interpretable and can have an insights of both linear and non linear relationship of the data. In the second phase, the predictive responses from the previously constructed LASH Tree are collected using Ensemble Bagging approach, and the dominant base learner is selected by the Meta Learner. The proposed frame work is designed in such a way to reduce the memory usage and overfitting issue of the existing algorithms. It is also designed to enhance the prediction accuracy.",2019,2019 3rd International Conference on Trends in Electronics and Informatics (ICOEI)
Linear Convergence of the Alternating Direction Method of Multipliers for a Class of Convex Optimization Problems,"The numerical success of the alternating direction method of multipliers (ADMM) inspires much attention in analyzing its theoretical convergence rate. While there are several results on the iterative complexity results implying sublinear convergence rate for the general case, there are only a few results for the special cases such as linear programming, quadratic programming, and nonlinear programming with strongly convex functions. In this paper, we consider the convergence rate of ADMM when applying to the convex optimization problems that the subdifferentials of the underlying functions are piecewise linear multifunctions, including LASSO, a well-known regression model in statistics, as a special case. We prove that due to its inherent polyhedral structure, a recent global error bound holds for this class of problems. Based on this error bound, we derive the linear rate of convergence for ADMM. We also consider the proximal based ADMM and derive its linear convergence rate.",2016,SIAM J. Numerical Analysis
Which environmental factors are associated with performance when controlling for capacity?,"OBJECTIVE
To determine which environmental factors are associated with performance when controlling for capacity, using the International Classification of Functioning, Disability and Health (ICF).


METHODS
A psychometric study using a sample of 296 persons with musculoskeletal health conditions as a case in point. The following steps were carried out: (i) Rasch analyses created 2 interval measurement scales, capacity and performance, based on 22 Activities and Participation ICF categories that had been rated as capacity and performance. Capacity and performance scores, ranging from 0 (low level) to 100 (high level) were calculated; (ii) group lasso regression was used to identify the environmental factors associated with a person's performance when controlling for capacity. Gender, age and health condition were forced to remain in the model.


RESULTS
A capacity scale based on 16 ICF categories (rated as capacity) and a performance scale based on 18 categories (rated as performance) were created. Thirteen environmental factors ICF categories covering the physical, social, attitudinal and political environment were identified as highly associated with patient's performance.


CONCLUSION
Using an exclusively statistical approach this study identified environmental factors associated with a person's performance.",2014,Journal of rehabilitation medicine
Classifying Big Data Over Networks Via The Logistic Network Lasso,We apply network Lasso to solve binary classification and clustering problems on network structured data. In particular we generalize ordinary logistic regression to non-Euclidean data defined over a complex network structure. The resulting logistic network Lasso classifier amounts to solving a convex optimization problem. A scalable classification algorithm is obtained by applying the alternating direction methods of multipliers.,2018,"2018 52nd Asilomar Conference on Signals, Systems, and Computers"
Heterogeneous feature selection by group lasso with logistic regression,"The selection of groups of discriminative features is critical for image understanding since the irrelevant features could deteriorate the performance of image understanding. This paper formulates the selection of groups of discriminative features by the extension of group lasso with logistic regression for high-dimensional feature setting, we call it as the heterogeneous feature selection by Group Lasso with Logistic Regression (GLLR). GLLR encodes a sparse grouping prior to seek after a more interpretable model for feature selection and can identify most of discriminative groups of homogeneous features. The utilization of GLLR for image annotation shows the proposed GLLR achieves a better performance.",2010,
GRB10 and E2F3 as Diagnostic Markers of Osteoarthritis and Their Correlation with Immune Infiltration,"This study aimed to find potential diagnostic markers for osteoarthritis (OA) and analyze the role of immune cells infiltration in this pathology. We used OA datasets from the Gene Expression Omnibus database. First, R software was used to identify differentially expressed genes (DEGs) and perform functional correlation analysis. Then least absolute shrinkage and selection operator (LASSO) logistic regression and support vector machine-recursive feature elimination algorithms were used to screen and verify the diagnostic markers of OA. Finally, CIBERSORT was used to evaluate the infiltration of immune cells in OA tissues, and the correlation between diagnostic markers and infiltrating immune cells was analyzed. A total of 458 DEGs were screened in this study. GRB10 and E2F3 (AUC = 0.962) were identified as diagnostic markers of OA. Immune cell infiltration analysis found that resting mast cells, T regulatory cells, CD4 memory resting T cells, activated NK cells, and eosinophils may be involved in the OA process. In addition, GRB10 was correlated with NK resting cells, naive CD4 + T cells, and M1 macrophages, while E2F3 was correlated with resting mast cells. In conclusion, GRB10 and E2F3 can be used as diagnostic markers of osteoarthritis, and immune cell infiltration plays an important role in the occurrence and progression of OA.",2020,Diagnostics
Development of a Predictive Model of Difficult Hemostasis following Endobronchial Biopsy in Lung Cancer Patients,"Endobronchial biopsy (EBB)-induced bleeding is fairly common; however, it can be potentially life-threatening due to difficult hemostasis following EBB. The aim of this study was to develop a predictive model of difficult hemostasis post-EBB. A total of 620 consecutive patients with primary lung cancer who had undergone EBB between 2014 and 2018 in a large tertiary hospital were enrolled in this retrospective single-center cohort study. Patients were classified into the difficult hemostasis group and the nondifficult hemostasis group according to hemostatic measures used following EBB. The LASSO regression method was used to select predictors and multivariate logistic regression was applied to develop the predictive model. The area under the curve (AUC) of the model was calculated. Bootstrapping method was applied for internal validation. Calibration curve analysis and decision curve analysis (DCA) were also performed. A nomogram was constructed to display the model. The incidence of difficult hemostasis post-EBB was 11.9% (74/620). Eight variables were selected by the LASSO regression analysis and seven (histological type of cancer, lesion location, neutrophil percentage, activated partial thromboplastin time, low density lipoprotein cholesterol, apolipoprotein-E, and pulmonary infection) of them were finally included in the predictive model. The AUC of the model was 0.822 (95% CI, 0.777-0.868), and it was 0.808 (95% CI, 0.761-0.856) in the internal validation. The predictive model was well calibrated and DCA indicated its potential clinical usefulness, which suggests that the model has great potential to predict lung cancer patients with a more difficult post-EBB hemostasis.",2019,BioMed Research International
Un sÃ©lecteur de Dantzig pour l'apprentissage par diffÃ©rences temporelles,"En apprentissage par renforcement, LSTD est l'un des algorithmes d'approximation de la fonction de valeur les plus populaires. Lorsqu'il y a plus de fonctions de base que d'exemples, un probleme se pose, qui peut etre traite en combinant LSTD avec une forme de regularisation. En particulier, les methodes de regularisation 1 tendent a selectionner les fonctions de base (en favorisant la parcimonie des solutions) et sont donc particulierement adaptees pour les problemes de grande dimension. Toutefois, LSTD n'est pas un simple algorithme de regression ; il resout un probleme de point fixe, l'integration d'une regularisation 1 n'est pas evidente et peut entrainer certains inconvenients (comme l'hypothese de P-matrice pour LASSO-TD). Cette contribution introduit un nouvel algorithme qui integre LSTD au selecteur de Dantzig, generalisant ce dernier a l'apprentissage par differences temporelles. En particulier, nous etudions les performances de l'algorithme propose ainsi que son lien avec les approches de l'etat de l'art, notamment la facon dont il surmonte certains inconvenients des solutions existantes.",2012,
Deep Learning-based Radiomic Features for Improving Neoadjuvant Chemoradiation Response Prediction in Locally Advanced Rectal Cancer,"PURPOSE
Radiomic features achieve promising results in cancer diagnosis and treatment response prediction. The goal of this study is to compare the handcrafted, or explicitly designed, radiomic features and deep learning (DL)-based radiomic features extracted from pre-treatment diffusion-weighted magnetic resonance images (DWIs) for predicting neoadjuvant chemoradiation treatment (nCRT) response in patients with locally advanced rectal cancer (LARC).


MATERIALS AND METHODS
43 patients receiving nCRT were included. All patients underwent DWIs before nCRT and total mesorectal excision surgery after nCRT. Gross tumor volume (GTV) contours were drawn by an experienced radiation oncologist on DWIs. The patient-cohort was split into the responder group (n=22) and the non-responder group (n=21) based on the post-nCRT response assessed by postoperative pathology, MRI or colonoscopy. Handcrafted and DL-based features were extracted from the apparent diffusion coefficient (ADC) map of the DWI using conventional computer-aided diagnosis methods and a pre-trained convolution neural network, respectively. Least absolute shrinkage and selection operator (LASSO)-logistic regression models were constructed using extracted features for predicting treatment response. The model performance was evaluated with repeated 20 times stratified 4-fold cross-validation using receiver operating characteristic (ROC) curves and compared using the corrected resampled t-test.


RESULTS
The model built with handcrafted features achieved the mean area under the ROC curve (AUC) of 0.64, while the one built with DL-based features yielded the mean AUC of 0.73. The corrected resampled t-test on AUC showed P-value < 0.05.


CONCLUSION
DL-based features extracted from pre-treatment DWIs achieved significantly better classification performance compared with handcrafted features for predicting nCRT response in LARC patients.",2020,Physics in medicine and biology
TRB 6 / 18 Differential Privacy for Regularised Linear Regression,"Recent attacks on machine learning models such as membership inference attacks increase the concern for privacy. Linear regression is such an essential statistical machine learning model at risk. For a given dataset, linear regression determines the parameters of the linear equation connecting the predictor variables to the response variable. As such linear regression yields a set of unstable and overfitted parameters. Regularisation terms are added to the loss function of linear regression in order to avoid overfitting. LASSO, ridge, and elastic net are three variants of regularised linear regression. We present an -differentially private functional mechanism for the aforementioned variants of regularised linear regression. We empirically and comparatively analyze its effectiveness. A functional mechanism achieves differential privacy for linear regression by adding noise to the loss function. We empirically show that an -differentially private functional mechanism causes more error than the non-private linear regression models whereas their performances are comparable. We also discuss caveats in the functional mechanism, such as non-convexity of the noisy loss function, which causes instability in the results of differentially private linear regression models. This discussion puts forth the need of designing a differentially private mechanism that produces a noisy loss function that is convex.",2018,
Sparsity Oracle Inequalities for Lasso and Dantzig Selector in High-Dimensional Nonparametric Regression,"Regularity conditions, such as the incoherence condition, restricted isometry property, compatibility condition and restricted eigenvalue assumption, play a pivotal role in high-dimensional regression and compressed sensing. Under these conditions, some interesting results for the Lasso and Dantzig selectors are derived. In this paper, we propose a modification of the compatibility condition, which is called modified compatibility condition. We show the oracle inequalities under the new condition and the methods which avoid using the sparsity condition. As a comparison with the results by Bickel et al. (2009) in high-dimensional nonparametric regression, more precise oracle inequalities for the prediction risk and bounds on the estimation loss are derived when the number of variables can be much larger than the sample size.",2013,International journal of applied mathematics and statistics
EEG UNDER ANESTHESIA A general method for calculation of depth of anesthesia,"This paper investigated the problem of automatic depth of anesthesia (DOA) estimation from electroencephalogram (EEG) recordings. Compared with the Bispectral Index (BIS), time-frequency domain signal processing technique and nonlinear dynamical analysis were combined for DOA assessment, multiple features were extracted from EEG, Lasso and Logistic regression were used to classify and calculate the index of DOA and evaluate its relationship with EEG features. In emulation and clinical practice, the index of DOA is very close to BIS. This method can enhance existing monitoring devices and work as a general method to find effective features for calculation of DOA. Â© 2011 Published by Elsevier Ltd. Selection and/or peer-review under responsibility of ICESB 2011",2011,
Bayesian Sparsity-Path-Analysis of Genetic Association Signal using Generalized t Priors,"We explore the use of generalized t priors on regression coefficients to help understand the nature of association signal within Â“hit regionsÂ” of genome-wide association studies. The particular generalized t distribution we adopt is a Student distribution on the absolute value of its argument. For low degrees of freedom, we show that the generalized t exhibits Â“sparsity-priorÂ” properties with some attractive features over other common forms of sparse priors and includes the well known double-exponential distribution as the degrees of freedom tends to infinity. We pay particular attention to graphical representations of posterior statistics obtained from sparsity-path-analysis (SPA) where we sweep over the setting of the scale (shrinkage/precision) parameter in the prior to explore the space of posterior models obtained over a range of complexities, from very sparse models with all coefficient distributions heavily concentrated around zero, to models with diffuse priors and coefficients distributed around their maximum likelihood estimates. The SPA plots are akin to LASSO plots of maximum a posteriori (MAP) estimates but they characterise the complete marginal posterior distributions of the coefficients plotted as a function of the precision of the prior. Generating posterior distributions over a range of prior precisions is computationally challenging but naturally amenable to sequential Monte Carlo (SMC) algorithms indexed on the scale parameter. We show how SMC simulation on graphic-processing-units (GPUs) provides very efficient inference for SPA. We also present a scale-mixture representation of the generalized t prior that leads to an expectation-maximization (EM) algorithm to obtain MAP estimates should only these be required.",2012,Statistical Applications in Genetics and Molecular Biology
A model-free approach for detecting interactions in genetic association studies,"Over the past few decades, genome-wide association studies analyzed by efficient statistical procedures have successfully identified single-nucleotide polymorphisms (SNPs) that are associated with complex traits or human diseases. However, due to the overwhelming number of SNPs, most approaches have focused on additive genetic model without genome-wide SNP-SNP interactions. In this study, we propose an efficient statistical procedure in a genetic model-free framework for detecting SNPs exhibiting main genetic effects as well as epistatic interactions. Specifically, the association between phenotype and genotype is characterized by an unknown function to be estimated using nonparametric techniques, and a two-stage non-parametric independence screening procedure is proposed to sequentially identify potentially important main genetic effects and interactions. Finally, the subset of genetic predictors implied by two-stage non-parametric independence screening is analyzed by penalized regressions such as LASSO, and a final model is identified. In this framework, specific genetic model is not assumed and interactions are not only among marginally important SNPs. Therefore, SNPs that are involved in genetic regulatory networks but missed by previous studies are expected to be recognized. In simulation studies, we show that the procedure is computationally efficient and has an outstanding finite sample performance in selecting potential SNPs as well as SNP-SNP interactions. A real data analysis further indicates the importance of epistatic interactions in explaining body mass index.",2014,Briefings in bioinformatics
Large-scale sparse regression models under weak assumptions,"Author(s): Raskutti, Garvesh | Advisor(s): Yu, Bin; Wainwright, Martin J | Abstract: Many modern problems in science and other areas involve extraction of useful information from so-called 'big data.' However, many classical statistical techniques are not equipped to meet with the challenges posed by big data problems. Furthermore, existing statistical methods often result in intractable algorithms. Consequently the last $15-20$ years has seen a flurry of research on adapting existing methods and developing new methods that overcome some of the statistical and computational challenges posed by problems involving big data. Regression is one of the oldest statistical techniques. For many modern regression problems involving big datasets, the number of predictors or covariates $\pdim$ is large compared the number of samples $n$, causing significant computational and statistical challenges. To overcome these challenges, many researchers have proposed imposing sparsity on the vector of regression co-efficients $\beta \in \mathbb{R}^{\pdim}$. Furthermore, researchers have proposed using $\ell_1$-based convex penalties for estimating $\beta$ under the sparsity assumption since they yield implementable algorithms with desirable performance guarantees. While there was already an established body of work on developing procedures for sparse regression models, most existing results rely on very restrictive model assumptions. These assumptions are often not satisfied for many scientific problems. In this thesis, we relax $3$ restrictive model assumptions that are commonly imposed in the literature for estimating sparse regression models. The $3$ assumptions are: (1) Strict sparsity, that is the vector of regression co-efficients $\beta$ contains only a small number of non-zeros; (2) The covariates or predictors are independent; (3) Response depends linearly on covariates. Given that these $3$ model assumptions are often not satisfied for many practical settings, it is important to understand whether existing theoretical results exhibit robustness to these assumptions. In Chapter 2, we impose a weaker notion of sparsity known as $\ell_q$-ball sparsity on $\beta$ which ensures the vector of regression co-efficients lies in an $\ell_q$ ball, but need not have any non-zeros. We prove that under the weaker $\ell_q$-ball sparsity assumption, it is possible to develop estimators with desirable mean-squared error behavior, even in the regime where $\pdim \gg n$.The weakest known condition under which the Lasso achieves optimal mean-squared error rate is the restricted eigenvalue condition~\cite{vandeGeer07,BicRitTsy08, Negahban09}. Existing results prove that in cases when the covariates are independent, the restricted eigenvalue condition is satisfied. However, the setting when predictors or covariates are correlated are also of interest and there was considerably less work dealing with this case. In Chapter 3, we prove that the restricted eigenvalue condition is satisfied for various correlated Gaussian designs, including time series models, spiked covariance models and others.Finally, in Chapter 4 we analyze sparse additive models, a non-parametric analog of sparse linear models, in which each component function lies in an ellipsoid or more formally a Reproducing kernel Hilbert space $\Hil$. Hence we weaken the assumption that our response depends on the covariate via a linear function. A new $\ell_1$-based polynomial-time method is developed and we prove that this method has desirable mean-squared error performance, even when $\pdim \gg n$. Furthermore, we prove lower bounds on the mean-squared error for estimating sparse additive models that match the upper bounds for our method. Hence our algorithm is optimal in terms of mean-squared error rate.",2012,
Genotype-phenotype association study via new multi-task learning model,"Research on the associations between genetic variations and imaging phenotypes is developing with the advance in high-throughput genotype and brain image techniques. Regression analysis of single nucleotide polymorphisms (SNPs) and imaging measures as quantitative traits (QTs) has been proposed to identify the quantitative trait loci (QTL) via multi-task learning models. Recent studies consider the interlinked structures within SNPs and imaging QTs through group lasso, e.g. â„“2, 1-norm, leading to better predictive results and insights of SNPs. However, group sparsity is not enough for representing the correlation between multiple tasks and â„“2, 1-norm regularization is not robust either. In this paper, we propose a new multi-task learning model to analyze the associations between SNPs and QTs. We suppose that low-rank structure is also beneficial to uncover the correlation between genetic variations and imaging phenotypes. Finally, we conduct regression analysis of SNPs and QTs. Experimental results show that our model is more accurate in prediction than compared methods and presents new insights of SNPs.",2018,Pacific Symposium on Biocomputing. Pacific Symposium on Biocomputing
Regularising the Factor Zoo with OWL : A Correlation-Robust Machine Learning Approach,"Cochrane (2011) points out that the burgeoning characteristic-related â€factor zooâ€ to explain the average returns in equity market are in disarray. This paper introduces a newly developed machine learning tool, ordered and weighted L1 norm regularisation (OWL) to â€regulariseâ€ this chaotic â€factor zooâ€. OWL permits high correlations among explanatory variables, which is novel in the finance literature and of great importance. Factor correlation prevails in high dimensionality (factor zoo) and distorts standard estimators such as Fama-MacBeth (FM) regression, LASSO, etc. I show OWL estimator is consistent with finite factors and derive the convergence rate with infinite factors. I also derive conditions that OWL groups highly correlated variables, while shrinks off useless/redundant variables simultaneously. Monte Carlo experiments show OWL outperforms LASSO, adaptive LASSO and Elastic Net (EN) in various settings, particularly when factors are highly correlated. Empirical evidence suggests that liquidity related factors are primary to drive asset prices. Following Freyberger et al. (2017), out-of-sample Sharpe ratio of hedge portfolios, formed using OWL selected factors as predictors are considerably larger than that of LASSO, EN and FM.",2019,
Group Fused Lasso,"We introduce the Group Total Variation (GTV) regularizer, a modification of Total Variation that uses the l2,1 norm instead of the l1 one to deal with multidimensional features. When used as the only regularizer, GTV can be applied jointly with iterative convex optimization algorithms such as FISTA. This requires to compute its proximal operator which we derive using a dual formulation. GTV can also be combined with a Group Lasso (GL) regularizer, leading to what we call Group Fused Lasso (GFL) whose proximal operator can now be computed combining the GTV and GL proximals through Dykstra algorithm. We will illustrate how to apply GFL in strongly structured but ill-posed regression problems as well as the use of GTV to denoise colour images.",2013,
A Two-Stage Penalized Least Squares Method for Constructing Large Systems of Structural Equations,"We propose a two-stage penalized least squares method to build large systems of structural equations based on the instrumental variables view of the classical two-stage least squares method. We show that, with large numbers of endogenous and exogenous variables, the system can be constructed via consistent estimation of a set of conditional expectations at the first stage, and consistent selection of regulatory effects at the second stage. While the consistent estimation at the first stage can be obtained via the ridge regression, the adaptive lasso is employed at the second stage to achieve the consistent selection. The resultant estimates of regulatory effects enjoy the oracle properties. This method is computationally fast and allows for parallel implementation. We demonstrate its effectiveness via simulation studies and real data analysis.",2018,J. Mach. Learn. Res.
Path consistent model selection in additive risk model via Lasso.,"As a flexible alternative to the Cox model, the additive risk model assumes that the hazard function is the sum of the baseline hazard and a regression function of covariates. For right censored survival data when variable selection is needed along with model estimation, we propose a path consistent model selector using a modified Lasso approach, under the additive risk model assumption. We show that the proposed estimator possesses the oracle variable selection and estimation property. Applications of the proposed approach to three right censored survival data sets show that the proposed modified Lasso yields parsimonious models with satisfactory estimation and prediction results.",2007,Statistics in medicine
The prediction of live weight of hair goats through penalized regression methods: LASSO and adaptive LASSO,"The least absolute selection and shrinkage operatorÂ (LASSO) and adaptive LASSO methods have become a popular model in the last decade, especially for data with a multicollinearity problem. This study was conducted to estimate the live weightÂ (LW) of Hair goats from biometric measurements and to select variables in order to reduce the model complexity by using penalized regression methods: LASSO and adaptive LASSO for Î³ = 0.5 and Î³ = 1 . The data were obtained from 132Â adult goats in Honaz district of Denizli province. Age, gender, forehead width, ear length, head length, chest width, rump height, withers height, back height, chest depth, chest girth, and body length were used as explanatory variables. The adjusted coefficient of determinationÂ ( R adj 2 ), root mean square errorÂ (RMSE), Akaike's information criterionÂ (AIC), Schwarz Bayesian criterionÂ (SBC), and average square errorÂ (ASE) were used in order to compare the effectiveness of the methods. It was concluded that adaptive LASSO ( Î³ = 1 ) estimated the LW with the highest accuracy for both male ( R adj 2 = 0.9048 ; RMSEâ€¯ = â€¯3.6250; AICâ€¯ = â€¯79.2974; SBCâ€¯ = â€¯65.2633; ASEâ€¯ = â€¯7.8843) and female ( R adj 2 = 0.7668 ; RMSEâ€¯ = â€¯4.4069; AICâ€¯ = â€¯392.5405; SBCâ€¯ = â€¯308.9888; ASEâ€¯ = â€¯18.2193) Hair goats when all the criteria were considered.",2018,Archives Animal Breeding
Compressed Sparse Linear Regression,"High-dimensional sparse linear regression is a basic problem in machine learning and statistics. Consider a linear model $y = X\theta^\star + w$, where $y \in \mathbb{R}^n$ is the vector of observations, $X \in \mathbb{R}^{n \times d}$ is the covariate matrix and $w \in \mathbb{R}^n$ is an unknown noise vector. In many applications, the linear regression model is high-dimensional in nature, meaning that the number of observations $n$ may be substantially smaller than the number of covariates $d$. In these cases, it is common to assume that $\theta^\star$ is sparse, and the goal in sparse linear regression is to estimate this sparse $\theta^\star$, given $(X,y)$. 
In this paper, we study a variant of the traditional sparse linear regression problem where each of the $n$ covariate vectors in $\mathbb{R}^d$ are individually projected by a random linear transformation to $\mathbb{R}^m$ with $m \ll d$. Such transformations are commonly applied in practice for computational savings in resources such as storage space, transmission bandwidth, and processing time. Our main result shows that one can estimate $\theta^\star$ with a low $\ell_2$-error, even with access to only these projected covariate vectors, under some mild assumptions on the problem instance. Our approach is based on solving a variant of the popular Lasso optimization problem. While the conditions (such as the restricted eigenvalue condition on $X$) for success of a Lasso formulation in estimating $\theta^\star$ are well-understood, we investigate conditions under which this variant of Lasso estimates $\theta^\star$. 
As a simple consequence, our approach also provides a new way for estimating $\theta^\star$ in the traditional sparse linear regression problem setting, which operates (even) under a weaker assumption on the design matrix than previously known, albeit achieving a weaker convergence bound.",2017,ArXiv
Joint testing and false discovery rate control in highâ€dimensional multivariate regression,"&NA; Multivariate regression with highâ€dimensional covariates has many applications in genomic and genetic research, in which some covariates are expected to be associated with multiple responses. This paper considers joint testing for regression coefficients over multiple responses and develops simultaneous testing methods with false discovery rate control. The test statistic is based on inverse regression and biasâ€corrected group lasso estimates of the regression coefficients and is shown to have an asymptotic chiâ€squared null distribution. A rowâ€wise multiple testing procedure is developed to identify the covariates associated with the responses. The procedure is shown to control the false discovery proportion and false discovery rate at a prespecified level asymptotically. Simulations demonstrate the gain in power, relative to entrywise testing, in detecting the covariates associated with the responses. The test is applied to an ovarian cancer dataset to identify the microRNA regulators that regulate protein expression.",2018,Biometrika
A Combined PLS and Negative Binomial Regression Model for Inferring Association Networks from Next-Generation Sequencing Count Data,"A major challenge of genomics data is to detect interactions displaying functional associations from large-scale observations. In this study, a new cPLS-algorithm combining partial least squares approach with negative binomial regression is suggested to reconstruct a genomic association network for high-dimensional next-generation sequencing count data. The suggested approach is applicable to the raw counts data, without requiring any further pre-processing steps. In the settings investigated, the cPLS-algorithm outperformed the two widely used comparative methods, graphical lasso, and weighted correlation network analysis. In addition, cPLS is able to estimate the full network for thousands of genes without major computational load. Finally, we demonstrate that cPLS is capable of finding biologically meaningful associations by analyzing an example data set from a previously published study to examine the molecular anatomy of the craniofacial development.",2018,IEEE/ACM Transactions on Computational Biology and Bioinformatics
Pitfalls in Prediction Modeling for Normal Tissue Toxicity in Radiation Therapy: An Illustration With the Individual Radiation Sensitivity and Mammary Carcinoma Risk Factor Investigation Cohorts.,"PURPOSE
To identify the main causes underlying the failure of prediction models for radiation therapy toxicity to replicate.


METHODS AND MATERIALS
Data were used from two German cohorts, Individual Radiation Sensitivity (ISE) (n=418) and Mammary Carcinoma Risk Factor Investigation (MARIE) (n=409), of breast cancer patients with similar characteristics and radiation therapy treatments. The toxicity endpoint chosen was telangiectasia. The LASSO (least absolute shrinkage and selection operator) logistic regression method was used to build a predictive model for a dichotomized endpoint (Radiation Therapy Oncology Group/European Organization for the Research and Treatment of Cancer score 0, 1, or â‰¥2). Internal areas under the receiver operating characteristic curve (inAUCs) were calculated by a naÃ¯ve approach whereby the training data (ISE) were also used for calculating the AUC. Cross-validation was also applied to calculate the AUC within the same cohort, a second type of inAUC. Internal AUCs from cross-validation were calculated within ISE and MARIE separately. Models trained on one dataset (ISE) were applied to a test dataset (MARIE) and AUCs calculated (exAUCs).


RESULTS
Internal AUCs from the naÃ¯ve approach were generally larger than inAUCs from cross-validation owing to overfitting the training data. Internal AUCs from cross-validation were also generally larger than the exAUCs, reflecting heterogeneity in the predictors between cohorts. The best models with largest inAUCs from cross-validation within both cohorts had a number of common predictors: hypertension, normalized total boost, and presence of estrogen receptors. Surprisingly, the effect (coefficient in the prediction model) of hypertension on telangiectasia incidence was positive in ISE and negative in MARIE. Other predictors were also not common between the 2 cohorts, illustrating that overcoming overfitting does not solve the problem of replication failure of prediction models completely.


CONCLUSIONS
Overfitting and cohort heterogeneity are the 2 main causes of replication failure of prediction models across cohorts. Cross-validation and similar techniques (eg, bootstrapping) cope with overfitting, but the development of validated predictive models for radiation therapy toxicity requires strategies that deal with cohort heterogeneity.",2016,"International journal of radiation oncology, biology, physics"
Development of a multivariable prediction model for identification of patients at risk for medication transfer errors at ICU discharge,"INTRODUCTION
Discharge from the intensive care unit (ICU) is a high-risk process, leading to numerous potentially harmful medication transfer errors (PH-MTE). PH-MTE could be prevented by medication reconciliation by ICU pharmacists, but resources are scarce, which renders the need for predicting which patients are at risk for PH-MTE. The aim of this study was to develop a prognostic multivariable model in patients discharged from the ICU to predict who is at increased risk for PH-MTE after ICU discharge, using predictors of PH-MTE that are readily available at the time of ICU discharge.


MATERIAL AND METHODS
Data for this study were derived from the Transfer ICU Medication reconciliation study, which included ICU patients and scored MTE at discharge of the ICU. The potential harm of every MTE was estimated with a validated score, where after MTE with potential for harm were indicated as PH-MTE. Predictors for PH-MTE at ICU discharge were identified using LASSO regression. The c statisticprovided a measure of the overall discriminative ability of the prediction model and the prediction model was internally validated by bootstrap resampling. Based on sensitivity and specificity, the cut-off point of the prediction model was determined.


RESULTS
The cohort contained 258 patients and six variables were identified as predictors for PH-MTE: length of ICU admission, number of home medications and patient taking one of the following medication groups at home: vitamin/mineral supplements, cardiovascular medication, psycholeptic/analeptic medication and medication for obstructive airway disease. The c of the final prediction model was 0.73 (95%CI 0.67-0.79) and decreased to 0.62 according to bootstrap resampling. At a cut-off score of two the prediction model yielded a sensitivity of 70% and a specificity of 61%.


CONCLUSIONS
A multivariable prediction model was developed to identify patients at risk for PH-MTE after ICU discharge. The model contains predictors that are available on the day of ICU discharge. Once external validation and evaluation of this model in daily practice has been performed, its incorporation into clinical practice could potentially allow institutions to identify patients at risk for PH-MTE after ICU discharge, on the day of ICU discharge, thus allowing for efficient, patient-specific allocation of clinical pharmacy services.


TRIAL REGISTRATION
Dutch trial register: NTR4159, 5 September 2013, retrospectively registered.",2019,PLoS ONE
Classification of spectral data using fused lasso logistic regression,"Abstract Spectral data contain powerful information that can be used to identify unknown compounds and their chemical structures. In this paper, we study fused lasso logistic regression (FLLR) to classify the spectral data into two groups. We show that the FLLR has a grouping property on regression coefficients, which simultaneously selects a group of highly correlated variables together. Both the sparsity and the grouping property of the FLLR provide great advantages in the analysis of the spectral data. In particular, it resolves the well-known peak misalignment problem of the spectral data by providing data dependent binning, and provides a better interpretable classifier than other l 1 -regularization methods. We also analyze the gas chromatography/mass spectrometry data to classify the origin of herbal medicines, and illustrate the advantages of the FLLR over other existing l 1 -regularized methods.",2015,Chemometrics and Intelligent Laboratory Systems
Privacy-preserving indoor localization via light transport analysis,"We propose a system for indoor localization using intensity-controllable LED light fixtures and light sensors mounted on the ceiling. While providing accurate location estimates, our approach preserves user privacy and is robust to ambient light conditions. We develop a LASSO algorithm and a localized ridge regression algorithm for locating a single object. In synthetic experiments, our localized ridge regression algorithm achieves an average localization error ranging from 0.24in to 1.39in, for different object sizes, in a 7Ã—12-foot room. The localized ridge regression algorithm also shows the ability to locate multiple objects in experiments with a real-world occupancy scenario.",2017,"2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
[The value of analysis of quantitative radiomics based on DTI in predicting astrocytoma IDH1 mutation].,"Objective: Non-invasive prediction of IDH1 mutations by establishing a quantitative radiographic model based on DTI-based whole-tumor texture analysis. Methods: Preoperative MRI images of patients with surgically confirmed astrocytoma were collected in the First Affiliated Hospital of Soochow University from February 2016 to June 2019, including T(1)WI, T(2)WI, DTI, and T(1)-contrast enhancement images.A total of 38 patients were included, consisting of 12 mutants and 26 wilds, 20 males and 18 females, the average age was (49Â±15) years old.The ROIs were drawn on each level of the T(2)WI image using MaZda software and copied to the ADC and FA maps to extract texture feature parameters. The LASSO regression was used to determine the best radiomics features, radiological scores were calculated, and binary Logistic regression was used to construct a prediction model, then the ROC curve was used to analyze the diagnostic efficiency and the calibration curve was used to evaluate model prediction performance. Results: The four most valuable radiomics features were determined by LASSO regression, and then the radiomics scores and Logistic regression models of each patient were established. The radiomics scores of the wild and mutant groups were 2.3Â±0.3 and 1.8Â±0.4. There were significant differences between the groups (P<0.05). The ROC curve analysis showed an AUC of 0.837 with sensitivity and specificity of 91.7% and 61.5%, respectively. The Logistic regression model had good predictive performance with AUC of 0.907, sensitivity and specificity of 91.7% and 84.6%. Conclusions: DTI-based whole tumor radiomics model is benefit for predicting astrocytoma IDH1 mutations.",2020,Zhonghua yi xue za zhi
