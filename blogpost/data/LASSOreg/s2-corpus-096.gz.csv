title,abstract,year,journal
Linear Regression with Limited Observation,"We consider the most common variants of linear regression, including Ridge, Lasso and Support-vector regression, in a setting where the learner is allowed to observe only a fixed number of attributes of each example at training time. We present simple and efficient algorithms for these problems: for Lasso and Ridge regression they need the same total number of attributes (up to constants) as do full-information algorithms, for reaching a certain accuracy. For Support-vector regression, we require exponentially less attributes compared to the state of the art. By that, we resolve an open problem recently posed by Cesa-Bianchi et al. (2010). 
 
Experiments show the theoretical bounds to be justified by superior performance compared to the state of the art.",2012,ArXiv
Pathway Detection Based on Hierarchical LASSO Regression Model,"Rapid and accurate identification of potentially interested pathways through the analysis of genome-wide expres- sion profiles remains an important challenge in bioinformatics. Most existing methods are based on hypothesis testing, such as GSEA. These methods mainly focus on individual pathways and rank them based on their individual strengths. However, biolog- ical pathways often work together to function. Therefore, it is important to consider their correlations in detection of pathways that are most closely related to the phenotypes. Considering this problem in the framework of variable selection, we propose a hierarchical LASSO regression (HLR) model to detect differen- tially expressed gene pathways, which automatically takes into account the correlation structure among the genes via regression. This approach is able to both select important gene pathways and remove unimportant genes within selected pathways. Both simulation and real data analysis show promising results.",2009,2009 2nd International Conference on Biomedical Engineering and Informatics
Reducing the Computational and Communication Complexity of a Distributed Optimization for Regularized Logistic Regression,"In this paper, we propose a new distributed optimization method that computes a Lasso estimator for logistic regression in the case when two parties have explanatory variables corresponding to distinct attributes. An existing protocol using the alternating direction method of multipliers (ADMM) for linear regression can be applied to logistic regression. However, this protocol needs an underlying iterative method such as the gradient method. We show that the proposed protocol using the generalized Bregman ADMM, which removes the necessity to use the underlying iterative method, requires lower computational and communication complexity.",2019,"2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)"
Prediction of Graduation Delay Based on Student Characterisitics and Performance,"A college studentâ€™s success depends on many factors including pre-university characteristics and university student support services. Student graduation rates are often used as an objective metric to measure institutional effectiveness. This work studies the impact of such factors on graduation rates, with a particular focus on delay in graduation. In this work, we used feature selection methods to identify a subset of the pre-institutional features with the highest discriminative power. In particular, Forward Selection with Linear Regression, Backward Elimination with Linear Regression, and Lasso Regression were applied. The feature sets were selected in a multivariate fashion. High school GPA, ACT scores, studentâ€™s high school, financial aid received, and first generation status were found to be important for predicting success. In order to predict delay in graduation, we trained predictive models using Support Vector Machines (SVMs), Gaussian Processes (GPs), and Deep Boltzmann Machines (DBMs) on real student data. The difference in performance among the models is negligible with respect to overall accuracies obtained. Further analysis showed that DBMs outperform SVMs in terms of precision and recall for individual",2017,
Classifying Lung Adenocarcinoma and Squamous Cell Carcinoma using RNA-Seq Data,"Background: Lung adenocarcinoma (LUAD) and lung squamous cell carcinoma (LUSC) are two primary subtypes of non-small cell lung carcinoma (NSCLC). Currently, the most widely used method to discriminate between LUAD and LUSC is hematoxylin-eosin (HE) staining. However, this method sometimes is unable to make the precise diagnosis on LUAD or LUSC. More accurate diagnostic approaches are highly desired. Methods: We propose to use gene expression profile to discriminate NSCLC patientâ€™s subtype. We leveraged RNA-Seq data from The Cancer Genome Atlas (TCGA) and randomly split the data into training and testing subsets. To construct classifiers based on the training data, we considered three methods: logistic regression on principal components (PCR), logistic regression with LASSO shrinkage (LASSO), and kth nearest neighbors (KNN). Performances of classifiers were evaluated and compared based on the testing data. Results: All gene expression-based classifiers show high accuracy in discriminating LUSC and LUAD. The classifier obtained by LASSO has the smallest overall misclassification rate of 3.42% (95% CI: 3.25%-3.60%) when using 0.5 as the cutoff value for the predicted probability of belonging to a subtype, followed by classifiers obtained by PCR (4.36%, 95% CI: 4.23%4.49%) and KNN (8.70%, 95% CI: 8.57%-8.83%). The LASSO classifier also has the highest average area under the receiver operating characteristic curve (AUC) value of 0.993, compared to PCR (0.987) and KNN (0.965). Conclusions: Our results suggest that mRNA expressions are highly informative for classifying NSCLC subtypes and may potentially be used to assist clinical diagnosis.",2017,
Serological and spatial analysis of alphavirus and flavivirus prevalence and risk factors in a rural community in western Kenya,"Alphaviruses, such as chikungunya virus, and flaviviruses, such as dengue virus, are (re)-emerging arboviruses that are endemic in tropical environments. In Africa, arbovirus infections are often undiagnosed and unreported, with febrile illnesses often assumed to be malaria. This cross-sectional study aimed to characterize the seroprevalence of alphaviruses and flaviviruses among children (ages 5-14, n = 250) and adults (ages 15 â‰¥ 75, n = 250) in western Kenya. Risk factors for seropositivity were explored using Lasso regression. Overall, 67% of participants showed alphavirus seropositivity (CI95 63%-70%), and 1.6% of participants showed flavivirus seropositivity (CI95 0.7%-3%). Children aged 10-14 were more likely to be seropositive to an alphavirus than adults (p < 0.001), suggesting a recent transmission period. Alphavirus and flavivirus seropositivity was detected in the youngest participants (age 5-9), providing evidence of inter-epidemic transmission. Demographic variables that were significantly different amongst those with previous infection versus those without infection included age, education level, and occupation. Behavioral and environmental variables significantly different amongst those in with previous infection to those without infection included taking animals for grazing, fishing, and recent village flooding. Experience of recent fever was also found to be a significant indicator of infection (p = 0.027). These results confirm alphavirus and flavivirus exposure in western Kenya, while illustrating significantly higher alphavirus transmission compared to previous studies.",2017,PLoS Neglected Tropical Diseases
On the variability of regression shrinkage methods for clinical prediction models: simulation study on predictive performance,"When developing risk prediction models, shrinkage methods are recommended, especially when the sample size is limited. Several earlier studies have shown that the shrinkage of model coefficients can reduce overfitting of the prediction model and subsequently result in better predictive performance on average. In this simulation study, we aimed to investigate the variability of regression shrinkage on predictive performance for a binary outcome, with focus on the calibration slope. The slope indicates whether risk predictions are too extreme (slope 1). We investigated the following shrinkage methods in comparison to standard maximum likelihood estimation: uniform shrinkage (likelihood-based and bootstrap-based), ridge regression, penalized maximum likelihood, LASSO regression, adaptive LASSO, non-negative garrote, and Firth's correction. There were three main findings. First, shrinkage improved calibration slopes on average. Second, the between-sample variability of calibration slopes was often increased relative to maximum likelihood. Among the shrinkage methods, the bootstrap-based uniform shrinkage worked well overall. In contrast to other shrinkage approaches, Firth's correction had only a small shrinkage effect but did so with low variability. Third, the correlation between the estimated shrinkage and the optimal shrinkage to remove overfitting was typically negative. Hence, although shrinkage improved predictions on average, it often worked poorly in individual datasets, in particular when shrinkage was most needed. The observed variability of shrinkage methods implies that these methods do not solve problems associated with small sample size or low number of events per variable.",2019,arXiv: Methodology
Statistical Methods in Mapping Complex Diseases,"Genome-wide association studies have become a standard tool for disease gene discovery over the past few years. These studies have successfully identified genetic variants attributed to complex diseases, such as cardiovascular disease, diabetes and cancer. Various statistical methods have been developed with the goal of improving power to find disease causing variants. The major focus of this dissertation is to develop statistical methods related to gene mapping studies with its application in real datasets to identify genetic markers associated with complex human diseases. In my first project, I developed a method to detect gene-gene interactions by incorporating linkage disequilibrium (LD) information provided by external datasets such as the International HapMap or the 1000 Genomes Projects. The next two projects in my dissertation are related to the analysis of secondary phenotypes in case-control genetic association studies. In these studies, a set of correlated secondary phenotypes that may share common genetic factors with disease status are often collected. However, due to unequal sampling probabilities between cases and controls, the standard regression approach for examination of these secondary phenotype can yield inflated type I error rates when the test SNPs are associated with the disease. To solve this issue, I propose a Gaussian copula approach to jointly model the disease status and the secondary phenotype. In my second project, I consider only one marker in the model and perform a test to access whether the marker is associated with the secondary phenotype in the Gaussian copula framework. In my third project, I extend the copula-based approach to include a large number of candidate SNPs in the model. I propose a variable selection approach to select markers which are associated with the secondary phenotype by applying a lasso penalty to the log-likelihood function. Degree Type Dissertation Degree Name Doctor of Philosophy (PhD) Graduate Group Epidemiology & Biostatistics This dissertation is available at ScholarlyCommons: http://repository.upenn.edu/edissertations/389 First Advisor Mingyao Li Second Advisor Hongzhe Li",2011,
GAP Safe screening rules for sparse multi-task and multi-class models,"High dimensional regression benefits from sparsity promoting regularizations. Screening rules leverage the known sparsity of the solution by ignoring some variables in the optimization, hence speeding up solvers. When the procedure is proven not to discard features wrongly the rules are said to be safe. In this paper we derive new safe rules for generalized linear models regularized with l1 and l1/ l2 norms. The rules are based on duality gap computations and spherical safe regions whose diameters converge to zero. This allows to discard safely more variables, in particular for low regularization parameters. The GAP Safe rule can cope with any iterative solver and we illustrate its performance on coordinate descent for multi-task Lasso, binary and multinomial logistic regression, demonstrating significant speed ups on all tested datasets with respect to previous safe rules.",2015,
Reliability analysis of slopes using UD-based response surface methods combined with LASSO,"Abstract The selection of experimental data points in response surface methods (RSMs) is still not based on precise guidelines or theory, and multicollinearityâ€”defined as the presence of linear correlations among the uncertain variablesâ€”hinders the estimation of the regression coefficients. As a result, a local optimal solution may be obtained. Uniform experimental design (UD) is a quasi Monte Carlo simulations (MCS) method, which has the advantage of coverage of all the experimental points, and is used to analyze the reliability of slopes in this framework. Least absolute shrinkage and selection operator (LASSO) is a regression method that is employed to estimate the regression coefficients. It involves the penalize the absolute size of the regression coefficients so that the coefficients can be estimated even if the variables are multicollinear. In this paper, a new approach is proposed in which a UD-based RSM is combined with LASSO. The response variable, namely, the factor of safety (Fs), is calculated using a 3D rigorous limit equilibrium method (3D RLEM). Two examples are used to demonstrate the validity and capability of the proposed approach. Notably, the probability of failure (Pf) obtained by the UD-based RSM is larger than that for other reliability methods without multicollinearity. Ignoring the influence of multicollinearity may lead to a nonconservative estimation of slope failure probability.",2018,Engineering Geology
Comparative evaluation of support vector machines for computer aided diagnosis of lung cancer in CT based on a multi-dimensional data set,"Lung cancer is one of the most common forms of cancer resulting in over a million deaths per year worldwide. In this paper, the usage of support vector machine (SVM) classification for lung cancer is investigated, presenting a systematic quantitative evaluation against Boosting, Decision trees, k-nearest neighbor, LASSO regressions, neural networks and random forests. A large database of 5984 regions of interest (ROIs) and 488 input features (including textural features, patient characteristics, and morphological features) were used to train the classifiers and evaluate for their performance. The evaluation for classifiers' performance was based on a tenfold cross validation framework, receiver operating characteristic curve (ROC), and Matthews correlation coefficient. Area under curve (AUC) of SVM, Boosting, Decision trees, k-nearest neighbor, LASSO, neural networks, random forests were 0.94, 0.86, 0.73, 0.72, 0.91, 0.92, and 0.85, respectively. It was proved that SVM classification offered significantly increased classification performance compared to the reference methods. This scheme may be used as an auxiliary tool to differentiate between benign and malignant SPNs of CT images in future.",2013,Computer methods and programs in biomedicine
Metabolic Network Construction Using Ensemble Algorithms,"One of the most important and challenging ""knowledge extraction"" tasks in bioinformatics is the reverse engineering of genes, proteins, and metabolites networks from biological data. Gaussian graphical models (GGMs) have been proven to be a very powerful formalism to infer biological networks. Standard GGM selection techniques can unfortunately not be used in the ""small N, large P"" data setting. Various methods to overcome this issue have been developed based on regularized estimation, partial least squares method, and limited-order partial correlation graphs. Several studies compared the performances among several network construction algorithms, such as PLSR, SCE, and ES, ICR and PCR, Ridge regression, Lasso and adaptive Lasso, to see which method is the best for biological network constructions. Each comparison analysis resulted in that each construction method has its own advantages as well as disadvantages according to different circumstances, such as the network complexity. However, it is almost impossible to recognize the complexity of the network before estimation. Thus, we develop an Ensemble method which is model averaging to construct a metabolic network. Our simulation studies show that the ensemble averaging based network construction has F1 score larger than these of other methods except only for Adaptive Lasso, reflecting its ability to account for uncertainty of network complexity.",2015,
Obliviousness Makes Poisoning Adversaries Weaker,"Poisoning attacks have emerged as a significant security threat to machine learning (ML) algorithms. It has been demonstrated that adversaries who make small changes to the training set, such as adding specially crafted data points, can hurt the performance of the output model. Most of these attacks require the full knowledge of training data or the underlying data distribution. In this paper we study the power of oblivious adversaries who do not have any information about the training set. We show a separation between oblivious and full-information poisoning adversaries. Specifically, we construct a sparse linear regression problem for which LASSO estimator is robust against oblivious adversaries whose goal is to add a non-relevant features to the model with certain poisoning budget. On the other hand, non-oblivious adversaries, with the same budget, can craft poisoning examples based on the rest of the training data and successfully add non-relevant features to the model.",2020,ArXiv
Virtual metrology modeling of time-dependent spectroscopic signals by a fused lasso algorithm,"Abstract This paper proposes a fused lasso model to identify significant features in the spectroscopic signals obtained from a semiconductor manufacturing process, and to construct a reliable virtual metrology (VM) model. Analysis of spectroscopic signals involves combinations of multiple samples collected over time, each with a vast number of highly correlated features. This leads to enormous amounts of data, which is a challenge even for modern-day computers to handle. To simplify such complex spectroscopic signals, dimension reduction is critical. The fused lasso is a regularized regression method that performs automatic variable selection for the predictive modeling of highly correlated datasets such as those of spectroscopic signals. Furthermore, the fused lasso is especially useful for analyzing high-dimensional data in which the features exhibit a natural order, as is the case in spectroscopic signals. In this paper, we conducted an experimental study to demonstrate the usefulness of a fused lasso-based VM model and compared it with other VM models based on the lasso and elastic-net models. The results showed that the VM model constructed with features selected by the fused lasso algorithm yields more accurate and robust predictions than the lasso- and elastic net-based VM models. To the best of our knowledge, ours is the first attempt to apply a fused lasso to VM modeling.",2016,Journal of Process Control
Development of a membrane lipid metabolismâ€“based signature to predict overall survival for personalized medicine in ccRCC patients,"Clear cell renal cell carcinoma (ccRCC) is the most common type of renal cell carcinoma and is characterized by a dysregulation of changes in cellular metabolism. Altered lipid metabolism contributes to ccRCC progression and malignancy. Associations among survival potential and each gene ontology (GO) term were analyzed by univariate Cox regression. The results revealed that membrane lipid metabolism had the greatest hazard ratio (HR). Weighted gene co-expression network analysis (WGCNA) was applied to determine the key genes associated with membrane lipid metabolism. Consensus clustering was used to identify novel molecular subtypes based on the key genes. LASSO Cox regression was performed to build a membrane lipid metabolismâ€“based signature. The random forest algorithm was applied to find the most important mutations associated with membrane lipid metabolism. Decision trees and nomograms were constructed to quantify risks for individual patients. Membrane lipid metabolism stratified ccRCC patients into high- and low-risk groups. Key genes were identified by WGCNA. Membrane lipid metabolismâ€“based signatures exhibited higher prediction efficiency than other clinicopathological traits in both whole cohort and subgroup analyses. The random forest algorithm revealed high associations among the membrane lipid metabolismâ€“based signature and BAP1, PBRM1 and VHL mutations. Decision trees and nomograms indicated high efficiency for risk stratification. Our study might contribute to the optimization of risk stratification for survival and personalized management of ccRCC patients.",2019,EPMA Journal
Large-Scale Structured Sparsity via Parallel Fused Lasso on Multiple GPUs,"ABSTRACTWe present a massively parallel algorithm for the fused lasso, powered by a multiple number of graphics processing units (GPUs). Our method is suitable for a class of large-scale sparse regression problems on which a two-dimensional lattice structure among the coefficients is imposed. This structure is important in many statistical applications, including image-based regression in which a set of images are used to locate image regions predictive of a response variable such as human behavior. Such large datasets are increasingly common. In our study, we employ the split Bregman method and the fast Fourier transform, which jointly have a high data-level parallelism that is distinct in a two-dimensional setting. Our multi-GPU parallelization achieves remarkably improved speed. Specifically, we obtained as much as 433 times improved speed over that of the reference CPU implementation. We demonstrate the speed and scalability of the algorithm using several datasets, including 8100 samples of 512 Ã— 512 ...",2017,Journal of Computational and Graphical Statistics
"Identification, development and testing of thermal error compensation model for a headstock assembly of CNC turning centre","In CNC machine tools, transient temperature variation in the headstock assembly is the major contributors for spindle thermal error. The compensation of thermal error is critical for ensuring the accuracy of machine tool. The performance of an error compensation system depends largely on the accuracy and robustness of the thermal error model. In the present work, a robust thermal error model is developed for minimizing the error in lateral direction of the spindle which significantly influences the geometrical accuracy of the workpiece. Analysis-of-variance (ANOVA) is applied to the results of the experiments in determining the percentage contribution of each individual temperature key point against a stated level of confidence. Based on the analysis of existing approaches for thermal error modeling of machine tools, an approach of LASSO (least absolute shrinkage and selection operator) is proposed in order to avoid the multi collinearity problem. The proposed method is an innovative variable selection method to remove redundant or unimportant temperature key points in the linear thermal error model and minimize the residual sum of squares. The predictive error model is found to have better robustness and accuracy in comparison to the combination of grey correlation and step wise linear regression for error compensation of CNC lathe.",2014,International journal of engineering and technology
On the prediction loss of the lasso in the partially labeled setting,"In this paper we revisit the risk bounds of the lasso estimator in the context of transductive and semi-supervised learning. In other terms, the setting under consideration is that of regression with random design under partial labeling. The main goal is to obtain user-friendly bounds on the off-sample prediction risk. To this end, the simple setting of bounded response variable and bounded (high-dimensional) covariates is considered. We propose some new adaptations of the lasso to these settings and establish oracle inequalities both in expectation and in deviation. These results provide non-asymptotic upper bounds on the risk that highlight the interplay between the bias due to the mis-specification of the linear model, the bias due to the approximate sparsity and the variance. They also demonstrate that the presence of a large number of unlabeled features may have significant positive impact in the situations where the restricted eigenvalue of the design matrix vanishes or is very small.",2016,arXiv: Statistics Theory
Bayesian empirical likelihood for ridge and lasso regressions,"Abstract Ridge and lasso regression models, which are also known as regularization methods, are widely used methods in machine learning and inverse problems that introduce additional information to solve ill-posed problems and/or perform feature selection. The ridge and lasso estimates for linear regression parameters can be interpreted as Bayesian posterior estimates when the regression parameters have Normal and independent Laplace (i.e., double-exponential) priors, respectively. A significant challenge in regularization problems is that these approaches assume that data are normally distributed, which makes them not robust to model misspecification. A Bayesian approach for ridge and lasso models based on empirical likelihood is proposed. This method is semiparametric because it combines a nonparametric model and a parametric model. Hence, problems with model misspecification are avoided. Under the Bayesian empirical likelihood approach, the resulting posterior distribution lacks a closed form and has a nonconvex support, which makes the implementation of traditional Markov chain Monte Carlo (MCMC) methods such as Gibbs sampling and Metropolisâ€“Hastings very challenging. To solve the nonconvex optimization and nonconvergence problems, the tailored Metropolisâ€“Hastings approach is implemented. The asymptotic Bayesian credible intervals are derived.",2020,Comput. Stat. Data Anal.
SmartGPA: how smartphones can assess and predict academic performance of college students,"Many cognitive, behavioral, and environmental factors impact student learning during college. The SmartGPA study uses passive sensing data and self-reports from students' smartphones to understand individual behavioral differences between high and low performers during a single 10-week term. We propose new methods for better understanding study (e.g., study duration) and social (e.g., partying) behavior of a group of undergraduates. We show that there are a number of important behavioral factors automatically inferred from smartphones that significantly correlate with term and cumulative GPA, including time series analysis of activity, conversational interaction, mobility, class attendance, studying, and partying. We propose a simple model based on linear regression with lasso regularization that can accurately predict cumulative GPA. The predicted GPA strongly correlates with the ground truth from students' transcripts (r = 0:81 and p < 0:001) and predicts GPA within Â±0:179 of the reported grades. Our results open the way for novel interventions to improve academic performance.",2015,Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing
Enhancing Accuracy in Cross-Domain Sentiment Classification by using Discounting Factor,"Sentiment Analysis involves in building a system to collect and examine opinions about the product made in blog posts, comments, reviews or tweets. Automatic classification of sentiment is important for applications such as opinion mining, opinion summarization, contextual advertising and market analysis. Sentiment is expressed differently in different domains and it is costly to annotate data for each new domain. In Cross-Domain Sentiment Classification, the features or words that appear in the source domain do not always appear in the target domain. So a classifier trained on one domain might not perform well on a different domain because it fails to learn the sentiment of the unseen words. One solution to this issue is to use a thesaurus which groups different words that express the same sentiment. Hence, feature expansion is required to augment a feature vector with additional related features to reduce the mismatch between features. The proposed method creates a thesaurus that is sensitive to the sentiment of words expressed in different domains. It utilizes both labeled as well as unlabeled data of the source domains and unlabeled data of the target domain. It uses pointwise mutual information to compute relatedness measure which in turn used to create thesaurus. The pointwise mutual information is biased towards infrequent elements/features. So a discounting factor is multiplied to the pointwise mutual information to overcome this problem. Then the proposed method uses the created thesaurus to expand feature vectors. Using these extended vectors, a Lasso Regularized Logistic Regression based binary classifier is trained to classify sentiment of the reviews in target domain. It gives improved prediction accuracy than existing Cross-Domain Sentiment Classification system.",2015,
Serum Alpha-2-Macroglobulin as an intrinsic radioprotective factor in patients undergoing thoracic radiation therapy,"Objective To investigate the impact of alpha-2-macroglobulin (A2M), a suspected intrinsic radioprotectant, on radiation pneumonitis and esophagitis. Additionally, we establish multifactorial predictive models for pneumonitis and esophagitis. Materials/Methods Baseline A2M levels were obtained for 258 patients prior to thoracic radiotherapy (RT). Dose-volume characteristics were extracted from treatment plans. Spearmanâ€™s correlation (Rs) test was used to correlate A2M levels, smoking status and dosimetric variables with toxicities. Esophagitis and pneumonitis prediction models were built using least absolute shrinkage and selection operator (LASSO) logistic regression on 1000 bootstrapped datasets. Models were built using 2/3 of the data for training and 1/3 for validation. Results There were 36 (14.0%) patients with grade â‰¥2 pneumonitis and 61 (23.6%) with grade â‰¥2 esophagitis. The median A2M level was 191 mg/dL (range: 94-511). Never/former/current smoker status was 47 (18.2%)/179 (69.4%)/32 (12.4%). We found a significant correlation between baseline A2M levels and esophagitis (Rs=-0.18/p=0.003) and between A2M and smoking status (former or current) (Rs=0.13/p=0.04) but not between A2M and pneumonitis. On univariate analysis, significant parameters for grade â‰¥2 esophagitis included number of fractions (Rs=0.47/p<0.0001), treatment days (Rs=0.44/p<0.0001), chemotherapy use (Rs=0.40/p<0.0001), dose per fraction (Rs=-0.34/p<0.0001), total dose (Rs=0.29/p<0.0001), age (Rs=-0.22/p=0.0003), and several dosimetric variables in esophagus with Rs>0.5 (p<0.0001). For pneumonitis, significant clinical parameters were treatment days (Rs=0.24/p=0.0001), chemotherapy use (Rs=0.22/p=0.0004), number of fractions (Rs=0.21/p=0.0007), dose per fraction (Rs=-0.18/p=0.0035), and total dose (Rs=0.15/p=0.013). The most significant dosimetric variable in lung and heart was D70 (Rs=0.28/p<0.0001) and max dose (Rs=0.27/p<0.0001), respectively. LASSO bootstrap logistic regression models on the validation data resulted in the area under the receiver operating characteristic curve of 0.84 and 0.75 for esophagitis and pneumonitis, respectively. Conclusion Our findings show an association of higher A2M values with lower risk of radiation esophagitis and smoking status. Multivariate predictive models also confirmed a role of heart dose in the risk of pneumonitis.",2019,bioRxiv
Weighted LAD-LASSO method for robust parameter estimation and variable selection in regression,"The weighted least absolute deviation (WLAD) regression estimation method and the adaptive least absolute shrinkage and selection operator (LASSO) are combined to achieve robust parameter estimation and variable selection in regression simultaneously. Compared with the LAD-LASSO method, the weighted LAD-LASSO (WLAD-LASSO) method will resist to the heavy-tailed errors and outliers in explanatory variables. Properties of the WLAD-LASSO estimators are investigated. A small simulation study and an example are provided to demonstrate the superiority of the WLAD-LASSO method over the LAD-LASSO method in the presence of outliers in the explanatory variables and the heavy-tailed error distribution.",2012,Comput. Stat. Data Anal.
Consistent model identification of varying coefficient quantile regression with BIC tuning parameter selection,"ABSTRACT Quantile regression provides a flexible platform for evaluating covariate effects on different segments of the conditional distribution of response. As the effects of covariates may change with quantile level, contemporaneously examining a spectrum of quantiles is expected to have a better capacity to identify variables with either partial or full effects on the response distribution, as compared to focusing on a single quantile. Under this motivation, we study a general adaptively weighted LASSO penalization strategy in the quantile regression setting, where a continuum of quantile index is considered and coefficients are allowed to vary with quantile index. We establish the oracle properties of the resulting estimator of coefficient function. Furthermore, we formally investigate a Bayesian information criterion (BIC)-type uniform tuning parameter selector and show that it can ensure consistent model selection. Our numerical studies confirm the theoretical findings and illustrate an application of the new variable selection procedure.",2017,Communications in Statistics - Theory and Methods
Marginal Regression For Multitask Learning,"Variable selection is an important and practical problem that arises in analysis of many high-dimensional datasets. Convex optimization procedures that arise from relaxing the NP-hard subset selection procedure, e.g., the Lasso or Dantzig selector, have become the focus of intense theoretical investigations. Although many efficient algorithms exist that solve these problems, finding a solution when the number of variables is large, e.g., several hundreds of thousands in problems arising in genome-wide association analysis, is still computationally challenging. A practical solution for these high-dimensional problems is marginal regression, where the output is regressed on each variable separately. We investigate theoretical properties of marginal regression in a multitask framework. Our contribution include: i) sharp analysis for marginal regression in a single task setting with random design, ii) sufficient conditions for the multitask screening to select the relevant variables, iii) a lower bound on the Hamming distance convergence for multitask variable selection problems. A simulation study further demonstrates the performance of marginal regression.",2012,
The Effects of Variable Selection Methods on Linear Regression-Based Effort Estimation Models,"Stepwise regression has often been used for variable selection of effort estimation models. However it has been criticized for inappropriate selection, and another method is recommended. We thus examined the effects of Lasso, which is one of such variable selection methods. An experiment with datasets from PROMISE repository revealed that Lasso-based selection stably selected better variables than stepwise in predictive performance. We thus concluded Lasso-based selection is preferable to stepwise regression.",2013,2013 Joint Conference of the 23rd International Workshop on Software Measurement and the 8th International Conference on Software Process and Product Measurement
"Understanding and Predicting Travel Time with Spatio-Temporal Features of Network Traffic Flow, Weather and Incidents","Travel time on a route varies substantially by time of day and from day to day. It is critical to understand to what extent this variation is correlated with various factors, such as weather, incidents, events or travel demand level in the context of dynamic networks. This helps a better decision making for infrastructure planning and real-time traffic operation. We propose a data-driven approach to understand and predict highway travel time using spatio-temporal features of those factors, all of which are acquired from multiple data sources. The prediction model holistically selects the most related features from a highdimensional feature space by correlation analysis, principle component analysis and LASSO. We test and compare the performance of several regression models in predicting travel time 30 min in advance via two case studies: (1) a 6-mile highway corridor of I-270N in D.C. region, and (2) a 2.3-mile corridor of I-376E in Pittsburgh region. We found that some bottlenecks scattered in the network can imply congestion on those corridors at least 30 minutes in advance, including those on the alternative route to the corridors of study. In addition, real-time travel time is statistically related to incidents on some specific locations, morning/afternoon travel demand, visibility, precipitation, wind speed/gust and the weather type. All those spatiotemporal information together help improve prediction accuracy, comparing to using only speed data. In both case studies, random forest shows the most promise, reaching a root-mean-squared error of 16.6% and 17.0% respectively in afternoon peak hours for the entire year of 2014.",2019,IEEE Intelligent Transportation Systems Magazine
Bayesian semi-parametric multiple shrinkage,"High dimensional and highly correlated data leading to nonor weakly-identified effects are commonplace. Maximum likelihood will typically fail in such situations and a variety of shrinkage methods have been proposed. Standard techniques, such as ridge regression or the lasso, shrink estimates toward zero, with some approaches allowing coefficients to be selected out of the model by achieving a value of zero. When substantive information is available, estimates can be shrunk to non-null values; however, such information may not be available. We propose a Bayesian semi-parametric approach that allows shrinkage to multiple locations. Coefficients are given a mixture of heavy tailed double exponential priors, with location and scale parameters assigned Dirichlet process hyperpriors to allow groups of coefficients to be shrunk toward the same, possibly non-zero, mean. Our approach favors sparse, but flexible structure, by shrinking towards a small number of random locations. The methods are illustrated using a study of genetic polymorphisms and multiple myeloma.",2007,
Penalized Lasso Methods in Health Data: application to trauma and influenza data of Kerman,"Background: Two main issues that challenge model building are number of Events Per Variable and multicollinearity among exploratory variables. Our aim is to review statistical methods that tackle these issues with emphasize on penalized Lasso regression model. Â The present study aimed to explain problems of traditional regressions due to small sample size and multi-colinearity in trauma and influenza data and to introduce Lasso regression as the most modern shrinkage method. Methods: Two data sets, corresponded to Events Per Variable of 1.5 and 3.4, were used. The outcomes of these two data sets were hospitalization due to trauma and hospitalization of patients suffering influenza respectively. In total, four models were developed: classic Cox and logistic regression models, as well as their penalized lasso form. The tuning parameters were selected through 10-fold cross validation. Results: Traditional Cox model was not able to detect significance of any of variables. Lasso Cox model revealed significance of respiratory rate, focused assessment with sonography in trauma, difference between blood sugar on admission and 3Â h after admission, and international normalized ratio. In the second data set, while lasso logistic selected four variables as being significant, classic logistic was able to identify only the importance of one variable. Conclusion: The AIC for lasso models was lower than that for traditional regression models. Lasso method has practical appeal when Events Per Variable is low and multicollinearity exists in the data.",2019,
Efficient support recovery via weighted maximum-contrast subagging,"In this paper, we study finite sample properties of subagging for nonsmooth estimation and model selection in sparse and large-scale regression settings where both the number of parameters and the number of samples can be extremely large. This setup is very different from high-dimensional regression and is such that Lasso estimator might be inappropriate for computational, rather than statistical reasons. We show that subagging of Lasso estimators results in discontinuous estimated support set and is never able to recover sparsity set when at least one of aggregated estimators has probability of support recovery strictly less than 1. Therefore, we propose its randomized and smoothed alternative, which we call weighted maximum-contrast subagging. We develop theory in support of the claim that proposed method has tight error control over both false positives and false negatives, regardless of the size of a dataset. Unlike existing methods, it allows for oracle-like properties, even in cases of non-oracle-like properties of aggregated estimators. Furthermore, we design an adaptive procedure for selecting tuning parameters and appropriate optimal weighting scheme. Finally, we validate our theoretical findings through extensive simulation study and analysis of a part of the million-song-challenge dataset.",2013,
"ELASTICREGRESS: Stata module to perform elastic net regression, lasso regression, ridge regression","elasticregress calculates an elastic net-regularized regression: an estimator of a linear model in which larger parameters are discouraged. This estimator nests the LASSO and the ridge regression, which can be estimated by setting alpha equal to 1 and 0 respectively. lassoregress estimates the LASSO; it is a convenience command equivalent to elasticregress with the option alpha(1). ridgeregress estimates a ridge regression; it is a convenience command equivalent to elasticregress with the option alpha(0). elasticregress implements the coordinate descent algorithm in Friedman, Hastie and Tibshirani (J.Stat.Software, 2008) to produce very efficient estimates of the LASSO, of the ridge regression and of a generalization of the two. The algorithm used is the 'covariance updates' algorithm presented in Section 2 of that paper, except that it does not exploit sparsity in the covariates.",2017,Statistical Software Components
Nomograms for predicting the overall and cause-specific survival in patients with malignant peripheral nerve sheath tumor: a population-based study,"BackgroundMalignant peripheral nerve sheath tumor (MPNST) is extremely rare in soft tissue sarcoma, with a high rate of recurrence and metastasis. Due to its rarity, the epidemiological features and prognostic factors are still uncertain. Moreover, nomograms for patients with MPNST have not been constructed and validated until now.Patients and methodsPatients diagnosed with MPNST between 1973 and 2014 were selected from the Surveillance, Epidemiology, and End Results (SEER) database. Survival analysis, machine learning and Lasso regression were used to identify the prognostic factors for overall survival (OS) and cause-specific survival (CSS). Significant prognostic factors were integrated to construct nomograms and then the nomograms were validated externally with a separate cohort from our own institution.ResultsA total of 689 patients were included in the training set and 42 patients in the validation set. Multivariate analysis suggested that age, histology, historic stage and chemotherapy were independent prognostic factors for OS and primary site, surgery, historic stage and chemotherapy for CSS. The nomograms based on multivariate models were developed and validated for predicting 3- and 5-year OS and CSS, with a C-index of 0.686 and 0.707, respectively. In the external validation set, the C-index was 0.700 for OS and 0.722 for CSS.ConclusionICD-O-3 histology, historic stage and chemotherapy were independent prognostic factors for OS and primary site, surgery, historic stage and chemotherapy for CSS. The constructed nomograms could provide individual prediction for MPNST patients and assist oncologists in making accurate survival evaluation.",2019,Journal of Neuro-Oncology
Redundancy Techniques for Straggler Mitigation in Distributed Optimization and Learning,"Performance of distributed optimization and learning systems is bottlenecked by ""straggler"" nodes and slow communication links, which significantly delay computation. We propose a distributed optimization framework where the dataset is ""encoded"" to have an over-complete representation with built-in redundancy, and the straggling nodes in the system are dynamically left out of the computation at every iteration, whose loss is compensated by the embedded redundancy. We show that oblivious application of several popular optimization algorithms on encoded data, including gradient descent, L-BFGS, proximal gradient under data parallelism, and coordinate descent under model parallelism, converge to either approximate or exact solutions of the original problem when stragglers are treated as erasures. These convergence results are deterministic, i.e., they establish sample path convergence for arbitrary sequences of delay patterns or distributions on the nodes, and are independent of the tail behavior of the delay distribution. We demonstrate that equiangular tight frames have desirable properties as encoding matrices, and propose efficient mechanisms for encoding large-scale data. We implement the proposed technique on Amazon EC2 clusters, and demonstrate its performance over several learning problems, including matrix factorization, LASSO, ridge regression and logistic regression, and compare the proposed method with uncoded, asynchronous, and data replication strategies.",2019,J. Mach. Learn. Res.
Metabolomic profiles and development of metabolic risk during the pubertal transition: A prospective study in the ELEMENT Project,"Objectives(1) Examine associations of a branched-chain amino acid (BCAA) metabolite pattern with metabolic risk across adolescence; (2) use Least Absolute Shrinkage and Selection Operator (LASSO) to identify novel metabolites of metabolic risk.MethodsWe used linear regression to examine associations of a BCAA score with change (âˆ†) in metabolic biomarkers over 5-year follow-up in 179 adolescents 8â€“14 years at baseline. Next, we applied LASSO, a regularized regression technique well suited for reduction of high-dimensional data, to identify metabolite predictors of âˆ†biomarkers.ResultsIn boys, the BCAA score corresponded with decreasing C-peptide, C-peptide-based insulin resistance (CP-IR), total cholesterol (TC), and low-density-lipoprotein cholesterol (LDL). In pubertal girls, the BCAA pattern corresponded with increasing C-peptide and leptin. LASSO identified asparagine as a predictor of decreasing C-peptide (Î²â€‰=â€‰âˆ’0.33) and CP-IR (Î²â€‰=â€‰âˆ’0.012), and acetyl-carnitine (Î²â€‰=â€‰2.098), 4-hydroxyproline (Î²â€‰=â€‰âˆ’0.050), ornithine (Î²â€‰=â€‰âˆ’0.353), and Î±-aminoisobutyric acid (Î²â€‰=â€‰âˆ’0.793) as determinants of TC in boys. In girls, histidine was a negative determinant of TC (Î²â€‰=â€‰âˆ’0.033).ConclusionsThe BCAA pattern was associated with âˆ†glycemia and âˆ†lipids in a sex-specific manner. LASSO identified asparagine, which influences growth hormone secretion, as a determinant of decreasing C-peptide and CP-IR in boys, and metabolites on lipid metabolism pathways as determinants of decreasing cholesterol in both sexes.",2018,Pediatric research
Learning Gene Regulatory Networks with High-Dimensional Heterogeneous Data,"The Gaussian graphical model is a widely used tool for learning gene regulatory networks with high-dimensional gene expression data. Most existing methods for Gaussian graphical models assume that the data are homogeneous, i.e., all samples are drawn from a single Gaussian distribution. However, for many real problems, the data are heterogeneous, which may contain some subgroups or come from different resources. This paper proposes to model the heterogeneous data using a mixture Gaussian graphical model, and apply the imputation-consistency algorithm, combining with the Ïˆ-learning algorithm, to estimate the parameters of the mixture model and cluster the samples to different subgroups. An integrated Gaussian graphical network is learned across the subgroups along with the iterations of the imputation-consistency algorithm. The proposed method is compared with an existing method for learning mixture Gaussian graphical models as well as a few other methods developed for homogeneous data, such as graphical Lasso, nodewise regression, and Ïˆ-learning. The numerical results indicate superiority of the proposed method in all aspects of parameter estimation, cluster identification, and network construction. The numerical results also indicate generality of the proposed method: it can be applied to homogeneous data without significant harms. The accompanied R package GGMM is available at https://cran.r-project.org.",2018,arXiv: Methodology
