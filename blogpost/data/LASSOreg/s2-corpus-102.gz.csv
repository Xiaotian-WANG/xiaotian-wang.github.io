title,abstract,year,journal
Closing the U.S. gender wage gap requires understanding its heterogeneity,"In 2016, the majority of full-time employed women in the U.S. earned significantly less than comparable men. The extent to which women were affected by gender inequality in earnings, however, depended greatly on socio-economic characteristics, such as marital status or educational attainment. In this paper, we analyzed data from the 2016 American Community Survey using a high-dimensional wage regression and applying double lasso to quantify heterogeneity in the gender wage gap. We found that the gap varied substantially across women and was driven primarily by marital status, having children at home, race, occupation, industry, and educational attainment. We recommend that policy makers use these insights to design policies that will reduce discrimination and unequal pay more effectively.",2018,arXiv: Econometrics
OP0021 Draft classification criteria for the anca associated vasculitides,"Background Classification criteria for the ANCA-associated vasculitides (AAVs) were developed in the 1980s prior to the use of ANCA testing and newer imaging techniques. The Diagnostic and Classification of the Systemic Vasculitides (DCVAS) study is an international project to update classification criteria for the systemic vasculitides. Objectives Development of draft classification criteria for Granulomatosis with Polyangiitis (GPA), Microscopic Polyangiitis (MPA) and Eosinophilic Granulomatosis with Polyangiitis (EGPA). Methods Three phases: 1) Expert panel review of cases to identify gold standard set of new cases of small vessel vasculitis; 2) Item reduction of >8000â€‰individual DCVAS items using data-driven and consensus methodology; 3) Lasso logistic regression models within each development set comparing each of the AAV types to other small and medium vessel vasculitides. Final criteria derived through clinical consensus, tested in validation set. The classification project has received financial support from the ACR and EULAR. Results The expert review process approved 2072/2871 (72%) of physician diagnosed DCVAS cases, including [724â€‰GPA, 291â€‰MPA, 226 EGPA, 51 polyarteritis nodose (PAN), 220 other small vessel disease (SVV)]. Data driven and expert consensus resulted in 91 items retained. Draft criteria, and sensitivity and specificity in table 1. Conclusions Draft classification criteria for GPA, MPA and EGPA have been created which reflect current practice and have good sensitivity and specificity. Acknowledgements DCVAS sites and expert panel members Disclosure of Interest None declared",2018,Annals of the Rheumatic Diseases
ParamÃ¨tres dâ€™analyse de texture du 18F-FDG TEP/CT prÃ©-traitement comme biomarqueurs prÃ©dictifs de survie sans progression chez les patients prÃ©sentant un lymphome folliculaire traitÃ© par immunochimiothÃ©rapie et maintenance par rituximab,"Etat de la question Identifier les parametres de lâ€™analyse de texture du scanner issu du 18F-FDG TEP/CT pouvant predire la survie sans progression (PFS), la survie sans progression des 24Â premiers mois (PFS 24) et le temps jusquâ€™a nouveau traitement (TTNT) chez les patients atteints dâ€™un lymphome folliculaire traites par immunochimiotherapie et maintenance par rituximab. Materiel et methodes Cette etude retrospective monocentrique a inclus 72Â patients presentant un lymphome folliculaire. Lâ€™analyse de texture a ete realisee sur les scanners sans injection issus des 18F-FDG TEP/CT obtenus dans le mois precedent lâ€™initiation du traitement, avec recueil des valeurs de mediane, derivation standard, entropie, kurtosis et skewness. Des modeles de regression de Cox avec selection de variables par methode Lasso ont ete realises afin dâ€™identifier les parametres predictifs de PFS, PFS 24Â et TTNT. Le score FLIPI et le T-MTVT ont egalement ete testes. Resultats Le skewness etait un facteur predictif independant de la PFS (SSFÂ =Â 2Â ; HR (CI 95Â %)Â Â =Â 3,72 (1,14, 12,11), pÂ =Â 0,029), de la PFS 24 (SSFÂ =Â 2Â ; HR (CI 95Â %)Â =Â 13,84 (1,29, 138,13), pÂ =Â 0,029), et du TTNT (SSFÂ =Â 2Â ; HR (CI 95Â %)Â =Â 5,11 (1,18, 22,13), pÂ =Â 0,029). Les valeurs de skewness superieures a âˆ’0,015Â a filtre SSFÂ =Â 2Â etaient significativement associees a des PFS, PFS 24Â et TTNT diminues. Le kurtosis sans filtre spatial etait un facteur predictif independant de la PFS (SSFÂ =Â 0Â ; HR (CI 95Â %)Â =Â 1,22 (1,02, 1,47), pÂ =Â 0,033) et du TTNT (SSFÂ =Â 0Â ; HR (CI 95Â %)Â =Â 1,23 (1,04, 1,46), pÂ =Â 0,01). Conclusion Les parametres skewness et kurtosis issus de lâ€™analyse de texture du scanner extrait du 18F-FDG TEP/CT pre-traitement peuvent etre des biomarqueurs predictifs de la PFS, de la PFS 24Â et du TTNT chez les patients presentant un lymphome folliculaire.",2020,Revue D Epidemiologie Et De Sante Publique
Semiparametric Model Selection in Panel Data Models with Deterministic Trends and Cross-Sectional Dependence By Jia Chen,"In this paper, we consider a model selection issue in semiparametric panel data models with fixed effects. The modelling framework under investigation can accommodate both nonlinear deterministic trends and cross-sectional dependence. And we consider the so-called â€œlarge panelsâ€ where both the time series and cross sectional sizes are very large. A penalised profile least squares method with first-stage local linear smoothing is developed to select the significant covariates and estimate the regression coefficients simultaneously. The convergence rate and the oracle property of the resulting semiparametric estimator are established by the joint limit approach. The developed semiparametric model selection methodology is illustrated by two Monte-Carlo simulation studies, where we compare the performance in model selection and estimation of three penalties, i.e., the least absolute shrinkage and selection operator (LASSO), the smoothly clipped absolute deviation (SCAD), and the minimax concave penalty (MCP). JEL classification: C13, C14, C23.",2014,
Penalized Estimation of Panel Vector Autoregressive Models : A Lasso Approach,"This paper proposes a new least absolute shrinkage and selection operator (lasso) for estimating panel vector autoregressive (PVAR) models. By allowing for interdependencies and heterogeneities across cross-sectional units, typically the number of parameters of PVAR models is too large to estimate using ordinary least squares. The penalized regression this paper introduces ensures the feasibility of the estimation by specifying a shrinkage penalty that contains time series and cross section characteristics; thereby, accounting for the inherent panel structure within the data. Furthermore, using the weighted sum of squared residuals as the loss function enables the lasso for PVAR models to take into account correlations between cross-sectional units in the penalized regression. Given large and sparse models, simulation results point towards advantages of using lasso for PVARs over OLS, standard lasso techniques as well as Bayesian estimators in terms of mean squared errors and forecast accuracy. Empirical forecasting applications with up to ten countries and four variables support these findings.",2017,
Risk factors and Predictors of Mortality in Streptococcal Necrotizing Soft-Tissue Infections: A Multicenter Prospective Study.,"BACKGROUND
Necrotizing soft-tissue infections (NSTI) are life-threatening conditions often caused by Î²-hemolytic streptococci, group A streptococcus (GAS) in particular. Optimal treatment is contentious. The INFECT cohort includes the largest set of prospectively enrolled streptococcal NSTI cases to date.


METHODS
Â From the INFECT cohort of 409 adults admitted with NSTI to five clinical centers in Scandinavia, patients culture-positive for GAS or Streptococcus dysgalactiae (SD) were selected. Risk factors were identified by comparison with a cohort of non-necrotizing streptococcal cellulitis. The impact of baseline factors and treatment on 90-day mortality was explored using Lasso regression. Whole-genome sequencing of bacterial isolates was used for emm typing and virulence gene profiling.


RESULTS
Â The 126 GAS NSTI cases and 27 cases caused by SD constituted 31% and 7% of the whole NSTI cohort, respectively. When comparing to non-necrotizing streptococcal cellulitis, streptococcal NSTI was associated to blunt trauma, absence of pre-existing skin lesions, and a lower BMI. Septic shock was significantly more frequent in GAS (65%) compared to SD (41%) and polymicrobial, non-streptococcal NSTI (46%). Age, male sex, septic shock, and no administration of intravenous immunoglobulin (IVIG) were among factors associated with 90-day mortality. Predominant emm types were emm1, emm3 and emm28 in GAS and stG62647 in SD.


CONCLUSIONS
Â  Streptococcal NSTI was associated with several risk factors, including blunt trauma. Septic shock was more frequent in NSTI caused by GAS than in cases due to SD. Factors associated with mortality in GAS NSTI included age, septic shock and no administration of IVIG.",2020,Clinical infectious diseases : an official publication of the Infectious Diseases Society of America
Prognostic value of tissueâ€based biomarker signature in clear cell renal cell carcinoma,"OBJECTIVE
To improve risk stratification for recurrence prognostication in patients with localised clear cell renal cell carcinoma (ccRCC).


PATIENTS AND METHODS
In all, 367 patients with non-metastatic ccRCC were included. The cohort was divided into a training and validation set. Using tissue microarrays, immunostaining was performed for 24 biomarkers representative of key pathways in ccRCC. Using Least Absolute Shrinkage and Selection Operator (LASSO) Cox regression, we identified several markers that were used to construct a risk classifier for risk of disease recurrence.


RESULTS
The median (interquartile range) follow-up was 63.5 (24.0-85.3) months. Five out of 24 markers were selected by LASSO Cox regression for the risk classifier: N-cadherin, E-cadherin, Ki67, cyclin D1 and phosphorylated eukaryotic initiation factor 4E binding protein-1 (p-4EBP1). Patients were classified as either low, intermediate or high risk of disease recurrence by tertiles of risk score. The 5-year recurrence-free survival (RFS) was 93.8%, 87.7% and 70% for patients with low-, intermediate- and high-risk scores, respectively (P < 0.001). Patients with a high marker score had worse RFS on multivariate analysis adjusted for age, gender, race and the Mayo Clinic Stage, Size, Grade, and Necrosis (SSIGN) score (hazard ratio 3.66, 95% confidence interval 1.58-8.49, P = 0.003 for high vs low marker score in the overall cohort). The five-marker classifier increased the concordance index of the clinical model in both the training and validation sets.


CONCLUSION
We developed a five-marker-based prognostic tool that can effectively classify patients with ccRCC according to risk of disease recurrence after surgery. This tool, if prospectively validated, could provide individualised risk estimation for patients with ccRCC.",2017,BJU International
A study of variable selection using g-prior distribution with ridge parameter,"In the Bayesian stochastic search variable selection framework, a common prior distribution for the regression coefficients is the g-prior of Zellner. However there are two standard cases where the associated covariance matrix does not exist and the conventional prior of Zellner cannot be used: if the number of observations is lower than the number of variables (large p and small n paradigm), or if some variables are linear combinations of others. In such situations, a prior distribution derived from the prior of Zellner can be considered by introducing a ridge parameter. This prior is a flexible and simple adaptation of the g-prior and its influence on the selection of variables is studied. A simple way to choose the associated hyper-parameters is proposed. The method is valid for any generalized linear mixed model and particular attention is paid to the study of probit mixed models when some variables are linear combinations of others. The method is applied to both simulated and real datasets obtained from Affymetrix microarray experiments. Results are compared to those obtained with the Bayesian Lasso.",2012,Comput. Stat. Data Anal.
Multivariate Discrimination by Shape I N Relation T O S I Z E,"Huinphries, J . M., F . L. Bookstein, B. Chernoff, G . R. Smith, R. L. Elder, and S. G. Poss (Museum of Zoology, Centerfor Human Growth and Development, Museuiiz of Paleontology, and Division of Biological Sciences, The University of Michigan, Ann Arbor, Michigan 48109) 1981. Multivariate discrimination by shape in relation to size. Syst. Zool., 30:291308.--The diverse methods for analyzing size-free shape differences tend to be guided by computational expediency rather than geometric principles. We question the use of ratios and ad hoc combinations of spatially unrelated measures. Neither are linear discriminant functions or series of independent regressions helpful to the visualization of shape differences. A bridge is needed between traditional quantitative methods and the geometrical analysis of shape. In principle any measured transects between landmarks of a form can serve as characters in a morpholnetric analysis. Systematic studies use a highly non-random sample of these, particularly biased regarding geometrical infolmation. We suggest defining size and shape in terms of factors-estimates of information comrnon to a universe of measured distances. The model presented here calculates a linear combination of variables that quantifies shape differences among populations, independent of size. In analyses in which the first two principal cornponents confound size and shape, size is removed from one axis with shear coefficients derived from regression of general size on principal cornponents centered by group. The general size factor is estimated by the principal axis of the within-group covariance matrix of the log-transformed data. Residuals from the regression of general size on the transfolmed axes approximate a shape-discriminating factor that is uncorrelated with size within group and displays the interpopulation shape differences borne by the first two principal components. The results bear a direct and interpretable correspondence to biorthogonal analysis of shape difference. [Multivariate analysis; principal components; discriminant functions; morphometrics; size-free shape; allometry; fishes.] Svsteinatists need ~rocedures that alassociations among the distance mealow them to discriminate ainong groups sures. Size, in particular, is not a single of organisms that vary in size. The groups variable such as biomass or a standard included in a studv can be chosen a length, but a factor which, when called priori (e.g., several species or geographic upon to predict all the distance measures populations within a species) or a poswithin a population, leaves the smallest teriori (as a conclusion resulting from mean squared residual. We prefer a factor some method of analysis). However the whose algebraic form acknowledges the groups are chosen, it has long been conallometric relationship (Jolicoeur, 1963). sidered desirable to discriminate ainong Our shape discriininators need to be inthem on the basis of size-free s h a ~ e dedependent of size (Flessa and Bray, 1977; rived from distance measures. Mosiinann and James, 1979) in order to The terms shape and size have been partition out the effects of growth (e.g., used in various and sometimes conflictindividuals of differing age and size). In ing ways (Huxley, 1932; Thompson, general, shape can be defined as the ge1942; Simpson, Roe and Lewontin, 1960; ometry of the organism after ""information Gould, 1966; Mosimann, 1970; Sprent, about position, scale, and orientation"" 1972: Bookstein. 1978). We construe size has been removed (Bookstein, 1978:8). and s'hape not as ineas'ured variables, but There is then an endless variety of shape as general factors, linear combinations information remaining. While the quanmost parsiinoniously accounting for the tification of size as a general factor de292 SYSTEMATIC ZOOLOGY VOL. 30 pends on the population under study, a measure of shape will depend on the research context as well. Our shape descriptors will always be shape discriminators defined in the context of multiple groups. Size can be a significant component of among-group differences (Calhoon and Jameson, 1970; Gould, 1977; Harding and Barnes, 1977; Mosiinann and James, 1979). Therefore, size coinparisons should not be automatically excluded from study (we disagree with Thorpe, 1976), but rather should be an independent part of the analysis. Three pr i~nary methods have been used to compare shape among groups while removing size: ratios, regressions, and factor or co~nponent analyses. The use of ratios has long been entrenched in the practice of morphometsics, and there is ample debate concerning the appropriateness of their use (Atchley et al., 1976; Corruccini, 1977; Mosimann and James, 1979; Hills, 1978; Dodson, 1978; Albrecht, 1978; Atchley and Anderson, 1978; Atchley, 1978). We agree with the position taken by Karl Pearson (1897), Atchley e t al. (1976), Atchley and Anderson (1978) and others that ratios should be avoided in morphometric studies because of statistical and conceptual difficulties which may seriously affect conclusions. These studies have demonstrated (1)that ratios can behave spuriously depending upon the correlation of the numerator and denominator; (2) that the ratio is not necessarily independent of the denominator; and (3) that the use of ratios alters the factor structure of the covariance matrix, especially for small sample sizes. Contrary to Blackith and Reyment (1971) and Dodson (1978), we believe that these with ratios are not automatically ameliorated by their logtransformation. Mosimann and James (1979), for instance, note that not all shape variables (ratios) are jointly lognormally distributed. Also, the purpose of constructing a ratio-to approxiinately ~ a r t i a l out the effect of size-is not autoinatically attained. Different nurnerators must be divided by different size variables to achieve the desired independence of ratios from denominators. A further danger is that ratios are not always interpretable as ineasures of shape. For example, Mosimann and James (1979:452) list log (toe + tarsus length/bill length) and log (tail lengthhoe + tarsus length) as shape variables. A second procedure used to standardize distance measures by size is univariate regression analysis of the distance measures on some measure of size (Mosimann, 1956; Gould, 1966; Thorpe, 1976, 1980). The regression can be calculated either separately by groups, or jointly using the pooled within-group covariance matrix. Each distance variable is replaced by its residual after the regression. If the pooled matrix is used, the underlying assumption is that the withingroup regression slopes are equal (Thorpe, 1976); if not, then size has not been removed. Also, regression analysis simply partials out the effect of the independent variable from the dependent variable. If size is designated as a single variable, then only that variable is partialed out. However, size is not equal to any single length, and those studies that regress distance ineasures against a single standard variable (e.g., snout-vent length in reptiles, standard length in fishes, or wing length in birds) do not necessarily remove the effects of size. For among-group comparisons, analysis of either ratios or regression residuals may be considered a univariate method that does not attend to the entire covariance matrix. This univariate approach is, perhaps, suitable for small numbers of variables, but as the number of variables becomes large (e.g., 111variables, Moss et al., 1977) the problems of describing the differences among the groups become increasingly difficult. We differ with the univariate approach on a inore fundamental level and agree with Sprent (1972) that the analysis of shape requires a multivariate context. A third technique for comparing shape 1981 SHAPE IN RELATION TO SIZE 293 and shape change among groups is inultivariate ordination (see Blackith and Reyment, 1971; Sneath and Sokal, 1973; Thorpe, 1976, 1980). Discriminant functions, with canonical axes, maximize the differences among groups with respect to the within-group covariance structure. The method requires a priori assignment of individuals to groups. When size is "" removed"" before discriminant analysis, the resulting functions are considered shape discriminators. When size is not removed the resulting discriminators are mixtures of size and shape. These functions are highly sample-dependent because the coefficients of the functions all have the determinant of the covariance matrix in the denominator, and as most variables, even ratios, are highly correlated, this determinant is close to zero even for large samples. Discriminant functions lose information abdut the correlations among the variables because the eigenvalues are derived by maximizing a function of the ratio of the amonggroup to within-group covariance matrices. The use of an optimization criterion does not constrain the resulting eigenvectors, and therefore the variableweights are spurious in relation to shape. From within a set of correlated characters only the variable with the highest F-statistic will be weighted heavily. Within that set, variables that do not contribute added discrimination will have low coefficients even though they contain nearly as much information about shape as the variable with the high F-statistic. In the analysis of shape it is imperative to include factors which accurately represent the degree to which the distance measures covary. We turn to principal components analysis which was specifically designed to analyze sets of correlated variables (Morrison, 1967). Principal component or factor analyses do not presume multiple groups and thus allow for their discovery. Furthermore, their coefficients, being ordinary regression coefficients of the indicators on the factors, are more easily interpretable (Stroud, 1953). In morphometric studies of groups differing in size the first factor or component has sometimes been interpreted as s",2007,
Quantile regression and variable selection for partially linear single-index models with missing censoring indicators,"Abstract Partially linear single-index models have been studied extensively under censorship setting, but typically all of the censoring indicators are assumed to be observed. This paper focuses on the quantile regression (QR) estimation for the partially linear single-index models where the data are right censored and the censoring indicators are missing at random. We propose weighted QR estimators of unknown parameters and link function based on the regression calibration, imputation and inverse probability weighting approaches. The asymptotic properties of the proposed weighted QR estimators for unknown parameters and the link function are established. Moreover, to select the important predictors, a variable selection procedure is introduced by applying adaptive LASSO penalized and the oracle property of the proposed weighted penalized estimators is obtained simultaneously. The finite sample performance of the proposed estimation methods and variable selection procedure are evaluated via simulation study. We also illustrate the proposed methods by using a dataset from a breast cancer clinical trial.",2019,Journal of Statistical Planning and Inference
[Preoperative prediction for lymph node metastasis of rectal nonmucinous adenocarcinoma based on radiomics classifier].,"OBJECTIVE
To determine the value of radiomics in identifying lymph node (LN) metastasis in patients with rectal nonmucinous adenocarcinoma.â€© Methods: Imaging data of 91 patients were retrospectively analyzed (61 in the training set and 30 in the test set). A total of 1 301 radiomics features were extracted from high-resolution T2-weighted images of the whole primary tumor. The least absolute shrinkage and selection operator (LASSO) logistic regression was performed to choose the optimal features and construct a radiomics classifier in the training set. Its discrimination performance was compared with that of morphological criteria by receiver operating characteristic (ROC) curve analysis, which was validated in the test set.â€© Results: The radiomics classifier combined with five key features was significantly associated with LN metastasis, which distinguished LN metastasis with an area under curve (AUC) at 0.874 (95% CI 0.787 to 0.960) in the training set, and the performance was similar in the test set (AUC 0.878, 95% CI 0.727 to 1.000). The AUCs according to the morphological criteria in the training set and test set were 0.619 (95% CI 0.487 to 0.752) and 0.556 (95% CI 0.355 to 0.756), respectively. Discrimination of the radiomics classifier was superior to that of morphological criteria in both the two datasets (both P <0.05).â€© Conclusion: The radiomics classifier provides individualized risk estimation for LN metastasis in rectal nonmucinous adenocarcinoma patients and it has the advantage over the morphological criteria.",2019,Zhong nan da xue xue bao. Yi xue ban = Journal of Central South University. Medical sciences
Discriminative Regression Machine: A Classifier for High-Dimensional Data or Imbalanced Data,"We introduce a discriminative regression approach to supervised classification in this paper. It estimates a representation model while accounting for discriminativeness between classes, thereby enabling accurate derivation of categorical information. This new type of regression models extends existing models such as ridge, lasso, and group lasso through explicitly incorporating discriminative information. As a special case we focus on a quadratic model that admits a closed-form analytical solution. The corresponding classifier is called discriminative regression machine (DRM). Three iterative algorithms are further established for the DRM to enhance the efficiency and scalability for real applications. Our approach and the algorithms are applicable to general types of data including images, high-dimensional data, and imbalanced data. We compare the DRM with currently state-of-the-art classifiers. Our extensive experimental results show superior performance of the DRM and confirm the effectiveness of the proposed approach.",2019,ArXiv
Balancing Performance and Interpretability: Selecting Features with Bootstrapped Ridge Regression,"Informctticists sometimes attempt to predict chronic healthcare events that are not fully understood. The resulting models often incorporate copious numbers of predictors derived across diverse datasets. This approach may yield desirable performance characteristics, but it sacrifices interpretability and portability. The Bootstrapped Ridge Selector (BoRidge) offers a tool to balance performance with interpretability. Compared to two modern feature selection methods, Bootstrapped LASSO regression (BoLASSO) and a minimal-redundancy-maximal-relevance selector (mRMR), the BoRidge bested them for binary classification on artificially generated data (sensitivity: 0.83, specificity:0.72) versus BoLASSO (sensitivity: 0.1, specificity:1) and mRMR (sensitivity: 0.69, specificity: 0.69). On a dataset used to validate a published suicide risk prediction model, the BoRidge selected an equally precise model to the publication, with far fewer predictors (114 versus the 1,538 used in the published model). The BoRidge has the potential to simplify classification models for complex problems, making them easier to translate and act upon.",2018,AMIA ... Annual Symposium proceedings. AMIA Symposium
Variable Selection for Functional Logistic Regression in fMRI Data Analysis,"ABS TRACT This study was motivated by classification problem in Functional Magnetic Resonance Imaging (fMRI), a noninvasive imaging technique which allows an experimenter to take images of a subjectâ€™s brain over time. As fMRI studies usually have a small number of subjects and we assume that there is a smooth, underlying curve describing the observations in fMRI data, this results in incredibly high-dimensional datasets that are functional in nature. High dimensionality is one of the biggest problems in statistical analysis of fMRI data. There is also a need for the development of better classification methods. One of the best things about fMRI technique is its noninvasiveness. If statistical classification methods are improved, it could aid the advancement of noninvasive diagnostic techniques for mental illness or even degenerative diseases such as Alzheimerâ€™s. In this paper, we develop a variable selection technique, which tackles high dimensionality and correlation problems in fMRI data, based on L1 regularization-group lasso for the functional logistic regression model where the response is binary and represent two separate classes; the predictors are functional. We assess our method with a simulation study and an application to a real fMRI dataset.",2015,Turkiye Klinikleri Journal of Biostatistics
High-throughput metabolite profiling: identification of plasma taurine as a potential biomarker of functional outcome after aneurysmal subarachnoid hemorrhage.,"OBJECTIVE
Metabolite profiling (or metabolomics) can identify candidate biomarkers for disease and potentially uncover new pathways for intervention. The goal of this study was to identify potential biomarkers of functional outcome after subarachnoid hemorrhage (SAH).


METHODS
The authors performed high-throughput metabolite profiling across a broad spectrum of chemical classes (163 metabolites) on plasma samples taken from 191 patients with SAH who presented to Massachusetts General Hospital between May 2011 and October 2016. Samples were drawn at 3 time points following ictus: 0-5, 6-10, and 11-14 days. Elastic net (EN) and LASSO (least absolute shrinkage and selection operator) machine learning analyses were performed to identify metabolites associated with 90-day functional outcomes as assessed by the modified Rankin Scale (mRS). Additional univariate and multivariate analyses were then conducted to further examine the relationship between metabolites and clinical variables and 90-day functional outcomes.


RESULTS
One hundred thirty-seven (71.7%) patients with aneurysmal SAH met the criteria for inclusion. A good functional outcome (mRS score 0-2) at 90 days was found in 79 (57.7%) patients. Patients with good outcomes were younger (p = 0.002), had lower admission Hunt and Hess grades (p < 0.0001) and modified Fisher grades (p < 0.0001), and did not develop hydrocephalus (p < 0.0001) or delayed cerebral ischemia (DCI) (p = 0.049). EN and LASSO machine learning methods identified taurine as the leading metabolite associated with 90-day functional outcome (p < 0.0001). Plasma concentrations of the amino acid taurine from samples collected between days 0 and 5 after aneurysmal SAH were 21.9% (p = 0.002) higher in patients with good versus poor outcomes. Logistic regression demonstrated that taurine remained a significant predictor of functional outcome (p = 0.013; OR 3.41, 95% CI 1.28-11.4), after adjusting for age, Hunt and Hess grade, modified Fisher grade, hydrocephalus, and DCI.


CONCLUSIONS
Elevated plasma taurine levels following aneurysmal SAH predict a good 90-day functional outcome. While experimental evidence in animals suggests that this effect may be mediated through downregulation of pro-inflammatory cytokines, additional studies are required to validate this hypothesis in humans.",2019,Journal of neurosurgery
Efficient Lasso training from a geometrical perspective,"The Lasso (L1-penalized regression) has drawn great interests in machine learning and statistics due to its robustness and high accuracy. A variety of methods have been proposed for solving the Lasso. But for large scale problems, the presence of L1 norm constraint significantly impedes the efficiency. Inspired by recent theoretical and practical contributions on the close relation between Lasso and SVMs, we reformulate the Lasso as a problem of finding the nearest point in a polytope to the origin, which circumvents the L1 norm constraint. This problem can be solved efficiently from a geometric perspective using the Wolfe's method. Comparing with least angle regression (LARS), which is a conventional method to solve Lasso, the proposed algorithm is advantageous in both efficiency and numerical stability. Experimental results show that the proposed approach is competitive with other state-of-the-art Lasso solvers on large scale problems.",2015,Neurocomputing
"Examining the Correlates of Sexually Transmitted Infection Testing Among Men Who Have Sex With Men in Ouagadougou and Bobo-Dioulasso, Burkina Faso.","BACKGROUND
Men who have sex with men (MSM) are a population at risk for HIV acquisition and transmission and other sexually transmitted infections (STIs). In Burkina Faso, the prevalence of HIV among MSM is higher than that of other reproductive-aged adults. Early and frequent STI testing and treatment can help prevent HIV acquisition and transmission and may improve linkage to care.


METHODS
A cross-sectional study used respondent-driven sampling of MSM in the urban centers of Ouagadougou and Bobo-Dioulasso, Burkina Faso, to complete a questionnaire and HIV and syphilis testing. The binary-dependent variable in these analyses was self-reported prior STI testing in the past 12 months. Independent variables included sociodemographic characteristics, sexual behaviors, and psychosocial factors, selected according to the modified social ecological model. Bivariate associations at the P<0.05 level were used to create a manual forward stepwise multivariable logistic regression.


RESULTS
Seventy-six percent of participants (511/672) did not test for STIs in the last 12 months. Testing for STIs was associated with STI symptoms (odds ratio [OR], 2.56; 95% confidence interval [95% CI], 1.39-4.76) and independently associated with depressive symptoms (adjusted OR, 1.49; 95% CI, 1.01-2.20) and discussing HIV and STIs with main male partners (adjusted OR, 1.73; 95% CI, 1.23-1.76).


CONCLUSIONS
These data suggest that periodic targeted STI screening for MSM in Burkina Faso may represent an important component of comprehensive HIV prevention programming. The relationship between depression and STI risks is well established, and these data further indicate that screening for depression may be warranted during these clinical encounters.",2016,Sexually transmitted diseases
Bayesian Regularized Quantile Regression Analysis Based on Asymmetric Laplace Distribution,"In recent years, variable selection based on penalty likelihood methods has aroused great concern. Based on the Gibbs sampling algorithm of asymmetric Laplace distribution, this paper considers the quantile regression with adaptive Lasso and Lasso penalty from a Bayesian point of view. Under the non-Bayesian and Bayesian framework, several regularization quantile regression methods are systematically compared for error terms with different distributions and heteroscedasticity. Under the error term of asymmetric Laplace distribution, statistical simulation results show that the Bayesian regularized quantile regression is superior to other distributions in all quantiles. And based on the asymmetric Laplace distribution, the Bayesian regularized quantile regression approach performs better than the non-Bayesian approach in parameter estimation and prediction. Through real data analyses, we also confirm the above conclusions.",2020,Journal of Applied Mathematics and Physics
Structured sparse model based feature selection and classification for hyperspectral imagery,"Sparse modeling is a powerful framework for data analysis and processing. It is especially useful for high-dimensional regression and classification problems in which a large number of feature variables exist but the amount of training samples is limited. In this paper, we address the problems of feature description, feature selection and classifier design for hyperspectral images using structured sparse models. A linear sparse logistic regression model is proposed to combine feature selection and pixel classification into a regularized optimization problem with the constraint of sparsity. To explore the structured features, three-dimensional discrete wavelet transform (3D-DWT) is employed, which processes the hyperspectral data cube as a whole tensor instead of adapting the data to a vector or matrix. This allows more effective capturing of the spatial and spectral structure. The structure of the 3D-DWT features is imposed on the sparse model by group LASSO which selects the features on the group level. The advantages of our method are validated on the real hyperspectral data.",2011,2011 IEEE International Geoscience and Remote Sensing Symposium
Class-imbalanced subsampling lasso algorithm for discovering adverse drug reactions,"Background All methods routinely used to generate safety signals from pharmacovigilance databases rely on disproportionality analyses of counts aggregating patientsâ€™ spontaneous reports. Recently, it was proposed to analyze individual spontaneous reports directly using Bayesian lasso logistic regressions. Nevertheless, this raises the issue of choosing an adequate regularization parameter in a variable selection framework while accounting for computational constraints due to the high dimension of the data. Purpose Our main objective is to propose a method, which exploits the subsampling idea from Stability Selection, a variable selection procedure combining subsampling with a high-dimensional selection algorithm, and adapts it to the specificities of the spontaneous reporting data, the latter being characterized by their large size, their binary nature and their sparsity. Materials and method Given the large imbalance existing between the presence and absence of a given adverse event, we propose an alternative subsampling scheme to that of Stability Selection resulting in an over-representation of the minority class and a drastic reduction in the number of observations in each subsample. Simulations are used to help define the detection threshold as regards the average proportion of false signals. They are also used to compare the performances of the proposed sampling scheme with that originally proposed for Stability Selection. Finally, we compare the proposed method to the gamma Poisson shrinker, a disproportionality method, and to a lasso logistic regression approach through an empirical study conducted on the French national pharmacovigilance database and two sets of reference signals. Results Simulations show that the proposed sampling strategy performs better in terms of false discoveries and is faster than the equiprobable sampling of Stability Selection. The empirical evaluation illustrates the better performances of the proposed method compared with gamma Poisson shrinker and the lasso in terms of number of reference signals retrieved.",2018,Statistical Methods in Medical Research
Multi-model ensemble for short-term traffic flow prediction under normal and abnormal conditions,"Accurate traffic flow prediction under abnormal conditions, such as accidents, adverse weather, work zones, and holidays, is significant for proactive traffic control. Here, the authors focus on a special challenge of how to develop robust responsive algorithms and prediction models for short-term traffic forecasting in different traffic conditions. To this end, this study presents an ensemble learning algorithm for the short-term traffic flow prediction via the integration of gradient boosting regression trees (GBRT) and the least absolute shrinkage and selection operator (Lasso). Four different model structures whether considering the feature selection are proposed and tested for multi-step-ahead prediction under both normal and abnormal conditions. The results indicate that the proposed multi-model ensemble models are superior to the benchmark algorithms, i.e., support vector regression, and random forests, the GBRT model outperforms the Lasso model under normal traffic conditions, and the Lasso model has a better prediction accuracy under abnormal traffic conditions. In addition, the Lasso model with the feature selection is superior to the full feature model under either normal or abnormal conditions, while the GBRT model is not always better under normal conditions. The proposed integration framework is general and flexible to assemble various traffic prediction algorithms.",2019,Iet Intelligent Transport Systems
Prediction of the onset of epileptic seizures from iEEG data CS 229 Final Report,"Seizures in epileptic patients can be anxiety-inducing, and the resulting medical and social issues can cause distress to a patient. A predictive mechanism to anticipate the onset of a seizure could help a patient prepare for a seizure and take medication. The literature has shown that the onset of a seizure is directly correlated with a distinct neurological change that can be detected using iEEG measurements. Thus the development of a classification model based on IEEG data can be created to aid epileptic patients. In this project we develop a feature set based on spectral and statistical features of the IEEG data, with and without ICA pre-processing of the data. Several classication algorithms were trained on the resulting data sets, including Naive-Bayes, SVM, logistic regression, and lasso-regularized logistic regression. It was found that the optimal results were given for regularized logistic regression on PIB features without ICA pre-processing, though generally all algorithms performed better than a random naive predictor model.",2014,
A Partial Correlation Screening Approach for Controlling the False Positive Rate in Sparse Gaussian Graphical Models,"Gaussian Graphical Models (GGMs) are extensively used in many research areas, such as genomics, proteomics, neuroimaging, and psychology, to study the partial correlation structure of a set of variables. This structure is visualized by drawing an undirected network, in which the variables constitute the nodes and the partial correlations the edges. In many applications, it makes sense to impose sparsity (i.e., some of the partial correlations are forced to zero) as sparsity is theoretically meaningful and/or because it improves the predictive accuracy of the fitted model. However, as we will show by means of extensive simulations, state-of-the-art estimation approaches for imposing sparsity on GGMs, such as the Graphical lasso, â„“1 regularized nodewise regression, and joint sparse regression, fall short because they often yield too many false positives (i.e., partial correlations that are not properly set to zero). In this paper we present a new estimation approach that allows to control the false positive rate better. Our approach consists of two steps: First, we estimate an undirected network using one of the three state-of-the-art estimation approaches. Second, we try to detect the false positives, by flagging the partial correlations that are smaller in absolute value than a given threshold, which is determined through cross-validation; the flagged correlations are set to zero. Applying this new approach to the same simulated data, shows that it indeed performs better. We also illustrate our approach by using it to estimate (1) a gene regulatory network for breast cancer data, (2) a symptom network of patients with a diagnosis within the nonaffective psychotic spectrum and (3) a symptom network of patients with PTSD.",2019,Scientific Reports
Self-adaptive Lasso and its Bayesian Estimation,"In this paper, we proposed a self-adaptive lasso method for variable selection in regression problems. Unlike the popular lasso method, the proposed method introduces a specific tuning parameter for each regression coefficient. We modeled self-adaptive lasso in a Bayesian framework and developed an efficient Gibbs sampling algorithm to automatically select these tuning parameters and estimate the parameters. This algorithm also brings in some convenience for conducting statistical inference for selected variables. Several synthetic and real examples in this paper demonstrate flexibility of the tuning parameters enhance the performance of self-adaptive lasso in terms of both prediction and variable selection. Finally, we also extend the self-adaptive lasso to account for elastic net and fused lasso.",2010,
Gully erosion spatial modelling: Role of machine learning algorithms in selection of the best controlling factors and modelling process,"Abstract This investigation assessed the efficacy of 10 widely used machine learning algorithms (MLA) comprising the least absolute shrinkage and selection operator (LASSO), generalized linear model (GLM), stepwise generalized linear model (SGLM), elastic net (ENET), partial least square (PLS), ridge regression, support vector machine (SVM), classification and regression trees (CART), bagged CART, and random forest (RF) for gully erosion susceptibility mapping (GESM) in Iran. The location of 462 previously existing gully erosion sites were mapped through widespread field investigations, of which 70% (323) and 30% (139) of observations were arbitrarily divided for algorithm calibration and validation. Twelve controlling factors for gully erosion, namely, soil texture, annual mean rainfall, digital elevation model (DEM), drainage density, slope, lithology, topographic wetness index (TWI), distance from rivers, aspect, distance from roads, plan curvature, and profile curvature were ranked in terms of their importance using each MLA. The MLA were compared using a training dataset for gully erosion and statistical measures such as RMSE (root mean square error), MAE (mean absolute error), and R-squared. Based on the comparisons among MLA, the RF algorithm exhibited the minimum RMSE and MAE and the maximum value of R-squared, and was therefore selected as the best model. The variable importance evaluation using the RF model revealed that distance from rivers had the highest significance in influencing the occurrence of gully erosion whereas plan curvature had the least importance. According to the GESM generated using RF, most of the study area is predicted to have a low (53.72%) or moderate (29.65%) susceptibility to gully erosion, whereas only a small area is identified to have a high (12.56%) or very high (4.07%) susceptibility. The outcome generated by RF model is validated using the ROC (Receiver Operating Characteristics) curve approach, which returned an area under the curve (AUC) of 0.985, proving the excellent forecasting ability of the model. The GESM prepared using the RF algorithm can aid decision-makers in targeting remedial actions for minimizing the damage caused by gully erosion.",2020,Geoscience frontiers
Distributionally Robust Optimization and its Applications in Machine Learning,"Distributionally Robust Optimization and its Applications in Machine Learning Yang Kang The goal of Distributionally Robust Optimization (DRO) is to minimize the cost of running a stochastic system, under the assumption that an adversary can replace the underlying baseline stochastic model by another model within a family known as the distributional uncertainty region. This dissertation focuses on a class of DRO problems which are data-driven, which generally speaking means that the baseline stochastic model corresponds to the empirical distribution of a given sample. One of the main contributions of this dissertation is to show that the class of data-driven DRO problems that we study unify many successful machine learning algorithms, including square root Lasso, support vector machines, and generalized logistic regression, among others. A key distinctive feature of the class of DRO problems that we consider here is that our distributional uncertainty region is based on optimal transport costs. In contrast, most of the DRO formulations that exist to date take advantage of a likelihood based formulation (such as Kullback-Leibler divergence, among others). Optimal transport costs include as a special case the so-called Wasserstein distance, which is popular in various statistical applications. The use of optimal transport costs is advantageous relative to the use of divergencebased formulations because the region of distributional uncertainty contains distributions which explore samples outside of the support of the empirical measure, therefore explaining why many machine learning algorithms have the ability to improve generalization. Moreover, the DRO representations that we use to unify the previously mentioned machine learning algorithms, provide a clear interpretation of the so-called regularization parameter, which is known to play a crucial role in controlling generalization error. As we establish, the regularization parameter corresponds exactly to the size of the distributional uncertainty region. Another contribution of this dissertation is the development of statistical methodology to study data-driven DRO formulations based on optimal transport costs. Using this theory, for example, we provide a sharp characterization of the optimal selection of regularization parameters in machine learning settings such as square-root Lasso and regularized logistic regression. Our statistical methodology relies on the construction of a key object which we call the robust Wasserstein profile function (RWP function). The RWP function similar in spirit to the empirical likelihood profile function in the context of empirical likelihood (EL). But the asymptotic analysis of the RWP function is different because of a certain lack of smoothness which arises in a suitable Lagrangian formulation. Optimal transport costs have many advantages in terms of statistical modeling. For example, we show how to define a class of novel semi-supervised learning estimators which are natural companions of the standard supervised counterparts (such as square root Lasso, support vector machines, and logistic regression). We also show how to define the distributional uncertainty region in a purely data-driven way. Precisely, the optimal transport formulation allows us to inform the shape of the distributional uncertainty, not only its center (which given by the empirical distribution). This shape is informed by establishing connections to the metric learning literature. We develop a class of metric learning algorithms which are based on robust optimization. We use the robust-optimization-based metric learning algorithms to inform the distributional uncertainty region in our data-driven DRO problem. This means that we endow the adversary with additional which force him to spend effort on regions of importance to further improve generalization properties of machine learning algorithms. In summary, we explain how the use of optimal transport costs allow constructing what we call double-robust statistical procedures. We test all of the procedures proposed in this paper in various data sets, showing significant improvement in generalization ability over a wide range of state-of-the-art procedures. Finally, we also discuss a class of stochastic optimization algorithms of independent interest which are particularly useful to solve DRO problems, especially those which arise when the distributional uncertainty region is based on optimal transport costs.",2017,
DiffGRN: differential gene regulatory network analysis.,"Identification of differential gene regulators with significant changes under disparate conditions is essential to understand complex biological mechanism in a disease. Differential Network Analysis (DiNA) examines different biological processes based on gene regulatory networks that represent regulatory interactions between genes with a graph model. While most studies in DiNA have considered correlation-based inference to construct gene regulatory networks from gene expression data due to its intuitive representation and simple implementation, the approach lacks in the representation of causal effects and multivariate effects between genes. In this paper, we propose an approach named Differential Gene Regulatory Network (DiffGRN) that infers differential gene regulation between two groups. We infer gene regulatory networks of two groups using Random LASSO, and then we identify differential gene regulations by the proposed significance test. The advantages of DiffGRN are to capture multivariate effects of genes that regulate a gene simultaneously, to identify causality of gene regulations, and to discover differential gene regulators between regression-based gene regulatory networks. We assessed DiffGRN by simulation experiments and showed its outstanding performance than the current state-of-the-art correlation-based method, DINGO. DiffGRN is applied to gene expression data in asthma. The DiNA with asthma data showed a number of gene regulations, such as ADAM12 and RELB, reported in biological literature.",2018,International journal of data mining and bioinformatics
Evaluation of DysLexML ; A Screening Tool for Dyslexia Using Machine Learning,"Eye movements during text reading can provide insights about reading disorders. Via eye-trackers, we can measure when, where and how eyes move with relation to the words they read. Machine Learning (ML) algorithms can decode this information and provide differential analysis. In our earlier work 1 , we developed DysLexML, a screening tool for developmental dyslexia that applies various ML algorithms to analyze fixation points recorded via eye-tracking during silent reading of children. We had evaluated its performance using measurements collected in the first systematic field study with 69 native Greek speakers, children, 32 of which were diagnosed as dyslexic by the official governmental agency for diagnosing learning and reading difficulties in Greece. In that field study, the measurements were collected using a custom-made eye-tracker developed by Medotics AG 2 . Here, we evaluate our system using a larger dataset from the second field study which consists of 135 children, 62 of which were diagnosed as dyslexic. In both works, we examined a large set of features based on statistical properties of fixations and saccadic movements and identified the ones with prominent predictive power, performing dimensionality reduction. I. DYSLEXML The main modules of the DysLexML algorithm include the feature extraction, the feature selection for identifying the dominant features, and its classifiers that employ these dominant features. DysLexML extracts general (non-wordspecific) features and word-specific ones that take into account the word the subject is looking at. Examples of non-word specific features are the number of fixations on the screen, mean and median duration of fixations and related to saccades, the mean and median length of saccades, i.e., the Euclidean distance between consecutive fixations, and characterization of the types of eye movements. DysLexML creates a feature vector of 35 features in total. DysLexML consists of two phases: It first employs the LASSO Regression five-fold crossvalidation to identify the dominant features. Based on the dominant features, it applies various classification algorithms. Â¶Contact author: Maria Papadopouli (mgp@ics.forth.gr) 1T. Asvestopoulou, V. Manousaki, A. Psistakis, I. Smyrnakis, V. Andreadakis, I. Aslanides, M. Papadopouli, â€DysLexML: Screening Tool for Dyslexia Using Machine Learningâ€ submitted at Eusipco, 2019 2I. Smyrnakis, V. Andreadakis, V. Selimis, M. Kalaitzakis, T. Bachourou, G. Kaloutsakis, G. D. Kymionis, S. Smirnakis, I. M. Aslanides, RADAR: A novel fast-screening method for reading difficulties with special focus on dyslexia., PLoS ONE, 2017. II. PERFORMANCE ANALYSIS Field Study: A commercial tracker was employed for the acquisition of data. The model of the eye tracker was the Tobii 4C eye tracker, and was designed and manufactured by Tobii AB. This tracker has 90Hz frequency, whether the tracker in our previous study was 60Hz. In addition, the use of chin rest is not necessary anymore, hence the participants can freely move their head making the procedure completely noninvasive. Moreover, this study is much larger than the previous one, almost double in participants number. We have acquired data from 135 participants in total, 73 of them classified as typical readers and 62 of them classified as dyslexics. All the participants were native Greek speakers and the age span was from 7 years old to 17 years old. Fig. 1. Reading â€pathâ€ from a typical reader (top) and from a reader with dyslexia (bottom). The blue circles are the fixations and the orange lines the saccadic movements. The larger the circle, the longer the fixation. III. CONCLUSION Our system achieves its best performance using linear SVM model, with an accuracy of 97% for the small dataset and 74% using the K-means algorithm for the larger dataset. This performance is achieved over a small feature set, namely saccade length, number of short forward movements, and number of multiply fixated words. Furthermore, we analyzed the impact of noise on the fixation positions and showed that DysLexML is accurate and robust in the presence of noise. These encouraging results set the basis for developing screening tools in less controlled, larger-scale environments, with inexpensive eye-trackers, potentially reaching a larger population for early intervention.",2019,
Gap Safe screening rules for sparsity enforcing penalties,"In high dimensional regression settings, sparsity enforcing penalties have proved useful to regularize the data-fitting term. A recently introduced technique called screening rules propose to ignore some variables in the optimization leveraging the expected sparsity of the solutions and consequently leading to faster solvers. When the procedure is guaranteed not to discard variables wrongly the rules are said to be safe. In this work, we propose a unifying framework for generalized linear models regularized with standard sparsity enforcing penalties such as $\ell_1$ or $\ell_1/\ell_2$ norms. Our technique allows to discard safely more variables than previously considered safe rules, particularly for low regularization parameters. Our proposed Gap Safe rules (so called because they rely on duality gap computation) can cope with any iterative solver but are particularly well suited to (block) coordinate descent methods. Applied to many standard learning tasks, Lasso, Sparse-Group Lasso, multi-task Lasso, binary and multinomial logistic regression, etc., we report significant speed-ups compared to previously proposed safe rules on all tested data sets.",2017,J. Mach. Learn. Res.
Construction and Validation of a 9-Gene Signature for Predicting Prognosis in Stage III Clear Cell Renal Cell Carcinoma,"Purpose: Aim of this study was to develop a multi-gene signature to help better predict prognosis for stage III renal cell carcinoma (RCC) patients. Methods: Fourteen pairs of stage III tumor and normal tissues mRNA expression data from GSE53757 and 16 pairs mRNA expression data from TCGA clear cell RCC database were used to analyze differentially expressed genes between tumor and normal tissues. Common different expressed genes in both datasets were used for further modeling. Lasso Cox regression analysis was performed to select and build prognostic multi-gene signature in TCGA stage III kidney cancer patients (N = 122). Then, the multi-gene signature was validated in stage III renal cancer cases in Fudan University Shanghai Cancer Center (N = 77). C-index and time-dependent ROC were used to test the efficiency of this signature in predicting overall survival. Results: In total, 1,370 common different expressed genes were found between tumor and normal tissues in both datasets. After Lasso Cox modeling, nine mRNAs were finally identified to build a classifier. Using this classifier, we could classify stage III clear cell RCC patients into high-risk group and low-risk group. Prognosis was significantly different between these groups in discovery TCGA cohort, validation FUSCC cohort and entire set (All P < 0.001). Multivariate cox regression in entire set (N = 199) revealed that risk group classified by 9-gene signature, age of diagnosis, pN stage and ISUP grade were independent prognostic factor of overall survival in stage III kidney cancer patients. Conclusion: We developed a robust multi-gene classifier that can effectively classify stage III RCC patients into groups with low and high risk of poor prognosis. This signature may help select high-risk patients who require more aggressive adjuvant target therapy or immune therapy.",2019,Frontiers in Oncology
Statistical approaches to detect epistasis in genome wide association studies,"De nombreux travaux de recherche portent sur la detection et lâ€™etude des interactions dans les etudes dâ€™association pangenomique (GWAS). La plupart des methodes proposees se concentrent principalement sur les interactions entre polymorphismes simples de lâ€™ADN (SNPs), mais des strategies de regroupement peuvent egalement etre envisagees.Dans cette these, nous developpons une approche originale pour la detection des interactions a lâ€™echelle des genes. De nouvelles variables representant les interactions entre deux genes sont definies a lâ€™aide de methodes de reduction de dimension. Ainsi, toutes les informations apportees par les marqueurs genetiques sont resumees au niveau du gene. Ces nouvelles variables dâ€™interaction sont ensuite introduites dans un modele de regression. La selection des effets significatifs est realisee a lâ€™aide dâ€™une methode de regression penalisee basee sur le Group LASSO avec controle du taux de fausse decouvertes.Nous comparons les differentes methodes de modelisation des variables dâ€™interaction a travers des etudes de simulations afin de montrer les bonnes performances de notre approche. Enfin, nous illustrons son utilisation pratique pour identifier des interactions entre genes en analysant deux jeux de donnees reelles.",2017,
Variable selection in the cox regression model with covariates missing at random.,"We consider variable selection in the Cox regression model (Cox, 1975, Biometrika 362, 269-276) with covariates missing at random. We investigate the smoothly clipped absolute deviation penalty and adaptive least absolute shrinkage and selection operator (LASSO) penalty, and propose a unified model selection and estimation procedure. A computationally attractive algorithm is developed, which simultaneously optimizes the penalized likelihood function and penalty parameters. We also optimize a model selection criterion, called the IC(Q) statistic (Ibrahim, Zhu, and Tang, 2008, Journal of the American Statistical Association 103, 1648-1658), to estimate the penalty parameters and show that it consistently selects all important covariates. Simulations are performed to evaluate the finite sample performance of the penalty estimates. Also, two lung cancer data sets are analyzed to demonstrate the proposed methodology.",2010,Biometrics
Regularizers for structured sparsity,"We study the problem of learning a sparse linear regression vector under additional conditions on the structure of its sparsity pattern. This problem is relevant in machine learning, statistics and signal processing. It is well known that a linear regression can benefit from knowledge that the underlying regression vector is sparse. The combinatorial problem of selecting the nonzero components of this vector can be â€œrelaxedâ€ by regularizing the squared error with a convex penalty function like the â„“1 norm. However, in many applications, additional conditions on the structure of the regression vector and its sparsity pattern are available. Incorporating this information into the learning method may lead to a significant decrease of the estimation error. In this paper, we present a family of convex penalty functions, which encode prior knowledge on the structure of the vector formed by the absolute values of the regression coefficients. This family subsumes the â„“1 norm and is flexible enough to include different models of sparsity patterns, which are of practical and theoretical importance. We establish the basic properties of these penalty functions and discuss some examples where they can be computed explicitly. Moreover, we present a convergent optimization algorithm for solving regularized least squares with these penalty functions. Numerical simulations highlight the benefit of structured sparsity and the advantage offered by our approach over the Lasso method and other related methods.",2013,Advances in Computational Mathematics
