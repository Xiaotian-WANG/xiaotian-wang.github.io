title,abstract,year,journal
Bayesian Variable Selection Regression of Multivariate Responses for Group Data,"We propose two multivariate extensions of the Bayesian group lasso for variable selection and estimation for data with high dimensional predictors and multi-dimensional response variables. The methods utilize spike and slab priors to yield solutions which are sparse at either a group level or both a group and individual feature level. The incorporation of group structure in a predictor matrix is a key factor in obtaining better estimators and identifying associations between multiple responses and predictors. The approach is suited to many biological studies where the response is multivariate and each predictor is embedded in some biological grouping structure such as gene pathways. Our Bayesian models are connected with penalized regression, and we prove both oracle and asymptotic distribution properties under an orthogonal design. We derive efficient Gibbs sampling algorithms for our models and provide the implementation in a comprehensive R package called MBSGS available on the Comprehensive R Archive Network (CRAN). The performance of the proposed approaches is compared to state-of-the-art variable selection strategies on simulated data sets. The proposed methodology is illustrated on a genetic dataset in order to identify markers grouping across chromosomes that explain the joint variability of gene expression in multiple tissues.",2017,Bayesian Analysis
Essays in High Dimensional Time Series Analysis,"Essays in High Dimensional Time Series Analysis Kashif Yousuf Due to the rapid improvements in the information technology, high dimensional time series datasets are frequently encountered in a variety of fields such as macroeconomics, finance, neuroscience, and meteorology. Some examples in economics and finance include forecasting low frequency macroeconomic indicators, such as GDP or inflation rate, or financial asset returns using a large number of macroeconomic and financial time series and their lags as possible covariates. In these settings, the number of candidate predictors (pT ) can be much larger than the number of samples (T ), and accurate estimation and prediction is made possible by relying on some form of dimension reduction. Given this ubiquity of time series data, it is surprising that few works on high dimensional statistics discuss the time series setting, and even fewer works have developed methods which utilize the unique features of time series data. This chapter consists of three chapters, and each one is self contained. The first chapter deals with high dimensional predictive regressions which are widely used in economics and finance. However, the theory and methodology is mainly developed assuming that the model is stationary with time invariant parameters. This is at odds with the prevalent evidence for parameter instability in economic time series. To remedy this, we present two L2 boosting algorithms for estimating high dimensional models in which the coefficients are modeled as functions evolving smoothly over time and the predictors are locally stationary. The first method uses componentwise local constant estimators as base learner, while the second relies on componentwise local linear estimators. We establish consistency of both methods, and address the practical issues of choosing the bandwidth for the base learners and the number of boosting iterations. In an extensive application to macroeconomic forecasting with many potential predictors, we find that the benefits to modeling time variation are substantial and are present across a wide range of economic series. Furthermore, these benefits increase with the forecast horizon and with the length of the time series available for estimation. This chapter is jointly written with Serena Ng. The second chapter deals with high dimensional non-linear time series models, and deals with the topic of variable screening/targeting predictors. Rather than assume a specific parametric model a priori, this chapter introduces several model free screening methods based on the partial distance correlation and developed specifically to deal with time dependent data. Methods are developed both for univariate models, such as nonlinear autoregressive models with exogenous predictors (NARX), and multivariate models such as linear or nonlinear VAR models. Sure screening properties are proved for our methods, which depend on the moment conditions, and the strength of dependence in the response and covariate processes, amongst other factors. Finite sample performance of our methods is shown through extensive simulation studies, and we show the effectiveness of our algorithms at forecasting US market returns. This chapter is jointly written with Yang Feng. The third chapter deals with variable selection for high dimensional linear stationary time series models. This chapter analyzes the theoretical properties of Sure Independence Screening (SIS), and its two stage combination with the adaptive Lasso, for high dimensional linear models with dependent and/or heavy tailed covariates and errors. We also introduce a generalized least squares screening (GLSS) procedure which utilizes the serial correlation present in the data. By utilizing this serial correlation when estimating our marginal effects, GLSS is shown to outperform SIS in many cases. For both procedures we prove two stage variable selection consistency when combined with the adaptive Lasso.",2019,
Risk estimation for high-dimensional lasso regression,"In high-dimensional estimation, analysts are faced with more parameters $p$ than available observations $n$, and asymptotic analysis of performance allows the ratio $p/n\rightarrow \infty$. This situation makes regularization both necessary and desirable in order for estimators to possess theoretical guarantees. However, the amount of regularization, often determined by one or more tuning parameters, is integral to achieving good performance. In practice, choosing the tuning parameter is done through resampling methods (e.g. cross-validation), generalized information criteria, or reformulating the optimization problem (e.g. square-root lasso or scaled sparse regression). Each of these techniques comes with varying levels of theoretical guarantee for the low- or high-dimensional regimes. However, there are some notable deficiencies in the literature. The theory, and sometimes practice, of many methods relies on either the knowledge or estimation of the variance parameter, which is difficult to estimate in high dimensions. In this paper, we provide theoretical intuition suggesting that some previously proposed approaches based on information criteria work poorly in high dimensions. We introduce a suite of new risk estimators leveraging the burgeoning literature on high-dimensional variance estimation. Finally, we compare our proposal to many existing methods for choosing the tuning parameters for lasso regression by providing an extensive simulation to examine their finite sample performance. We find that our new estimators perform quite well, often better than the existing approaches across a wide range of simulation conditions and evaluation criteria.",2016,arXiv: Methodology
Prediction of treatment benefit in high-dimensional cox models via gene signatures in randomized clinical trials,"Methods We investigated four approaches: penalize biomarker main effects and biomarker-by-treatment interactions using a lasso penalty (full-lasso); control of main effects by principal components or ridge penalty, and lasso on interactions (sPCA+lasso or ridge+lasso); and â€˜modified covariatesâ€™ in a penalized regression model (Tian et al. 2014). We performed simulations under null and alternative scenarios by varying the sample size n, number of biomarkers H, number of true main effects or treatment-modifiers, effect sizes and correlations. We proposed two novel measures of treatment effect prediction for gene signatures: a difference in C-indices and a Wald-based interaction statistic. We used gene expression data from a RCT of adjuvant chemotherapy in non-small cell lung cancer (n=133) for illustration.",2015,Trials
Generalization Error Minimization: A New Approach to Model Evaluation and Selection with an Application to Penalized Regression,"We study model evaluation and model selection from the perspective of generalization ability (GA): the ability of a model to predict outcomes in new samples from the same population. We believe that GA is one way formally to address concerns about the external validity of a model. The GA of a model estimated on a sample can be measured by its empirical out-of-sample errors, called the generalization errors (GE). We derive upper bounds for the GE, which depend on sample sizes, model complexity and the distribution of the loss function. The upper bounds can be used to evaluate the GA of a model, ex ante. We propose using generalization error minimization (GEM) as a framework for model selection. Using GEM, we are able to unify a big class of penalized regression estimators, including lasso, ridge and bridge, under the same set of assumptions. We establish finite-sample and asymptotic properties (including $\mathcal{L}_2$-consistency) of the GEM estimator for both the $n \geqslant p$ and the $n",2016,arXiv: Machine Learning
Integrating satellite and climate data to predict wheat yield in Australia using machine learning approaches,"Abstract Wheat is the most important staple crop grown in Australia, and Australia is one of the top wheat exporting countries globally. Timely and reliable wheat yield prediction in Australia is important for regional and global food security. Prior studies use either climate data, or satellite data, or a combination of these two to build empirical models to predict crop yield. However, though the performance of yield prediction using empirical methods is improved by combining the use of climate and satellite data, the contributions from different data sources are still not clear. In addition, how the regression-based methods compare with various machine-learning based methods in their performance in yield prediction is also not well understood and needs in-depth investigation. This work integrated various sources of data to predict wheat yield across Australia from 2000 to 2014 at the statistical division (SD) level. We adopted a well-known regression method (LASSO, as a benchmark) and three mainstream machine learning methods (support vector machine, random forest, and neural network) to build various empirical models for yield prediction. For satellite data, we used the enhanced vegetation index (EVI) from MODIS and solar-induced chlorophyll fluorescence (SIF) from GOME-2 and SCIAMACHY as metrics to approximate crop productivity. The machine-learning based methods outperform the regression method in modeling crop yield. Our results confirm that combining climate and satellite data can achieve high performance of yield prediction at the SD level (R2 Ëœ 0.75). The satellite data track crop growth condition and gradually capture the variability of yield evolving with the growing season, and their contributions to yield prediction usually saturate at the peak of the growing season. Climate data provide extra and unique information beyond what the satellite data have offered for yield prediction, and our empirical modeling work shows the added values of climate variables exist across the whole season, not only at some certain stages. We also find that using EVI as an input can achieve better performance in yield prediction than SIF, primarily due to the large noise in the satellite-based SIF data (i.e. coarse resolution in both space and time). In addition, we also explored the potential for timely wheat yield prediction in Australia, and we can achieve the optimal prediction performance with approximately two-month lead time before wheat maturity. The proposed methodology in this paper can be extended to different crops and different regions for crop yield prediction.",2019,Agricultural and Forest Meteorology
Internet-Based Motivation Program for Women With Eating Disorders: Eating Disorder Pathology and Depressive Mood Predict Dropout,"BACKGROUND
One of the main problems of Internet-delivered interventions for a range of disorders is the high dropout rate, yet little is known about the factors associated with this. We recently developed and tested a Web-based 6-session program to enhance motivation to change for women with anorexia nervosa, bulimia nervosa, or related subthreshold eating pathology.


OBJECTIVE
The aim of the present study was to identify predictors of dropout from this Web program.


METHODS
A total of 179 women took part in the study. We used survival analyses (Cox regression) to investigate the predictive effect of eating disorder pathology (assessed by the Eating Disorders Examination-Questionnaire; EDE-Q), depressive mood (Hopkins Symptom Checklist), motivation to change (University of Rhode Island Change Assessment Scale; URICA), and participants' age at dropout. To identify predictors, we used the least absolute shrinkage and selection operator (LASSO) method.


RESULTS
The dropout rate was 50.8% (91/179) and was equally distributed across the 6 treatment sessions. The LASSO analysis revealed that higher scores on the Shape Concerns subscale of the EDE-Q, a higher frequency of binge eating episodes and vomiting, as well as higher depression scores significantly increased the probability of dropout. However, we did not find any effect of the URICA or age on dropout.


CONCLUSIONS
Women with more severe eating disorder pathology and depressive mood had a higher likelihood of dropping out from a Web-based motivational enhancement program. Interventions such as ours need to address the specific needs of women with more severe eating disorder pathology and depressive mood and offer them additional support to prevent them from prematurely discontinuing treatment.",2014,Journal of Medical Internet Research
A note on shrinkage sliced inverse regression,"We employ Lasso shrinkage within the context of sufficient dimension reduction to obtain a shrinkage sliced inverse regression estimator, which provides easier interpretations and better prediction accuracy without assuming a parametric model. The shrinkage sliced inverse regression approach can be employed for both single-index and multiple-index models. Simulation studies suggest that the new estimator performs well when its tuning parameter is selected by either the Bayesian information criterion or the residual information criterion. Copyright 2005, Oxford University Press.",2005,Biometrika
Wavelet-based scalar-on-function finite mixture regression models,"Classical finite mixture regression is useful for modeling the relationship between scalar predictors and scalar responses arising from subpopulations defined by the di ering associations between those predictors and responses. The classical finite mixture regression model is extended to incorporate functional predictors by taking a wavelet-based approach in which both the functional predictors and the component-specific coefficient functions are represented in terms of an appropriate wavelet basis. By using the wavelet representation of the model, the coefficients corresponding to the functional covariates become the predictors. In this setting, there are typically many more predictors than observations. Hence a lasso-type penalization is employed to simultaneously perform feature selection and estimation. Specification of the model is discussed and a fitting algorithm is provided. The wavelet-based approach is evaluated on synthetic data as well as applied to a real data set from a study of the relationship between cognitive ability and di usion tensor imaging measures in subjects with multiple sclerosis.",2016,Computational statistics & data analysis
associaÃ§Ã£o de ascaris lumbricoides com a asma e sua distribuiÃ§Ã£o espacial no bairro do pedregal â€“ campina grande â€“ pb,"Ascaris lumbricoides ASSOCIATION WITH ASTHMA AND ITS SPATIAL DISTRIBUTION IN THE NEIGHBORHOOD OF PEDREGAL CAMPINA GRANDE PB OBJECTIVE: Studying the association between asthma and ascariasis and its spatial distribution in children 2-10 years of age, in the neighborhood of Pedregal Campina Grande PB METHODS: Cross-sectional study between January and November 2007. 1004 questionnaires were administered standard International Study of Asthma And Allergy in Childwood (ISAAC), and delivered the container to collect fecal material. In search of parasites in faeces was used the method of Ritchie, for calculating the parasite load was used Kato-Katz method. Was taken for geographical position of households with a GARMIN GPS device. We used the t test and chi-square (ï£) Pearson and Linear Trend and Logistic Regression, with Odds Ratios (OR) and Confidence Interval (CI). To analyze the spatial database and the geographic coordinates were organized in the program ArcGIS 9.3, was defined a bandwidth of 50 meters and regular grid consisting of 5 x 5 cells. RESULTS: Association of light and heavy parasite loads were significant for all symptoms of asthma (p <0.05). Significant associations were also found between infected and asthma to age, household income, and holders with co infection A. lumbricoides and T. trichiura (p <0.05). The density of the Kernelassociated infection by A. lumbricoides with asthma, showed that there is no homogeneity in the cases distribution, that the clusters tend to focus on the higher areas of the neighborhood, relatively distant from the trench open sewer that runs through the neighborhood. Logistic regression was appropriate to identify the predictors of asthma. CONCLUSION: The low parasite load of infection by A. lumbricoides appeared as a protective factor for asthma and its symptoms while high parasite load, characterized as a risk factor. Analyses Kernel (density and hazard ratio) indicated the locations of highest concentration of contamination by A. lumbricoides, and logistic regression identified the independent variables were statistically significant for the risk of asthma.",2013,
A Systematic Evaluation of Feature Selection and Classification Algorithms Using Simulated and Real miRNA Sequencing Data,"Sequencing is widely used to discover associations between microRNAs (miRNAs) and diseases. However, the negative binomial distribution (NB) and high dimensionality of data obtained using sequencing can lead to low-power results and low reproducibility. Several statistical learning algorithms have been proposed to address sequencing data, and although evaluation of these methods is essential, such studies are relatively rare. The performance of seven feature selection (FS) algorithms, including baySeq, DESeq, edgeR, the rank sum test, lasso, particle swarm optimistic decision tree, and random forest (RF), was compared by simulation under different conditions based on the difference of the mean, the dispersion parameter of the NB, and the signal to noise ratio. Real data were used to evaluate the performance of RF, logistic regression, and support vector machine. Based on the simulation and real data, we discuss the behaviour of the FS and classification algorithms. The Apriori algorithm identified frequent item sets (mir-133a, mir-133b, mir-183, mir-937, and mir-96) from among the deregulated miRNAs of six datasets from The Cancer Genomics Atlas. Taking these findings altogether and considering computational memory requirements, we propose a strategy that combines edgeR and DESeq for large sample sizes.",2015,Computational and Mathematical Methods in Medicine
Spatio Temporal EEG Source Imaging with the Hierarchical Bayesian Elastic Net and Elitist Lasso Models,"The estimation of EEG generating sources constitutes an Inverse Problem (IP) in Neuroscience. This is an ill-posed problem due to the non-uniqueness of the solution and regularization or prior information is needed to undertake Electrophysiology Source Imaging. Structured Sparsity priors can be attained through combinations of (L1 norm-based) and (L2 norm-based) constraints such as the Elastic Net (ENET) and Elitist Lasso (ELASSO) models. The former model is used to find solutions with a small number of smooth nonzero patches, while the latter imposes different degrees of sparsity simultaneously along different dimensions of the spatio-temporal matrix solutions. Both models have been addressed within the penalized regression approach, where the regularization parameters are selected heuristically, leading usually to non-optimal and computationally expensive solutions. The existing Bayesian formulation of ENET allows hyperparameter learning, but using the computationally intensive Monte Carlo/Expectation Maximization methods, which makes impractical its application to the EEG IP. While the ELASSO have not been considered before into the Bayesian context. In this work, we attempt to solve the EEG IP using a Bayesian framework for ENET and ELASSO models. We propose a Structured Sparse Bayesian Learning algorithm based on combining the Empirical Bayes and the iterative coordinate descent procedures to estimate both the parameters and hyperparameters. Using realistic simulations and avoiding the inverse crime we illustrate that our methods are able to recover complicated source setups more accurately and with a more robust estimation of the hyperparameters and behavior under different sparsity scenarios than classical LORETA, ENET and LASSO Fusion solutions. We also solve the EEG IP using data from a visual attention experiment, finding more interpretable neurophysiological patterns with our methods. The Matlab codes used in this work, including Simulations, Methods, Quality Measures and Visualization Routines are freely available in a public website.",2017,Frontiers in Neuroscience
Statistical Guarantee for Non-Convex Optimization,"The aim of this thesis is to systematically study the statistical guarantee for two
representative non-convex optimization problems arsing in the statistics community.
The first one is the high-dimensional Gaussian mixture model, which is motivated by
the estimation of multiple graphical models arising from heterogeneous observations.
The second one is the low-rank tensor estimation model, which is motivated by
high-dimensional interaction model. Both optimal statistical rates and numerical
comparisons are studied in depth.
In the first part of my thesis, we consider joint estimation of multiple graphical
models arising from heterogeneous and high-dimensional observations. Unlike most
previous approaches which assume that the cluster structure is given in advance, an
appealing feature of our method is to learn cluster structure while estimating heterogeneous graphical models. This is achieved via a high dimensional version of Expectation
Conditional Maximization (ECM) algorithm. A joint graphical lasso penalty is
imposed on the conditional maximization step to extract both homogeneity and heterogeneity components across all clusters. Our algorithm is computationally efficient
due to fast sparse learning routines and can be implemented without unsupervised
learning knowledge. The superior performance of our method is demonstrated by extensive experiments and its application to a Glioblastoma cancer dataset reveals some
new insights in understanding the Glioblastoma cancer. In theory, a non-asymptotic
error bound is established for the output directly from our high dimensional ECM
algorithm, and it consists of two quantities: statistical error (statistical accuracy) and optimization error (computational complexity). Such a result gives a theoretical
guideline in terminating our ECM iterations.
In the second part of my thesis, we propose a general framework for sparse and low-rank tensor estimation from cubic sketchings. A two-stage non-convex implementation
is developed based on sparse tensor decomposition and thresholded gradient descent,
which ensures exact recovery in the noiseless case and stable recovery in the noisy
case with high probability. The non-asymptotic analysis sheds light on an interplay
between optimization error and statistical error. The proposed procedure is shown to
be rate-optimal under certain conditions. As a technical by-product, novel high-order
concentration inequalities are derived for studying high-moment sub-Gaussian tensors.
An interesting tensor formulation illustrates the potential application to high-order
interaction pursuit in high-dimensional linear regression",2019,
"Toward Data-driven, Semi-automatic Inference of Phenomenological Physical Models: Application to Eastern Sahel Rainfall","First-principles based predictive understanding of complex, dynamic physical phenomena, such as regional precipitation or hurricane intensity and frequency, is quite limited due to the lack of complete phenomenological models underlying their physics. To address this gap, hypothesis-driven, manually-constructed, conceptual hurricane models and models for regional-scale precipitation extremes have been emerging. To complement both approaches, we propose a methodology for data-driven, semi-automatic inference of plausible phenomenological models and apply it to derive the model for eastern Sahel rainfall, an important factor for socioeconomic growth and development of this region. At its core, our methodology derives cause-effect relationships using the Lasso multivariate regression model and quantifies compound affect that the complex interplay among the key predictors at their prominent temporal phases plays on the response (rainfall). Specifically, we propose methods for (a) detecting and ranking predictorsâ€™ prominent temporal phases, (b) optimizing the regularization penalty, (c) assessing predictor statistical significance, (d) performing impact analysis of data normalization on model inference, and (e) calculating the Expected Causality Impact (ECI) score to quantify impact analysis. The culmination of this study is the plausible phenomenological model of the eastern Sahel seasonal rainfall and quantified key climate drivers involved in the rainfall variability at different time lags. To the best of our knowledge, this is the first phenomenological model of this phenomenon; several of its components are consistent with the known evidence from literature.",2012,
Pivotal Estimation of Nonparametric Functions via Square-root Lasso,"In a nonparametric linear regression model we study a variant of LASSO, called p LASSO, which does not require the knowledge of the scaling parameter Ïƒ of the noise or bounds for it. This work derives new finite sample upper bounds for prediction norm rate of convergence, l1-rate of converge, lâˆž-rate of convergence, and sparsity of the p LASSO estimator. A lower bound for the prediction norm rate of convergence is also established. In many non-Gaussian noise cases, we rely on moderate deviation theory for self- normalized sums and on new data-dependent empirical process inequalities to achieve Gaussian-like results provided log p = o(n 1/3 ) improving upon results derived in the para- metric case that required log p . log n. In addition, we derive finite sample bounds on the performance of ordinary least square (OLS) applied tom the model selected by p LASSO accounting for possible misspecification of the selected model. In particular, we provide mild conditions under which the rate of convergence of OLS post p LASSO is not worse than p LASSO. We also study two extreme cases: parametric noiseless and nonparametric unbounded variance. p LASSO does have interesting theoretical guarantees for these two extreme cases. For the parametric noiseless case, differently than LASSO, p LASSO is capable of exact recovery. In the unbounded variance case it can still be consistent since its penalty choice does not depend on Ïƒ. Finally, we conduct Monte carlo experiments which show that the empirical performance of p LASSO is very similar to the performance of LASSO when Ïƒ is known. We also emphasize that p LASSO can be formulated as a convex programming problem and its computation burden is similar to LASSO. We provide theoretical and empirical evidence",2011,
Prevalence and risk of HIV infection among female sex workers in Burkina Faso,"Summary: Little information is available regarding human immunodeficiency virus (HIV) infection among female sex workers (FSW) in Burkina Faso, West Africa. A cross-sectional study was conducted in Ouagadougou and Bobo-Dioulasso, the 2 largest cities of the country, to determine the prevalence of HIV infection and other sexually transmitted diseases (STDs) among FSWs, and to investigate the factors which were associated with HIV infection in this population. From October to November 1994, 426 FSWs were recruited. The method of anonymous and unlinked HIV screening recommended by the World Health Organization (WHO) was used. The overall HIV seroprevalence was 58.2% (95% confidence interval: 53.4-62.9) and 52.6% of FSWs had at least one STD agent. The most common STDs were trichomoniasis (23%), syphilis (15%) and gonorrhoea (13%). In a logistic regression analysis, risk factors for HIV infection were high gravidity ( 2 pregnancies), low perception of personal risk of HIV infection, syphilis and the presence of genital warts. These results suggest that FSWs in Burkina Faso need better information about HIV transmission and prevention and then need better access to STD detection and management services.",1998,International Journal of STD and AIDS
High Dimensional Model Selection and Validation: A Comparison Study,"Model selection is a challenging issue in high dimensional statistical analysis, and many approaches have been proposed in recent years. In this thesis, we compare the performance of three penalized logistic regression approaches (Ridge, Lasso, and Elastic Net) and three information criteria (AIC, BIC, and EBIC) on binary response variable in high dimensional situation through extensive simulation study. The models are built and selected on the training datasets, and their performance are evaluated through AUC on the validation datasets. We also display the comparison results on two real datasets (Arcene Data and University Retention Data). The performance differences among those approaches are discussed at the end.",2015,
Nearly optimal sample size in hypothesis testing for high-dimensional regression,"We consider the problem of fitting the parameters of a high-dimensional linear regression model. In the regime where the number of parameters p is comparable to or exceeds the sample size n, a successful approach uses an â„“1-penalized least squares estimator, known as Lasso. Unfortunately, unlike for linear estimators (e.g. ordinary least squares), no well-established method exists to compute confidence intervals or p-values on the basis of the Lasso estimator. Very recently, a line of work [8], [7], [13] has addressed this problem by constructing a debiased version of the Lasso estimator. We propose a special debiasing method that is well suited for random designs with sparse inverse covariance. Our approach improves over the state of the art in that it yields nearly optimal average testing power if sample size n asymptotically dominates s0(logp)2, with s0 being the sparsity level (number of non-zero coefficients). Earlier work achieved similar performances only for much larger sample size, namely it requires n to asymptotically dominates (s0 log p)2. We evaluate our method on synthetic data, and compare it with earlier proposals.",2013,"2013 51st Annual Allerton Conference on Communication, Control, and Computing (Allerton)"
An Homotopy Algorithm for the Lasso with Online Observations,"It has been shown that the problem of l1-penalized least-square regression commonly referred to as the Lasso or Basis Pursuit DeNoising leads to solutions that are sparse and therefore achieves model selection. We propose in this paper RecLasso, an algorithm to solve the Lasso with online (sequential) observations. We introduce an optimization problem that allows us to compute an homotopy from the current solution to the solution after observing a new data point. We compare our method to Lars and Coordinate Descent, and present an application to compressive sensing with sequential observations. Our approach can easily be extended to compute an homotopy from the current solution to the solution that corresponds to removing a data point, which leads to an efficient algorithm for leave-one-out cross-validation. We also propose an algorithm to automatically update the regularization parameter after observing a new data point.",2008,
Challenges and caveats of a multi-center retrospective radiomics study: an example of early treatment response assessment for NSCLC patients using FDG-PET/CT radiomics,"BACKGROUND
Prognostic models based on individual patient characteristics can improve treatment decisions and outcome in the future. In many (radiomic) studies, small size and heterogeneity of datasets is a challenge that often limits performance and potential clinical applicability of these models. The current study is example of a retrospective multi-centric study with challenges and caveats. To highlight common issues and emphasize potential pitfalls, we aimed for an extensive analysis of these multi-center pre-treatment datasets, with an additional 18F-fluorodeoxyglucose (FDG) positron emission tomography/computed tomography (PET/CT) scan acquired during treatment.


METHODS
The dataset consisted of 138 stage II-IV non-small cell lung cancer (NSCLC) patients from four different cohorts acquired from three different institutes. The differences between the cohorts were compared in terms of clinical characteristics and using the so-called 'cohort differences model' approach. Moreover, the potential prognostic performances for overall survival of radiomic features extracted from CT or FDG-PET, or relative or absolute differences between the scans at the two time points, were assessed using the LASSO regression method. Furthermore, the performances of five different classifiers were evaluated for all image sets.


RESULTS
The individual cohorts substantially differed in terms of patient characteristics. Moreover, the cohort differences model indicated statistically significant differences between the cohorts. Neither LASSO nor any of the tested classifiers resulted in a clinical relevant prognostic model that could be validated on the available datasets.


CONCLUSION
The results imply that the study might have been influenced by a limited sample size, heterogeneous patient characteristics, and inconsistent imaging parameters. No prognostic performance of FDG-PET or CT based radiomics models can be reported. This study highlights the necessity of extensive evaluations of cohorts and of validation datasets, especially in retrospective multi-centric datasets.",2019,PLoS ONE
Investigation of regions impacting inbreeding depression and their association with the additive genetic effect for United States and Australia Jersey dairy cattle,"BackgroundVariation in environment, management practices, nutrition or selection objectives has led to a variety of different choices being made in the use of genetic material between countries. Differences in genome-level homozygosity between countries may give rise to regions that result in inbreeding depression to differ. The objective of this study was to characterize regions that have an impact on a runs of homozygosity (ROH) metric and estimate their association with the additive genetic effect of milk (MY), fat (FY) and protein yield (PY) and calving interval (CI) using Australia (AU) and United States (US) Jersey cows.MethodsGenotyped cows with phenotypes on MY, FY and PY (nâ€‰=â€‰6751 US; nâ€‰=â€‰3974 AU) and CI (nâ€‰=â€‰5816 US; nâ€‰=â€‰3905 AU) were used in a two-stage analysis. A ROH statistic (ROH4Mb), which counts the frequency of a SNP being in a ROH of at least 4 Mb was calculated across the genome. In the first stage, residuals were obtained from a model that accounted for the portion explained by the estimated breeding value. In the second stage, these residuals were regressed on ROH4Mb using a single marker regression model and a gradient boosted machine (GBM) algorithm. The relationship between the additive and ROH4Mb of a region was characterized based on the (co)variance of 500 kb estimated genomic breeding values derived from a Bayesian LASSO analysis. Phenotypes to determine ROH4Mb and additive effects were residuals from the two-stage approach and yield deviations, respectively.ResultsAssociations between yield traits and ROH4Mb were found for regions on BTA13, BTA23 and BTA25 for the US population and BTA3, BTA7, BTA17 for the AU population. Only one association (BTA7) was found for CI and ROH4Mb for the US population. Multiple potential epistatic interactions were characterized based on the GBM analysis. Lastly, the covariance sign between ROH4Mb and additive SNP effect of a region was heterogeneous across the genome.ConclusionWe identified multiple genomic regions associated with ROH4Mb in US and AU Jersey females. The covariance of regions impacting ROH4Mb and the additive genetic effect were positive and negative, which provides evidence that the homozygosity effect is location dependent.",2015,BMC Genomics
Four transcription profileâ€“based models identify novel prognostic signatures in oesophageal cancer,"Oesophageal cancer (ESCA) is a clinically challenging disease with poor prognosis and health-related quality of life. Here, we investigated the transcriptome of ESCA to identify high risk-related signatures. A total of 159 ESCA patients of The Cancer Genome Atlas (TCGA) were sorted by three phases. In the discovery phase, differentially expressed transcripts were filtered; in the training phase, two adjusted Cox regressions and two machine leaning models were used to construct and estimate signatures; and in the validation phase, prognostic signatures were validated in the testing dataset and the independent external cohort. We constructed two signatures from three types of RNA markers by Akaike information criterion (AIC) and least absolute shrinkage and selection operator (LASSO) Cox regressions, respectively, and all candidate markers were further estimated by Random Forest (RFS) and Support Vector Machine (SVM) algorithms. Both signatures had good predictive performances in the independent external oesophageal squamous cell carcinoma (ESCC) cohort and performed better than common clinicopathological indicators in the TCGA dataset. Machine learning algorithms predicted prognosis with high specificities and measured the importance of markers to verify the risk weightings. Furthermore, the cell function and immunohistochemical (IHC) staining assays identified that the common risky marker FABP3 is a novel oncogene in ESCA.",2019,Journal of Cellular and Molecular Medicine
Variable Selection in Kernel Regression Using Measurement Error Selection Likelihoods.,"This paper develops a nonparametric shrinkage and selection estimator via the measurement error selection likelihood approach recently proposed by Stefanski, Wu, and White. The Measurement Error Kernel Regression Operator (MEKRO) has the same form as the Nadaraya-Watson kernel estimator, but optimizes a measurement error model selection likelihood to estimate the kernel bandwidths. Much like LASSO or COSSO solution paths, MEKRO results in solution paths depending on a tuning parameter that controls shrinkage and selection via a bound on the harmonic mean of the pseudo-measurement error standard deviations. We use small-sample-corrected AIC to select the tuning parameter. Large-sample properties of MEKRO are studied and small-sample properties are explored via Monte Carlo experiments and applications to data.",2017,Journal of the American Statistical Association
Double machine learning for treatment and causal parameters,"Most modern supervised statistical/machine learning (ML) methods are explicitly designed to solve prediction problems very well. Achieving this goal does not imply that these methods automatically deliver good estimators of causal parameters. Examples of such parameters include individual regression coffiecients, average treatment e ffects, average lifts, and demand or supply elasticities. In fact, estimators of such causal parameters obtained via naively plugging ML estimators into estimating equations for such parameters can behave very poorly. For example, the resulting estimators may formally have inferior rates of convergence with respect to the sample size n caused by regularization bias. Fortunately, this regularization bias can be removed by solving auxiliary prediction problems via ML tools. Speci ficially, we can form an efficient score for the target low-dimensional parameter by combining auxiliary and main ML predictions. The efficient score may then be used to build an efficient estimator of the target parameter which typically will converge at the fastest possible 1/v n rate and be approximately unbiased and normal, allowing simple construction of valid con fidence intervals for parameters of interest. The resulting method thus could be called a ""double ML"" method because it relies on estimating primary and auxiliary predictive models. Such double ML estimators achieve the fastest rates of convergence and exhibit robust good behavior with respect to a broader class of probability distributions than naive ""single"" ML estimators. In order to avoid overfi tting, following [3], our construction also makes use of the K-fold sample splitting, which we call cross- fitting. The use of sample splitting allows us to use a very broad set of ML predictive methods in solving the auxiliary and main prediction problems, such as random forests, lasso, ridge, deep neural nets, boosted trees, as well as various hybrids and aggregates of these methods (e.g. a hybrid of a random forest and lasso). We illustrate the application of the general theory through application to the leading cases of estimation and inference on the main parameter in a partially linear regression model and estimation and inference on average treatment eff ects and average treatment e ffects on the treated under conditional random assignment of the treatment. These applications cover randomized control trials as a special case. We then use the methods in an empirical application which estimates the e ffect of 401(k) eligibility on accumulated financial assets.",2016,arXiv: Machine Learning
Exploiting Genome Structure in Association Analysis,"A genome-wide association study involves examining a large number of single-nucleotide polymorphisms (SNPs) to identify SNPs that are significantly associated with the given phenotype, while trying to reduce the false positive rate. Although haplotype-based association methods have been proposed to accommodate correlation information across nearby SNPs that are in linkage disequilibrium, none of these methods directly incorporated the structural information such as recombination events along chromosome. In this paper, we propose a new approach called stochastic block lasso for association mapping that exploits prior knowledge on linkage disequilibrium structure in the genome such as recombination rates and distances between adjacent SNPs in order to increase the power of detecting true associations while reducing false positives. Following a typical linear regression framework with the genotypes as inputs and the phenotype as output, our proposed method employs a sparsity-enforcing Laplacian prior for the regression coefficients, augmented by a first-order Markov process along the sequence of SNPs that incorporates the prior information on the linkage disequilibrium structure. The Markov-chain prior models the structural dependencies between a pair of adjacent SNPs, and allows us to look for association SNPs in a coupled manner, combining strength from multiple nearby SNPs. Our results on HapMap-simulated datasets and mouse datasets show that there is a significant advantage in incorporating the prior knowledge on linkage disequilibrium structure for marker identification under whole-genome association.",2014,Journal of computational biology : a journal of computational molecular cell biology
Development and Validation of an Algorithm to Identify Nonalcoholic Fatty Liver Disease in the Electronic Medical Record,"Background and AimsNonalcoholic fatty liver disease (NAFLD) is the most common cause of chronic liver disease worldwide. Risk factors for NAFLD disease progression and liver-related outcomes remain incompletely understood due to the lack of computational identification methods. The present study sought to design a classification algorithm for NAFLD within the electronic medical record (EMR) for the development of large-scale longitudinal cohorts.MethodsWe implemented feature selection using logistic regression with adaptive LASSO. A training set of 620 patients was randomly selected from the Research Patient Data Registry at Partners Healthcare. To assess a true diagnosis for NAFLD we performed chart reviews and considered either a documentation of a biopsy or a clinical diagnosis of NAFLD. We included in our model variables laboratory measurements, diagnosis codes, and concepts extracted from medical notes. Variables with PÂ <Â 0.05 were included in the multivariable analysis.ResultsThe NAFLD classification algorithm included number of natural language mentions of NAFLD in the EMR, lifetime number of ICD-9 codes for NAFLD, and triglyceride level. This classification algorithm was superior to an algorithm using ICD-9 data alone with AUC of 0.85 versus 0.75 (PÂ <Â 0.0001) and leads to the creation of a new independent cohort of 8458 individuals with a high probability for NAFLD.ConclusionsThe NAFLD classification algorithm is superior to ICD-9 billing data alone. This approach is simple to develop, deploy, and can be applied across different institutions to create EMR-based cohorts of individuals with NAFLD.",2015,Digestive Diseases and Sciences
Development of a Predictive Model of Difficult Hemostasis following Endobronchial Biopsy in Lung Cancer Patients,"Endobronchial biopsy (EBB)-induced bleeding is fairly common; however, it can be potentially life-threatening due to difficult hemostasis following EBB. The aim of this study was to develop a predictive model of difficult hemostasis post-EBB. A total of 620 consecutive patients with primary lung cancer who had undergone EBB between 2014 and 2018 in a large tertiary hospital were enrolled in this retrospective single-center cohort study. Patients were classified into the difficult hemostasis group and the nondifficult hemostasis group according to hemostatic measures used following EBB. The LASSO regression method was used to select predictors andmultivariate logistic regressionwas applied to develop the predictivemodel.The area under the curve (AUC) of the model was calculated. Bootstrapping method was applied for internal validation. Calibration curve analysis and decision curve analysis (DCA) were also performed. A nomogram was constructed to display the model. The incidence of difficult hemostasis post-EBB was 11.9% (74/620). Eight variables were selected by the LASSO regression analysis and seven (histological type of cancer, lesion location, neutrophil percentage, activated partial thromboplastin time, low density lipoprotein cholesterol, apolipoproteinE, and pulmonary infection) of them were finally included in the predictive model. The AUC of the model was 0.822 (95% CI, 0.777-0.868), and it was 0.808 (95% CI, 0.761-0.856) in the internal validation. The predictive model was well calibrated and DCA indicated its potential clinical usefulness, which suggests that the model has great potential to predict lung cancer patients with a more difficult post-EBB hemostasis.",2019,
Ensemble Methods for MiRNA Target Prediction from Expression Data,"BACKGROUND
microRNAs (miRNAs) are short regulatory RNAs that are involved in several diseases, including cancers. Identifying miRNA functions is very important in understanding disease mechanisms and determining the efficacy of drugs. An increasing number of computational methods have been developed to explore miRNA functions by inferring the miRNA-mRNA regulatory relationships from data. Each of the methods is developed based on some assumptions and constraints, for instance, assuming linear relationships between variables. For such reasons, computational methods are often subject to the problem of inconsistent performance across different datasets. On the other hand, ensemble methods integrate the results from individual methods and have been proved to outperform each of their individual component methods in theory.


RESULTS
In this paper, we investigate the performance of some ensemble methods over the commonly used miRNA target prediction methods. We apply eight different popular miRNA target prediction methods to three cancer datasets, and compare their performance with the ensemble methods which integrate the results from each combination of the individual methods. The validation results using experimentally confirmed databases show that the results of the ensemble methods complement those obtained by the individual methods and the ensemble methods perform better than the individual methods across different datasets. The ensemble method, Pearson+IDA+Lasso, which combines methods in different approaches, including a correlation method, a causal inference method, and a regression method, is the best performed ensemble method in this study. Further analysis of the results of this ensemble method shows that the ensemble method can obtain more targets which could not be found by any of the single methods, and the discovered targets are more statistically significant and functionally enriched. The source codes, datasets, miRNA target predictions by all methods, and the ground truth for validation are available in the Supplementary materials.",2015,PLoS ONE
Accuracy of genomic selection in biparental populations of flax (Linum usitatissimum L.),"Abstract Flax is an important economic crop for seed oil and stem fiber. Phenotyping of traits such as seed yield, seed quality, stem fiber yield, and quality characteristics is expensive and time consuming. Genomic selection (GS) refers to a breeding approach aimed at selecting preferred individuals based on genomic estimated breeding values predicted by a statistical model based on the relationship between phenotypes and genome-wide genetic markers. We evaluated the prediction accuracy of GS ( r MP ) and the efficiency of GS relative to phenotypic selection ( RE ) for three GS models: ridge regression best linear unbiased prediction (RR-BLUP), Bayesian LASSO (BL), and Bayesian ridge regression (BRR), for seed yield, oil content, iodine value, linoleic, and linolenic acid content with a full and a common set of genome-wide simple sequence repeat markers in each of three biparental populations. The three GS models generated similar r MP and RE , while BRR displayed a higher coefficient of determination ( R 2 ) of the fitted models than did RR-BLUP or BL. The mean r MP and RE varied for traits with different heritabilities and was affected by the genetic variation of the traits in the populations. GS for seed yield generated a mean RE of 1.52 across populations and marker sets, a value significantly superior to that for direct phenotypic selection. Our empirical results provide the first validation of GS in flax and demonstrate that GS could increase genetic gain per unit time for linseed breeding. Further studies for selection of training populations and markers are warranted.",2016,Crop Journal
A Least Absolute Shrinkage and Selection Operator (lasso) for Nonlinear System Identification,"Abstract Identification of parametric nonlinear models involves estimating unknown parameters and detecting its underlying structure. Structure computation is concerned with selecting a subset of parameters to give a parsimonious description of the system which may afford greater insight into the functionality of the system or a simpler controller design. In this study, a least absolute shrinkage and selection operator (LASSO) technique is investigated for computing efficient model descriptions of nonlinear systems. The LASSO minimises the residual sum of squares by the addition of a l1 penalty term on the parameter vector of the traditional l2 minimisation problem. Its use for structure detection is a natural extension of this constrained minimisation approach to pseudolinear regression problems which produces some model parameters that are exactly zero and, therefore, yields a parsimonious system description. The performance of this LASSO structure detection method was evaluated by using it to estimate the structure of a nonlinear polynomial model. Applicability of the method to more complex systems such as those encountered in aerospace applications was shown by identifying a parsimonious system description of the F/A-18 Active Aeroelastic Wing using flight test data.",2006,IFAC Proceedings Volumes
Comprehensive analysis and selection of anthrax vaccine adsorbed immune correlates of protection in rhesus macaques.,"Humoral and cell-mediated immune correlates of protection (COP) for inhalation anthrax in a rhesus macaque (Macaca mulatta) model were determined. The immunological and survival data were from 114 vaccinated and 23 control animals exposed to Bacillus anthracis spores at 12, 30, or 52 months after the first vaccination. The vaccinated animals received a 3-dose intramuscular priming series (3-i.m.) of anthrax vaccine adsorbed (AVA) (BioThrax) at 0, 1, and 6 months. The immune responses were modulated by administering a range of vaccine dilutions. Together with the vaccine dilution dose and interval between the first vaccination and challenge, each of 80 immune response variables to anthrax toxin protective antigen (PA) at every available study time point was analyzed as a potential COP by logistic regression penalized by least absolute shrinkage and selection operator (LASSO) or elastic net. The anti-PA IgG level at the last available time point before challenge (last) and lymphocyte stimulation index (SI) at months 2 and 6 were identified consistently as a COP. Anti-PA IgG levels and lethal toxin neutralization activity (TNA) at months 6 and 7 (peak) and the frequency of gamma interferon (IFN-Î³)-secreting cells at month 6 also had statistically significant positive correlations with survival. The ratio of interleukin 4 (IL-4) mRNA to IFN-Î³ mRNA at month 6 also had a statistically significant negative correlation with survival. TNA had lower accuracy as a COP than did anti-PA IgG response. Following the 3-i.m. priming with AVA, the anti-PA IgG responses at the time of exposure or at month 7 were practicable and accurate metrics for correlating vaccine-induced immunity with protection against inhalation anthrax.",2014,Clinical and vaccine immunology : CVI
Feature selection in feature network models: finding predictive subsets of features with the Positive Lasso.,"A set of features is the basis for the network representation of proximity data achieved by feature network models (FNMs). Features are binary variables that characterize the objects in an experiment, with some measure of proximity as response variable. Sometimes features are provided by theory and play an important role in the construction of the experimental conditions. In some research settings, the features are not known a priori. This paper shows how to generate features in this situation and how to select an adequate subset of features that takes into account a good compromise between model fit and model complexity, using a new version of least angle regression that restricts coefficients to be non-negative, called the Positive Lasso. It will be shown that features can be generated efficiently with Gray codes that are naturally linked to the FNMs. The model selection strategy makes use of the fact that FNM can be considered as univariate multiple regression model. A simulation study shows that the proposed strategy leads to satisfactory results if the number of objects is less than or equal to 22. If the number of objects is larger than 22, the number of features selected by our method exceeds the true number of features in some conditions.",2008,The British journal of mathematical and statistical psychology
Abstracts 10-4,"Clash of Career and Family: Fertility Decisions after Job Displacement Emilia Del Bono, University of Essex, Andrea Weber, University of Mannheim, and Rudolf WinterEbme University of Linz and IHS Vienna In this paper we investigate how career considerations may affect fertility decisions in the presence of a temporary employment shock. We compare the birth rates of women displaced by a plant closure with those of women unaffected by job loss after establishing the pre-displacement comparability of these groups. Our results reveal that job displacement reduces average fertility by 5%-10%, and that these effects are largely explained by the response of women in more skilled occupations. We offer an explanation of our results based on career interruptions of women. The Political Economy of Flexicurity Tito Boeri, UniversitÃ  Bocconi and Fondazione Rodolfo Debenedetti, Jose Ignacio Conde-Ruiz, Universidad Complutense de Madrid and FEDEA, and Vincenzo Galasso, UniversitÃ  Bocconi and Dondena We document the presence of a trade-off in the labor market between the protection of jobs and the support offered to unemployed people. Different countriesâ€™ locations along this trade-off represent stable politico-economic equilibria. We develop a model in which individuals determine the mix of job protection and support to the unemployed in a political environment. Agents are heterogeneous along two dimensions: employment status (insiders and outsiders) and skills (low and high). Unlike previous work on the political economy of labor market institutions, we emphasize the role of job protection and unemployment benefits in the wage setting process. A key implication of the model is that flexicurity configurations with low job protection and high support to the unemployed should emerge in presence of a highly educated workforce. Panel regressions of countriesâ€™ locations along this institutional trade-off are consistent with the implications of our model. Evolving International Inflation Dynamics: World and Country Specific Factors Haroon Mumtaz, Bank of England and Paolo Surico, London Business School The decline in the level and persistence of inflation over the 1980s is a common feature of the most industrialized economies in the world. The rise in inflation volatility of the late 1970s and the subsequent fall of the 1980s is country specific for the U.K., Canada and, to a lesser extent, U.S., Italy and Japan. Since the late 1980s, inflation predictability has declined significantly across the industrialized world. We link the empirical results to recent theories of international inflation. Do Hiring Subsidies Reduce Unemployment among Older Workers? Evidence from Natural Experiments Bernhard Boockmann, Institute for Applied Economic Research (IAW) and University of TÃ¼bingen, Thomas Zwick, Ludwig Maximilians University and ZEW, Andreas AmmermÃ¼ller, Federal Ministry of Labour and Social Affairs (BMAS), and Michael Maier, University of Mannheim We estimate the effects of hiring subsidies for older workers on transitions from unemployment to employment in Germany. Using a natural experiment, our first set of estimates is based on a legal change extending the group of eligible unemployed persons. A subsequent legal change in the opposite direction is used to validate these results. Our data cover the population of unemployed jobseekers in Germany and was specifically made available for our purposes from administrative data. Consistent support for an employment effect of hiring subsidies can only be found for women in East Germany. Concerning other population groups, firmsâ€™ hiring behavior is hardly influenced by the program and hiring subsidies mainly lead to deadweight effects.",2012,
QSAR modeling of benzimidazole derivatives as potent inhibitors of trichomoniasis / QSAR modellemesi ile Benzimidazole tÃ¼revlerinin trikomoniasis iÃ§in etkili inhibitÃ¶rler olarak kullanÄ±lmasÄ±,"Abstract Objective: Trichomoniasis can increase the risk of getting or spreading other sexually transmitted infections. It can cause genital inflammation that can increase the likelihood of infection with HIV or passing HIV on to a sex partner. The present study used QSAR modeling to study benzimidazole derivatives as potent inhibitors of trichomonicidal activity. A set of 10 compounds was randomly removed from the dataset to be used as the prediction set (PSET). The remaining 60 compounds were utilized as the training set (TSET). Methods: Genetic algorithms, artificial neural networks (ANN), multiple linear regression (MLR), partial least squares (PLS), principal component regression (PCR) and a least absolute shrinkage and selection operator (LASSO) were used to create the QSAR models. Geometry optimization of compounds was carried out using the B3LYP method employing a 6-31G (2d) basis set. HyperChem, Gaussian 03W and Dragon (version 5.5) software were used for geometric optimization of the molecules and calculation of the quantum chemical descriptors. Unscrambler was used for analysis of data. Results: The MLR method obtained a root mean square error (RMSE) for calibration of 0.0803 and R2 of 0.99. The R2 and RMSE of calibration using PCR were 0.58 and 0.5236, respectively. Conclusion: The results found that the ANN and MLR models were the most favorable of the methods tested and are suitable for use in QSAR models. Ã–zet Amac: Trikomoniasis diÄŸer cinsel yolla bulaÅŸan enfeksiyonlarÄ±n bulaÅŸÄ±cÄ±lÄ±ÄŸÄ±nÄ± ve yayÄ±lmacÄ±lÄ±ÄŸÄ±nÄ± arttÄ±rabiliyor. Sebep olduÄŸu jenital enflamasyon nedeniyle HIV virusunun bulaÅŸma veya baÅŸka birine aktarÄ±lma ihtimalini yukseltiyor. Bu araÅŸtÄ±rmada QSAR modellemesi ile incelenen Benzimidazol turevlerinin trikomonisidal faaliyetinde etkili inhibitorler olduÄŸu goruÅŸmuÅŸtur. Veri setinden geliÅŸiguzel olarak 10 bileÅŸik seti cÄ±karÄ±lmÄ±ÅŸ ve ongoru seti olarak ayrÄ±lmÄ±ÅŸtÄ±r. (PSET) Kalan 60 bileÅŸik calÄ±ÅŸma seti olarak kullanÄ±lmÄ±ÅŸtÄ±r (TSET). Metod: Malzemeler ve yontemler: Genetik algoritmalar, yapay noron aÄŸlarÄ±/noral aÄŸlar (ANN), coklu dikey regresyon (MLR), kÄ±smi en az kareler (PLS), baÅŸ bileÅŸen regresyonlarÄ± (PCR) ve En Kucuk Mutlak Cekme ve Secme Operatoru (LASSO) kullanÄ±larak QSAR modelleri oluÅŸturuldu. BileÅŸiklerin geometric optimizasyonu 6-31G (2d) baz seti kullanÄ±larak B3LYP metodu ile yapÄ±ldÄ±. Kuantum kimyasal hesaplamalarÄ± ve molekullerin optimizasyonlarÄ± icin HyperChem, Gaussian 03W ve Dragon (version 5.5) yazÄ±lÄ±mlarÄ± kullanÄ±ldÄ±. Veri analizi icin de Unscrambler yazÄ±lÄ±mÄ± kullanÄ±lmÄ±ÅŸtÄ±r. Bulgular: 0.0803 ve 0.99 R2. R2 and RMSEâ€™nin PCR kullanÄ±larak hesaplamalarÄ± sÄ±rasÄ± ile 0.58 and 0.5236 olarak bulundu. Sonuc: Sonuclara gore ANN ve MLR modelleri en iyi sonuclarÄ± veren yontemler olmuÅŸtur ve QSAR modellerinde kullanmak icin uygundur.",2015,Turkish Journal of Biochemistry
Predicting the Success of Bank Telemarketing using various Classification Algorithms,"In this thesis, well known methods of classification Support Vector Machine, Decision Trees, Random Forest and Artificial Neural Network have been performed. To reduce the dimensionality, feature selection best subset Logistic Regression, Least Absolute Shrinkage and Selection Operator (LASSO), Random Forest approaches have been utilized. The focus was to check the prediction accuracy and performance of these classification methods after feature selection on full as well as reduced model. The output showed that reduce subset of variables which is attained through Random Forest has best accuracy with the classification method random forest. In this way, we can rely on the subset of variables obtained from Random Forest as it has almost same percentage of accuracy as compared to the full model.",2018,
An MM-Based Optimization Algorithm for Sparse Linear Modeling on Microarray Data Analysis,"Sparsity is crucial for high-dimensional statistical modeling. On one hand, dimensionality reduction can reduce the variability of estimation and thus provide reliable predictive power. On the other hand, the selected sub-model can discover and emphasize the underlying dependencies, which is useful for objective interpretation. Many variable selection methods have been proposed in literatures. For a prominent example, Least Absolute Shrinkage and Selection Operator (lasso) in linear regression context has been extensively explored. This paper discusses a class of scaled mixture of Gaussian models from both a penalized likelihood and a Bayesian regression point of view. We propose an Majorize-Minimize (MM) algorithm to find the Maximum A Posteriori (MAP) estimator, where the EM algorithm can be stuck at local optimum for some members in this class. Simulation studies show the outperformance of proposed algorithm in nonstochastic design variable selection scenario. The proposed algorithm is applied to a real large-scale E.coli data set with known bona fide interactions for constructing sparse gene regulatory networks. We show that our regression networks with a properly chosen prior can perform comparably to state-of-the-art regulatory network construction algorithms.",2009,2009 3rd International Conference on Bioinformatics and Biomedical Engineering
