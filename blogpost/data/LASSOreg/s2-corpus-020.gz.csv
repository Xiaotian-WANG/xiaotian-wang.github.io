title,abstract,year,journal
Prevalence and risk factors for hearing loss in high-risk neonates in Germany.,"AIM
Hearing loss in infants is often diagnosed late, despite universal screening programmes. Risk factors of hearing impairment in high-risk neonates, identified from population-based studies, can inform policy around targeted screening. Our aim was to determine the prevalence and the risk factors of hearing loss in a high-risk neonatal population.


METHODS
This was a retrospective cohort study of neonates hospitalised at the University Hospital Cologne, Germany from January 2009 to December 2014 and were part of the newborn hearing screening programme. Multivariable regression analyses using the lasso approach was performed.


RESULTS
Data were available for 4512 (43% female) neonates with a mean gestational age at birth of 35.5Â weeks. The prevalence of hearing loss was 1.6%, and 42 (0.9%) neonates had permanent hearing loss. Craniofacial anomalies, hyperbilirubinaemia requiring exchange transfusion, oxygen supplementation after 36Â weeks of gestation and hydrops fetalis showed associations with permanent hearing loss.


CONCLUSION
Our findings of risk factors for hearing loss were consistent with other studies. However, some commonly demonstrated risk factors such as perinatal infections, meningitis, sepsis and ototoxic drugs did not show significant associations in our cohort. Targeted screening based on risk factors may help early identification of hearing loss in neonates.",2019,Acta paediatrica
Confidence Bands in Quantile Regression and Generalized Dynamic Semiparametric Factor Models,"In many applications it is necessary to know the stochastic fluctuation of the maximal deviations of the nonparametric quantile estimates, e.g. for various parametric models check. Uniform confidence bands are therefore constructed for nonparametric quantile estimates of regression functions. The first method is based on the strong approximations of the empirical process and extreme value theory. The strong uniform consistency rate is also established under general conditions. The second method is based on the bootstrap resampling method. It is proved that the bootstrap approximation provides a substantial improvement. The case of multidimensional and discrete regressor variables is dealt with using a partial linear model. A labor market analysis is provided to illustrate the method. High dimensional time series which reveal nonstationary and possibly periodic behavior occur frequently in many fields of science, e.g. macroeconomics, meteorology, medicine and financial engineering. One of the common approach is to separate the modeling of high dimensional time series to time propagation of low dimensional time series and high dimensional time invariant functions via dynamic factor analysis. We propose a two-step estimation procedure. At the first step, we detrend the time series by incorporating time basis selected by the group Lasso-type technique and choose the space basis based on smoothed functional principal component analysis. We show properties of this estimator under the dependent scenario. At the second step, we obtain the detrended low dimensional stochastic process (stationary), but it also poses an important question: is it justified, from an inferential point of view, to base further statistical inference on the estimated stochastic time series? We show that the difference of the inference based on the estimated time series and â€œtrueâ€ unobserved time series is asymptotically negligible, which finally allows one to study the dynamics of the whole high-dimensional system with a low dimensional representation together with the deterministic trend. We apply the method to our motivating empirical problems: studies of the dynamic behavior of temperatures (further used for pricing weather derivatives), implied volatilities and risk patterns and correlated brain activities (neuroeconomics related) using fMRI data, where a panel version model is also presented.",2010,
The effect of machine learning regression algorithms and sample size on individualized behavioral prediction with functional connectivity features,"Abstract Individualized behavioral/cognitive prediction using machine learning (ML) regression approaches is becoming increasingly applied. The specific ML regression algorithm and sample size are two key factors that nonâ€trivially influence prediction accuracies. However, the effects of the ML regression algorithm and sample size on individualized behavioral/cognitive prediction performance have not been comprehensively assessed. To address this issue, the present study included six commonly used ML regression algorithms: ordinary least squares (OLS) regression, least absolute shrinkage and selection operator (LASSO) regression, ridge regression, elasticâ€net regression, linear support vector regression (LSVR), and relevance vector regression (RVR), to perform specific behavioral/cognitive predictions based on different sample sizes. Specifically, the publicly available restingâ€state functional MRI (rsâ€fMRI) dataset from the Human Connectome Project (HCP) was used, and wholeâ€brain restingâ€state functional connectivity (rsFC) or rsFC strength (rsFCS) were extracted as prediction features. Twentyâ€five sample sizes (ranged from 20 to 700) were studied by subâ€sampling from the entire HCP cohort. The analyses showed that rsFCâ€based LASSO regression performed remarkably worse than the other algorithms, and rsFCSâ€based OLS regression performed markedly worse than the other algorithms. Regardless of the algorithm and feature type, both the prediction accuracy and its stability exponentially increased with increasing sample size. The specific patterns of the observed algorithm and sample size effects were well replicated in the prediction using reâ€testing fMRI data, data processed by different imaging preprocessing schemes, and different behavioral/cognitive scores, thus indicating excellent robustness/generalization of the effects. The current findings provide critical insight into how the selected ML regression algorithm and sample size influence individualized predictions of behavior/cognition and offer important guidance for choosing the ML regression algorithm or sample size in relevant investigations. HighlightsIndividualized prediction is influenced by regression algorithm and sample size.LASSO regression performed worse than other algorithms using rsFC feature.OLS regression performed worse than other algorithms using rsFCS feature.Prediction accuracy and its stability exponentially increased with sample size.The observed algorithm and sample size effects are robust and generalizable.",2018,NeuroImage
Maximizing Interpretability and Cost-Effectiveness of Surgical Site Infection (SSI) Predictive Models Using Feature-Specific Regularized Logistic Regression on Preoperative Temporal Data,"This study describes a novel approach to solve the surgical site infection (SSI) classification problem. Feature engineering has traditionally been one of the most important steps in solving complex classification problems, especially in cases with temporal data. The described novel approach is based on abstraction of temporal data recorded in three temporal windows. Maximum likelihood L1-norm (lasso) regularization was used in penalized logistic regression to predict the onset of surgical site infection occurrence based on available patient blood testing results up to the day of surgery. Prior knowledge of predictors (blood tests) was integrated in the modelling by introduction of penalty factors depending on blood test prices and an early stopping parameter limiting the maximum number of selected features used in predictive modelling. Finally, solutions resulting in higher interpretability and cost-effectiveness were demonstrated. Using repeated holdout cross-validation, the baseline C-reactive protein (CRP) classifier achieved a mean AUC of 0.801, whereas our best full lasso model achieved a mean AUC of 0.956. Best model testing results were achieved for full lasso model with maximum number of features limited at 20 features with an AUC of 0.967. Presented models showed the potential to not only support domain experts in their decision making but could also prove invaluable for improvement in prediction of SSI occurrence, which may even help setting new guidelines in the field of preoperative SSI prevention and surveillance.",2019,Computational and Mathematical Methods in Medicine
Bayesian Modeling and Inference for Quantile Mixture Regression,"Author(s): Yan, Yifei | Advisor(s): Kottas, Athanasios | Abstract: The focus of this work is to develop a Bayesian framework to combine information from multiple parts of the response distribution characterized with different quantiles. The goal is to obtain a synthesized estimate of the covariate effects on the response variable as well as to identify the more influential predictors. This framework naturally relates to the traditional quantile regression, which studies the relationship between the covariates and the conditional quantile of the response variable and serves as an attractive alternative to the more widely used mean regression methods. We achieve the objectives through constructing a Bayesian mixture model using quantile regressions as the mixture components.The first stage of the research involves the development of a parametric family of distributions to provide the mixture kernel for the Bayesian quantile mixture regression. We derive a new family of error distributions for model-based quantile regression called generalized asymmetric Laplace distribution, which is constructed through a structured mixture of normal distributions. The construction enables fixing specific percentiles of the distribution while, at the same time, allowing for varying mode, skewness and tail behavior. This family provides a practically important extension of the asymmetric Laplace distribution, which is the standard error distribution for parametric quantile regression. We develop a Bayesian formulation for the proposed quantile regression model, including conditional lasso regularized quantile regression based on a hierarchical Laplace prior for the regression coefficients, and a Tobit quantile regression model.Next, we develop the main framework to model the conditional distribution of the response with a weighted mixture of quantile regression components. We specify a common regression coefficient vector for all components to synthesize information from multiple parts of the response distribution, each modeled with one quantile regression component. The goal is to obtain a combined estimate of the predictive effect of each covariate. We consider the following two choices of kernel densities for the mixture model. When the probability of the quantile in each regression component is known, we model the components with the generalized asymmetric Laplace distribution, as its shape parameter introduces flexibility in shape and skewness to the kernel; else when the quantile probabilities are unknown, we use the asymmetric Laplace distribution as kernel density and view its skewness parameter, which is also the quantile probability of the component, as a random quantity and estimate it from the data. Under each kernel density, we formulate the hierarchical structure of the mixture weights and develop the approach to the posterior inference. We consider both parametric and nonparametric priors for the framework, and explore inferences for the number of components to be included. We demonstrate the performance of the method in identification of influential variables with simulation examples and illustrate the posterior predictive inferences in a realty price data from the Boston metropolitan area.Finally, we extend the framework to apply the methods to specific problems in survival analysis and epidemiology. Both applications involve analyses of two cohorts, which oftentimes exhibit differing responses given the same predictor input. We adapt the proposed framework to model the survival data with right-censoring. For applications in epidemiology, we study the ordering properties of the mixture kernels and incorporate stochastic ordering in the two-cohort mixture framework through structured priors, which conforms with the assumption in certain circumstances of receiver operating characteristic curve estimation. With the adapted models, we carry out cohort-specific identification of influential variables and gain insights into the contribution in estimation and prediction from different parts of the response distribution, which are depicted by the corresponding quantile regression components. We illustrate the applications with a time-to-event data set on length of stay at nursing home and two disease diagnosis data sets, one on adolescent depression and the other on cattle epidemics.",2017,
Dietâ€Related Metabolites Associated with Cognitive Decline Revealed by Untargeted Metabolomics in a Prospective Cohort,"SCOPE
Untargeted metabolomics may reveal preventive targets in cognitive aging, including within the food metabolome.


METHODS AND RESULTS
A case-control study nested in the prospective Three-City study included participants aged â‰¥65 years and initially free of dementia. We contrasted 209 cases of cognitive decline and 209 controls (matched for age, gender and educational level) with slower cognitive decline over up to 12 years. This article is protected by copyright. All rights reserved Using a bootstrap-enhanced LASSO regression on the baseline serum metabolomes analyzed with LC-QTOF, we identified a signature of 22 metabolites associated with subsequent cognitive decline. The signature included three coffee metabolites, a biomarker of citrus intake, a cocoa metabolite, two metabolites putatively derived from fish and wine, three medium-chain acylcarnitines, glycodeoxycholic acid, lysoPC(18:3), trimethyllysine, glucose, cortisol, creatinine and arginine. Adding the 22 metabolites to a reference predictive model for cognitive decline (conditioned on age, gender and education and including ApoE-Îµ4, diabetes, BMI and number of medications) substantially increased the predictive performance: cross-validated Area Under the Receiver Operating Curve = 75% [95% CI 70-80%] compared to 62% [95% CI 56-67%].


CONCLUSIONS
Our untargeted metabolomics study supports a protective role of specific foods (e.g., coffee, cocoa, fish) and various alterations in the endogenous metabolism responsive to diet in cognitive aging.",2019,Molecular Nutrition & Food Research
ColoGuideEx: a robust gene classifier specific for stage II colorectal cancer prognosis.,"BACKGROUND AND AIMS
Several clinical factors have an impact on prognosis in stage II colorectal cancer (CRC), but as yet they are inadequate for risk assessment. The present study aimed to develop a gene expression classifier for improved risk stratification of patients with stage II CRC.


METHODS
315 CRC samples were included in the study. Gene expression measurements from 207 CRC samples (stage I-IV) from two independent Norwegian clinical series were obtained using Affymetrix exon-level microarrays. Differentially expressed genes between stage I and stage IV samples from the test series were identified and used as input for L1 (lasso) penalised Cox proportional hazards analyses of patients with stage II CRC from the same series. A second validation was performed in 108 stage II CRC samples from other populations (USA and Australia).


RESULTS
An optimal 13-gene expression classifier (PIGR, CXCL13, MMP3, TUBA1B, SESN1, AZGP1, KLK6, EPHA7, SEMA3A, DSC3, CXCL10, ENPP3, BNIP3) for prediction of relapse among patients with stage II CRC was developed using a consecutive Norwegian test series from patients treated according to current standard protocols (n=44, p<0.001, HR=18.2), and its predictive value was successfully validated for patients with stage II CRC in a second Norwegian CRC series collected two decades previously (n=52, p=0.02, HR=3.6). Further validation of the classifier was obtained in a recent external dataset of patients with stage II CRC from other populations (n=108, p=0.001, HR=6.5). Multivariate Cox regression analyses, including all three sample series and various clinicopathological variables, confirmed the independent prognostic value of the classifier (pâ‰¤0.004). The classifier was shown to be specific to stage II CRC and does not provide prognostic stratification of patients with stage III CRC.


CONCLUSION
This study presents the development and validation of a 13-gene expression classifier, ColoGuideEx, for prognosis prediction specific to patients with stage II CRC. The robustness was shown across patient series, populations and different microarray versions.",2012,Gut
How to Use Fewer Markers in Admixture Studies,"Summary Swiss Fleckvieh has been established from 1970 as a composite of Simmental and Red Holstein Friesian cattle. Breed composition is currently reported based on pedigree information. Information on ancestry informative molecular markers potentially provides more accurate information. For the analysis Illumina Bovine SNP50 Beadchip data for 495 bulls were used. Markers were selected based on diff erence in allele frequencies in the pure populations, using FST as an indicator. Performance of sets with decreasing number of markers was compared. Th e scope of the study was to see how much we can reduce the number of markers based on FST to get a reliability that is close to that with the full set of markers. On these sets of markers hidden Markov models (HMM) and methods used in genomic selection (BayesB, partial least squares regression, LASSO variable selection) were applied. Correlations of admixture levels were estimated and compared with admixture levels based on pedigree information. FST chosen SNP gave very high correlations with pedigree based admixture. Only when using 96 and 48 SNP with the highest F ST , correlations dropped to 0.92 and 0.90, respectively.",2011,
Deregressed EBV as the response variable yield more reliable genomic predictions than traditional EBV in pure-bred pigs,"BackgroundGenomic selection can be implemented by a multi-step procedure, which requires a response variable and a statistical method. For pure-bred pigs, it was hypothesised that deregressed estimated breeding values (EBV) with the parent average removed as the response variable generate higher reliabilities of genomic breeding values than EBV, and that the normal, thick-tailed and mixture-distribution models yield similar reliabilities.MethodsReliabilities of genomic breeding values were estimated with EBV and deregressed EBV as response variables and under the three statistical methods, genomic BLUP, Bayesian Lasso and MIXTURE. The methods were examined by splitting data into a reference data set of 1375 genotyped animals that were performance tested before October 2008, and 536 genotyped validation animals that were performance tested after October 2008. The traits examined were daily gain and feed conversion ratio.ResultsUsing deregressed EBV as the response variable yielded 18 to 39% higher reliabilities of the genomic breeding values than using EBV as the response variable. For daily gain, the increase in reliability due to deregression was significant and approximately 35%, whereas for feed conversion ratio it ranged between 18 and 39% and was significant only when MIXTURE was used. Genomic BLUP, Bayesian Lasso and MIXTURE had similar reliabilities.ConclusionsDeregressed EBV is the preferred response variable, whereas the choice of statistical method is less critical for pure-bred pigs. The increase of 18 to 39% in reliability is worthwhile, since the reliabilities of the genomic breeding values directly affect the returns from genomic selection.",2011,"Genetics, Selection, Evolution : GSE"
Application of Machine Learning for Predicting Clinically Meaningful Outcome After Arthroscopic Femoroacetabular Impingement Surgery,"Background: Hip arthroscopy has become an important tool for surgical treatment of intra-articular hip pathology. Predictive models for clinically meaningful outcomes in patients undergoing hip arthroscopy for femoroacetabular impingement syndrome (FAIS) are unknown. Purpose: To apply a machine learning model to determine preoperative variables predictive for achieving the minimal clinically important difference (MCID) at 2 years after hip arthroscopy for FAIS. Study Design: Case-control study; Level of evidence, 3. Methods: Data were analyzed for patients who underwent hip arthroscopy for FAIS by a high-volume fellowship-trained surgeon between January 2012 and July 2016. The MCID cutoffs for the Hip Outcome Scoreâ€“Activities of Daily Living (HOS-ADL), HOSâ€“Sport Specific (HOS-SS), and modified Harris Hip Score (mHHS) were 9.8, 14.4, and 9.14, respectively. Predictive models for achieving the MCID with respect to each were built with the LASSO algorithm (least absolute shrinkage and selection operator) for feature selection, followed by logistic regression on the selected features. Study data were analyzed with PatientIQ, a cloud-based research and analytics platform for health care. Results: Of 1103 patients who met inclusion criteria, 898 (81.4%) had a minimum of 2-year reported outcomes and were entered into the modeling algorithm. A total of 74.0%, 73.5%, and 79.9% met the HOS-ADL, HOS-SS, and mHHS threshold scores for achieving the MCID. Predictors of not achieving the HOS-ADL MCID included anxiety/depression, symptom duration for >2 years before surgery, higher body mass index, high preoperative HOS-ADL score, and preoperative hip injection (all P < .05). Predictors of not achieving the HOS-SS MCID included anxiety/depression, preoperative symptom duration for >2 years, high preoperative HOS-SS score, and preoperative hip injection, while running at least at the recreational level was a predictor of achieving HOS-SS MCID (all P < .05). Predictors of not achieving the mHHS MCID included history of anxiety or depression, high preoperative mHHS score, and hip injections, while being female was predictive of achieving the MCID (all P < .05). Conclusion: This study identified predictive variables for achieving clinically meaningful outcome after hip arthroscopy for FAIS. Patient factors including anxiety/depression, symptom duration >2 years, preoperative intra-articular injection, and high preoperative outcome scores are most consistently predictive of inability to achieve clinically meaningful outcome. These findings have important implications for shared decision-making algorithms and management of preoperative expectations after hip arthroscopy for FAI.",2019,The American Journal of Sports Medicine
Probabilistic Signal Recovery and Random Matrices,"Abstract : Our research program spanned several areas of mathematics and data science. In the area of highdimensionalinference, we showed that classical methods for linear regression (such as Lasso) areapplicable for non-linear data. This surprising finding has already found several applications in theanalysis of genetic, fMRI and proteomic data, compressed sensing, coding and quantization. In the area ofnetwork analysis, we showed how to detect communities in sparse networks by using semidefiniteprogramming and regularized spectral clustering. In high dimensional convex geometry, we studied thecomplexity of convex sets. In numerical linear algebra, we analyzed the fastest known randomizedapproximation algorithm for computing the permanents of matrices with non-negative entries. Incomputational graph theory, we studied a randomized algorithm for estimating the number of perfectmatchings in general graphs. In random matrix theory, we established delocalization of eigenvectors for awide class of random matrices, proved a sharp invertibility result for sparse random matrices, showed howto improve the norm of a general random matrix by removing a small submatrix, and developed a simpleand general tool for bounding the deviation of random matrices on arbitrary geometric sets. This has applications for dimension reduction, regression and compressed sensing.",2016,
On Shrinkage Estimation: Non-orthogonal Case,"In this paper, we consider the estimation of the parameters of the non-orthogonal regression model, when we suspect a sparsity condition. We provide with a comparative performance characteristics of the primary penalty estimators, namely, the ridge and the LASSO, with the least square estimator, restricted LSE, preliminary testÂ  and Stein-type ofÂ  estimators, when the dimension of the parameterÂ  space is less than the dimensionÂ  of the sample space. Using the principle of marginal distribution theory, the analysis of risks leads to the following conclusions: (i) ridge estimator outperforms least squares, preliminary test and Stein-type estimators uniformly, (ii) The restricted least squares estimator and LASSO are competitive, although LASSO lags behind the restricted least squares estimator uniformly. Both estimators outperform the least squares, preliminary test, and Stein-type estimators in a subspace, respectively.Â  (iii) The lower bound risk expression of LASSO does not depend on the threshold parameter. (iv) Performance of the estimators depends upon the size of numbers of active coefficients, non-active coefficients, and the divergence parameter. In support of our conclusion, we prepare some tables and graphs relevant to the properties of the estimators.",2018,"Statistics, Optimization and Information Computing"
Reducing Wait Time Prediction In Hospital Emergency Room: Lean Analysis Using a Random Forest Model,"Most of the patients visiting emergency departments face long waiting times due to overcrowding which is a major concern across the hospital in the United States. Emergency Department (ED) overcrowding is a common phenomenon across hospitals, which leads to issues for the hospital management, such as increased patient's dissatisfaction and an increase in the number of patients choosing to terminate their ED visit without being attended to by a medical healthcare professional. Patients who have to Leave Without Being Seen (LWBS) by doctors often leads to loss of revenue to hospitals encouraging healthcare professionals to analyze ways to improve operational efficiency and reduce the operational expenses of an emergency department. To keep patients informed of the conditions in the emergency room, recently hospitals have started publishing wait times online. Posted wait times help patients to choose the ED which is least overcrowded thus benefiting patients with shortest waiting time and allowing hospitals to allocate and plan resources appropriately. This requires an accurate and efficient method to model the experienced waiting time for patients visiting an emergency medical services unit. In this thesis, the author seeks to estimate the waiting time for low acuity patients within an ED setting; using regularized regression methods such as Lasso, Ridge, Elastic Net, SCAD and MCP; along with tree-based regression (Random Forest). For accurately capturing the dynamic state of emergency rooms, queues of patients at various stage of ED is used as candidate predictor variables along with time patient's arrival time to account for diurnal variation. Best waiting time prediction model is selected based on the analysis of historical data from the hospital. Tree-based regression model predicts wait time of",2017,
Lasso based gene selection for linear classifiers,"Selecting a subset of genes with strong discriminative power is a very important step in classification problems based on gene expression data. Lasso is known to have automatic variable selection ability in linear regression analysis. This paper uses Lasso to select most informative genes to represent the class label as a linear function of the gene expression data. The selected genes are further used to fit linear classifiers for tumor classification. The proposed approach (gene selection and linear classification) was applied to 5 publicly available cancer datasets. Compared to other methods in literature, the proposed method achieves similar or higher classification accuracy with fewer genes.",2009,2009 IEEE International Conference on Bioinformatics and Biomedicine Workshop
Machine Learning Methods in Gastroenterology.,"2 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 Dear Sir: We read with interest the article by Hsu et al, recently published in Gastroenterology. Predictive scores are very useful to develop more proper screening strategies and the paper by Hsu et al is of paramount importance, because it incorporates information from common genetic susceptibility loci. The correct merging of clinical, epidemiologic, and genetic parameters certainly adds to the performance of the final predictive risk model. Hsu et al included 27 validated common colorectal cancer susceptibility loci identified from genome-wide association studies together with clinical parameters such as age, sex, family history, and history of endoscopy (often neglected in previous works). However, in our opinion, some statistical concerns should be raised with regard to the methodology adopted in building the model. The authors quantify the association of colorectal cancer risk with baseline and genetic parameters by means of logistic regression and express it in terms of odds ratios and 95% CIs. The accuracy of classic logistic regression models may be impaired if many predictor variables are included (particularly when genetic parameters such as susceptibility loci or alleles are considered). In these circumstances, this results in inaccurate regression coefficients and large CIs. Sometimes, when the number of predictors is comparable with or even greater than the number of subjects enrolled, certain models will not converge at all (the calculation for the prediction cannot be performed and the equation cannot be solved). The study by Hsu et al has the great strength to present a very large sample size both with regard to training set and validation cohort. However, we think that a proper approach to address the aforementioned limitation of classic regression would be to apply a least absolute shrinkage and selection operator (LASSO) technique. The LASSO is a regression method that penalizes the absolute size of regression coefficients (or equivalently constrains the sum of the absolute values of the estimates), thus leading to a situation where some of the parameter estimates may be exactly zero. The larger the penalty applies, the further the estimates are shrunk toward zero. In this way, LASSO facilitates variable selection by incorporating only those variables most strongly correlated with the outcome of interest. In creating a more condensed clinical prediction model that only includes variables highly associated with the outcome, the condensed model can outperform a more comprehensive approach, particularly in presence of a high number of parameters.",2015,Gastroenterology
Least squares approximation with a diverging number of parameters,"Regularized regression with the l1 penalty is a popular approach for variable selection and coefficient estimation. For a unified treatment of the l1-constrained model selection, Wang and Leng (2007) proposed the least squares approximation method (LSA) for a fixed dimension. LSA makes use of a quadratic expansion of the loss function and takes full advantage of the fast Lasso algorithm in Efron etÃ‚ al. (2004). In this paper, we extend the fixed dimension LSA to the situation with a diverging number of parameters. We show that LSA possesses the oracle properties under appropriate conditions when the number of variables grows with the sample size. We propose a new tuning parameter selection method which achieves the oracle properties. Extensive simulation studies confirmed the theoretical results.",2010,Statistics & Probability Letters
Discovery of pre-therapy 2-deoxy-2-18F-fluoro-D-glucose positron emission tomography-based radiomics classifiers of survival outcome in non-small-cell lung cancer patients,"PurposeThe aim of this multi-center study was to discover and validate radiomics classifiers as image-derived biomarkers for risk stratification of non-small-cell lung cancer (NSCLC).Patients and methodsPre-therapy PET scans from a total of 358 Stage Iâ€“III NSCLC patients scheduled for radiotherapy/chemo-radiotherapy acquired between October 2008 and December 2013 were included in this seven-institution study. A semi-automatic threshold method was used to segment the primary tumors. Radiomics predictive classifiers were derived from a training set of 133 scans using TexLAB v2. Least absolute shrinkage and selection operator (LASSO) regression analysis was used for data dimension reduction and radiomics feature vector (FV) discovery. Multivariable analysis was performed to establish the relationship between FV, stage and overall survival (OS). Performance of the optimal FV was tested in an independent validation set of 204 patients, and a further independent set of 21 (TESTI) patients.ResultsOf 358 patients, 249 died within the follow-up period [median 22 (range 0â€“85) months]. From each primary tumor, 665 three-dimensional radiomics features from each of seven gray levels were extracted. The most predictive feature vector discovered (FVX) was independent of known prognostic factors, such as stage and tumor volume, and of interest to multi-center studies, invariant to the type of PET/CT manufacturer. Using the median cut-off, FVX predicted a 14-month survival difference in the validation cohort (NÂ =â€‰204, pÂ =â€‰0.00465; HRâ€‰=â€‰1.61, 95% CI 1.16â€“2.24). In the TESTI cohort, a smaller cohort that presented with unusually poor survival of stage I cancers, FVX correctly indicated a lack of survival difference (NÂ =â€‰21, pÂ =â€‰0.501). In contrast to the radiomics classifier, clinically routine PET variables including SUVmax, SUVmean and SUVpeak lacked any prognostic information.ConclusionPET-based radiomics classifiers derived from routine pre-treatment imaging possess intrinsic prognostic information for risk stratification of NSCLC patients to radiotherapy/chemo-radiotherapy.",2018,European Journal of Nuclear Medicine and Molecular Imaging
Connectivity-based parcellation of putamen using resting state fMRI data,"In this paper, we present a novel framework for parcellation of a brain region into functional sub-regions based on connectivity patterns between brain regions. The proposed method takes the prior neurological information into consideration and aims at finding spatially continuous and functionally consistent sub-regions in a given brain area. The proposed framework relies on 1) a sparse spatially regularized fused lasso regression model for encouraging spatially and functionally adjacent voxels to share similar regression coefficients despite of spatial noise; 2) an iterative voxels (groups) merging and adaptive parameter tuning process; and 3) a Graph-Cut optimization algorithm for assigning overlapped voxels into separate sub-regions. With spatial information incorporated, spatially continuous and functionally consistent sub-regions could be obtained and further used for subsequent brain connectivity analysis.",2015,2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI)
Super learning for daily streamflow forecasting: Large-scale demonstration and comparison with multiple machine learning algorithms,"Daily streamflow forecasting through data-driven approaches is traditionally performed using a single machine learning algorithm. Existing applications are mostly restricted to examination of few case studies, not allowing accurate assessment of the predictive performance of the algorithms involved. Here we propose super learning (a type of ensemble learning) by combining 10 machine learning algorithms. We apply the proposed algorithm in one-step ahead forecasting mode. For the application, we exploit a big dataset consisting of 10-year long time series of daily streamflow, precipitation and temperature from 511 basins. The super learner improves over the performance of the linear regression algorithm by 20.06%, outperforming the ""hard to beat in practice"" equal weight combiner. The latter improves over the performance of the linear regression algorithm by 19.21%. The best performing individual machine learning algorithm is neural networks, which improves over the performance of the linear regression algorithm by 16.73%, followed by extremely randomized trees (16.40%), XGBoost (15.92%), loess (15.36%), random forests (12.75%), polyMARS (12.36%), MARS (4.74%), lasso (0.11%) and support vector regression (-0.45%). Based on the obtained large-scale results, we propose super learning for daily streamflow forecasting.",2019,ArXiv
Multiple single nucleotide polymorphism analysis using penalized regression in nonlinear mixed-effect pharmacokinetic models.,"CONTEXT
Studies on the influence of single nucleotide polymorphisms (SNPs) on drug pharmacokinetics (PK) have usually been limited to the analysis of observed drug concentration or area under the concentration versus time curve. Nonlinear mixed effects models enable analysis of the entire curve, even for sparse data, but until recently, there has been no systematic method to examine the effects of multiple SNPs on the model parameters.


OBJECTIVE
The aim of this study was to assess different penalized regression methods for including SNPs in PK analyses.


METHODS
A total of 200 data sets were simulated under both the null and an alternative hypothesis. In each data set for each of the 300 participants, a PK profile at six sampling times was simulated and 1227 genotypes were generated through haplotypes. After modelling the PK profiles using an expectation maximization algorithm, genetic association with individual parameters was investigated using the following approaches: (i) a classical stepwise approach, (ii) ridge regression modified to include a test, (iii) Lasso and (iv) a generalization of Lasso, the HyperLasso.


RESULTS
Penalized regression approaches are often much faster than the stepwise approach. There are significantly fewer true positives for ridge regression than for the stepwise procedure and HyperLasso. The higher number of true positives in the stepwise procedure was accompanied by a higher count of false positives (not significant).


CONCLUSION
We find that all approaches except ridge regression show similar power, but penalized regression can be much less computationally demanding. We conclude that penalized regression should be preferred over stepwise procedures for PK analyses with a large panel of genetic covariates.",2013,Pharmacogenetics and genomics
Benefits of dimension reduction in penalized regression methods for high-dimensional grouped data: a case study in low sample size,"MOTIVATION
In some prediction analyses, predictors have a natural grouping structure and selecting predictors accounting for this additional information could be more effective for predicting the outcome accurately. Moreover, in a high dimension low sample size framework, obtaining a good predictive model becomes very challenging. The objective of this work was to investigate the benefits of dimension reduction in penalized regression methods, in terms of prediction performance and variable selection consistency, in high dimension low sample size data. Using two real datasets, we compared the performances of lasso, elastic net, group lasso, sparse group lasso, sparse partial least squares (PLS), group PLS and sparse group PLS.


RESULTS
Considering dimension reduction in penalized regression methods improved the prediction accuracy. The sparse group PLS reached the lowest prediction error while consistently selecting a few predictors from a single group.


AVAILABILITY AND IMPLEMENTATION
R codes for the prediction methods are freely available at https://github.com/SoufianeAjana/Blisar.


SUPPLEMENTARY INFORMATION
Supplementary data are available at Bioinformatics online.",2019,Bioinformatics
Gray matter network differences between behavioral variant frontotemporal dementia and Alzheimer's disease,"We set out to study whether single-subject gray matter (GM) networks show disturbances that are specific for Alzheimer's disease (AD; nÂ = 90) or behavioral variant frontotemporal dementia (bvFTD; nÂ = 59), and whether such disturbances would be related to cognitive deficits measured with mini-mental state examination and a neuropsychological battery, using subjective cognitive decline subjects as reference. AD and bvFTD patients had a lower degree, connectivity density, clustering, path length, betweenness centrality, and small world values compared with subjective cognitive decline. AD patients had a lower connectivity density than bvFTD patients (FÂ = 5.79, pÂ = 0.02; mean Â± standard deviation bvFTD 16.10 Â± 1.19%; mean Â± standard deviation AD 15.64 Â± 1.02%). Lasso logistic regression showed that connectivity differences between bvFTD and AD were specific to 23 anatomical areas, in terms of local GM volume, degree, and clustering. Lower clustering values and lower degree values were specifically associated with worse mini-mental state examination scores and lower performance on the neuropsychological tests. GM showed disease-specific alterations, when comparing bvFTD with AD patients, and these alterations were associated with cognitive deficits.",2017,Neurobiology of Aging
Non-Invasive Continuous Glucose Monitoring: Identification of Models for Multi-Sensor Systems,"Diabetes is a disease that undermines the normal regulation of glucose levels in the blood. In people with diabetes, the body does not secrete insulin (Type 1 diabetes) or derangements occur in both insulin secretion and action (Type 2 diabetes). In 
spite of the therapy, which is mainly based on controlled regimens of insulin and drug administration, diet, and physical exercise, tuned according to self-monitoring of blood glucose (SMBG) levels 3-4 times a day, blood glucose concentration often exceeds the normal range thresholds of 70-180 mg/dL. While hyperglycaemia mostly affects long-term complications (such as neuropathy, retinopathy, cardiovascular, and heart diseases), hypoglycaemia can be very dangerous in the short-term and, in the worst-case scenario, may bring the patient into hypoglycaemic coma. New scenarios in diabetes treatment have been opened in the last 15 years, when continuous glucose monitoring (CGM) sensors, able to monitor glucose concentration continuously (i.e. with a reading every 1 to 5 min) over several days, entered clinical research. CGM sensors can be used both retrospectively, e.g., to optimize the metabolic control, and in real-time applications, e.g., in the ""smart"" CGM sensors, able to generate alerts when glucose concentrations are predicted to exceed the normal range thresholds or in the so-called ""artificial pancreas"". Most CGM sensors exploit needles and are thus invasive, although minimally. In order to improve patients comfort, Non-Invasive Continuous Glucose Monitoring (NI-CGM) technologies have been widely investigated in the last years and their ability to monitor glucose changes in the human body has been demonstrated under highly controlled (e.g. in-clinic) conditions. 
As soon as these conditions become less favourable (e.g. in daily-life use) several problems have been experienced that can be associated with physiological and environmental 
perturbations. To tackle this issue, the multisensor concept received greater attention in the last few years. A multisensor consists in the embedding of sensors of different nature 
within the same device, allowing the measurement of endogenous (glucose, skin perfusion, sweating, movement, etc.) as well as exogenous (temperature, humidity, etc.) factors. 
The main glucose related signals and those measuring specific detrimental processes have to be combined through a suitable mathematical model with the final goal of estimating glucose non-invasively. White-box models, where differential equations are used to describe the internal behavior of the system, can be rarely considered to combine multisensor measurements because a physical/mechanistic model linking multisensor data 
to glucose is not easily available. A more viable approach considers black-box models, which do not describe the internal mechanisms of the system under study, but rather depict how the inputs (channels from the non-invasive device) determine the output (estimated glucose values) through a transfer function (which we restrict to the class of multivariate linear models). Unfortunately, numerical problems usually arise in the 
identication of model parameters, since the multisensor channels are highly correlated (especially for spectroscopy based devices) and for the potentially high dimension of the 
measurement space. 
The aim of the thesis is to investigate and evaluate different techniques usable for the identication of the multivariate linear regression models parameters linking multisensor data and glucose. In particular, the following methods are considered: Ordinary Least Squares (OLS); Partial Least Squares (PLS); the Least Absolute Shrinkage and Selection Operator (LASSO) based on l1 norm regularization; Ridge regression based on l2 norm regularization; Elastic Net (EN), based on the combination of the two previous norms. As a case study, we consider data from the Multisensor device mainly based on dielectric 
and optical sensors developed by Solianis Monitoring AG (Zurich, Switzerland) which partially sponsored the PhD scholarship. Solianis Monitoring AG IP portfolio is now 
held by Biovotion AG (Zurich, Switzerland). Forty-five recording sessions provided by Solianis Monitoring AG and collected in 6 diabetic human beings undertaken hypo and 
hyperglycaemic protocols performed at the University Hospital Zurich are considered. The models identified with the aforementioned techniques using a data subset are then 
assessed against an independent test data subset. Results show that methods controlling complexity outperform OLS during model test. In general, regularization techniques outperform PLS, especially those embedding the l1 norm (LASSO end EN), because they set many channel weights to zero thus resulting more robust to occasional spikes occurring in the Multisensor channels. In particular, the EN model results the best one, 
sharing both the properties of sparseness and the grouping effect induced by the l1 and l2 norms respectively. In general, results indicate that, although the performance, in terms of overall accuracy, is not yet comparable with that of SMBG enzyme-based needle sensors, the Multisensor platform combined with the Elastic-Net (EN) models is a valid tool for the real-time monitoring of glycaemic trends. An effective application concerns the complement of sparse SMBG measures with glucose trend information within the recently developed concept of dynamic risk for the correct judgment of dangerous events such as hypoglycaemia. 
The body of the thesis is organized into three main parts: Part I (including Chapters 1 to 4), first gives an introduction of the diabetes disease and of the current technologies for NI-CGM (including the Multisensor device by Solianis) and then states the aims of the thesis; Part II (which includes Chapters 5 to 9), first describes some of the issues to be faced in high dimensional regression problems, and then presents OLS, PLS, LASSO, Ridge and EN using a tutorial example to highlight their advantages and drawbacks; Finally, Part III (including Chapters 10-12), presents the case study with the data set and results. Some concluding remarks and possible future developments end the thesis. In particular, a Monte Carlo procedure to evaluate robustness of the calibration procedure for the Solianis Multisensor device is proposed, together with a new cost function to be used for identifying models.",2013,
Bayesian Network Classifier Based on L1 Regularization,"Variable order-based Bayesian network classifiers ignore the information of the selected variables in their sequence and their class label,which significantly hurts the classification accuracy.To address this problem,we proposed a simple and efficient L1 regularized Bayesian network classifier(L1-BNC).Through adjusting the constraint value of Lasso and fully taking advantage of the regression residuals of the information,L1-BNC takes the information of the sequence of selected variables and the class label into account,and then generates an excellent variable ordering sequence(L1 regularization path) for constructing a good Bayesian network classifier by the K2 algorithm.Experimental results show that L1-BNC outperforms existing state-of-the-art Bayesian network classifiers.In addition,in comparison with SVM,Knn and J48 classification algorithms,L1-BNC is also superior to those algorithms on most datasets.",2012,Computer Science
A Unified and Comprehensible View of Parametric and Kernel Methods for Genomic Prediction with Application to Rice,"One objective of this study was to provide readers with a clear and unified understanding of parametric statistical and kernel methods, used for genomic prediction, and to compare some of these in the context of rice breeding for quantitative traits. Furthermore, another objective was to provide a simple and user-friendly R package, named KRMM, which allows users to perform RKHS regression with several kernels. After introducing the concept of regularized empirical risk minimization, the connections between well-known parametric and kernel methods such as Ridge regression [i.e., genomic best linear unbiased predictor (GBLUP)] and reproducing kernel Hilbert space (RKHS) regression were reviewed. Ridge regression was then reformulated so as to show and emphasize the advantage of the kernel ""trick"" concept, exploited by kernel methods in the context of epistatic genetic architectures, over parametric frameworks used by conventional methods. Some parametric and kernel methods; least absolute shrinkage and selection operator (LASSO), GBLUP, support vector machine regression (SVR) and RKHS regression were thereupon compared for their genomic predictive ability in the context of rice breeding using three real data sets. Among the compared methods, RKHS regression and SVR were often the most accurate methods for prediction followed by GBLUP and LASSO. An R function which allows users to perform RR-BLUP of marker effects, GBLUP and RKHS regression, with a Gaussian, Laplacian, polynomial or ANOVA kernel, in a reasonable computation time has been developed. Moreover, a modified version of this function, which allows users to tune kernels for RKHS regression, has also been developed and parallelized for HPC Linux clusters. The corresponding KRMM package and all scripts have been made publicly available.",2016,Frontiers in Genetics
A LASSO-Based Diagnostic Framework for Multivariate Statistical Process Control,"In monitoring complex systems, apart from quick detection of abnormal changes of system performance and key parameters, accurate fault diagnosis of responsible factors has become increasingly critical in a variety of applications that involve rich process data. Conventional statistical process control (SPC) methods, such as interpretation and decomposition of Hotellingâ€™s T2-type statistic, are often computationally expensive in such high-dimensional problems. In this article, we frame fault isolation as a two-sample variable selection problem to provide a unified diagnosis framework based on Bayesian information criterion (BIC). We propose a practical LASSO-based diagnostic procedure which combines BIC with the popular adaptive LASSO variable selection method. Given the oracle property of LASSO and its algorithm, the diagnostic result can be obtained easily and quickly with a similar computational effort as least squares regression. More importantly, the proposed method does not require making any extra t...",2011,Technometrics
Efficient methods for estimating constrained parameters with applications to regularized (lasso) logistic regression,"Fitting logistic regression models is challenging when their parameters are restricted. In this article, we first develop a quadratic lower-bound (QLB) algorithm for optimization with box or linear inequality constraints and derive the fastest QLB algorithm corresponding to the smallest global majorization matrix. The proposed QLB algorithm is particularly suited to problems to which the EM-type algorithms are not applicable (e.g., logistic, multinomial logistic, and Cox's proportional hazards models) while it retains the same EM ascent property and thus assures the monotonic convergence. Secondly, we generalize the QLB algorithm to penalized problems in which the penalty functions may not be totally differentiable. The proposed method thus provides an alternative algorithm for estimation in lasso logistic regression, where the convergence of the existing lasso algorithm is not generally ensured. Finally, by relaxing the ascent requirement, convergence speed can be further accelerated. We introduce a pseudo-Newton method that retains the simplicity of the QLB algorithm and the fast convergence of the Newton method. Theoretical justification and numerical examples show that the pseudo-Newton method is up to 71 (in terms of CPU time) or 107 (in terms of number of iterations) times faster than the fastest QLB algorithm and thus makes bootstrap variance estimation feasible. Simulations and comparisons are performed and three real examples (Down syndrome data, kyphosis data, and colon microarray data) are analyzed to illustrate the proposed methods.",2008,Comput. Stat. Data Anal.
Distance metric choice can both reduce and induce collinearity in geographically weighted regression,"This paper explores the impact of different distance metrics on collinearity in local regression models such as geographically weighted regression. Using a case study of house price data collected in Ha Ná»™i, Vietnam, and by fully varying both power and rotation parameters to create different Minkowski distances, the analysis shows that local collinearity can be both negatively and positively affected by distance metric choice. The Minkowski distance that maximised collinearity in a geographically weighted regression was approximate to a Manhattan distance with (powerâ€‰=â€‰0.70) with a rotation of 30Â°, and that which minimised collinearity was parameterised with powerâ€‰=â€‰0.05 and a rotation of 70Â°. The results indicate that distance metric choice can provide a useful extra tuning component to address local collinearity issues in spatially varying coefficient modelling and that understanding the interaction of distance metric and collinearity can provide insight into the nature and structure of the data relationships. The discussion considers first, the exploration and selection of different distance metrics to minimise collinearity as an alternative to localised ridge regression, lasso and elastic net approaches. Second, it discusses the how distance metric choice could extend the methods that additionally optimise local model fit (lasso and elastic net) by selecting a distance metric that further helped minimise local collinearity. Third, it identifies the need to investigate the relationship between kernel bandwidth, distance metrics and collinearity as an area of further work.",2020,
Structure learning with large sparse undirected graphs and its applications,"Learning the structures of large undirected graphical models from data is an active research area and has many potential applications in various domains, including molecular biology, social science, marketing data analysis, among others. The estimated structures provide semantic clarity, the possibility of causal interpretation, and ease of integration with a variety of tools. For example, one very important direction in system biology is to discover gene regulatory networks from microarray data (together with other data sources) based on the observed mRNA levels of thousands of genes under various conditions. The basic assumption is that if two genes are co-regulated by the same proteins, then they tend to have similar patterns at the mRNA levels. Thus it is possible to learn a gene regulatory network from microarray data if we treat each gene as a node variable and each condition as a configuration instance. Structure learning for undirected graphs is an open challenge in machine learning. Most probabilistic structure learning approaches enforce sparsity on the estimated structure by penalizing the number of edges in the graph, which leads to a non-convex optimization problem. Thus these approaches have to search for locally optimal solutions through the combinatorial space of structures, which makes them unscalable for large graphs. Furthermore, the local optimal solution they find could be far away from the global optimal solution, especially when the number of configuration instances is small compared with the number of nodes in the graph. This thesis tries to address these issues by developing a novel structure learning approach that can learn large undirected graphs efficiently in a probabilistic framework. We use the Graphical Gaussian Model (GGM) as the underlying model and propose a novel ARD style Wishart prior for the precision matrix of the GGM, which encodes the graph structure we want to learn. With this prior, we can get the MAP estimation of the precision matrix by solving a modified version of Lasso regression and thus achieve a global optimal sparse solution. By proposing a generalized version of Lasso regression, which is called the Feature Vector Machine (FVM), our structure learning model is further extended so that it can capture non-linear dependencies between node variables. In particular, the optimization problem in our model remains convex even in non-linear cases, which makes our solution globally optimal. We have also developed a graph-based classification approach for predicting node labels given network structures, either observed or automatically induced. This approach is especially suitable when edges in the networks contain multiple input features.",2007,
Accurately forecasting temperatures in smart buildings using fewer sensors,"Forecasts of temperature in a â€œsmartâ€ building, i.e. one that is outfitted with sensors, are computed from data gathered by these sensors. Model predictive controllers can use accurate temperature forecasts to save energy by optimally using heating, ventilation and air conditioners while achieving comfort. We report on experiments from such a house. We select different sets of sensors, build a temperature model from each set, and compare the accuracy of these models. While a primary goal of this research area is to reduce energy consumption, in this paper, besides the cost of energy, we consider the cost of data collection and management. Our approach informs the selection of an optimal set of sensors for any model predictive controller to reduce overall costs, using any forecasting methodology. We use lasso regression with lagged observations, which compares favourably to previous methods using the same data.",2017,Personal and Ubiquitous Computing
Prognostic value of an immunohistochemical signature in patients with esophageal squamous cell carcinoma undergoing radical esophagectomy,"Here, we aimed to identify an immunohistochemical (IHC)-based classifier as a prognostic factor in patients with esophageal squamous cell carcinoma (ESCC). A cohort of 235 patients with ESCC undergoing radical esophagectomy (with complete clinical and pathological information) were enrolled in the study. Using the least absolute shrinkage and selection operator (LASSO) regression model, we extracted six IHC features associated with progression-free survival (PFS) and then built a classifier in the discovery cohort (nÂ =Â 141). The prognostic value of this classifier was further confirmed in the validation cohort (nÂ =Â 94). Additionally, we developed a nomogram integrating the IHC-based classifier to predict the PFS. We used the IHC-based classifier to stratify patients into high- and low-risk groups. In the discovery cohort, 5-year PFS was 22.4% (95% CI: 0.14-0.36) for the high-risk group and 43.3% (95% CI: 0.32-0.58) for the low-risk group (PÂ =Â 0.00064), and in the validation cohort, 5-year PFS was 20.58% (95% CI: 0.12-0.36) for the high-risk group and 36.43% (95% CI: 0.22-0.60) for the low-risk group (PÂ =Â 0.0082). Multivariable analysis demonstrated that the IHC-based classifier was an independent prognostic factor for predicting PFS of patients with ESCC. We further developed a nomogram integrating the IHC-based classifier and clinicopathological risk factors (gender, American Joint Committee on Cancer staging, and vascular invasion status) to predict the 3- and 5-year PFS. The performance of the nomogram was evaluated and proved to be clinically useful. Our 6-IHC marker-based classifier is a reliable prognostic tool to facilitate theÂ individual management of patients with ESCC after radical esophagectomy.",2018,Molecular Oncology
What is the relative impact of primary health care quality and conditional cash transfer program in child mortality?,"Evaluate how coverage and quality of primary health care (PHC) and a conditional cash transfer (CCT) program associate with child mortality in Brazil. Multivariate linear regression models and least absolute shrinkage and selection estimator (LASSO) were utilized with the municipal level child mortality rate as the key dependent variable. PHC quality with PHC and CCT coverage were the independent variables. The quality of the Brazilian PHC was assessed using the Brazilian National Program for Access and Quality Improvement in PHC data. PHC and CCT coverage were calculated based on Brazilian official databases. Human developmental index (HDI), municipality size, and country region were used as control variables. A total of 3441 municipalities were evaluated. We found that ESF (EstratÃ©gia SaÃºde da FamÃ­lia) quality variables PLANNING [Family Health Team Planning activities], CITYSUPPORT [municipality support for Family Health Strategy activities], EXAMS [exams offered and priority groups seen by the family health team], and PRENATAL [prenatal care and exams provided by the family health team], as well as HDI, percentage of PHC coverage, percentage of CCT coverage, and population size have significant and negative relationships with 1-year-old child mortality. LASSO regression results confirmed these associations. Quality is an important element of effective social service provision. This exploration represents one of the first investigations into the role of PHC system quality, and how it is related to health outcomes, while also considering PHC and conditional cash transfer program coverage. Quality of PHC, measured by work process variables, plays an important role in child mortality. Efforts on PHC quality and coverage, as well as on CCT program coverage, are important to child mortality reduction. Therefore, this is an important finding to other PHC public health services.",2019,Canadian Journal of Public Health
Sparse Subspace Clustering via Two-Step Reweighted L1-Minimization: Algorithm and Provable Neighbor Recovery Rates,"Sparse subspace clustering (SSC) relies on sparse regression for accurate neighbor identification. Inspired by recent progress in compressive sensing, this paper proposes a new sparse regression scheme for SSC via two-step reweighted $\ell_1$-minimization, which also generalizes a two-step $\ell_1$-minimization algorithm introduced by E. J. Cand\`es et al in [The Annals of Statistics, vol. 42, no. 2, pp. 669-699, 2014] without incurring extra algorithmic complexity. To fully exploit the prior information offered by the computed sparse representation vector in the first step, our approach places a weight on each component of the regression vector, and solves a weighted LASSO in the second step. We propose a data weighting rule suitable for enhancing neighbor identification accuracy. Then, under the formulation of the dual problem of weighted LASSO, we study in depth the theoretical neighbor recovery rates of the proposed scheme. Specifically, an interesting connection between the locations of nonzeros of the optimal sparse solution to the weighted LASSO and the indexes of the active constraints of the dual problem is established. Afterwards, under the semi-random model, analytic probability lower/upper bounds for various neighbor recovery events are derived. Our analytic results confirm that, with the aid of data weighting and if the prior neighbor information is enough accurate, the proposed scheme with a higher probability can produce many correct neighbors and few incorrect neighbors as compared to the solution without data weighting. Computer simulations are provided to validate our analytic study and evidence the effectiveness of the proposed approach.",2019,ArXiv
