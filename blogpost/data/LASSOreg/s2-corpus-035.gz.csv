title,abstract,year,journal
"Fabrication, calibration, and recovery of chemical nanosensor array for ammonia detection","The low selectivity of nanosensors is one of their major obstacles for their wide deployment. To enhance the selectivity, nanosensor array made from zinc oxide (ZnO) nanowires and carbon nanotubes (CNTs) was assembled through dielectrophoresis (DEP). The fabricated nanosensor array was used to detect ammonia (NH3) in a well-controlled environment at room temperature. Because of their opposite material types, ZnO nanowire based sensor behaved oppositely to CNT based sensor. In this study, it is also demonstrated that DC biases can quickly recover both sensing elements. After collecting sensing signals from two transducers under different NH3 concentrations, the concentration of NH3 can be estimated through regression methods. It is shown that quadratic model with the lasso performs well on the collected data.",2017,2017 IEEE 17th International Conference on Nanotechnology (IEEE-NANO)
Fast FSR methods for second-order linear regression models,"CREWS, HUGH BATES. Fast FSR Methods for Second-Order Linear Regression Models. (Under the direction of Leonard Stefanski and Dennis Boos.) Many variable selection techniques have been developed that focus on first-order linear regression models. In some applications, such as modeling response surfaces, fitting second-order terms can improve predictive accuracy. However, the number of spurious interactions can be large leading to poor results with many methods. We focus on forward selection, describing algorithms that use the natural hierarchy existing in second-order linear regression models to limit spurious interactions. We then develop stopping rules by extending False Selection Rate methodology to these algorithms. In addition, we describe alternative estimation methods for fitting regression models including the LASSO, CART, and MARS. We also propose a general method for controlling multiple-group false selection rates, which we apply to second-order linear regression models. By estimating a separate entry level for first-order and secondorder terms, we obtain equal contributions to the false selection rate from each group. We compare the methods via Monte Carlo simulation and apply them to optimizing response surface experimental designs. Fast FSR Methods for Second-Order Linear Regression Models",2008,
Statistical and Network Analysis of Metabolomics Data,"Metabolomics encompasses analysis of metabolites using profiling techniques such as mass spectroscopy (MS) and nuclear magnetic resonance (NMR). Statistical analysis is performed on the profiled data to determine variations in the levels of metabolites. The goal here is to reveal relationships between the variations in the concentrations of metabolites and specific pathophysiological conditions such as diseases or external factors. Metabolomics has been widely used to characterize metabolites in various body fluids such as saliva, serum and urine in various fields of medical research including cancer [3], cardialogy [6], diabetes [5], human infections [12], neurology [7], neonatology [4] and respiratory diseases [2] to name a few.
 In the statistical analysis of metabolomics data, many methods are used which can be categorized as univariate and multivariate analysis methods. Univariate methods are very commonly applied due to their ease of use and interpretation. These methods consider metabolomic features (variables) one at a time independent of each other, thus, ignoring correlations with other features. Moreover, as pointed by Alonso et al. [1], these methods ignore confounding variables such as age, gender, body mass index (BMI), which may lead to incorrect results [13, 15]. On the other hand, multivariate methods consider all the features and their correlations during data analysis. These methods include unsupervised methods such as principal component analysis (PCA), and supervised methods such as partial least squares (PLS) and support vector machine (SVM). Alonso et al. has provided a review of univariate and multivariate methods used in metabolomics. To the best of our knowledge, there are many state of the art statistical methods that have not be used for metabolomic data analysis. A significant advantage of these methods over commonly used methods is their ability to process high-dimensional data. Along with state-of-the-art statistical methods we have used differential network analysis to identify variations at system level.
 In this work we have analyzed urine samples from Qatar Metabolomics Study on Diabetes (QMDiab) for identification of potential biomarkers. QMDiab was conducted by Hamad Medical Corporation, Qatar (HMC) and Weill Cornell Medical College, Qatar in 2012 with approval from the Institutional Review Boards of HMC and Weill Cornell Medical College-Qatar (Research Protocol number 11131/11). Written informed consent was obtained from all participants. Subjects in the study included males and females from Arab and Asian ethnicities aging 17-81 years. Urine samples were sent to Chenomx Inc., Alberta, Canada for proton nuclear magnetic resonance (1H NMR). Although the original study was targeting investigation of type 2 diabetes, in this paper we are focusing on obesity as well by using BMI as a representative measure of obesity.
 In this work we have used regularization models and differential network analysis. We have used the elastic net, glinternet, the lasso projection and high-dimensional inference. The elastic net uses L1 and L2 penalty resulting in a mix of ridge and lasso regression. The glinternet is a group-lasso based method developed by Lim and Hastie [9]. The method learns pairwise interactions of variables in linear regression models satisfying strong hierarchy. The lasso projection (lasso proj) or de-sparsified lasso is a regularization based method that performs statistical inference of low dimensional parameters with high dimensional data [17]. The method uses low dimension projection approach to construct confidence intervals for the estimated regression parameters. The high-dimensional inference computes P-values of variables and associated confidence intervals in high-dimensional data [10].
 Further, we performed differential network analysis to identify variable interactions, which differentiate between diabetic and non-diabetic, or obese and lean subjects. The network is constructed using mutual information between the variables for different groups of samples. We applied the differential network analysis, dGHD algorithm, proposed by Ruan et al. [14] for detecting interaction patterns, which differentiate two networks. The algorithm uses the Generalised Hamming Distance (GHD) for calculating topological differences between the networks along with computation of their statistical significance.
 It is astonishing that the proposed methods, which have not been applied in the field yet, identify potential biomarkers, proposed in the literature by previous studies, in a small dataset. The results for the elastic net, the glinternet and the lasso proj are summarized in Table 1. For diabetes analysis, identified significant variables include age, betaine, glycolate and glucose, well known biomarkers for diabetes [8, 11]. For obesity analysis, identified significant variables include age, dimethylamine, succinate and cis-aconitate, previously identified by [16]. The high-dimensional inference only identified age and betaine for diabetes study.
 We conclude that state-of-the-art statistical and network analysis methods can be used for metabolomics data analysis for datasets with limited number of samples. The number of metabolomic features is increasing with the advancement of technologies. The ability of these methods to handle high-dimensional data make them suitable in the settings where the number of samples is smaller than the number of features. These methods can help in identification potential biomarkers in future studies.",2016,"Proceedings of the 7th ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics"
Blood Saturation Decreasing Level Based on the Features of a Spirogram Signal,"Obstructive sleep apnea is a common disease associated with respiratory processes during sleep due to the narrowing of the upper respiratory airways, which leads to a decrease in the amount of oxygen in arterial blood flow - SpO2 â€“ to human internal organs. Low blood saturation values can lead to serious consequences in the patient's health, and it can be monitored, for example, using pulsoximetry sensors. In this article a comparative analysis of various regression approaches â€“ Support Vector Regression (SVR), Gradient Boosting, Lasso Regression, Ridge Regression and Sequential Neural Network-was carried out in order to discover an opportunity for predicting SpO2 level decreasing basing on the features of a spirogram for 4 different patients separately. The best results of R2 determination score were shown by models based on SVR and Neural Network: 0.6878 and 0.6697. The best correlation coefficient between the ground truth blood saturation decreasing and the predicted values was 0.9019 for the Support Vector Regressor model.",2019,"2019 Ural Symposium on Biomedical Engineering, Radioelectronics and Information Technology (USBEREIT)"
Selecting Shrinkage Parameters for Effect Estimation: The Multi-Ethnic Study of Atherosclerosis,"We present a method for improving estimation in linear regression models in samples of moderate size, using shrinkage techniques. Our work connects the theory of causal inference, which describes how variable adjustment should be performed with large samples, with shrinkage estimators such as ridge regression and the least absolute shrinkage and selection operator (LASSO), which can perform better in sample sizes seen in epidemiologic practice. Shrinkage methods reduce mean squared error by trading off some amount of bias for a reduction in variance. However, when inference is the goal, there are no standard methods for choosing the penalty ""tuning"" parameters that govern these tradeoffs. We propose selecting the penalty parameters for these shrinkage estimators by minimizing bias and variance in future similar data sets drawn from the posterior predictive distribution. Our method provides both the point estimate of interest and corresponding standard error estimates. Through simulations, we demonstrate that it can achieve better mean squared error than using cross-validation for penalty parameter selection. We apply our method to a cross-sectional analysis of the association between smoking and carotid intima-media thickness in the Multi-Ethnic Study of Atherosclerosis (multiple US locations, 2000-2002) and compare it with similar analyses of these data.",2018,American Journal of Epidemiology
Mod eling Propensity for Readmissions with Claims Data,"We built a predictive model for hospital readmissions using claims data. The probability of hospital readmission was computed as a logit model. The coefficients for the logit model were trained with a Lasso regression on a set of 237,129 patients and tested on 70,465. The modelâ€™s discriminative power evaluated via ROC was 0.813 and its calibration had p <0.05 assessed using the Hosmer-Lemeshow test.",2011,
Oracle Efficient Estimation and Forecasting with the Adaptive Lasso and the Adaptive Group Lasso in Vector Autoregressions,"We show that the adaptive Lasso (aLasso) and the adaptive group Lasso (agLasso) are oracle efficient in stationary vector autoregressions where the number of parameters per equation is smaller than the number of observations. In particular, this means that the parameters are estimated consistently at root T rate, that the truly zero parameters are classiffied as such asymptotically and that the non-zero parameters are estimated as efficiently as if only the relevant variables had been included in the model from the outset. The group adaptive Lasso differs from the adaptive Lasso by dividing the covariates into groups whose members are all relevant or all irrelevant. Both estimators have the property that they perform variable selection and estimation in one step. We evaluate the forecasting accuracy of these estimators for a large set of macroeconomic variables. The Lasso is found to be the most precise procedure overall. The adaptive and the adaptive group Lasso are less stable but mostly perform at par with the common factor models.",2012,
Inference of high-dimensional linear models with time-varying coefficients,"We propose a pointwise inference algorithm for high-dimensional linear models with time-varying coefficients. The method is based on a novel combination of the nonparametric kernel smoothing technique and a Lasso bias-corrected ridge regression estimator. First, due to the non-stationarity feature of the model, dynamic bias-variance decomposition of the estimator is obtained. Second, by a bias-correction procedure according to our fundamental representation, the local null distribution of the proposed estimator of the time-varying coefficient vector is characterized to compute the p-values for the iid Gaussian and heavy-tailed errors. In addition, the limiting null distribution is also established for Gaussian process errors and we show that the asymptotic properties have dichotomy features between short-range and long-range dependent errors. Third, the p-values are further adjusted by a Bonferroni-type correction procedure, which is provably to control the familywise error rate (FWER) in the asymptotic sense at each time point. Finally, finite sample size performance of the proposed inference algorithm on a synthetic data and a real application to learn brain connectivity by using the resting-state fMRI data for Parkinson's disease are illustrated.",2015,arXiv: Methodology
Asset Allocation Under Predictability and Parameter Uncertainty Using LASSO,"We consider a short-term investor who exploits return predictability in stocks and bonds to maximize mean-variance utility. Since the true parameters are unknown, we resort to portfolio optimization in form of linear regression with LASSO in order to mitigate problems related to estimation errors as done by Li (2015). As standard cross-validation relies on the assumption of i.i.d. returns, we propose a new type of cross-validation that selects Î» from simulated returns sampled from a multivariate normal distribution. We find an inverse U-shaped relationship between selected Î» and expected utility, and we show that the optimal value of Î» declines as the number of observations used to estimate the parameters increases. We finally show how our strategy outperforms some commonly employed benchmarks.",2019,
Markers of neuroinflammation associated with Alzheimerâ€™s disease pathology in older adults,"BACKGROUND
In vitro and animal studies have linked neuroinflammation to Alzheimer's disease (AD) pathology. Studies on markers of inflammation in subjects with mild cognitive impairment or AD dementia provided inconsistent results. We hypothesized that distinct blood and cerebrospinal fluid (CSF) inflammatory markers are associated with biomarkers of amyloid and tau pathology in older adults without cognitive impairment or with beginning cognitive decline.


OBJECTIVE
To identify blood-based and CSF neuroinflammation marker signatures associated with AD pathology (i.e. an AD CSF biomarker profile) and to investigate associations of inflammation markers with CSF biomarkers of amyloid, tau pathology, and neuronal injury.


DESIGN/METHODS
Cross-sectional analysis was performed on data from 120 older community-dwelling adults with normal cognition (n=48) or with cognitive impairment (n=72). CSF AÎ²1-42, tau and p-tau181, and a panel of 37 neuroinflammatory markers in both CSF and serum were quantified. Least absolute shrinkage and selection operator (LASSO) regression was applied to determine a reference model that best predicts an AD CSF biomarker profile defined a priori as p-tau181/AÎ²1-42 ratio >0.0779. It was then compared to a second model that included the inflammatory markers from either serum or CSF. In addition, the correlations between inflammatory markers and CSF AÎ²1-42, tau and p-tau181 levels were assessed.


RESULTS
Forty-two subjects met criteria for having an AD CSF biomarker profile. The best predictive models included 8 serum or 3 CSF neuroinflammatory markers related to cytokine mediated inflammation, vascular injury, and angiogenesis. Both models improved the accuracy to predict an AD biomarker profile when compared to the reference model. In analyses separately performed in the subgroup of participants with cognitive impairment, adding the serum or the CSF neuroinflammation markers also improved the accuracy of the diagnosis of AD pathology. None of the inflammatory markers correlated with the CSF AÎ²1-42 levels. Six CSF markers (IL-15, MCP-1, VEGFR-1, sICAM1, sVCAM-1, and VEGF-D) correlated with the CSF tau and p-tau181 levels, and these associations remained significant after controlling for age, sex, cognitive impairment, and APOEÎµ4 status.


CONCLUSIONS
The identified serum and CSF neuroinflammation biomarker signatures improve the accuracy of classification for AD pathology in older adults. Our results suggest that inflammation, vascular injury, and angiogenesis as reflected by CSF markers are closely related to cerebral tau pathology.",2017,"Brain, Behavior, and Immunity"
Metabolic Biosynthesis Pathways Identified from Fecal Microbiome Associated with Prostate Cancer.,"BACKGROUND
The fecal microbiome is associated with prostate cancer risk factors (obesity, inflammation) and can metabolize and produce various products that may influence cancer but have yet to be defined in prostate cancer.


OBJECTIVE
To investigate gut bacterial diversity, identify specific metabolic pathways associated with disease, and develop a microbiome risk profile for prostate cancer.


DESIGN, SETTING, AND PARTICIPANTS
After prospective collection of 133 rectal swab samples 2 wk before the transrectal prostate biopsy, we perform 16S rRNA amplicon sequencing on 105 samples (64 with cancer, 41 without cancer). Phylogenetic Investigation of Communities by Reconstruction of Unobserved States (PICRUSt) was applied to infer functional categories associated with taxonomic composition. The p values were adjusted using the false discovery rate. The Î±- and Î²-diversity analyses were performed using QIIME. The Mann-Whitney U test was employed to evaluate the statistical significance of Î²-diversity distances within and between groups of interest, and least absolute shrinkage and selection operator (LASSO) regression analysis was used to determine pathway significance.


OUTCOME MEASUREMENTS AND STATISTICAL ANALYSIS
The detection of prostate cancer on transrectal prostate needle biopsy and 16s microbiome profile.


RESULTS AND LIMITATIONS
We identified significant associations between total community composition and cancer/non-cancer status (Bray-Curtis distance metric, p<0.01). We identified significant differences in enrichments of Bacteroides and Streptococcus species in cancer (all p<0.04). Folate (LDA 3.8) and arginine (LDA 4.1) were the most significantly altered pathways. We formed a novel microbiome-derived risk factor for prostate cancer based on 10 aberrant metabolic pathways (area under curve=0.64, p=0.02).


CONCLUSIONS
Microbiome analyses on men undergoing prostate biopsy noted mostly similar bacterial species diversity among men diagnosed with and without prostate cancer. The microbiome may have subtle influences on prostate cancer but are likely patient-specific and would require paired analysis and precise manipulation, such as improvement of natural bacterial folate production.


PATIENT SUMMARY
Microbiome evaluation may provide patients with personalized data regarding the presence or absence of particular bacteria that have metabolic functions and implications regarding prostate cancer risk. The study provides a basis to investigate the manipulation of aberrant microbiomes to reduce prostate cancer risk.",2018,European urology
"Bioinformatic Profiling Identifies a Fatty Acid Metabolism-Related Gene Risk Signature for Malignancy, Prognosis, and Immune Phenotype of Glioma","Cancer cells commonly have metabolic abnormalities. Aside from altered glucose and amino acid metabolism, cancers cells often share the attribute of fatty acid metabolic alterations. However, fatty acid metabolism related-gene set has not been systematically investigated in gliomas. Here, we provide a bioinformatic profiling of the fatty acid catabolic metabolism-related gene risk signature for the malignancy, prognosis and immune phenotype of glioma. In this study, a cohort of 325 patients with whole genome RNA-seq expression data from the Chinese Glioma Genome Atlas (CGGA) dataset was used as training set, while another cohort of 667 patients from The Cancer Genome Atlas (TCGA) dataset was used as validating set. After confirmed that fatty acid catabolic metabolism-related gene set could distinguish clinicopathological features of gliomas, we used LASSO regression analysis to develop a fatty-acid metabolism-related gene risk signature for glioma. This 8-gene risk signature was found to be a good predictor of clinical and molecular features involved in the malignancy of gliomas. We also identified that this 8-gene risk signature had high prognostic values in patients with gliomas. Correlation analysis showed that our risk signature was closely associated with the immune cells involved in the microenvironment of glioma. Furthermore, the fatty acid catabolic metabolism-related gene risk signature was also found to be significantly correlated with immune checkpoint members B7-H3 and Tim-3. In summary, we have identified a fatty acid metabolism-related gene risk signature for malignancy, prognosis, and immune phenotype of glioma; and our study might contribute to better understanding of metabolic pathways and further developing of novel therapeutic approaches for gliomas.",2019,Disease Markers
The Remarkable Multidimensionality in the Cross-Section of Expected U.S. Stock Returns,"20+ years after Fama & French (1992), we re-measure the dimensionality of the cross-section of expected U.S. monthly stock returns in light of the large number of return predictive signals (RPS) that have been identified by business academics over the past 40 years. Using 100 readily programmed RPS, we find that a remarkable 24 are multidimensionally priced as defined by their mean coefficients having an absolute t-statistic ï‚³ 3.0 in Fama-MacBeth regressions where all RPS are simultaneously projected onto 1-month ahead returns during 1980-2012. We confirm the high degree of dimensionality in returns using factor analysis of RPS, factor analysis of long/short RPS hedge returns, LASSO regression, regressions of portfolio returns on RPS factor returns, and out-ofsample RPS hedge portfolio returns. We put forward a new empirically determined 10-RPS model of expected returns for consideration by researchers and practitioners. We also discuss other implications of our findings, chief of which is the need for research that explains why stock returns are so multidimensional and why the most empirically important RPS are priced the way they are. This version: April 2, 2014 * Corresponding author. Our paper has greatly benefitted from the comments of Jeff Abarbanell, Sanjeev Bhojraj, Matt Bloomfield, John Cochrane, Oleg Grudin, Bruce Jacobs, Bryan Kelly, Juhani Linnainmaa, Ed Maydew, Scott Richardson, Jacob Sagi, Eric Yeung, and workshop participants at the University of Chicago, Cornell University, UNC Chapel Hill, the Fall 2013 Conference of the Society of Quantitative Analysts, and the Fall 2013 Chicago Quantitative Alliance Conference. The SAS programs we use to create our RPS data and execute most of our statistical analyses will be made publicly available on 7/1/14.",2013,
Probing sequence-level instructions for gene expression,"Gene regulation is tightly controlled to ensure a wide variety of cell types and functions. These controls take place at different levels and are associated with different genomic regulatory regions. An actual challenge is to understand how the gene regulation machinery works in each cell type and to identify the most important regulators. Several studies attempt to understand the regulatory mechanisms by modeling gene expression using epigenetic marks. Nonetheless, these approaches rely on experimental data which are limited to some samples, costly and time-consuming. Besides, the important component of gene regulation based at the sequence level cannot be captured by these approaches. The main objective of this thesis is to explain mRNA expression based only on DNA sequences features. In a first work, we use Lasso penalized linear regression to predict gene expression using DNA features such as transcription factor binding site (motifs) and nucleotide compositions. We measured the accuracy of our approach on several data from the TCGA database and find similar performance as that of models fitted with experimental data. In addition, we show that nucleotide compositions of different regulatory regions have a major impact on gene expression. Furthermore, we rank the influence of each regulatory regions and show a strong effect of the gene body, especially introns.In a second part, we try to increase the performances of the model. We first consider adding interactions between nucleotide compositions and applying non-linear transformations on predictive variables. This induces a slight increase in model performances.To go one step further, we then learn deep neuronal networks. We consider two types of neural networks: multilayer perceptrons and convolution networks. Hyperparameters of each network are optimized. The performances of both types of networks appear slightly higher than those of a Lasso penalized linear model. In this thesis, we were able to (i) demonstrate the existence of sequence-level instructions for gene expression and (ii) provide different frameworks based on complementary approaches. Additional work is ongoing, in particular with the last direction based on deep learning, with the aim of detecting additional information present in the sequence.",2018,
Identification of Transcriptional Regulatory Elements: Novel Machine Learning Approaches,"The identification of transcriptional regulatory elements is of pivotal importance in understanding the molecular mechanisms that govern specific expression patterns. Despite the fast development of next generation sequencing (NGS) technology for profiling genome-wide transcription factors (TFs) binding, DNA methylation and other epigenetic features, the identification of transcription factor binding sites (TFBSs) with active functional relevance remains a challenging task in system biology. Therefore, computational prediction still plays an important role. However, the sequence-based prediction can not represent the dynamics of transcription regulation in cell-specific or condition-specific manner. In addition, the overwhelming potential TFBSs obtained through experimental or computation procedures with an unknown false positive (FP) rate also prohibit the reliable biological findings. The integration of other types of functional genomics data are crucial for the elucidation of regulatory mechanism. 

In this thesis, we explore the potential of machine learning methods in distinguishing the most causal TFBSs from enormous predicted candidates. We first focus on the reconstruction of transcriptional regulatory network(TRN) using TFBSs predicted in the promoter regions of co-expressed genes, under the commonly-accepted assumption that a set of genes showing similar expression profiles are likely to be commonly regulated by a collection of TFs. We propose a penalized multinomial logistic regression model to prioritize the most representative TFBSs for each set of co-expressed genes, among multiple sets simultaneously. Joint effects of the TF interaction are also considered. The results through cross-validation show that the minimum classification error rate can be reached at 0.302 and the prioritized TFBSs are not obtained by chance. On this basis, we further model gene expression time course using the predicted TFBSs from a set of co-expressed genes to reconstruct TRN. The expression of one gene at one time point is modeled as the linear combination of expression of all non-TF coding genes and the expression of all TF coding genes weight by transformed TFBSs binding scores at the previous time point. The evaluation shows high performance in simulation study (AUC=0.85). Our model also successfully identify the TF coding genes causal for cell apoptosis in MCF-7:5C cell line which is sensitive to E2-induced apoptosis.

Lastly, we integrate DNA methylation and gene expression data to identify the TFBSs located in remote regulatory regions. Previous studies have indicated that Low-methylated regions (LMRs) are potential active distal regulatory regions (enhancers) in mammalian genomes. We propose several lasso-penalized logistic regression models to predict the directional change of differentially expressed (DE) genes using predicted TFBSs in pairwise cell-type-specific LMRs (dLMRs). The models are evaluated on pairs from four cell types. The AUCs from 10-fold cross-validation procedure show that the model using TFBSs in dLMRs in intergenic or genebody region has more predictive power (AUC 0.71 and 0.66 respectively), comparing with the one using TFBSs from promoter regions alone (AUC 0.62). When using the TFBSs in dLMRs from both intergenic and genebody regions together, the best prediction was obtained (AUC=0.78). Our models are capable to identify subsets of LMRs in which the binding sites of the insulator protein CTCF, p300 co-activator and other TFs verified before by ChIP-seq are significantly enriched. In summary, our models provide tools that detect distal and proximal TFBSs which may causally regulate gene expression.",2015,
"Peptide-Cation Systems: Conformational Search, Benchmark Evaluation, and Force Field Parameter Adjustment Using Regularized Linear Regression","Metal cations often play an important role in shaping the three-dimensional structure of peptides. As an example, the model system AcPheAla5LysH + is investigated in order to fully understand the forces that stabilize its helical structure. In particular, the question of whether the local fixation of the positive charge at the peptideâ€™s C-terminus is a prerequisite for forming helices is addressed by replacing the protonated lysine residue by alanine and a sodium cation. The combination of gas-phase cold-ion vibrational spectroscopy with molecular simulations based on density-functional theory (DFT) revealed that the charge localization at the C-terminus is imperative for helix formation in the gas phase as this stabilizes the structure through a cation-helix dipole interaction. For sodiated AcPheAla6, globular rather than helical structures were found caused by the strong cation-backbone and cation-Ï€ interactions. Interestingly, the global minimum-energy structure from simulation is not present in the experiment where the system remains kinetically trapped in a solution-state structure. Thereby calculated energies and IR spectra that are sufficiently accurate relied on DFT with computationally costly hybrid functionals, while for the structure search low-computationalcost force field (FF) models are crucial. This inspired a study where the goodness of commonly applied levels of theory, i.e. FFs, semi-empirical methods, density-functional approximations, composite methods, and wavefunction-based methods are being evaluated with respect to benchmark-grade coupled-cluster calculations. Acetylhistidine â€“ either bare or in presence of a zinc cation â€“ thereby serves as a molecular benchmark system. Neither FFs nor semi-empirical methods are reliable enough for a description of these systems within â€œchemical accuracyâ€ of 1kcal/mol. Accurate energetic description within chemical accuracy is achieved for all systems using the meta-GGA SCAN or computationally more demanding hybrid functionals. The double-hybrid functional B3LYP+XYG3 is best resembling the benchmark method DLPNOCCSD(T). Despite poor energetic performances of conventional FFs for peptides in the gas phase, their low computational costs still render them appealing tools for large-scale structure searches. Consequently, a machine learning approach is presented where the torsional parameters and (if desired) van der Waals parameters in the potential-energy function of a particular FF are adjusted by fitting it against DFT energies using regularized regression models like LASSO or Ridge regression. For the peptide AcAla2NMe, this resulted in a significant improvement when comparing to standard OPLS-AA FF parameters. For more challenging peptide-cation systems, e.g. AcAla2NMe+Na, this approach does not give satisfying results, which is caused iii by the formulation of the potential energy of the FF itself: While derived empirical partial charges using Hirshfeld partitioning or the electrostatic potential (ESP) decrease the accuracy, part of the energetic discrepancy can be â€œcompensatedâ€ due to the flexibility of the torsional contributions in terms of the energetic description.",2018,
Minimax rates of convergence for high-dimensional regression under â„“q-ball sparsity,"Consider the standard linear regression model y = XÃŸâˆ— + w, where y âˆŠ R<sup>n</sup> is an observation vector, X âˆŠ R<sup>nÃ—d</sup> is a measurement matrix, ÃŸâˆ— âˆŠ R<sup>d</sup> is the unknown regression vector, and w ~ N (0, Ïƒ<sup>2</sup>Î™) is additive Gaussian noise. This paper determines sharp minimax rates of convergence for estimation of ÃŸâˆ— in l<inf>2</inf> norm, assuming that Î²âˆ— belongs to a weak l<inf>b</inf>-ball B<inf>q</inf>(Ã±<inf>q</inf>) for some q âˆŠ [0,1]. We show that under suitable regularity conditions on the design matrix X, the minimax error in squared l<inf>2</inf>-norm scales as R<inf>q</inf>(log d Ã· n)<sup>1 âˆ’qÃ·2</sup>. In addition, we provide lower bounds on rates of convergence for general l<inf>p</inf> norm (for all p âˆŠ [l,+âˆž], p â‰  q). Our proofs of the lower bounds are information-theoretic in nature, based on Fano's inequality and results on the metric entropy of the balls B<inf>q</inf>(R<inf>q</inf>). Matching upper bounds are derived by direct analysis of the solution to an optimization algorithm over B<inf>q</inf>(R<inf>q</inf>). We prove that the conditions on X required by optimal algorithms are satisfied with high probability by broad classes of non-i.i.d. Gaussian random matrices, for which RIP or other sparse eigenvalue conditions are violated. For q = 0, t<inf>1</inf>-based methods (Lasso and Dantzig selector) achieve the minimax optimal rates in t<inf>2</inf> error, but require stronger regularity conditions on the design than the non-convex optimization algorithm used to determine the minimax upper bounds.",2009,"2009 47th Annual Allerton Conference on Communication, Control, and Computing (Allerton)"
Research and applications: Dynamic contrast-enhanced MRI-based biomarkers of therapeutic response in triple-negative breast cancer,"OBJECTIVE
To predict the response of breast cancer patients to neoadjuvant chemotherapy (NAC) using features derived from dynamic contrast-enhanced (DCE) MRI.


MATERIALS AND METHODS
60 patients with triple-negative early-stage breast cancer receiving NAC were evaluated. Features assessed included clinical data, patterns of tumor response to treatment determined by DCE-MRI, MRI breast imaging-reporting and data system descriptors, and quantitative lesion kinetic texture derived from the gray-level co-occurrence matrix (GLCM). All features except for patterns of response were derived before chemotherapy; GLCM features were determined before and after chemotherapy. Treatment response was defined by the presence of residual invasive tumor and/or positive lymph nodes after chemotherapy. Statistical modeling was performed using Lasso logistic regression.


RESULTS
Pre-chemotherapy imaging features predicted all measures of response except for residual tumor. Feature sets varied in effectiveness at predicting different definitions of treatment response, but in general, pre-chemotherapy imaging features were able to predict pathological complete response with area under the curve (AUC)=0.68, residual lymph node metastases with AUC=0.84 and residual tumor with lymph node metastases with AUC=0.83. Imaging features assessed after chemotherapy yielded significantly improved model performance over those assessed before chemotherapy for predicting residual tumor, but no other outcomes.


CONCLUSIONS
DCE-MRI features can be used to predict whether triple-negative breast cancer patients will respond to NAC. Models such as the ones presented could help to identify patients not likely to respond to treatment and to direct them towards alternative therapies.",2013,Journal of the American Medical Informatics Association : JAMIA
Regularized estimation for the accelerated failure time model.,"SUMMARY
In the presence of high-dimensional predictors, it is challenging to develop reliable regression models that can be used to accurately predict future outcomes. Further complications arise when the outcome of interest is an event time, which is often not fully observed due to censoring. In this article, we develop robust prediction models for event time outcomes by regularizing the Gehan's estimator for the accelerated failure time (AFT) model (Tsiatis, 1996, Annals of Statistics 18, 305-328) with least absolute shrinkage and selection operator (LASSO) penalty. Unlike existing methods based on the inverse probability weighting and the Buckley and James estimator (Buckley and James, 1979, Biometrika 66, 429-436), the proposed approach does not require additional assumptions about the censoring and always yields a solution that is convergent. Furthermore, the proposed estimator leads to a stable regression model for prediction even if the AFT model fails to hold. To facilitate the adaptive selection of the tuning parameter, we detail an efficient numerical algorithm for obtaining the entire regularization path. The proposed procedures are applied to a breast cancer dataset to derive a reliable regression model for predicting patient survival based on a set of clinical prognostic factors and gene signatures. Finite sample performances of the procedures are evaluated through a simulation study.",2009,Biometrics
Sparse Recovery With Unknown Variance: A LASSO-Type Approach,"We address the issue of estimating the regression vector Î² in the generic s-sparse linear model y = XÎ² + z, with Î² âˆˆ â„<sup>p</sup>, y âˆˆ â„<sup>n</sup>, z ~ )V (0, Ïƒ<sup>2</sup>I), and p > n when the variance Ïƒ2 is unknown. We study two least absolute shrinkage and selection operator (LASSO)-type methods that jointly estimate Î² and the variance. These estimators are minimizers of the l1 penalized least-squares functional, where the relaxation parameter is tuned according to two different strategies. In the first strategy, the relaxation parameter is of the order ÏƒÌ‚âˆšlog p, where ÏƒÌ‚<sup>2</sup> is the empirical variance. In the second strategy, the relaxation parameter is chosen so as to enforce a tradeoff between the fidelity and the penalty terms at optimality. For both estimators, our assumptions are similar to the ones proposed by CandeÌ€s and Plan in Ann. Stat. (2009), for the case where Ïƒ<sup>2</sup> is known. We prove that our estimators ensure exact recovery of the support and sign pattern of Î² with high probability. We present simulation results showing that the first estimator enjoys nearly the same performances in practice as the standard LASSO (known variance case) for a wide range of the signal-to-noise ratio. Our second estimator is shown to outperform both in terms of false detection, when the signal-to-noise ratio is low.",2014,IEEE Transactions on Information Theory
DNA methylation profiling to predict overall survival risk in gastric cancer: development and validation of a nomogram to optimize clinical management,"DNA methylation has been reported to serve an important role in the carcinogenesis and development of gastric cancer. Our aim was to systematically develop an individualized prediction model of the survival risk combing clinical and methylation factors in gastric cancer. A univariate Cox proportional risk regression analysis was used to identify the prognosis-associated methylation sites based on the differentially expressed methylation sites between early and advanced gastric cancer group, then we applied least absolute shrinkage and selection operator (LASSO) Cox regression model to screen candidate methylation sites. Subsequently, multivariate Cox proportional risk regression analysis was conducted to identify predictive signature according to the candidate sites. Relative operating characteristic curve (ROC) analysis manifested that an 11-methylation signature exhibited great predictive efficiency for 1-, 3-, 5-year survival events. Patients in the low-risk group classified according to 11-methylation signature-based risk score yield significantly better survival than that in high-risk group. Moreover, Cox regression analysis combing methylation-based risk score and other clinical factors indicated that 11-methylation signature served as an independent risk factor. The predictive value of risk score was validated in the testing dataset. In addition, a nomogram was constructed and the ROC as well as calibration plots analysis demonstrated the good performance and clinical application of the nomogram. In conclusion, the result suggested the 11-DNA methylation signature may be potentially independent prognostic marker and functioned as a significant tool for guiding the clinical prediction of gastric cancer patientsâ€™ overall survival.",2020,
Altered relaxation times in MRI indicate bronchopulmonary dysplasia.,"We developed a MRI protocol using transverse (T2) and longitudinal (T1) mapping sequences to characterise lung structural changes in preterm infants with bronchopulmonary dysplasia (BPD). We prospectively enrolled 61 infants to perform 3-Tesla MRI of the lung in quiet sleep. Statistical analysis was performed using logistic Group Lasso regression and logistic regression. Increased lung T2 relaxation time and decreased lung T1 relaxation time indicated BPD yielding an area under the curve (AUC) of 0.80. Results were confirmed in an independent study cohort (AUC 0.75) and mirrored by lung function testing, indicating the high potential for MRI in future BPD diagnostics. TRIAL REGISTRATION: DRKS00004600.",2019,Thorax
Regularized Transformation Models: The tramnet Package,"The tramnet package implements regularized linear transformation models by combining the flexible class of transformation models from tram with constrained convex optimization implemented in CVXR. Regularized transformation models unify many existing and novel regularized regression models under one theoretical and computational framework. Regularization strategies implemented for transformation models in tramnet include the LASSO, ridge regression and the elastic net and follow the parametrization in glmnet. Several functionalities for optimizing the hyperparameters, including model-based optimization based on the mlrMBO package, are implemented. A multitude of S3 methods are deployed for visualization, handling and simulation purposes. This work aims at illustrating all facets of tramnet in realistic settings and comparing regularized transformation models with existing implementations of similar models.",2020,
Forecasting the aggregate oil price volatility in a data-rich environment,"This paper explores the effectiveness of a large set of indicators in forecasting crude oil price volatility, including uncertainty and market sentiment, macroeconomic indicators, and technical indicators. Using the OLS, LASSO regression, and various combination forecasts, we obtain several noteworthy findings. First, we determine which indicators most effectively forecast oil price volatility. Specifically, the uncertainty index is notable. Second, in general, combination strategies and LASSO produce statistically and economically significant forecasts. Third, the combined and LASSO strategies perform considerably better during recessions than expansions. Overall, our study provides which indicators and strategies can improve forecasting accuracy in the oil market.",2018,Economic Modelling
Bundle Methods for Regularized Risk Minimization,"A wide variety of machine learning problems can be described as minimizing a regularized risk functional, with different algorithms using different notions of risk and different regularizers. Examples include linear Support Vector Machines (SVMs), Gaussian Processes, Logistic Regression, Conditional Random Fields (CRFs), and Lasso amongst others. This paper describes the theory and implementation of a scalable and modular convex solver which solves all these estimation problems. It can be parallelized on a cluster of workstations, allows for data-locality, and can deal with regularizers such as L1 and L2 penalties. In addition to the unified framework we present tight convergence bounds, which show that our algorithm converges in O(1/e) steps to e precision for general convex problems and in O(log (1/e)) steps for continuously differentiable problems. We demonstrate the performance of our general purpose solver on a variety of publicly available data sets.",2010,J. Mach. Learn. Res.
Learning Financial Networks using Quantile Granger Causality,"In the post-crisis era, financial regulators and policymakers require data-driven tools to quantify systemic risk and to identify systemically important firms. We propose a statistical method that measures connectivity in the financial sector using time series of firms' stock returns. Our method is based on system-wide lower-tail analysis, whereby we estimate linkages between firms that occur when those firms are distressed and that exist conditional on the financial information of all other firms in the sample. This is achieved using Lasso-penalized quantile vector autoregression. By considering centrality measures of the estimated networks, we can assess the build-up of systemic risk and identify risk propagation channels. We apply our method to monthly returns of large U.S. firms, demonstrating that we are able to detect many of the most recent systemic events, in addition to identifying key players in the 2007-2009 U.S. financial crisis. Importantly, these players are not identified using standard Granger causality, which estimates connectivity by averaging across good, bad, and normal days of the market.",2018,Proceedings of the Fourth International Workshop on Data Science for Macro-Modeling with Financial and Economic Datasets
Abstract 469: Microrna signature as a potential biomarker for predicting survival in colon cancer,"Background: Colon cancer is one of the most common cancers with increasing incidence and high mortality worldwide. Prognosis and choice of treatment is largely based on the tumor stage at presentation. Thus, finding novel biomarkers for predicting survival is highly desirable. Lately, several studies have been looking at microRNAs (miRNAs) in several cancers, including colon cancer. MicroRNAs are conserved, non-coding RNA molecules that play an important role in the regulation of post-transcriptional gene expression. Material and Methods: In the present study, we have profiled miRNA in one hundred and seventy two TNM stage I-IV colon cancer patients and 10 corresponding normal colon tissue samples. Total RNA was extracted from freshly frozen tissues, and the expression of miRNA profile were assessed using Pick and Mix focus panels from Exiqon containing 84 miRNAs that have been linked to cancer. Results: The results were visualized in a heatmap (Qlucore omics Software) and more than 20 miRNAs were found to be differentially expressed in tumors compared to the normal colon. Further, twelve miRNAs were found to discriminate between relapse and no-relapse patients in TNM- stage II and III, and four of these miRNAs (miR-23a, miR-25, miR-30d and miR-31) were found to be statistically significant in binary logistic regression with relapse as outcome variable. In univariate analysis, low expression of the four-miRNA signature was associated with better 3-year disease-free survival (DFS), 88 % versus 63% in low versus high signature, respectively (P=0.001). Moreover, the signature was a predictor of poor relapse-free survival in multivariate analyses (P=0.001; HR 31; 95% CI: 3.8-248.9). Another regression analyses method (LASSO) identified a 16-miRNA signature, and the four miRNAs found earlier were among them. The 16-miRNA signature was associated with better survival (P Conclusion: The present study has identified a four-miRNA signature predicting relapse in colon cancer stage II and III patients. Citation Format: Havjin Jacob, Luka Stanisavljevic, Kristian Eeg Storli, Olav Dahl, Mette Pernille Myklebust. Microrna signature as a potential biomarker for predicting survival in colon cancer [abstract]. In: Proceedings of the American Association for Cancer Research Annual Meeting 2017; 2017 Apr 1-5; Washington, DC. Philadelphia (PA): AACR; Cancer Res 2017;77(13 Suppl):Abstract nr 469. doi:10.1158/1538-7445.AM2017-469",2017,Cancer Research
High-dimensional Joint Sparsity Random Effects Model for Multi-task Learning,"Joint sparsity regularization in multi-task learning has attracted much attention in recent years. The traditional convex formulation employs the group Lasso relaxation to achieve joint sparsity across tasks. Although this approach leads to a simple convex formulation, it suffers from several issues due to the looseness of the relaxation. To remedy this problem, we view jointly sparse multi-task learning as a specialized random effects model, and derive a convex relaxation approach that involves two steps. The first step learns the covariance matrix of the coefficients using a convex formulation which we refer to as sparse covariance coding; the second step solves a ridge regression problem with a sparse quadratic regularizer based on the covariance matrix obtained in the first step. It is shown that this approach produces an asymptotically optimal quadratic regularizer in the multitask learning setting when the number of tasks approaches infinity. Experimental results demonstrate that the convex formulation obtained via the proposed model significantly outperforms group Lasso (and related multi-stage formulations",2013,ArXiv
"Approches statistiques avancÃ©es pour la modÃ©lisation des sÃ©ries chronologiques en rÃ©gression, appliquÃ©es Ã  lâ€™Ã©pidÃ©miologie environnementale.","La sante des populations est un des defis majeurs lies a lâ€™adaptation aux changements climatiques. Lâ€™effet des vagues de chaleur est notamment deja visible alors que ces evenements devraient se multiplier dans les annees a venir. Les maladies cardiovasculaires representent une des classes de maladies les plus touchees, tout en etant deja un probleme majeur de sante publique a lâ€™heure actuelle. De plus en plus dâ€™etudes en epidemiologie environnementale visent a identifier lâ€™effet de la meteorologie sur la sante, afin dâ€™anticiper les changements climatiques et mettre en place des alertes appropriees. Les etudes dâ€™epidemiologie environnementales sâ€™appuient notamment sur des modeles de regression, lesquels sont appliques avec des donnees pouvant prendre la forme de series chronologiques (on peut aussi citer les donnees de type spatial). Les series chronologiques violent notamment les hypotheses dâ€™independance et de distribution identique des residus dans la regression, et necessitent donc des methodes mieux adaptees. Cette these propose donc des methodologies statistiques visant a repondre aux problemes crees par lâ€™utilisation de series chronologiques dans la regression. Les methodologies consistent toutes en un pretraitement des donnees puis a lâ€™application de modeles de regression adaptes pour prendre en compte les caracteristiques des donnees transformees. Elles sont ensuite appliquees a lâ€™etude du lien existant entre la meteorologie, en particulier la temperature et lâ€™humidite, sur la mortalite par maladie cardiovasculaire dans la communaute metropolitaine de Montreal. 
Les donnees sanitaires utilisees dependent notamment de lâ€™organisation des services medicaux. Or, cette organisation entraine la presence de bruit dans les donnees (p. ex. davantage de personnel de jour que de nuit), pouvant rendre plus difficile lâ€™estimation de la relation entre une variable explicative et une reponse. Il est ainsi propose dâ€™agreger temporellement les series de donnees sanitaires afin de faire ressortir le signal du a la meteorologie, puis dâ€™appliquer un modele de regression pour serie temporelle visant a modeliser la dependance temporelle dans les residus. La comparaison de cette methodologie avec un modele classique dâ€™epidemiologie environnementale montre quâ€™elle permet un meilleur ajustement du modele aux donnees. La comparaison de diverses strategies dâ€™agregation mene cependant a la conclusion que la fenetre dâ€™agregation ne doit pas etre superieure a une semaine. 
Une problematique plus generale des etudes concernant des processus naturels est la presence de saisonnalite et tendance entre autres menant a des cas de regression fallacieuse. Il est ainsi propose dans la these de decomposer les differents motifs reguliers presents dans les series de donnees par decomposition modale empirique. Les composantes en resultant sont ensuite utilisees dans la regression au lieu des series de donnees dâ€™origine, en utilisant la technique du Lasso (operateur de selection et reduction par moindres valeurs absolues) pour ne conserver que les composantes les plus importantes pour lâ€™explication de la reponse. Lâ€™application de cette methodologie aux donnees de mortalite, temperature et humidite permet de mettre en evidence des aspects de la relation habituellement invisibles dans les modeles statistiques. Cette methodologie permet ainsi un regard alternatif et detaille sur la relation entre des series de donnees chronologiques. 
De nombreux problemes lies a lâ€™utilisation de series chronologiques dans la regression tels que lâ€™autocorrelation et la non-stationnarite sont issus du fait quâ€™elles sont en fait des discretisations de processus intrinsequement continus. La these propose donc de considerer les series sanitaires et meteorologiques comme des courbes continues en utilisant le cadre de lâ€™analyse de donnees fonctionnelle. Notamment, les modeles de regression fonctionnelle sont adaptes aux problematiques inherentes au domaine de lâ€™epidemiologie environnementale. Les resultats montrent le potentiel de la regression fonctionnelle pour comprendre le lien entre la meteorologie et la sante dans sa globalite, en retranscrivant notamment les processus dâ€™adaptation physiologique des individus. Abstract In the context of climate change adaptation, public health management is a major challenge. For instance, the frequency and strength of heatwaves are expected to increase in the future while their effect on mortality is already well-known. Among the affected disease classes are cardiovascular diseases, which are already an important public health issue. Nowadays, many environmental epidemiology studies seek to understand precisely the effect of weather on population health, in order to accurately anticipate the future. Studies of the effect of meteorological factors on health often rely on regression models applied on time series data (although other types of data exist such as spatial data). However, several assumptions of regression models do not hold in presence of time series data, i.e. the assumptions of independence and same distribution of the residuals. Therefore, the purpose of the present thesis is to propose a number of regression methodologies addressing several issues caused by the temporal structure of data. The methodologies all rely on data preprocessing, in order to obtain transformed data that could be used in existing and efficient regression methods. They are illustrated on the relationship between weather and cardiovascular mortality in the census metropolitan area of Montreal, Canada. 
Health data are often noisy because of organisational factors in hospitals, which complicate the task of estimating the effect of a weather exposure on a health issue. It is herein proposed to temporally aggregate the health response before using it in a regression model. A time series regression model is then used to account for the temporal dependence of data. Comparing this methodology with classical regression models show that it leads to a better fit to health data as well as unveiling the relationship at a the weekly scale than classical regression. Moreover, several aggregation strategies are tried and it is shown that the best results are obtained using aggregations with small time windows. 
Many natural time series contains nonstationary patterns such as seasonality and trend, which could lead to spurious regression. The present thesis proposes to decompose time series data into basic oscillating components through empirical mode decomposition in order to use the components as new variables in a regression model. The use of the Lasso (least absolute shrinkage and selection operator) allows keeping only the most important components in order to explain the health response. The application of this methodology on temperature and humidity related to cardiovascular morbidity unveils little known aspects of the relationship, in addition to providing a good fit of the data. Hence, it is argued that this methodology represents a tool to understand more accurately than classical models any relationship between time-related processes. 
Many time series related issues in regression models are due to the fact that time series can be viewed as the discretization of intrinsically continuous processes. Therefore, the present thesis argues for the use functional data analysis which deals with data as continuous curves instead of discrete series. In particular, functional regression models are adapted to the particular issues of environmental epidemiology. The application of such models on the temperature-related cardiovascular mortality shows that they are able to describe an overall relationship. Functional models especially bring a tool that allows representing the physiological adaptation of populations, rarely taken into account in classical models.",2017,
MEBoost: Variable selection in the presence of measurement error.,"We present a novel method for variable selection in regression models when covariates are measured with error. The iterative algorithm we propose, Measurement Error Boosting (MEBoost), follows a path defined by estimating equations that correct for covariate measurement error. We illustrate the use of MEBoost in practice by analyzing data from the Box Lunch Study, a clinical trial in nutrition where several variables are based on self-report and, hence, measured with error, where we are interested in performing model selection from a large data set to select variables that are related to the number of times a subject binge ate in the last 28 days. Furthermore, we evaluated our method and compared its performance to the recently proposed Convex Conditioned Lasso and to the ""naive"" Lasso, which does not correct for measurement error through a simulation study. Increasing the degree of measurement error increased prediction error and decreased the probability of accurate covariate selection, but this loss of accuracy occurred to a lesser degree when using MEBoost. Through simulations, we also make a case for the consistency of the model selected.",2019,Statistics in medicine
Multi-population GWA mapping via multi-task regularized regression,"MOTIVATION
Population heterogeneity through admixing of different founder populations can produce spurious associations in genome-wide association studies that are linked to the population structure rather than the phenotype. Since samples from the same population generally co-evolve, different populations may or may not share the same genetic underpinnings for the seemingly common phenotype. Our goal is to develop a unified framework for detecting causal genetic markers through a joint association analysis of multiple populations.


RESULTS
Based on a multi-task regression principle, we present a multi-population group lasso algorithm using L(1)/L(2)-regularized regression for joint association analysis of multiple populations that are stratified either via population survey or computational estimation. Our algorithm combines information from genetic markers across populations, to identify causal markers. It also implicitly accounts for correlations between the genetic markers, thus enabling better control over false positive rates. Joint analysis across populations enables the detection of weak associations common to all populations with greater power than in a separate analysis of each population. At the same time, the regression-based framework allows causal alleles that are unique to a subset of the populations to be correctly identified. We demonstrate the effectiveness of our method on HapMap-simulated and lactase persistence datasets, where we significantly outperform state of the art methods, with greater power for detecting weak associations and reduced spurious associations.


AVAILABILITY
Software will be available at http://www.sailing.cs.cmu.edu/.",2010,Bioinformatics
Variable selection via Group LASSO Approach : Application to the Cox Regression and frailty model,"In the analysis of survival outcome supplemented with both clinical information and high-dimensional gene expression data, use of the traditional Cox proportional hazards model (1972) fails to meet some emerging needs in biomedical research. First, the number of covariates is generally much larger the sample size. Secondly, predicting an outcome based on individual gene expression is inadequate because multiple biological processes and functional pathways regulate the expression associated with a gene. Another challenge is that the Cox model assumes that populations are homogenous, implying that all individuals have the same risk of death, which is rarely true due to unmeasured risk factors among populations. In this paper we propose group LASSO with gamma-distributed frailty for variable selection in Cox regression by extending previous scholarship to account for heterogeneity among group structures related to exposure and susceptibility. The consistency property of the proposed method is established. This method is appropriate for addressing a wide variety of research questions from genetics to air pollution. Simulated analysis shows promising performance by group LASSO compared with other methods, including group SCAD and group MCP. Future directions include expanding the use of frailty with adaptive group LASSO and sparse group LASS.",2018,arXiv: Computation
Integration of single nucleotide variants and whole-genome DNA methylation profiles for classification of rheumatoid arthritis cases from controls,"This study evaluated the use of multiomics data for classification accuracy of rheumatoid arthritis (RA). Three approaches were used and compared in terms of prediction accuracy: (1) whole-genome prediction (WGP) using SNP marker information only, (2) whole-methylome prediction (WMP) using methylation profiles only, and (3) whole-genome/methylome prediction (WGMP) with combining both omics layers. The number of SNP and of methylation sites varied in each scenario, with either 1, 10, or 50% of these preselected based on four approaches: randomly, evenly spaced, lowest p value (genome-wide association or epigenome-wide association study), and estimated effect size using a Bayesian ridge regression (BRR) model. To remove effects of high levels of pairwise linkage disequilibrium (LD), SNPs were also preselected with an LD-pruning method. Five Bayesian regression models were studied for classification, including BRR, Bayes-A, Bayes-B, Bayes-C, and the Bayesian LASSO. Adjusting methylation profiles for cellular heterogeneity within whole blood samples had a detrimental effect on the classification ability of the models. Overall, WGMP using Bayes-B model has the best performance. In particular, selecting SNPs based on LD-pruning with 1% of the methylation sites selected based on BRR included in the model, and fitting the most significant SNP as a fixed effect was the best method for predicting disease risk with a classification accuracy of 0.975. Our results showed that multiomics data can be used to effectively predict the risk of RA and identify cases in early stages to prevent or alter disease progression via appropriate interventions.",2020,Heredity
Goodness of fit tests and lasso variable selection in time series analysis,"This thesis examines various aspects of time series and their applications. In the rst part, we study numerical and asymptotic properties of Box-Pierce family of portmanteau tests. We compare size and power properties of time series model diagnostic tests using their asymptotic c2 distribution and bootstrap distribution (dynamic and fixed design) against various linear and non-linear alternatives. In general, our results show that dynamic bootstrapping provides a better approximation of the distribution underlying these statistics. Moreover, we find that Box-Pierce type tests are powerful against linear alternatives while the CvM due to Escanciano (2006b) test performs better against non linear alternative models.
The most challenging scenario for these portmanteau tests is when the process is close to the stationary boundary and value of m, the maximum lag considered in the portmanteau test, is very small. In these situations, the c2 distribution is a poor approximation of the null asymptotic distribution. Katayama (2008) suggested a bias correction term to improve the approximation in these situations. We numerically study Katayama's bias correction in Ljung and Box (1978) test. Our results show that Katayama's correction works well and conrms the results as shown in Katayama (2008). We also provide a number of algorithms for performing the necessary calculations efciently.
We notice that the bootstrap automatically does bias correction in Ljung-Box statistic. It motivates us to look at theoretical properties of the dynamic bootstrap in this context. Moreover, noticing the good performance of Katayama's correction, we suggest a bias correction term for the Monti (1994) test on the lines of Katayama's correction. We show that our suggestion improves Monti's statistic in a similar way to what Katayama's suggestion does for Ljung-Box test. We also make a novel suggestion of using the pivotal portmanteau test. Our suggestion is to use two separate values of m, one a large value for the calculation of the information matrix and a smaller choice for diagnostic purposes. This results in a pivotal statistic which automatically corrects the bias correction in Ljung-Box test. Our suggested novel algorithm efciently computes this novel portmanteau test.
In the second part, we implement lasso-type shrinkage methods to linear regression and time series models. We look through simulations in various examples to study the oracle properties of these methods via the adaptive lasso due to Zou (2006). We study consistent variable selection by the lasso and adaptive lasso and consider a result in the literature which states that the lasso cannot be consistent in variable selection if a necessary condition does not hold for the model. We notice that lasso methods have nice theoretical properties but it is not very easy to achieve them in practice.
The choice of tuning parameter is crucial for these methods. So far there is not any fully explicit way of choosing the appropriate value of tuning parameter, so it is hard to achieve the oracle properties in practice. In our numerical study, we compare the performance of k-fold cross-validation with the BIC method of Wang et al. (2007) for selecting the appropriate value of the tuning parameter. We show that k-fold crossvalidation is not a reliable method for choosing the value of the tuning parameter for consistent variable selection.
We also look at ways to implement lasso-type methods time series models. In our numerical results we show that the oracle properties of lasso-type methods can also be achieved for time series models. We derive the necessary condition for consistent variable selection by lasso-type methods in the time series context. We also prove the oracle properties of the adaptive lasso for stationary time series.",2011,
Comparing Approaches to Treatment Effect Estimation for Subgroups in Clinical Trials,"ABSTRACTIdentifying subgroups, which respond differently to a treatment, both in terms of efficacy and safety, is an important part of drug development. A well-known challenge in exploratory subgroup analyses is the small sample size in the considered subgroups, which is usually too low to allow for definite comparisons. In early phase trials, this problem is further exaggerated, because limited or no clinical prior information on the drug and plausible subgroups is available. We evaluate novel strategies for treatment effect estimation in these settings in a simulation study motivated by real clinical trial situations. We compare several approaches to estimate treatment effects for selected subgroups, employing model averaging, resampling, and Lasso regression methods. Two subgroup identification approaches are employed, one based on categorization of covariates and the other based on splines. Our results show that naive estimation of the treatment effect, which ignores that a selection has taken place, ...",2016,Statistics in Biopharmaceutical Research
A sparse version of the ridge logistic regression for large-scale text categorization,"The ridge logistic regression has successfully been used in text categorization problems and it has been shown to reach the same performance as the Support Vector Machine but with the main advantage of computing a probability value rather than a score. However, the dense solution of the ridge makes its use unpractical for large scale categorization. On the other side, LASSO regularization is able to produce sparse solutions but its performance is dominated by the ridge when the number of features is larger than the number of observations and/or when the features are highly correlated. In this paper, we propose a new model selection method which tries to approach the ridge solution by a sparse solution. The method first computes the ridge solution and then performs feature selection. The experimental evaluations show that our method gives a solution which is a good trade-off between the ridge and LASSO solutions.",2011,Pattern Recognit. Lett.
Least Angle Regression and Partial Least Squares Regression on Process Data with High Collinearity,"Overview Collinearity is a common problem met in regression analysis for chemical process data. A popular method to deal with collinearity is partial least squares regression (PLS). It uses projections of the original variables to a reduce number of latent variables to circumvent the task of model selection. On the other hand, recent advances in statistics and machine learning provide promising methods of sparse analytics, which lead to a natural way to exclude variables that are irrelevant or redundant. To study the effectiveness of these methods, we evaluate the performance of least angle regression (LARS) on an industrial boiler dataset with high collinearity, and compare its performance with those of least absolute shrinkage and selection operator (LASSO) and PLS. The results show that LARS has a better performance on highly collinear data. It produces sparse coefficients like LASSO, which PLS cannot achieve, and it allows for an easier selection of a best set of coefficients comparative to LASSO.",2019,
A note on path-based variable selection in the penalized proportional hazards model,"We propose an efficient and adaptive shrinkage method for variable selection in the Cox model. The method constructs a piecewise-linear regularization path connecting the maximum partial likelihood estimator and the origin. Then a model is selected along the path. We show that the constructed path is adaptive in the sense that, with a proper choice of regularization parameter, the fitted model works as well as if the true underlying submodel were given in advance. A modified algorithm of the least-angle-regression type efficiently computes the entire regularization path of the new estimator. Furthermore, we show that, with a proper choice of shrinkage parameter, the method is consistent in variable selection and efficient in estimation. Simulation shows that the new method tends to outperform the lasso and the smoothly-clipped-absolute-deviation estimators with moderate samples. We apply the methodology to data concerning nursing homes. Copyright 2008, Oxford University Press.",2008,Biometrika
A note on the asymptotic distribution of LASSO estimator for correlated data,"The asymptotic distribution of the Lasso estimator for regression models with independent errors has been investigated by Knight and Fu (2000). In this note we extend these results to regression models with a general weak dependence structure. We determine the asymptotic distribution of the Lasso estimator when the number of parameters M is fixed and the number of observations, n, converges to infinity. We show that, for an appropriate choice of the tuning parameter of the method, this asymptotic distribution reduces to a multivariate normal distribution. As an illustrative example, the special case of AR(1) is also investigated.",2012,
Standardized comparison of the relative impacts of HIV-1 reverse transcriptase (RT) mutations on nucleoside RT inhibitor susceptibility.,"Determining the phenotypic impacts of reverse transcriptase (RT) mutations on individual nucleoside RT inhibitors (NRTIs) has remained a statistical challenge because clinical NRTI-resistant HIV-1 isolates usually contain multiple mutations, often in complex patterns, complicating the task of determining the relative contribution of each mutation to HIV drug resistance. Furthermore, the NRTIs have highly variable dynamic susceptibility ranges, making it difficult to determine the relative effect of an RT mutation on susceptibility to different NRTIs. In this study, we analyzed 1,273 genotyped HIV-1 isolates for which phenotypic results were obtained using the PhenoSense assay (Monogram, South San Francisco, CA). We used a parsimonious feature selection algorithm, LASSO, to assess the possible contributions of 177 mutations that occurred in 10 or more isolates in our data set. We then used least-squares regression to quantify the impact of each LASSO-selected mutation on each NRTI. Our study provides a comprehensive view of the most common NRTI resistance mutations. Because our results were standardized, the study provides the first analysis that quantifies the relative phenotypic effects of NRTI resistance mutations on each of the NRTIs. In addition, the study contains new findings on the relative impacts of thymidine analog mutations (TAMs) on susceptibility to abacavir and tenofovir; the impacts of several known but incompletely characterized mutations, including E40F, V75T, Y115F, and K219R; and a tentative role in reduced NRTI susceptibility for K64H, a novel NRTI resistance mutation.",2012,Antimicrobial agents and chemotherapy
Detecting genetic risk factors for Alzheimer's disease in whole genome sequence data via Lasso screening,"Genetic factors play a key role in Alzheimer's disease (AD). The Alzheimer's Disease Neuroimaging Initiative (ADNI) whole genome sequence (WGS) data offers new power to investigate mechanisms of AD by combining entire genome sequences with neuroimaging and clinical data. Here we explore the ADNI WGS SNP (single nucleotide polymorphism) data in depth and extract approximately six million valid SNP features. We investigate imaging genetics associations using Lasso regression - a widely used sparse learning technique. To solve the large-scale Lasso problem more efficiently, we employ a highly efficient screening rule for Lasso - called dual polytope projections (DPP) - to remove irrelevant features from the optimization problem. Experiments demonstrate that the DPP can effectively identify irrelevant features and leads to a 400Ã— speedup. This allows us for the first time to run the compute-intensive model selection procedure called stability selection to rank SNPs that may affect the brain and AD risk.",2015,2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI)
Title Sieve likelihood ratio statistics and Wilks phenomenon,"Variable selection is vital to statistical data analyses. Many of procedures in use are ad hoc stepwise selection procedures, which are computationally expensive and ignore stochastic errors in the variable selection process of previous steps. An automatic and simultaneous variable selection procedure can be obtained by using a penalized likelihood method. In traditional linear models, the best subset selection and stepwise deletion methods coincide with a penalized leastsquares method when design matrices are orthonormal. In this paper, we propose a few new approaches to selecting variables for linear models, robust regression models and generalized linear models based on a penalized likelihood approach. A family of thresholding functions are proposed. The LASSO proposed by Tibshirani (1996) is a member of the penalized leastsquares with the L1-penalty. A smoothly clipped absolute deviation (SCAD) penalty function is introduced to ameliorate the properties of L1-penalty. A uni ed algorithm is introduced, which is backed up by statistical theory. The new approaches are compared with the ordinary leastsquares methods, the garrote method by Breiman (1995) and the LASSO method by Tibshirani (1996). Our simulation results show that the newly proposed methods compare favorably with other approaches as an automatic variable selection technique. Because of simultaneous selection of variables and estimation of parameters, we are able to give a simple estimated standard error formula, which is tested to be accurate enough for practical applications. Two real data examples illustrate the versatility and e ectiveness of the proposed approaches.",1999,
"The effects of data sources, cohort selection, and outcome definition on a predictive model of risk of thirty-day hospital readmissions","BACKGROUND
Hospital readmission risk prediction remains a motivated area of investigation and operations in light of the hospital readmissions reduction program through CMS. Multiple models of risk have been reported with variable discriminatory performances, and it remains unclear how design factors affect performance.


OBJECTIVES
To study the effects of varying three factors of model development in the prediction of risk based on health record data: (1) reason for readmission (primary readmission diagnosis); (2) available data and data types (e.g. visit history, laboratory results, etc); (3) cohort selection.


METHODS
Regularized regression (LASSO) to generate predictions of readmissions risk using prevalence sampling. Support Vector Machine (SVM) used for comparison in cohort selection testing. Calibration by model refitting to outcome prevalence.


RESULTS
Predicting readmission risk across multiple reasons for readmission resulted in ROC areas ranging from 0.92 for readmission for congestive heart failure to 0.71 for syncope and 0.68 for all-cause readmission. Visit history and laboratory tests contributed the most predictive value; contributions varied by readmission diagnosis. Cohort definition affected performance for both parametric and nonparametric algorithms. Compared to all patients, limiting the cohort to patients whose index admission and readmission diagnoses matched resulted in a decrease in average ROC from 0.78 to 0.55 (difference in ROC 0.23, p value 0.01). Calibration plots demonstrate good calibration with low mean squared error.


CONCLUSION
Targeting reason for readmission in risk prediction impacted discriminatory performance. In general, laboratory data and visit history data contributed the most to prediction; data source contributions varied by reason for readmission. Cohort selection had a large impact on model performance, and these results demonstrate the difficulty of comparing results across different studies of predictive risk modeling.",2014,Journal of biomedical informatics
Clinico-pathological and transcriptomic determinants of SLFN11 expression in invasive breast carcinoma,"SLFN11 is a putative DNA/RNA helicase we discovered as causally associated with sensitivity to DNA damaging agents, such as platinum salts, topoisomerase I and II inhibitors, and other alkylators in the NCI-60 panel of cancer cell lines [1]. Later, SLFN11 was identified as an early interferon response gene, in association with HIV infection [2]. Here we assessed SLFN11 determinants in a gene expression meta-set of 5,061 breast cancer patients annotated with clinical data and multigene signatures obtained with the package genefu [3]. By correlation analysis, we found 537 transcripts above the 95th percentile of Pearsonâ€™s coefficients with SLFN11, identifying â€œimmune responseâ€, â€œlymphocyte activationâ€, and â€œT cell activationâ€ as top Gene Ontology enriched processes [4]. Through multiple correspondence analysis, we discovered a subgroup of patients characterized by high SLFN11 levels, ER negativity, basal phenotype, elevated CD3D, STAT1 signature [5], and young age. Fitting a penalized maximum likelihood lasso regression model [6], we found a strong multivariable association of SLN11 with the stroma 1 and stroma 2 signatures [7,8], associated with basal cancer and response to chemotherapy in ER- tumors. Finally, using Cox proportional hazard regression, ER-, high proliferation, high SLFN11 patients undergoing chemotherapy treatment showed a significantly longer disease-free interval than other patient categories included in our model.",2015,Journal for Immunotherapy of Cancer
