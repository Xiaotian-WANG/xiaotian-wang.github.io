title,abstract,year,journal
A prognostic signature for lower-grade gliomas based on expression of long noncoding RNAs,"Diffuse low-grade and intermediate-grade gliomas (together known as lower-grade gliomas, WHO grade II and III) develop in the supporting glial cells of brain and are the most common types of primary brain tumor. Despite a better prognosis for lower-grade gliomas, 70% of patients undergo high-grade transformation within 10 years, stressing the importance of better prognosis. Long non-coding RNAs (lncRNAs) are gaining attention as potential biomarkers for cancer diagnosis and prognosis. We have developed a computational model, UVA8, for prognosis of lower-grade gliomas by combining lncRNA expression, Cox regression and L1-LASSO penalization. The model was trained on a subset of patients in TCGA. Patients in TCGA, as well as a completely independent validation set (CGGA) could be dichotomized based on their risk score, a linear combination of the level of each prognostic lncRNA weighted by its multivariable cox regression coefficient. UVA8 is an independent predictor of survival and outperforms standard epidemiological approaches and previous published lncRNA-based predictors as a survival model. Guilt-by-association studies of the lncRNAs in UVA8, all of which predict good outcome, suggest they have a role in suppressing interferon stimulated response and epithelial to mesenchymal transition. The expression levels of 8 lncRNAs can be combined to produce a prognostic tool applicable to diverse populations of glioma patients. The 8 lncRNA (UVA8) based score can identify grade II and grade III glioma patients with poor outcome and thus identify patients who should receive more aggressive therapy at the outset.",2018,bioRxiv
To construct a ceRNA regulatory network as prognostic biomarkers for bladder cancer.,"Emerging evidence demonstrates that competing endogenous RNA (ceRNA) hypothesis has played a role in molecular biological mechanisms of cancer occurrence and development. But the effect of ceRNA network in bladder cancer (BC), especially lncRNA-miRNA-mRNA regulatory network of BC, was not completely expounded. By means of The Cancer Genome Atlas (TCGA) database, we compared the expression of RNA sequencing (RNA-Seq) data between 19 normal bladder tissue and 414 primary bladder tumours. Then, weighted gene co-expression network analysis (WGCNA) was conducted to analyse the correlation between two sets of genes with traits. Interactions between miRNAs, lncRNAs and target mRNAs were predicted by MiRcode, miRDB, starBase, miRTarBase and TargetScan. Next, by univariate Cox regression and LASSO regression analysis, the 86 mRNAs obtained by prediction were used to construct a prognostic model which contained 4 mRNAs (ACTC1Â +Â FAM129AÂ +Â OSBPL10Â +Â EPHA2). Then, by the 4 mRNAs in the prognostic model, a ceRNA regulatory network with 48 lncRNAs, 14 miRNAs and 4 mRNAs was constructed. To sum up, the ceRNA network can further explore gene regulation and predict the prognosis of BC patients.",2020,Journal of cellular and molecular medicine
Case Complexity as a Guide for Psychological Treatment Selection,"Objective: Some cases are thought to be more complex and difficult to treat, although there is little consensus on how to define complexity in psychological care. This study proposes an actuarial, data-driven method of identifying complex cases based on their individual characteristics. Method: Clinical records for 1,512 patients accessing low- and high-intensity psychological treatments were partitioned in 2 random subsamples. Prognostic indices predicting post-treatment reliable and clinically significant improvement (RCSI) in depression (Patient Health Questionnaire-9; Kroenke, Spitzer, & Williams, 2001) and anxiety (Generalized Anxiety Disorder-7; Spitzer, Kroenke, Williams, & LÃ¶we, 2006) symptoms were estimated in 1 subsample using penalized (Lasso) regressions with optimal scaling. A PI-based algorithm was used to classify patients as standard (St) or complex (Cx) cases in the second (cross-validation) subsample. RCSI rates were compared between Cx cases that accessed treatments of different intensities using logistic regression. Results: St cases had significantly higher RCSI rates compared to Cx cases (OR = 1.81 to 2.81). Cx cases tended to attain better depression outcomes if they were initially assigned to high-intensity (vs. low intensity) interventions (OR = 2.23); a similar pattern was observed for anxiety but the odds ratio (1.74) was not statistically significant. Conclusions: Complex cases could be detected early and matched to high-intensity interventions to improve outcomes.",2017,Journal of Consulting and Clinical Psychology
"Optimal Scaling, A Wonderful Method for Analysing Clinical Trials with Imperfect Data","Context: In clinical trials the research question is often measured with multiple variables, and multiple regression is commonly used for analysis. The problem with multiple regression is that consecutive levels of the variables are assumed to be equal, while in practice this is virtually never true. Optimal scaling is a method designed to maximize the relationship between a predictor and an outcome variable by adjusting their scales. Aims: To assess the performance of optimal scaling in clinical research. Settings and design: A simulated example of a drug efficacy trial was used. The SPSS module Optimal Scaling with ridge regression, lasso and elastic net regression was used. Results: The ridge optimal scaling model produced eight p-values < 0.01, while traditional regression and unregularized optimal scaling produced only 3 and 2 p-values < 0.01. Lasso optimal scaling eliminated 4 of 12 predictors from the analysis, while, of the remainder, only two were significant at p < 0.01. Similarly elastic net optimal scaling did not provide additional benefit. Conclusions: 1/ Optimal scaling shows similarly sized effects compared to traditional regression. In order to benefit from optimal scaling a regularization procedure for the purpose of correcting overdispersion is needed. 2/ Ridge optimal scaling performed much better than did traditional regression giving rise to many more statistically significant predictors. 3/ Lasso optimal scaling shrinks some b-values to zero, and is particularly suitable if you are looking for a limited number of strong predictors. 4/ Elastic net optimal scaling works better than lasso if the number of predictors is larger than the number of observations.",2013,
Thermal performance of a novel ultrasonic evaporator based on machine learning algorithms,"Abstract Ultrasound is a promising method to enhance heat transfer in industrial evaporators. However, there are very limited studies on this topic. This paper explores thermal performance of a novel ultrasonic evaporator based on machine learning methods. The results indicate that the overall heat transfer coefficients can be increased by around 15â€“20% after adding ultrasound due to acoustic cavitation and acoustic streaming. Two machine learning-based global sensitivity analysis (treed Gaussian Process and polynomial chaos expansion) methods are used to identify important variables influencing overall heat transfer coefficients in the ultrasonic evaporator. It is found that the temperature difference between evaporation and heating steam is the dominant factor affecting thermal performance in this case study. Ultrasound has complicated interactions and non-linear effects in the ultrasonic evaporator. Seven machine learning algorithms are created to compare predictive thermal performance of this evaporator, including linear model, Lasso (least absolute shrinkage and selection operator), MARS (multivariate adaptive regression splines), NNet (averaging neural network), CB (Cubist model), GP (Gaussian process), and SVM (support vector machine). The SVM and NNet models among these seven models can provide accurate prediction of overall heat transfer coefficients in the ultrasonic evaporator.",2019,Applied Thermal Engineering
The Analysis of Impact Factors of Foreign Investment Based on Relaxed Lasso,"Relaxed Lasso method is used for variable selection to 15 main economic factors affecting foreign investment. While Relaxed Lasso method, method of least squares and regression are compared, the result further reveals the main problem facing foreign investment at the present stage.",2017,Journal of Applied Mathematics and Physics
Economic Sustainability in Franchising: A Model to Predict Franchisor Success or Failure,"As a business model, franchising makes a major contribution to gross domestic product (GDP). A model that predicts franchisor success or failure is therefore necessary to ensure economic sustainability. In this study, such a model was developed by applying Lasso regression to a sample of franchises operating between 2002 and 2013. For franchises with the highest likelihood of survival, the franchise fees and the ratio of company-owned to franchised outlets were suited to the age of the franchise. Surviving franchises were those that opened franchised outlets at a sustainable pace, increased the franchise fee as intangible assets increased, and effectively managed profitability and efficiency.",2017,Sustainability
Bridge regression: Adaptivity and group selection,"Abstract In high-dimensional regression problems regularization methods have been a popular choice to address variable selection and multicollinearity. In this paper we study bridge regression that adaptively selects the penalty order from data and produces flexible solutions in various settings. We implement bridge regression based on the local linear and quadratic approximations to circumvent the nonconvex optimization problem. Our numerical study shows that the proposed bridge estimators are a robust choice in various circumstances compared to other penalized regression methods such as the ridge, lasso, and elastic net. In addition, we propose group bridge estimators that select grouped variables and study their asymptotic properties when the number of covariates increases along with the sample size. These estimators are also applied to varying-coefficient models. Numerical examples show superior performances of the proposed group bridge estimators in comparisons with other existing methods.",2011,Fuel and Energy Abstracts
The Gamma Lasso,"This article describes a very fast algorithm for obtaining continuous regularization paths corresponding to cost functions spanning the range of concavity between L0 and L1 norms. The â€˜gamma lassoâ€™ heuristic does L1 (lasso) penalized regression estimation on a grid of decreasing penalties, but adapts coefficient-specific weights to decrease as a function of the estimated coefficient in the previous path segment. Our particular weight-updating scheme is motivated from a Bayesian model, and is related to estimation under log penalties. This very simple recipe is used to illustrate the large and difficult literature on concave penalization, with the hope that we can make the ideas more accessible to practitioners. The construction also leads us to a plug-in estimator for degrees of freedom; this is applied in model selection, and in experimentation our information criteria perform as well as cross-validation. The work is illustrated in linear regression simulations and in application of logistic regression to evaluate hockey players.",2013,
Subspace clustering via seeking neighbors with minimum reconstruction error,"Abstract Subspace clustering refers to segmenting data in accordance with the underlying subspaces. To this end, the state-of-the-art methods commonly obtain representations of samples on a dictionary to measure similarities among samples. However, most these subspace clustering methods are still very sensitive to noise in the data. To relieve the impact of the noise, we propose an iterative method by successively seeking k samples (k Sparse Representation Neighbors, k SRNs) with the highest similarity to a query. In each round, based on a fast constructed dictionary, a sparse representation is obtained using Lasso regression. Afterwards, we introduce a similarity measurement relevant with reconstruction error. By that means, the sample with minimum reconstruction error is selected as a SRN which is utilized to update the dictionary in the next round. Since each SRN has the minimum reconstruction error, the proposed method is robust to the noise in samples. Experimental results on the commonly used benchmark datasets show that the proposed method outperforms existing methods.",2018,Pattern Recognit. Lett.
A Deep Learning Approach to Predict Parking Occupancy using Cluster Augmented Learning Method,"This paper proposes a deep learning model for block-level parking occupancy prediction. The proposed model leverages Convolutional Neural Nets (CNN) to extract spatial relations of traffic flow and stacked LSTM autoencoder to capture temporal correlations. The paper also introduces Clustering Augmented Learning Method (CALM) which is based on concept of simultaneous heterogeneous clustering and regression learning to learn deep feature representations of spatio-temporal data obtained using the proposed embedding. The regression model that is considered in this work has a Feedforward Neural Net(FNN) architecture. With the aim of improving the accuracy of the regression model, CALM iterates between clustering and learning to form robust clusters, thereby leveraging the learning process of the FNN. A dissimilarity measure is proposed based on the weights of each feature of input data in the regression model to form the clusters. During each iteration of learning and clustering, a classifier is used to predict the current cluster labels, and the cluster belonging probabilities are used to control the subsequent re-estimation of cluster centers. Computational experiments on San Francisco Parking data set reveals that proposed approach CALM outperforms other baseline methods including multi-layer LSTM and Lasso with testing MAPE of 7.8% when predicting block-level parking occupancies. The findings highlight that the way in which the spatio - temporal information is exploited in order to aggregate the analogous data into clusters makes a significant difference and demonstrate the effectiveness of the model in processing parking data.",2019,2019 International Conference on Data Mining Workshops (ICDMW)
Analysis of a multiclass classification problem by Lasso Logistic Regression and Singular Value Decomposition to identify sound patterns in queenless bee colonies,"Abstract This study presents an analysis of a multiclass classification problem to identify queenless states by monitoring bee sound in two possible cases; a strong and healthy colony that lost its queen and a reduced population queenless colony. The sound patterns were compared with patterns of healthy queenright colonies. Five colonies of Carniola honey bee were monitored by using a system based on a Raspberry Pi 2 and omnidirectional microphones placed inside the hives. Feature extraction was carried out by Mel Frequency Cepstral Coefficients (MFCCs) method. A multiclass model with three outcome variables was constructed. For feature selection and regularization, a Lasso logistic Regression model was used along with one vs all strategy. To provide visual evidence and examine the results, data was analyzed by scatter plots of Singular Value Decomposition (SVD). The results show that is possible to detect the queenless state in both cases. Queenless or healthy colonies can generate slightly different patterns and the data clusters of the same condition tend to be close. The proposed methodology can be applied for the analysis of more conditions in bee colonies.",2019,Comput. Electron. Agric.
Multivariate sparse group lasso for the multivariate multiple linear regression with an arbitrary group structure.,"We propose a multivariate sparse group lasso variable selection and estimation method for data with high-dimensional predictors as well as high-dimensional response variables. The method is carried out through a penalized multivariate multiple linear regression model with an arbitrary group structure for the regression coefficient matrix. It suits many biology studies well in detecting associations between multiple traits and multiple predictors, with each trait and each predictor embedded in some biological functional groups such as genes, pathways or brain regions. The method is able to effectively remove unimportant groups as well as unimportant individual coefficients within important groups, particularly for large p small n problems, and is flexible in handling various complex group structures such as overlapping or nested or multilevel hierarchical structures. The method is evaluated through extensive simulations with comparisons to the conventional lasso and group lasso methods, and is applied to an eQTL association study.",2015,Biometrics
"SABR: sparse, anchor-based representation of the speech signal","We present SABR (Sparse, Anchor-Based Representation), an analysis technique to decompose the speech signal into speaker-dependent and speaker-independent components. Given a collection of utterances for a particular speaker, SABR uses the centroid for each phoneme as an acoustic â€œanchor,â€ then applies Lasso regularization to represent each speech frame as a sparse non-negative combination of the anchors. We illustrate the performance of the method on a speaker-independent phoneme recognition task and a voice conversion task. Using a linear classifier, SABR weights achieve significantly higher phoneme recognition rates than Mel frequency Cepstral coefficients. SABR weights can also be used directly to perform accent conversion without the need to train a speakerto-speaker regression model.",2015,
Identification of Geographical Segmentation of the Rental Apartment Market in the Tokyo Metropolitan Area (Short Paper),"It is often said that the real estate market is divided geographically in such a manner that the value of attributes of real estate properties is different for each area. This study proposes a new approach to the investigation of the geographical segmentation of the real estate market. We develop a price model with many regional explanatory variables, and implement the generalized fused lasso - a regression method for promoting sparsity - to extract the areas where the valuation standard is the same. The proposed method is applied to rental data of apartments in the Tokyo metropolitan area, and we find that the geographical segmentation displays hierarchal patterns. Specifically, we observe that the market is divided by wards, railway lines and stations, and neighbourhoods.",2018,
The walking estimated limitation stated by history (WELSH): a visual tool to self-reported walking impairment in a predominantly illiterate population.,"BACKGROUND
The prevalence of cardiovascular diseases is increasing in low-income countries. Various questionnaires to estimate walking capacity in patients are available in multiple languages but they are not suitable for illiterate patients.


OBJECTIVE
The walking estimated limitation stated by history (WELSH) tool aims at rating individual walking disability using only drawings and four items.


METHODS
A six-month prospective study was performed on new patients referred to the Department of Cardiology at the Centre Hospitalier Universitaire SourÃ´ Sanou in Bobo-Dioulasso, Burkina Faso. We administered the WELSH tool after a short oral presentation in the patient's language or dialect. Thereafter, patients performed a six-minute walking test in the hospital corridor under the supervision of a nurse who was blinded to the results of the WELSH score. We performed a step-by-step multilinear regression analysis to determine the factors predicting maximal walking distance (MWD).


RESULTS
There were 40 female and 10 male patients in this study. Their ages ranged from 54.8 Â± 10.7 years. Only 32% of the patients had attended primary school. Most patients were classified as stage I to III of the New York Heart Association (NYHA) classification. The objective measurement of MWD during a six-minute walking test showed no association with the subjects' educational level, body mass index, NYHA stage or gender, but a significant correlation with the WELSH scores. The Spearman r-value for the WELSH score-to-MWD relationship was 0.605 (p < 0.001).


CONCLUSIONS
The WELSH tool is feasible and correlated with measured MWD in a population of predominantly illiterate patients.",2019,Cardiovascular journal of Africa
Variable selection in regression using maximal correlation and distance correlation,"In most of the regression problems the first task is to select the most influential predictors explaining the response, and removing the others from the model. These problems are usually referred to as the variable selection problems in the statistical literature. Numerous methods have been proposed in this field, most of which address linear models. In this study we propose two variable selection criteria for regression based on two powerful dependence measures, maximal correlation and distance correlation. We focus on these two measures since they fully or partially satisfy the RÃ©nyi postulates for dependence measures, and thus they are able to detect nonlinear dependence structures. Therefore, our methods are considered to be appropriate in linear as well as nonlinear regression models. Both methods are easy to implement and they perform well. We illustrate the performances of the proposed methods via simulations, and compare them with two benchmark methods, stepwise Akaike information criterion and lasso. In several cases with linear dependence all four methods turned out to be comparable. In the presence of nonlinear or uncorrelated dependencies, we observed that our proposed methods may be favourable. An application of the proposed methods to a real financial data set is also provided.",2015,Journal of Statistical Computation and Simulation
Feature Selection Based on Structured Sparsity: A Comprehensive Study.,"Feature selection (FS) is an important component of many pattern recognition tasks. In these tasks, one is often confronted with very high-dimensional data. FS algorithms are designed to identify the relevant feature subset from the original features, which can facilitate subsequent analysis, such as clustering and classification. Structured sparsity-inducing feature selection (SSFS) methods have been widely studied in the last few years, and a number of algorithms have been proposed. However, there is no comprehensive study concerning the connections between different SSFS methods, and how they have evolved. In this paper, we attempt to provide a survey on various SSFS methods, including their motivations and mathematical representations. We then explore the relationship among different formulations and propose a taxonomy to elucidate their evolution. We group the existing SSFS methods into two categories, i.e., vector-based feature selection (feature selection based on lasso) and matrix-based feature selection (feature selection based on lr,p-norm). Furthermore, FS has been combined with other machine learning algorithms for specific applications, such as multitask learning, multilabel learning, multiview learning, classification, and clustering. This paper not only compares the differences and commonalities of these methods based on regression and regularization strategies, but also provides useful guidelines to practitioners working in related fields to guide them how to do feature selection.",2017,IEEE transactions on neural networks and learning systems
A regularization approach for estimation and variable selection in high dimensional regression models,"Model selection and estimation are important topics in econometric analysis which can become considerably complicated in high dimensional settings, where the set of possible regressors can become larger than the set of available observations. For large scale problems the penalized regression methods (e.g. Lasso) have become the de factor benchmark that can effectively trade off parsimony and fit. In this paper we introduce a regularized estimation and model selection approach that is based on sparse large covariance matrix estimation, introduced by Bickel and Levina (2008) and extended by Dendramis et al (2017). We provide asymptotic and small sample results that indicate that our approach can be an important alternative to the penalized regression. Moreover, we also introduce a number of extensions that can improve the asymptotic and small sample performance of the proposed method. The usefulness of what we propose is illustrated via Monte Carlo exercises and an empirical application in macroeconomic forecasting.",2018,
Performance Analysis of LASSO-based Signal Parameter Estimation,"The Least Absolute Shrinkage and Selection Operator (LASSO ) has gained attention in a wide class of continuous parametric estimation problems with pr omising results. In these applications the desired information is given by the unknown support of a spar e vector represented by some continuous parameters. The objective of this work is to provide a theore tical analysis of such a LASSO-based estimator in terms of the classical statistical measures, i . . variance and bias. Employing LASSO, which only admits a discrete set of candidate regressors, to a cont inu us valued problem complicates the analysis significantly. We respond to this dilemma by introducing a ne w approach considering an intermediate sparse estimator over the continuum, which we show to be asym ptotically equivalent to LASSO but easier to analyze. This provides us theoretical expression for the LASSO-based estimation error in the asymptotic case of high SNR and dense grids. We specifically s how that beyond the RIP-based results, such an asymptotic case may be consistent in many individual cases of interest. Without loss of generality, we present the comparative numerical results in the context of Direction of Arrival (DOA) estimation using a sensor array. Index Terms Compressed Sensing, performance analysis, sparse estimat ion, sparse regression, continuous regression",2012,
Predicting Time to Treatment in Follicular Lymphoma Using Population-Based Data,"Introduction

Patients diagnosed with Follicular Lymphoma (FL) either require immediate immuno-chemotherapy or, may simply be observed until the development of symptoms suggests that treatment should be initiated. Although there is no evidence to indicate that early intervention in asymptomatic, stage II - IV disease improves outcomes, both doctors and patients find ""watch and wait"" (W+W) strategies difficult to accept as it leaves considerable uncertainty as to the future. The ability to predict the likely time to treatment (TTT) being required would be helpful, both to give reassurance in cases where the likelihood of therapy in the near future is low and to identify higher risk cases where close observation or early intervention might be justified. Prognostic indices are usually derived using data from clinical trials or institutional datasets where patients who do not require treatment tend to be underrepresented; accordingly we constructed an improved prognostic index for TTT for those patients initially on a W+W strategy in an unselected, population-based cohort.

Methods

This study was based on an established population-based cohort, which since 2004 has tracked all patients newly diagnosed with a haematological malignancy in a representative UK population of nearly 4 million people (www.hmrn.org). All diagnoses, including disease progressions and transformations, are made by a single specialist haematopathology laboratory (www.hmds.info), and clinical teams work to UK guidelines (www.bcshguidlines.com). Clinical and treatment information is systematically collected for all patients and survival data is acquired through links with national data sources.

All 296 out of 741 patients newly diagnosed with FL from 2004-2011 initially managed by a W+W approach were included in the analyses and these patients were followed up to February 2015. Prognostic indices for TTT were constructed using the components of the Follicular Lymphoma International Prognostic Index (FLIPI) and other routinely measured clinical variables. Modern machine learning techniques were used, in particular the LASSO applied to semiparametric survival regression, using bootstrapped model selection to confirm Lasso variable selection. The appropriate functional forms for individual variables were chosen on the basis of the Akaike Information Criterion. Predictive performance was measured using area under the ROC curve (AUC) and the concordance index (C).

Results

With a median age of 65.4 years (range 21-95), 42% of patients were male, 16% had B-symptoms and 37% had stage IV disease at presentation. Median follow-up was 6.4 years; 83 patients were subsequently treated for FL, a further 34 patients transformed to diffuse large B-cell lymphoma and 9 others died from disease progression prior to receiving chemotherapy for FL; median time to these events was 1.4 years. Whilst the FLIPI score for patients initially managed on W+W was predictive for TTT (Figure 1) - achieving an AUC=0.64 and C=0.61, as a result of the model building process a proposed new index for TTT achieved AUC=0.75 and C=0.70 retaining blood albumin, haemoglobin, presence/absence of bulky disease (1/0 respectively) and a score based on the number of nodal sites in the prognostic model (Risk\_Score = Albumin (g/dL) x 0.0412 + 0.719 * bulky\_disease - 0.102 x Hb (g/dL) + 0.159 x nodal_score). The relation between index value and expected time-to-event for TTT is shown in Figure 2.

Conclusion

Our population-based data demonstrates that the FLIPI can be used to predict TTT in patients diagnosed with FL and put onto a W+W strategy. By utilising all of the information contained in the components of the FLIPI and by adding additional routine clinical factors we show that the accuracy of prediction of TTT can be improved leading to the production of an accurate and simple TTT curve that can be used in routine clinical practice.

![Figure 1.][1] 

Figure 1. 
Time-to-treatment stratified by FLIPI (p = 0.0004 log-rank test)





![Figure 2.][1] 

Figure 2. 
Expected Probability of Not Having Received Chemotherapy at 5 years After Diagnosis by Time-to-Treatment Risk Score



Disclosures Patmore: Gilead: Honoraria; Janssen: Honoraria.

 [1]: pending:yes",2015,Blood
Price jump prediction in a limit order book,"A limit order book provides information on available limit order prices and their volumes. Based on these quantities, we give an empirical result on the relationship between the bid-ask liquidity balance and trade sign and we show that liquidity balance on best bid/best ask is quite informative for predicting the future market order's direction. Moreover, we de ne price jump as a sell (buy) market order arrival which is executed at a price which is smaller (larger) than the best bid (best ask) price at the moment just after the precedent market order arrival. Features are then extracted related to limit order volumes, limit order price gaps, market order information and limit order event information. Logistic regression is applied to predict the price jump from the limit order book's feature. LASSO logistic regression is introduced to help us make variable selection from which we are capable to highlight the importance of di erent features in predicting the future price jump. In order to get rid of the intraday data seasonality, the analysis is based on two separated datasets: morning dataset and afternoon dataset. Based on an analysis on forty largest French stocks of CAC40, we nd that trade sign and market order size as well as the liquidity on the best bid (best ask) are consistently informative for predicting the incoming price jump.",2012,Journal of Mathematical Finance
Supervised & unsupervised transfer learning,"This thesis investigates transfer learning in two areas of data analysis, supervised 
and unsupervised learning. We study multi-task learning on vectorial 
data in a supervised setting and multi-view clustering on pairwise distance 
data in a Bayesian unsupervised approach. The aim in both areas is to transfer 
knowledge over different related data sets as opposed to learning on single 
data sets separately. 
 
In supervised learning, not only the input vectors but also the corresponding target vectors are observed. The aim is to learn a mapping from the input 
space to the target space to predict the target values for new samples. In 
standard classification or regression problems, one data set at a time is considered 
and the learning problem for every data set is solved separately. In 
this work, we are looking at the non-standard case of learning by exploiting 
the information given by multiple related tasks. Multi-task learning is based 
on the assumption that multiple tasks share some features or structures. One 
well-known technique solving multi-task problems is the Group-Lasso with 
2-norm regularization. The motivation for using the Group-Lasso is to couple 
the individual tasks via the group-structure of the constraint term. Our main 
contribution in the supervised learning part consists in deriving a complete 
analysis of the Group-Lasso for all p-norm regularizations, including results 
about uniqueness and completeness of solutions and coupling properties of 
different p-norms. In addition, a highly efficient active set algorithm for all 
p-norms is presented which is guaranteed to converge and which is able to 
operate on extremely high-dimensional input spaces. For the first time, this 
allows a direct comparison and evaluation of all possible Group-Lasso methods 
for all p-norms in large scale experiments. We show that in a multi-task 
setting, both, tight coupling norms with p >>Â 2 and loose coupling norms 
with p <<Â 2 significantly degrade the prediction performance. Moderate coupling 
norms seem to be the best compromise between coupling 
strength and robustness against systematic differences between the tasks. 
 
The second area of data analysis we look at is unsupervised learning. In unsupervised 
learning, the training data consists of input vectors without any 
corresponding target vectors. Classical problems in unsupervised learning 
are clustering, density estimation or dimensionality reduction. As in the supervised 
scenario, we are not only considering single data sets independently 
of each other, but we want to learn over two or more data sets simultaneously. 
A problem that arises frequently is that the data is only available as 
pairwise distances between objects (e.g. pairwise string alignment scores from 
protein sequences) and a loss-free embedding into a vector space is usually 
not possible. We propose a Bayesian clustering model that is able to operate 
on this kind of distance data without explicitly embedding it into a vector 
space. Our main contribution in the unsupervised learning part is twofold. 
Firstly, we derive a fully probabilistic clustering method based on pairwise 
Euclidean distances, that is rotation-, translation-, and scale- invariant and 
uses the Wishart distribution in the likelihood term. On the algorithmic 
side, a highly efficient sampling algorithm is presented. Experiments indicate 
the advantage of encoding the translation invariance into the likelihood, 
and our clustering algorithm clearly outperforms several hierarchical clustering 
methods. Secondly, we extend this clustering method to a novel Bayesian 
multi-view clustering approach based on distance data. We show that the 
multi-view clustering method reveals shared information between different 
views of a phenomenon and we obtain an improved clustering compared to 
clustering on every view separately.",2013,
Penalized regression for discrete structures,"Penalisierte Regressionsmodelle stellen eine Moglichkeit dar die Selektion von Kovariablen in die Schatzung eines Modells zu integrieren. Penalisierte Ansatze eignen sich insbesondere dafur, komplexen Strukturen in den Kovariablen eines Modells zu berucksichtigen. Diese Arbeit beschaftigt sich mit verschiedenen Penalisierungsansatzen fur diskrete Strukturen, wobei der Begriff ""diskrete Struktur"" in dieser Arbeit alle Arten von kategorialen Einflussgrosen, von effekt-modifizierenden, kategorialen Einflussgrosen sowie von gruppenspezifischen Effekten in hierarchisch strukturierten Daten bezeichnet. Ihnen ist gemein, dass sie zu einer verhaltnismasig grosen Anzahl an zu schatzenden Koeffizienten fuhren konnen. Deswegen besteht ein besonderes Interesse daran zu erfahren, welche Kategorien einer Einflussgrose die Zielgrose beeinflussen, und welche Kategorien unterschiedliche beziehungsweise ahnliche Effekte auf die Zielgrose haben. Kategorien mit ahnlichen Effekten konnen beispielsweise durch fused Lasso Penalties identifiziert werden. Jedoch beschranken sich einige, bestehende Ansatze auf das lineare Modell. Die vorliegende Arbeit ubertragt diese Ansatze auf die Klasse der generalisierten linearen Regressionsmodelle. Das beinhaltet computationale wie theoretische Aspekte. Konkret wird eine fused Lasso Penalty fur effekt-modifizierende kategoriale Einflussgrosen in generalisierten linearen Regressionsmodellen vorgeschlagen. Sie ermoglicht es, Einflussgrosen zu selektieren und Kategorien einer Einflussgrose zu fusionieren. Gruppenspezifische Effekte, die die Heterogenitat in hierarchisch strukturierten Daten berucksichtigen, sind ein Spezialfall einer solchen effekt-modifizierenden, kategorialen Grose. Hier bietet der penalisierte Ansatz zwei wesentliche Vorteile: (i) Im Gegensatz zu gemischten Modellen, die starkere Annahmen treffen, kann der Grad der Heterogenitat sehr leicht reduziert werden. (ii) Die Schatzung ist effizienter als im unpenalisierten Ansatz. In orthonormalen Settings konnen Fused Lasso Penalties konzeptionelle Nachteile haben. Als Alternative wird eine L0 Penalty fur diskrete Strukturen in generalisierten linearen Regressionsmodellen diskutiert, wobei die sogenannte L0 ""Norm"" eine Indikatorfunktion fur Argumente ungleich Null bezeichnet. Als Penalty ist diese Funktion so interessant wie anspruchsvoll. Betrachtet man eine Approximation der L0 Norm als Verlustfunktion wird im Grenzwert der bedingte Modus einer Zielgrose geschatzt.",2015,
Breast Cancer Risk Prediction Using Electronic Health Records,"Electronic health records (EHRs) represent an underused data source that has great research and clinical potential. Our goal was to quantify the value of EHRs in breast cancer risk prediction. We conducted a retrospective case-control study, gathering patients' ICD-9 diagnosis codes from an existing EHR data repository. Based on the hierarchical structure of ICD-9 codes, which are composed of 3-5 digits, three levels of data representation were studied: level 0, using only the first 3 digits; level 1, using up to the first 4 digits; and level 2, using up to the full 5 digits of each code. We created two models to predict breast cancer one year in advance based on diagnosis codes in three levels of data representation: logistic regression (LR) and LASSO logistic regression (LR+Lasso). Area under the ROC curve (AUC) was used to assess model performance. The LR+Lasso model demonstrated significantly higher predictive performance than the LR model when using the level 2 feature representation (0.648 vs 0.603, p=0.013). For both the level 1 representation and the level 0 representation, the predictive difference between LR+Lasso and LR model was not significant, (0.634 vs 0.604, p=0.081) and (0.612 vs 0.603, p=0.523), respectively. For LR model, predictive performance changed modestly across three levels. For LR+Lasso model, predictive performance also changed modestly from the level 0 to the level 1representation (p=0.168) and from the level 1 to the level 2 representation (p=0.374). However, the level 2 representation provided significantly higher predictive performance than the level 0 representation (p=0.034). The unabridged level 2 representation of the diagnosis codes contains the most valuable information that may contribute to breast cancer risk prediction. The performance of these models demonstrates that EHR data can be used to predict breast cancer risk, which provides the possibility to personalize care in clinical practice. In the future, we will combine coded EHR data with demographic risk factors, genetic variants, and imaging features to improve breast cancer risk prediction.",2017,2017 IEEE International Conference on Healthcare Informatics (ICHI)
Non-convex Global Minimization and False Discovery Rate Control for the TREX,"The TREX is a recently introduced method for performing sparse high-dimensional regression. Despite its statistical promise as an alternative to the lasso, square-root lasso, and scaled lasso, the TREX is computationally challenging in that it requires solving a non-convex optimization problem. This paper shows a remarkable result: despite the non-convexity of the TREX problem, there exists a polynomial-time algorithm that is guaranteed to find the global minimum. This result adds the TREX to a very short list of non-convex optimization problems that can be globally optimized (principal components analysis being a famous example). After deriving and developing this new approach, we demonstrate that (i) the ability of the preexisting TREX heuristic to reach the global minimum is strongly dependent on the difficulty of the underlying statistical problem, (ii) the new polynomial-time algorithm for TREX permits a novel variable ranking and selection scheme, (iii) this scheme can be incorporated into a rule that controls the false discovery rate (FDR) of included features in the model. To achieve this last aim, we provide an extension of the results of Barber & Candes (2015) to establish that the knockoff filter framework can be applied to the TREX. This investigation thus provides both a rare case study of a heuristic for non-convex optimization and a novel way of exploiting non-convexity for statistical inference.",2016,ArXiv
Model Selection via Minimum Description Length,"The minimum description length (MDL) principle originated from data compression literature and has been considered for deriving statistical model selection procedures. Most existing methods utilizing the MDL principle focus on models consisting of independent data, particularly in the context of linear regression. The data considered in this thesis are in the form of repeated measurements, and the exploration of MDL principle begins with classical linear mixed-effects models. We distinct two kinds of research focuses: one concerns the population parameters and the other concerns the cluster/subject parameters. When the research interest is on the population level, we propose a class of MDL procedures which incorporate the dependence structure within individual or cluster with data-adaptive penalties and enjoy the advantages of Bayesian information criteria. When the number of covariates is large, the penalty term is adjusted by data-adaptive structure to diminish the under selection issue in BIC and try to mimic the behaviour of AIC. Theoretical justifications are provided from both data compression and statistical perspectives. Extensions to categorical response modelled by generalized estimating equations and functional data modelled by functional principle components are illustrated. When the interest is on the cluster level, we use group LASSO to set up a class of candidate models. Then we derive a MDL criterion for this LASSO technique in a group manner to selection the final model via the tuning parameters. Extensive numerical experiments are conducted to demonstrate the usefulness of the proposed MDL procedures on both population level and cluster level.",2012,
Lasso for sparse linear regression with exponentially Î²-mixing errors,"We prove two consistency theorems for the lasso estimators of sparse linear regression models with exponentiallyÎ²-mixing errors, in which the number of regressors p is large, even much larger than the sample size n.",2017,Statistics & Probability Letters
Assessing Prior Pain Visits and Medical History Risk Factors for Opioid Overdose,"Objective:Â  Identifying text features of emergency department visits associated with risk of future drug overdose. Introduction:Â  Opioid overdoses are a growing cause of mortality in the United States. 1 Â Medical prescriptions for opioids are a risk factor for overdose 2 . This observation raises concerns that patients may seek multiple opioid prescriptions, possibly increasing their overdose risk. One route for obtaining those prescriptions is visiting the emergency department (ED) for pain-related complaints. Here, two hypotheses related to prescription seeking and overdoses are tested. (1) Overdose patients have a larger number of prior ED visits than matched controls. (2) Overdose patients have distinct patterns of pain-related complaints compared to matched controls. Methods:Â  ED registrations were collected via the EpiCenter syndromic surveillance system. Regular expression searches on chief complaints identified overdose visits. Overdose visits were matched with control visits from the same facility with maximal similarity of gender, age, home location and arrival time. A year of prior ED visits for cases and controls were matched using facility-specific patient identifiers or birthdate, gender and home location. Patient history chief complaints were sanitized to standardize spelling, expand abbreviations and consolidate phrases. Word frequency comparisons between groups identified candidate terms for modeling. Odds ratios of patient history terms were calculated with univariate logistic regression. Multivariate lasso logistic regression selected covariates for prediction. These models were fit to data from one quarter and cutoffs for covariate inclusion were validated on the following quarterâ€™s data. Model predictions were validated on a 1% sample of ED registrations from the next quarter. Results:Â  Quarter three of 2016 yielded 23,769 overdose ED visits and matching controls; quarter four yielded 21,957 pairs; and 15,824 ED visits were sampled from the first quarter of 2017 including 130 overdose visits. Contrary to expectations, patients in the control group averaged 0.7 additional ED visits in the prior year relative to controls; this pattern was consistent across quarters and regardless of how prior visits were matched (Fig 1). Prior visits for various pain categories were also more common among control patients than overdose patients (e.g. odds ratio for â€œback painâ€: 0.78). Terms associated with drug use (e.g. â€œdetoxâ€ odds ratio: 2.66) and mental health concerns (e.g. â€œpsychologicalâ€ odds ratio: 4.28) were most consistently overrepresented in the history of overdose patients (Table 1). Terms associated with chronic disease were most overrepresented in the history of control patients (Table 2). The best predictive model achieved a sensitivity of 57% and a specificity of 86% on test data (Fig 2). Conclusions:Â  While a history of more overall ED visits and more ED visits related to pain were not associated with overdose ED visits, vocabulary of prior ED visits did predict future overdose ED visits. Performance of predictive models exceeded expectations, given the relative scarcity of overdoses among ED visits and the simplicity of chief complaints used for prediction. The correlation between past and future overdose visits highlights the need for targeted intervention to break addiction cycles.",2018,Online Journal of Public Health Informatics
Multimodal classification of prostate tissue: a feasibility study on combining multiparametric MRI and ultrasound,"The common practice for biopsy guidance is through transrectal ultrasound, with the fusion of ultrasound and MRI-based targets when available. However, ultrasound is only used as a guidance modality in MR-targeted ultrasound-guided biopsy, even though previous work has shown the potential utility of ultrasound, particularly ultrasound vibro-elastography, as a tissue typing approach. We argue that multiparametric ultrasound, which includes B-mode and vibro-elastography images, could contain information that is not captured using multiparametric MRI (mpMRI) and therefore play a role in refining the biopsy and treatment strategies. In this work, we combine mpMRI with multiparametric ultrasound features from registered tissue areas to examine the potential improvement in cancer detection. All the images were acquired prior to radical prostatectomy and cancer detection was validated based on 36 whole mount histology slides. We calculated a set of 24 texture features from vibro-elastography and B-mode images, and five features from mpMRI. Then we used recursive feature elimination (RFE) and sparse regression through LASSO to find an optimal set of features to be used for tissue classification. We show that the set of these selected features increases the area under ROC curve from 0.87 with mpMRI alone to 0.94 with the selected mpMRI and multiparametric ultrasound features, when used with support vector machine classification on features extracted from peripheral zone. For features extracted from the whole-gland, the area under the curve was 0.75 and 0.82 for mpMRI and mpMRI along with ultrasound, respectively. These preliminary results provide evidence that ultrasound and ultrasound vibro-elastography could be used as modalities for improved cancer detection in combination with MRI.",2015,
Biomarkers Can Identify Pulmonary Tuberculosis in HIV-infected Drug Users Months Prior to Clinical Diagnosis,"BACKGROUND
Current diagnostic tests cannot identify which infected individuals are at risk for progression to tuberculosis (TB). Our aim was to identify biomarkers which can predict the development of TB prior to clinical diagnosis.


METHOD
In a retrospective case-control study, RNA of 14 HIV-infected drug users obtained before TB diagnosis (cases) and of 15 who did not develop TB (controls) was analyzed for the expression of 141 genes by dcRT-MLPA followed by Lasso regression analysis.


FINDINGS
A combined analysis of IL13 and AIRE had the highest discriminatory power to identify cases up to 8Â months prior to clinical diagnosis. Cases expressing IL13 had a gene expression pattern strongly enriched for type I IFN related signaling genes, suggesting that these genes represent processes that contribute to TB pathogenesis.


INTERPRETATION
We here demonstrated that biomarkers, such as IL13-AIRE, can identify individuals that progress to TB within a high risk population, months prior to clinical diagnosis.",2015,EBioMedicine
Split regression modeling,"In this note we study the benefits of splitting variables variables for reducing the variance of linear functions of the regression coefficient estimate. We show that splitting combined with shrinkage can result in estimators with smaller mean squared error compared to popular shrinkage estimators such as Lasso, ridge regression and garrote.",2018,arXiv: Methodology
Breast cancer diagnostics in daily practice: the place of PAMMOTH,"With incidence numbers still rising, breast cancer is the leading cancer diagnosis in women worldwide. Therefore, early detection and precise diagnosis of breast cancer is an important factor contributing to accurate therapy and better survival chances. The goal of this study is to outline the hospital-based diagnostic care pathway of patients with suspected breast cancer in the Netherlands and to identify features which influence the diagnostic pathway. Two different databases have been analysed; one â€™benignâ€™ database containing approximately 31,000 patients with suspected breast cancer, and one â€™malignantâ€™ database containing approximately 2,200 diagnosed breast cancer pa- tients. Information in the malignant database originates from the Netherlands Cancer Registry (NCR) and hospital-based financial data, accommodated by Performation. Information in the benign database was based on financial data only. Both databases have been carefully evaluated to reveal variation between and patterns within the diagnostic care pathways. In- fluencing features on the diagnosis of breast cancer, days until diagnosis and number of diagnostic care activities have been identified in the malignant diagnostic care pathway, using the Lasso method and cross-validation together with Cox and Poisson regression models.",2018,
An efficient stochastic approach for flow in porous media via sparse polynomial chaos expansion constructed by feature selection,"Abstract An efficient method for uncertainty quantification for flow in porous media is studied in this paper, where response surface of sparse polynomial chaos expansion (PCE) is constructed with the aid of feature selection method. The number of basis functions in PCE grows exponentially as the random dimensionality increases, which makes the computational cost unaffordable in high-dimensional problems. In this study, a feature selection method is introduced to select major stochastic features for the PCE by running a limited number of simulations, and the resultant PCE is termed as sparse PCE. Specifically, the least absolute shrinkage and selection operator modified least angle regression algorithm (LASSO-LAR) is applied for feature selection and the selected features are assessed by cross-validation (CV). Besides, inherited samples are utilized to make the algorithm self-adaptive. In this study, we test the performance of sparse PCE for uncertainty quantification for flow in heterogeneous media with different spatial variability. The statistical moments and probability density function of the output random field are accurately estimated through the sparse PCE, meanwhile the computational efforts are greatly reduced compared to the Monte Carlo method.",2017,Advances in Water Resources
Optimization of an H0 photonic crystal nanocavity using machine learning,"Using machine learning, we optimized an ultrasmall photonic crystal nanocavity to attain a high $Q$Q. Training data were collected via finite-difference time-domain simulation for models with randomly shifted holes, and a fully connected neural network (NN) was trained, resulting in a coefficient of determination between predicted and calculated values of 0.977. By repeating NN training and optimization of the $Q$Q value on the trained NN, the $Q$Q was roughly improved by a factor of 10â€“20 for various situations. Assuming a 180-nm-thick semiconductor slab at a wavelength approximately 1550 nm, we obtained $Q={1},\!{011},\!{400}$Q=1,011,400 in air; 283,200 in a solution, which was suitable for biosensing; and 44,600 with a nanoslot for high sensitivity. Important hole positions were also identified using the linear Lasso regression algorithm.",2020,Optics Letters
A model for predicting overall survival in men with metastatic castrate-resistant prostate cancer (CRPC) for whom first-line chemotherapy failed.,"24 Background: Several prognostic models for overall survival (OS) have been developed and validated in men with chemotherapy naÃ¯ve mCRPC. The primary objective was to develop and validate a prognostic model that can be used to predict OS in men who have failed first-line chemotherapy.


METHODS
Data was used from a phase III trial of 755 mCRPC men who had developed progressive disease following first-line chemotherapy and were randomized to cabazitaxel plus prednisone or mitoxantrone plus prednisone (TROPIC trial). The data was randomly split into training (n=507) and testing (n=248) sets. A separate data, consisting of 488 men previously treated with docetaxel who were randomly assigned to either satraplatin and prednisone or placebo and prednisone, was used as the validation set (SPARC trial). Penalized regression method was used to identify important prognostic factors. Adaptive Lasso selected nine variables of OS. A predictive score was computed from the estimated regression coefficients and used to classify patients into low (<-1.29) and high (>= -1.29) risk groups in the testing datasets. The model was assessed for its predictive accuracy using time dependent area under the curve (AUC) on the testing sets (TROPIC and SPARC trials).


RESULTS
The final selected model included: ECOG performance status, time since last docetaxel use, measurable disease, presence of visceral disease, pain, duration of prior hormonal use, hemoglobin, prostate specific antigen and alkaline phosphatase. In the TROPIC testing set, the median OS in high and low risk groups were 11 and 17 months, respectively, with a hazard ratio (HR)=2.47 (p-value<0.0001). Using the SPARC set, the median OS were 11 and 20 months in the high and low risk groups, respectively, with a HR=1.94 (p<0.0001). The time dependent AUC were 0.73 and 0.70 on the testing sets.


CONCLUSIONS
A prognostic model of OS in the post-docetaxel mCRPC setting was developed and validated and risk groups were identified. This model can be used to select patients based on their prognosis to participate in clinical trials. Prospective validation is needed.",2013,Journal of clinical oncology : official journal of the American Society of Clinical Oncology
Abstract MP20: The Discriminatory Characteristics of Neighborhood Socioeconomic Status in Predicting Cardiovascular Disease in Electronic Health Record Based Studies Differs by Age,"Introduction: Recent studies report an association between neighborhood residence and health outcomes. There is less information on the relative utility of neighborhood socioeconomic status (nSES) in models that predict future health outcomes and the impact that age may have on this. Objective: To quantify if nSES data alone or in concert with electronic health record (EHR) data can improve risk prediction for myocardial infarction (MI) and stroke beyond current models. Methods: Neighborhood SES was derived using the AHRQ SES index. Clinical and demographic data was obtained from the EHR of patients seen at the Duke University Health System from 2009-2015; it was split into a training set (2009-2012) and testing set (2012-2015). Age (in yrs) was categorized as young (18-44), middle age (45-64), and old (â‰¥65). Logistic regression models were fit for each outcome over 6 time horizons (30, 90, & 180 days; 1, 2, & 3 years) using machine learning methods (least absolute shrinkage and selection operator [LASSO]) for model selection to determine if nSES improved discrimination, as measured by the c-statistic. Results: Of 106703 patients, 63% were female, 41% were Black, 2.6% had CVD, 12% had diabetes, and 29% had hypertension at baseline with mean age of 47 years. The majority of the correlation between EHR variables and nSES (r 2 =0.31) was explained by demographic information within the EHR (r 2 =0.29; p Conclusions: The added value of nSES was less than expected as much variability in nSES may be phenotyped through demographic information in the EHR. In discrete instances, nSES can improve risk prediction but varies by age, clinical outcome, and time horizon.",2018,Circulation
M37: Taking a Polygenic Approach to GâŽg and GâŽe Interactions in the Prediction of Mdd,"Background Major Depressive Disorder (MDD) is a complex heterogeneous disorder, with an estimated heritability of ~40%. Known environmental risk factors (e.g. neuroticism, cognition) also play an important role in the development of MDD. Despite the successful identification of many associated loci in the most recent MDD GWAS, this required a huge sample size and so these loci explain only a small fraction of the heritability, making clinical application challenging. One way of revealing greater aetiological insight, and potentially increasing disease prediction, is to detect gene-gene and gene-environment interactions. The identification of these interactions is challenging due to the huge model space involved (esp. for GâŽG). While these individual variant interactions may exist, there may also be interactions â€“ more easily identified â€“ between the overall polygenic risk of a disorder and that of other disorders or environmental risk factors. Here we aim to 1) use Polygenic Risk Scores (PRS) to identify global genetic interaction effects, and 2) build a predictive model of MDD case-control status utilizing the rich phenotypic data available in the UK Biobank. Methods To build and validate predictive models of MDD, we split the UK Biobank data into discovery GWAS, testing and validation datasets. The predictive model was developed via model selection techniques such as stepwise AIC/BIC and lasso regression, with a large number of PRSâŽPRS and PRSâŽE predictors. Results We identify several intriguing PRS interaction effects and increase out-of-sample prediction of MDD case/control status significantly via inclusion of PRS interactions.",2019,European Neuropsychopharmacology
