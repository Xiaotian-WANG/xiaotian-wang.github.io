title,abstract,year,journal
"High-dimensional Bias-corrected Inference, with Applications to fMRI Studies","In neuroimaging studies, measures of neural structure and function are used to try to predict clinical outcomes in adults. Identifying biomarkers that reflect underlying neuropathological processes can provide promising neural targets for future therapeutic interventions. This identification is typically done using linear or generalized linear models (GLM) with many covariates and relatively few subjects. Thus, regularization is used to select the salient covariates in the model. In this thesis, we compare the performance of the least absolute shrinkage and selection operator (LASSO) regression, adaptive LASSO regression, debiased LASSO regression, and regularized zero-inflated Poisson (ZIP) regression model in two simulation settings. The performance of LASSO regression with Poisson and Gaussian models are similar but for all these approaches the zero-inflated model outperforms the rest. We apply these approaches to the data from the Longitudinal Assessment of Manic Symptoms (LAMS) study. We then study the bias correction of GLM and the application on ZIP data. We apply a decorrelated score approach to address Poisson distributed data and introduce Cornish-Fisher correction to the decorrelated score test. In high-dimension settings, the Cornish-Fisher correction can improve the performance of decorrelated score test for ZIP data.",2019,
Fast Algorithm for the Lasso based L1-Graph Construction,"The lasso-based L1-graph is used in many applications since it can effectively model a set of data points as a graph. The lasso is a popular regression approach and the L1-graph represents data points as nodes by using the regression result. More specifically, by solving the L1-optimization problem of the lasso, the sparse regression coefficients are used to obtain the weights of the edges in the graph. Conventional graph structures such as k-NN graph use two steps, adjacency searching and weight selection, for constructing the graph whereas the lasso-based L1-graph derives the adjacency structure as well as the edge weights simultaneously by using a coordinate descent. However, the construction cost of the lasso-based L1-graph is impractical for large data sets since the coordinate descent iteratively updates the weights of all edges until convergence. Our proposal, Castnet, can efficiently construct the lasso-based L1-graph. In order to avoid updating the weights of all edges, we prune edges that cannot have nonzero weights before entering the iterations. In addition, we update edge weights only if they are nonzero in the iterations. Experiments show that Castnet is significantly faster than existing approaches.",2016,Proc. VLDB Endow.
"High-Dimensional Sparse Matched Case-Control and Case-Crossover Data: A Review of Recent Works, Description of an R Tool and an Illustration of the Use in Epidemiological Studies","The conditional logistic regression model is the standard tool for the analysis of epidemiological studies in which one or more cases (the event of interest), are matched with one or more controls (not showing the event). These situations arise, for example, in matched caseâ€“control and caseâ€“crossover studies. In sparse and high-dimensional settings, penalized methods, such as the Lasso, have emerged as an alternative to conventional estimation and variable selection procedures. We describe the R package clogitLasso, which brings together algorithms to estimate parameters of conditional logistic models using sparsity-inducing penalties. Most individually matched designs are covered, and, beside Lasso, Elastic Net, adaptive Lasso and bootstrapped versions are available. Different criteria for choosing the regularization term are implemented, accounting for the dependency of data. Finally, stability is assessed by resampling methods. We previously review the recent works pertaining to clogitLasso. We also report the use in exploratory analysis of a large pharmacoepidemiological study.",2013,
Genomic Selection for Predicting Fusarium Head Blight Resistance in a Wheat Breeding Program,"Genomic selection (GS) is a breeding method that uses markerâ€“ trait models to predict unobserved phenotypes. This study developed GS models for predicting traits associated with resistance to Fusarium head blight (FHB) in wheat (Triticum aestivum L.). We used genotyping-by-sequencing (GBS) to identify 5054 singlenucleotide polymorphisms (SNPs), which were then treated as predictor variables in GS analysis. We compared how the prediction accuracy of the genomic-estimated breeding values (GEBVs) was affected by (i) five genotypic imputation methods (ran dom forest imputation [RFI], expectation maximization imputation [EMI], k-nearest neighbor imputation [kNNI], singular value decomposition imputation [SVDI], and the mean imputation [MNI]); (ii) three statistical models (ridge-regression best linear unbiased predictor [RR-BLUP], least absolute shrinkage and operator selector [LASSO], and elastic net); (iii) marker density (p = 500, 1500, 3000, and 4500 SNPs); (iv) training population (TP) size (nTP = 96, 144, 192, and 218); (v) marker-based and pedigree-based relationship matrices; and (vi) control for relatedness in TPs and validation populations (VPs). No discernable differences in prediction accuracy were observed among imputation methods. The RR-BLUP outperformed other models in nearly all scenarios. Accuracies decreased substantially when marker number decreased to 3000 or 1500 SNPs, depending on the trait; when sample size of the training set was less than 192; when using pedigree-based instead of marker-based matrix; or when no control for relatedness was implemented. Overall, moderate to high prediction accuracies were observed in this study, suggesting that GS is a very promising breeding strategy for FHB resistance in wheat.",2015,The Plant Genome
Analytic solution and stationary phase approximation for the Bayesian lasso and elastic net,"Regression shrinkage and variable selection are important concepts in high-dimensional statistics that allow the inference of robust models from large data sets. Bayesian methods achieve this by subjecting the model parameters to a prior distribution whose mass is centred around zero. In particular, the lasso and elastic net linear regression models employ a double-exponential distribution in their prior, which results in some maximum-likelihood regression coefficients being identically zero. Because of their ability to simultaneously perform parameter estimation and variable selection, these models have become enormously popular. However, there has been limited success in moving beyond maximum-likelihood estimation and deriving estimates for the posterior distribution of regression coefficients, due to a need for computationally expensive Gibbs sampling approaches to evaluate analytically intractable partition function integrals. Here, through the use of the Fourier transform, these integrals are expressed as complex-valued oscillatory integrals over ""regression frequencies"". This results in an analytic expansion and stationary phase approximation for the partition functions of the Bayesian lasso and elastic net, where the non-differentiability of the double-exponential prior distribution has so far eluded such an approach. Use of this approximation leads to highly accurate numerical estimates for the expectation values and marginal posterior distributions of the regression coefficients, thus allowing for Bayesian inference of much higher dimensional models than previously possible.",2018,ArXiv
Component Selection and Smoothing for Nonparametric Regression in Exponential Families,"We propose a new penalized likelihood method for model selection and nonparametric regression in exponential families. In the framework of smoothing spline ANOVA, our method employs a regularization with the penalty functional being the sum of the reproducing kernel Hilbert space norms of functional com- ponents in the ANOVA decomposition. It generalizes the LASSO in the linear regression to the nonparametric context, and conducts component selection and smoothing simultaneously. Continuous and categorical variables are treated in a unied fashion. We discuss the connection of the method to the traditional smooth- ing spline penalized likelihood estimation. We show that an equivalent formulation of the method leads naturally to an iterative algorithm. Simulations and examples are used to demonstrate the performances of the method.",2006,Statistica Sinica
Characterizing $L_{2}$Boosting,"We consider $L_2$Boosting, a special case of Friedman's generic boosting algorithm applied to linear regression under $L_2$-loss. We study $L_2$Boosting for an arbitrary regularization parameter and derive an exact closed form expression for the number of steps taken along a fixed coordinate direction. This relationship is used to describe $L_2$Boosting's solution path, to describe new tools for studying its path, and to characterize some of the algorithm's unique properties, including active set cycling, a property where the algorithm spends lengthy periods of time cycling between the same coordinates when the regularization parameter is arbitrarily small. Our fixed descent analysis also reveals a repressible condition that limits the effectiveness of $L_2$Boosting in correlated problems by preventing desirable variables from entering the solution path. As a simple remedy, a data augmentation method similar to that used for the elastic net is used to introduce $L_2$-penalization and is shown, in combination with decorrelation, to reverse the repressible condition and circumvents $L_2$Boosting's deficiencies in correlated problems. In itself, this presents a new explanation for why the elastic net is successful in correlated problems and why methods like LAR and lasso can perform poorly in such settings.",2012,Annals of Statistics
Modeling association between multivariate correlated outcomes and high-dimensional sparse covariates: the adaptive SVS method,"ABSTRACT The problem of modeling the relationship between a set of covariates and a multivariate response with correlated components often arises in many areas of research such as genetics, psychometrics, signal processing. In the linear regression framework, such task can be addressed using a number of existing methods. In the high-dimensional sparse setting, most of these methods rely on the idea of penalization in order to efficiently estimate the regression matrix. Examples of such methods include the lasso, the group lasso, the adaptive group lasso or the simultaneous variable selection (SVS) method. Crucially, a suitably chosen penalty also allows for an efficient exploitation of the correlation structure within the multivariate response. In this paper we introduce a novel variant of such method called the adaptive SVS, which is closely linked with the adaptive group lasso. Via a simulation study we investigate its performance in the high-dimensional sparse regression setting. We provide a comparison with a number of other popular methods under different scenarios and show that the adaptive SVS is a powerful tool for efficient recovery of signal in such setting. The methods are applied to genetic data.",2019,Journal of Applied Statistics
Survival Risk Prediction Models of Gliomas Based on IDH and 1p/19q,"Gliomas have been classified into different molecular subtypes based on their molecular features. To explore the prognostic factors of different subtypes of gliomas, we performed a univariate survival analysis based on the RNA-seq data of 653 patients obtained from The Cancer Genome Atlas. We identified 12205 (20.18%), 6125 (10.13%) and 5206 (8.61%) genes associated with the overall survival (OS) of the IDH-wildtype, IDH-mutation 1p/19q intact and IDH-mutation 1p/19q codeletion gliomas, respectively. Pathway enrichment analysis revealed that OS related genes were mainly involved in alcoholism, systemic lupus erythematosus, hematopoietic cell lineage and diabetes. The OS related genes were further selected using Lasso regression, and three prognostic risk score models were constructed to effectively predict the OS of the patients with different subtypes of gliomas. In total, 76 signature genes were identified and were selected to construct the three models. Moreover, neither of the 76 genes overlapped between different models, which suggested the enormous difference among the three subtypes, although some signature genes (SERPINA5, RP11.229A12.2 and RP11.62F24.2) were also identified as the OS related genes in different glioma subtypes. Interestingly, five genes (RP11.229A12.2, RP11.62F24.2, C3orf67, RP11.275H4.1 and TBX3) played opposing roles (protective or risk factor) in different subtypes. Additionally, the prognosis models consisted of a substantial proportion of non-coding RNA (58.74%, 70.13% and 58.11% in the IDH-wildtype, IDH-mutation 1p/19q intact and IDH-mutation 1p/19q codeletion). Furthermore, multivariate analysis integrating clinical variables demonstrated that risk group predicted by the prognostic models was an independent prognostic factor for gliomas. In conclusion, we have constructed and validated three models that have the potential to predict the prognosis of glioma patients. The genes and pathways identified in this study require further investigation for their underlying mechanisms and potential clinical significance in improving the OS of the glioma patients.",2020,
Concave selection in generalized linear models,"A family of concave penalties, including the smoothly clipped absolute deviation (SCAD) and minimax concave penalties (MCP), has been shown to have attractive properties in variable selection. The computation of concave penalized solutions, however, is a difficult task. We propose a majorization minimization by coordinate descent (MMCD) algorithm to compute the solutions of concave penalized generalized linear models (GLM). In contrast to the existing algorithms that uses local quadratic or local linear approximation of the penalty, the MMCD majorizes the negative log-likelihood by a quadratic loss, but does not use any approximation to the penalty. This strategy avoids the computation of scaling factors in iterative steps, hence improves the efficiency of coordinate descent. Under certain regularity conditions, we establish the theoretical convergence property of the MMCD algorithm. We implement this algorithm in a penalized logistic regression model using the SCAD and MCP penalties. Simulation studies and a data example demonstrate that the MMCD works sufficiently fast for the penalized logistic regression in high-dimensional settings where the number of covariates is much larger than the sample size. Grouping structure among predictors exists in many regression applications. We first propose an `2 grouped concave penalty to incorporate such group information in a regression model. The `2 grouped concave penalty performs group selection and includes group Lasso (Yuan and Lin (2006)) as a special case. An efficient algorithm is developed and its theoretical convergence property is established under certain",2012,
Total Variation Minimization for Compressed Sensing with â€œSmoothlyâ€ Varying Covariates,"The LASSO is a variable subset selection procedure in statistical linear regression based on sparsity promoting `1 penalization of the least-squares operator. In many applications, the design matrix has strongly correlated columns which are smoothly evolving with the column index. For such applications, the standard LASSO does not provide satisfactory solutions in practice because some incoherence is often needed for support recovery of sparse vectors. In this paper, we circumvent this problem by using a Total Variation penalty and obtain adaptive confidence intervals for the nonzero components of the signal. The relaxation parameter is calibrated by a new multiscale data acquisition scheme. This approach is illustrated by some simulations results for a source localization problem in a marine environment.",2016,2016 IEEE Intl Conference on Computational Science and Engineering (CSE) and IEEE Intl Conference on Embedded and Ubiquitous Computing (EUC) and 15th Intl Symposium on Distributed Computing and Applications for Business Engineering (DCABES)
Whiteout: Gaussian Adaptive Regularization Noise in Deep Neural Networks,"Noise injection is an off-the-shelf method to mitigate over-fitting in neural networks (NNs). The recent developments in Bernoulli noise injection as implemented in the dropout and shakeout procedures demonstrates the efficiency and feasibility of noise injection in regularizing deep NNs. We propose whiteout, a new regularization technique via injection of adaptive Gaussian noises into a deep NN. Whiteout offers three tuning parameters, offering flexibility during training of NNs. We show that whiteout is associated with a deterministic optimization objective function in the context of generalized linear models with a closed-form penalty term and includes lasso, ridge regression, adaptive lasso, and elastic net as special cases. We also demonstrate that whiteout can be viewed as robust learning of NN model in the presence of small and insignificant perturbations in input and hidden nodes. Compared to dropout, whiteout has better performance when training data of relatively small sizes with the sparsity introduced through the l1 regularization. Compared to shakeout, the penalized objective function in whiteout has better convergence behaviors and has a tighter bound for tail probabilities. We establish theoretically that the noiseperturbed empirical loss function with whiteout converges almost surely to the ideal loss function, and the estimates of NN parameters obtained from minimizing the former loss function are consistent with those obtained from minimizing the ideal loss function. Computationally, whiteout can be incorporated in the back-propagation algorithm and is computationally efficient. The superiority of whiteout over dropout and shakeout in training NNs in classification is demonstrated using the MNIST data. keywords: (adaptive) lasso; elastic net; regularization; robustness; consistency; backpropagation âˆ—Co-first authors â€ Corresponding author email: fang.liu.131@nd.ed 1 ar X iv :1 61 2. 01 49 0v 1 [ st at .M L ] 5 D ec 2 01 6",2016,
Targeting TWIST1 through loss of function inhibits tumorigenicity of human glioblastoma,"TWIST1 (TW) is a bHLH transcription factor (TF) and master regulator of the epithelial-to-mesenchymal transition (EMT). InÂ vitro, TW promotes mesenchymal change, invasion, and self-renewal in glioblastoma (GBM) cells. However, the potential therapeutic relevance of TW has not been established through loss-of-function studies in human GBM cell xenograft models. The effects of TW loss of function (gene editing and knockdown) on inhibition of tumorigenicity of U87MG and GBM4 glioma stem cells were tested in orthotopic xenograft models and conditional knockdown in established flank xenograft tumors. RNAseq and the analysis of tumors investigated putative TW-associated mechanisms. Multiple bioinformatic tools revealed significant alteration of ECM, membrane receptors, signaling transduction kinases, and cytoskeleton dynamics leading to identification of PI3K/AKT signaling. We experimentally show alteration of AKT activity and periostin (POSTN) expression inÂ vivo and/or inÂ vitro. For the first time, we show that effect of TW knockout inhibits AKT activity in U87MG cells inÂ vivo independent of PTEN mutation. The clinical relevance of TW and candidate mechanisms was established by analysis of the TCGA and ENCODE databases. TW expression was associated with decreased patient survival and LASSO regression analysis identified POSTN as one of top targets of TW in human GBM. While we previously demonstrated the role of TW in promoting EMT and invasion of glioma cells, these studies provide direct experimental evidence supporting protumorigenic role of TW independent of invasion inÂ vivo and the therapeutic relevance of targeting TW in human GBM. Further, the role of TW driving POSTN expression and AKT signaling suggests actionable targets, which could be leveraged to mitigate the oncogenic effects of TW in GBM.",2018,Molecular Oncology
Lad-lasso: Simulation Study of Robust Regression in High Dimensional Data,"The common issues in regression, there are a lot of cases in the condition number of predictor variables more than number of observations ( ) called high dimensional data. The classical problem always lies in this case, that is multicolinearity. It would be worse when the datasets subject to heavy-tailed errors or outliers that may appear in the responses and/or the predictors. As this reason, Wang et al in 2007 developed combined methods from Least Absolute Deviation (LAD) regression that is useful for robust regression, and also LASSO that is popular choice for shrinkage estimation and variable selection, becoming LAD-LASSO. Extensive simulation studies demonstrate satisfactory using LAD-LASSO in high dimensional datasets that lies outliers better than using LASSO. Keywords: high dimensional data, LAD-LASSO, robust regression",2015,
A Ranking Approach to Genomic Selection,"BACKGROUND
Genomic selection (GS) is a recent selective breeding method which uses predictive models based on whole-genome molecular markers. Until now, existing studies formulated GS as the problem of modeling an individual's breeding value for a particular trait of interest, i.e., as a regression problem. To assess predictive accuracy of the model, the Pearson correlation between observed and predicted trait values was used.


CONTRIBUTIONS
In this paper, we propose to formulate GS as the problem of ranking individuals according to their breeding value. Our proposed framework allows us to employ machine learning methods for ranking which had previously not been considered in the GS literature. To assess ranking accuracy of a model, we introduce a new measure originating from the information retrieval literature called normalized discounted cumulative gain (NDCG). NDCG rewards more strongly models which assign a high rank to individuals with high breeding value. Therefore, NDCG reflects a prerequisite objective in selective breeding: accurate selection of individuals with high breeding value.


RESULTS
We conducted a comparison of 10 existing regression methods and 3 new ranking methods on 6 datasets, consisting of 4 plant species and 25 traits. Our experimental results suggest that tree-based ensemble methods including McRank, Random Forests and Gradient Boosting Regression Trees achieve excellent ranking accuracy. RKHS regression and RankSVM also achieve good accuracy when used with an RBF kernel. Traditional regression methods such as Bayesian lasso, wBSR and BayesC were found less suitable for ranking. Pearson correlation was found to correlate poorly with NDCG. Our study suggests two important messages. First, ranking methods are a promising research direction in GS. Second, NDCG can be a useful evaluation measure for GS.",2015,PLoS ONE
Prediction of multiple infections after severe burn trauma: a prospective cohort study.,"OBJECTIVE
To develop predictive models for early triage of burn patients based on hypersusceptibility to repeated infections.


BACKGROUND
Infection remains a major cause of mortality and morbidity after severe trauma, demanding new strategies to combat infections. Models for infection prediction are lacking.


METHODS
Secondary analysis of 459 burn patients (â‰¥16 years old) with 20% or more total body surface area burns recruited from 6 US burn centers. We compared blood transcriptomes with a 180-hour cutoff on the injury-to-transcriptome interval of 47 patients (â‰¤1 infection episode) to those of 66 hypersusceptible patients [multiple (â‰¥2) infection episodes (MIE)]. We used LASSO regression to select biomarkers and multivariate logistic regression to built models, accuracy of which were assessed by area under receiver operating characteristic curve (AUROC) and cross-validation.


RESULTS
Three predictive models were developed using covariates of (1) clinical characteristics; (2) expression profiles of 14 genomic probes; (3) combining (1) and (2). The genomic and clinical models were highly predictive of MIE status [AUROCGenomic = 0.946 (95% CI: 0.906-0.986); AUROCClinical = 0.864 (CI: 0.794-0.933); AUROCGenomic/AUROCClinical P = 0.044]. Combined model has an increased AUROCCombined of 0.967 (CI: 0.940-0.993) compared with the individual models (AUROCCombined/AUROCClinical P = 0.0069). Hypersusceptible patients show early alterations in immune-related signaling pathways, epigenetic modulation, and chromatin remodeling.


CONCLUSIONS
Early triage of burn patients more susceptible to infections can be made using clinical characteristics and/or genomic signatures. Genomic signature suggests new insights into the pathophysiology of hypersusceptibility to infection may lead to novel potential therapeutic or prophylactic targets.",2015,Annals of surgery
Effect of genotype imputation on genome-enabled prediction of complex traits: an empirical study with mice data,"BackgroundGenotype imputation is an important tool for whole-genome prediction as it allows cost reduction of individual genotyping. However, benefits of genotype imputation have been evaluated mostly for linear additive genetic models. In this study we investigated the impact of employing imputed genotypes when using more elaborated models of phenotype prediction. Our hypothesis was that such models would be able to track genetic signals using the observed genotypes only, with no additional information to be gained from imputed genotypes.ResultsFor the present study, an outbred mice population containing 1,904 individuals and genotypes for 1,809 pre-selected markers was used. The effect of imputation was evaluated for a linear model (the Bayesian LASSO - BL) and for semi and non-parametric models (Reproducing Kernel Hilbert spaces regressions â€“ RKHS, and Bayesian Regularized Artificial Neural Networks â€“ BRANN, respectively). The RKHS method had the best predictive accuracy. Genotype imputation had a similar impact on the effectiveness of BL and RKHS. BRANN predictions were, apparently, more sensitive to imputation errors. In scenarios where the masking rates were 75% and 50%, the genotype imputation was not beneficial. However, genotype imputation incorporated information about important markers and improved predictive ability, especially for body mass index (BMI), when genotype information was sparse (90% masking), and for body weight (BW) when the reference sample for imputation was weakly related to the target population.ConclusionsIn conclusion, genotype imputation is not always helpful for phenotype prediction, and so it should be considered in a case-by-case basis. In summary, factors that can affect the usefulness of genotype imputation for prediction of yet-to-be observed traits are: the imputation accuracy itself, the structure of the population, the genetic architecture of the target trait and also the model used for phenotype prediction.",2014,BMC Genetics
Adaptive group LASSO selection in quantile models,"The paper considers a linear model with grouped explanatory variables. If the model errors are not with zero mean and bounded variance or if model contains outliers, then the least squares framework is not appropriate. Thus, the quantile regression is an interesting alternative. In order to automatically select the relevant variable groups, we propose and study here the adaptive group LASSO quantile estimator. We establish the sparsity and asymptotic normality of the proposed estimator in two cases: fixed number and divergent number of variable groups. Numerical study by Monte Carlo simulations confirms the theoretical results and illustrates the performance of the proposed estimator.",2016,Statistical Papers
Model-assisted inference for treatment effects using regularized calibrated estimation with high-dimensional data,"Consider the problem of estimating average treatment effects when a large number of covariates are used to adjust for possible confounding through outcome regression and propensity score models. The conventional approach of model building and fitting iteratively can be difficult to implement, depending on ad hoc choices of what variables are included. In addition, uncertainty from the iterative process of model selection is complicated and often ignored in subsequent inference about treatment effects. We develop new methods and theory to obtain not only doubly robust point estimators for average treatment effects, which remain consistent if either the propensity score model or the outcome regression model is correctly specified, but also model-assisted confidence intervals, which are valid when the propensity score model is correctly specified but the outcome regression model may be misspecified. With a linear outcome model, the confidence intervals are doubly robust, that is, being also valid when the outcome model is correctly specified but the propensity score model may be misspecified. Our methods involve regularized calibrated estimators with Lasso penalties, but carefully chosen loss functions, for fitting propensity score and outcome regression models. We provide high-dimensional analysis to establish the desired properties of our methods under comparable conditions to previous results, which give valid confidence intervals when both the propensity score and outcome regression are correctly specified. We present a simulation study and an empirical application which confirm the advantages of the proposed methods compared with related methods based on regularized maximum likelihood estimation.",2018,arXiv: Statistics Theory
Factors associated with performing tuberculosis screening of HIV-positive patients in Ghana: LASSO-based predictor selection in a large public health data set,"BackgroundThe purpose of this study is to propose the Least Absolute Shrinkage and Selection Operators procedure (LASSO) as an alternative to conventional variable selection models, as it allows for easy interpretation and handles multicollinearities. We developed a model on the basis of LASSO-selected parameters in order to link associated demographical, socio-economical, clinical and immunological factors to performing tuberculosis screening in HIV-positive patients in Ghana.MethodsApplying the LASSO method and multivariate logistic regression analysis on a large public health data set, we selected relevant predictors related to tuberculosis screening.ResultsOne Thousand Ninety Five patients infected with HIV were enrolled into this study with 691 (63.2Â %) of them having tuberculosis screening documented in their patient folders. Predictors found to be significantly associated with performance of tuberculosis screening can be classified into factors related to the clinicianâ€™s perception of the clinical state, as well as those related to PLHIVâ€™s awareness. These factors include newly diagnosed HIV infections (nâ€‰=â€‰354 (32.42Â %), aOR 1.84), current CD4+ T cell count (aOR 0.92), non-availability of HIV type (nâ€‰=â€‰787 (72.07Â %), aOR 0.56), chronic cough (nâ€‰=â€‰32 (2.93Â %), aOR 5.07), intake of co-trimoxazole (nâ€‰=â€‰271 (24.82Â %), aOR 2.31), vitamin supplementation (nâ€‰=â€‰220 (20.15Â %), aOR 2.64) as well as the use of mosquito bed nets (nâ€‰=â€‰613 (56.14Â %), aOR 1.53).ConclusionsAccelerated TB screening among newly diagnosed HIV-patients indicates that application of the WHO screening form for intensifying tuberculosis case finding among HIV-positive individuals in resource-limited settings is increasingly adopted. However, screening for TB in PLHIV is still impacted by clinicianâ€™s perception of patientâ€™s health state and PLHIVâ€™s health awareness. Education of staff, counselling of PLHIV and sufficient financing are needed for further improvement in implementation of TB screening for all PLHIV. The LASSO approach proved a convenient method for automatic variable selection in a large public health data set that requires efficient and fast algorithms.Trials registrationClinicalTrials.gov NCT01897909 (July 5, 2013).",2016,BMC Public Health
"On the sign recovery by LASSO, thresholded LASSO and thresholded Basis Pursuit Denoising","In the high-dimensional regression model Y = XÎ² + e, we provide new theoretical results on the probability of recovering the sign of Î² by the Least Absolute Selection and Shrinkage Operator (LASSO) and by the thresholded LASSO. It is well known that ""irrepresentability"" is a necessary condition for LASSO to recover the sign of Î² with a large probability. In this article we extend this result by providing a tight upper bound for the probability of LASSO sign recovery. This upper bound is smaller than 1/2 when the irrepresentable condition does not hold and thus generalizes Theorem 2 of Wainwright [27]. The bound depends on the tuning parameter Î» and is attained when non-null components of Î² tend to infinity; its value is equal to the limit of the probability that every null component of Î² is correctly estimated at 0. Consequently, this bound makes it possible to select Î» so as to control the probability of at least one false discovery. The ""irrepresentability"" is a stringent necessary condition to recover the sign of Î² by LASSO which can be substantially relaxed when LASSO estimates are additionally filtered out with an appropriately selected threshold. In this article we provide new theoretical results on thresholded LASSO and thresholded Basis Pursuit DeNoising (BPDN) in the asymptotic setup under which X is fixed and non-null components of Î² tend to infinity. Compared to the classical asymptotics, where X is a n Ã— p matrix and both n and p tend to +âˆž, our approach allows for reduction of the technical burden. Our main Theorem takes a simple form: When non-null components of Î² are sufficiently large, appropriately thresholded LASSO or thresholded BPDN can recover the sign of Î² if and only if Î² is identifiable with respect to the L1 norm, i.e. If XÎ³ = XÎ² and Î³ = Î² then Î³ 1 > Î² 1. To illustrate our results we present examples of irrepresentability and identifiability curves for some selected design matrices X. These curves provide the proportion of k sparse vectors Î² for which the irrep-* Corresponding author: tardivel@math.uni.wroc.pl 1 resentability and identifiability conditions hold. Our examples illustrate that ""irrepresentability"" is a much stronger condition than ""identifiability"", especially when the entries in each row of X are strongly correlated. Finally, we illustrate how the knockoff methodology [1, 8] can be used to select an appropriate threshold and that thresholded BPDN and LASSO can recover the sign of Î² with a larger probability than adaptive LASSO [32].",2019,arXiv: Methodology
A Study on Applying Shrinkage Method in Generalized Additive Model,"Generalized additive model(GAM) is the statistical model that resolves most of the problems existing in the traditional linear regression model. However, overfitting phenomenon can be aroused without applying any method to reduce the number of independent variables. Therefore, variable selection methods in generalized additive model are needed. Recently, Lasso related methods are popular for variable selection in regression analysis. In this research, we consider Group Lasso and Elastic net models for variable selection in GAM and propose an algorithm for finding solutions. We compare the proposed methods via Monte Carlo simulation and applying auto insurance data in the fiscal year 2005. lt is shown that the proposed methods result in the better performance.",2010,
StudentLife: Using Smartphones to Assess Mental Health and Academic Performance of College Students,"Much of the stress and strain of student life remains hidden. The StudentLife continuous sensing app assesses the day-to-day and week-by-week impact of workload on stress, sleep, activity, mood, sociability, mental well-being and academic performance of a single class of 48 students across a 10 weeks term at Dartmouth College using Android phones. Results from the StudentLife study show a number of significant correlations between the automatic objective sensor data from smartphones and mental health and educational outcomes of the student body. We propose a simple model based on linear regression with lasso regularization that can accurately predict cumulative GPA. We also identify a Dartmouth term lifecycle in the data that shows students start the term with high positive affect and conversation levels, low stress, and healthy sleep and daily activity patterns. As the term progresses and the workload increases, stress appreciably rises while positive affect, sleep, conversation and activity drops off. The StudentLife dataset is publicly available on the web.",2017,
Generalized Structured Component Analysis: A Component-Based Approach to Structural Equation Modeling,"Introduction Structural Equation Modeling Traditional Approaches to Structural Equation Modeling Why Generalized Structured Component Analysis? Generalized Structured Component Analysis Model Specification Estimation of Model Parameters Model Evaluation Example Other Related Component-Based Methods Appendix The Alternating Least-Squares Algorithm for Generalized Structured Component Analysis Appendix Extensions of the Least-Squares Criterion for Generalized Structured Component Analysis Appendix A Partial Least Squares Path Modeling Algorithm Basic Extensions of Generalized Structured Component Analysis Constrained Analysis Higher-Order Latent Variables Multiple Group Analysis Total and Indirect Effects Missing Data Summary Appendix The Imputation Phase of the Alternating Least-Squares Algorithm for Estimating Missing Observations Fuzzy Clusterwise Generalized Structured Component Analysis Fuzzy Clustering Fuzzy Clusterwise Generalized Structured Component Analysis Example: Clusterwise Latent Growth Curve Modeling of Alcohol Use among Adolescents Summary Appendix The Alternating Least-Squares Algorithm for Fuzzy Clusterwise Generalized Structured Component Analysis Appendix Regularized Fuzzy Clusterwise Generalized Structured Component Analysis Nonlinear Generalized Structured Component Analysis Introduction Nonlinear Generalized Structured Component Analysis Examples Summary Appendix Algorithms for Kruskal's (1964a, b) Least-Squares Monotonic Transformations Generalized Structured Component Analysis with Latent Interactions Generalized Structured Component Analysis with Latent Interactions Probing of Latent Interaction Effects Testing Latent Interaction Effects in Partial Least Squares Path Modeling Example Summary Appendix The Alternating Least-Squares Algorithm for Generalized Structured Component Analysis with Latent Interactions Multilevel Generalized Structured Component Analysis Model Specification Parameter Estimation Example: The ACSI Data Summary Appendix The Alternating Least-Squares Algorithm for Two-Level Multilevel Generalized Structured Component Analysis Appendix The Three-Level Generalized Structured Component Analysis Model Regularized Generalized Structured Component Analysis Ridge Regression Regularized Generalized Structured Component Analysis Example Summary Appendix The Alternating Regularized Least-Squares Algorithm for Regularized Generalized Structured Component Analysis Lasso Generalized Structured Component Analysis Lasso Regression Lasso Generalized Structured Component Analysis Example: The Company-Level ACSI Data Summary Appendix The Alternating Coordinate-Descent Algorithm for Lasso Generalized Structured Component Analysis Dynamic Generalized Structured Component Analysis Introduction The Method Examples of Application to Real Functional Neuroimaging Data Summary and Discussion Appendix Algorithm for Dynamic Generalized Structured Component Analysis Functional Generalized Structured Component Analysis Functional Generalized Structured Component Analysis A Related Method: Functional Extended Redundancy Analysis Examples Summary Appendix The Alternating Regularized Least-Squares Algorithm for Functional Generalized Structured Component Analysis References Index",2014,
