title,abstract,year,journal
"Seroprevalence and risk factors of Toxoplasma gondii infection in pregnant women from Bobo Dioulasso, Burkina Faso","BackgroundToxoplasmosis is one of the common worldwide parasitic zoonosis due to Toxoplasma gondii (T. gondii). Toxoplasmosis during pregnancy can result in fetal and neonatal death or various congenital defects. The aim of this study was to assess the seroprevalence and risk factors of T. gondii infection in pregnant women following antenatal care (ANC) services at Bobo Dioulasso.MethodsA cross-sectional study was conducted enrolling a sample of 316 pregnant women attending ANC at centers for maternal and child health of Bobo-Dioulasso town from March 2013 to February 2014. Data on socio-demographic and potential risk factors were collected from each study participant using structured questionnaire through face-to-face interview. Moreover, venous blood specimens were collected and tested for IgM and IgG anti-T. gondii antibodies by enzyme-linked immunosorbent assay and enzyme linked fluorescent assay, respectively. Multivariable logistic regression modeling was used to identify the potential predictor variables for T. gondii infection.ResultsThe overall seroprevalence for T. gondii infection was 31.1% (98/316). All the pregnant women were positive for IgG anti-bodies exclusively. Multivariable logistic regression analysis showed that having at least a secondary education level (AORÂ =Â 2.23; 95% CI: [1.04â€“4.63]); being urban resident (AORÂ =Â 2.81; 95% CI: [1.24â€“6.86]) and the consumption of meat combination (pork + beef + mutton + wild meat + poultry) (AORÂ =Â 4.00; 95% CI: [1.06â€“15.24]) were potential risk factors of T. gondii infection.ConclusionToxoplasmosis is frequent in pregnant women and studies that show incidence of T. gondii among the neonates have to be done to introduce routine antenatal screening program to control congenital toxoplasmosis. There is the need for preventive measures such as education of pregnant women about the transmission routes and prevention methods of toxoplasmosis at ANC clinics.",2017,BMC Infectious Diseases
"Country of origin and use of social benefits: A large, preregistered study of stereotype accuracy in Denmark","A nationally representative Danish sample was asked to estimate the percentage of persons aged 30-39 living in Denmark receiving social benefits for 70 countries of origin (N = 766). After extensive quality control procedures, a sample of 484 persons were available for analysis. Stereotypes were scored by accuracy by comparing the estimated values to values obtained from an official source. Individual stereotypes were found to be fairly accurate (median/mean correlation with criterion values = .48/.43), while the aggregate stereotype was found to be very accurate (r = .70). Both individual and aggregate-level stereotypes tended to underestimate the percentages of persons receiving social benefits and underestimate real group differences. 
 
In bivariate analysis, stereotype correlational accuracy was found to be predicted by a variety of predictors at above chance levels, including conservatism (r = .13), nationalism (r = .11), some immigration critical beliefs/preferences, agreement with a few political parties, educational attainment (r = .20), being male (d = .19) and cognitive ability (r = .22). Agreement with most political parties, experience with ghettos, age, and policy positions on immigrant questions had little or no predictive validity. 
 
In multivariate predictive analysis using LASSO regression, correlational accuracy was found to be predicted only by cognitive ability and educational attainment with even moderate level of reliability. In general, stereotype accuracy was not easy to predict, even using 24 predictors (k-fold cross-validated R2 = 4%). 
 
We examined whether stereotype accuracy was related to the proportion of Muslims in the groups. Stereotypes were found to be less accurate for the groups with higher proportions of Muslims in that participants underestimated the percentages of persons receiving social benefits (mean estimation error for Muslim groups relative to overall elevation error = -8.09 %points). 
 
The study was preregistered with most analyses being specified before data collection began.",2016,
Bayesian Perspectives on Sparse Empirical Bayes Analysis ( SEBA ),"We consider a joint processing of n independent similar sparse regression problems. Each is based on a sample (yi1, xi1) . . . , (yim, xim) of m i.i.d. observations from yi1 = xi1Î²i + Îµi1, yi1 âˆˆ R, xi1 âˆˆ R, and Îµi1 âˆ¼ N(0, Ïƒ), say. The dimension p is large enough so that the empirical risk minimizer is not feasible. We consider, from a Bayesian point of view, three possible extensions of the lasso. Each of the three estimators, the lassoes, the group lasso, and the RING lasso, utilizes different assumptions on the relation between the n vectors Î²1, . . . , Î²n. â€œ. . . and only a star or two set sparsedly in the vault of heaven; and you will find a sight as stimulating as the hoariest summit of the Alps.â€ R. L. Stevenson",2010,
Interval lasso regression based Extreme learning machine for nonlinear multivariate calibration of near infrared spectroscopic datasets,"As a nonlinear multivariate calibration method, extreme learning machine (ELM) has recently received increasing attention for its fast learning speed and excellent generalized performance. However, it is implemented normally under the empirical risk minimization scheme, and is prone to generate a large-scale and over-fitting model. Least absolute shrinkage and selection operator (LASSO) based ELM (LASSO-ELM) is a simple and efficient approach to avoid over-fitting and obtain an appropriate network structure. Unfortunately, when the initial hidden layer output matrix is in a high dimensional feature space, solving the LASSO problem remains a challenge. To improve the efficiency of solving high-dimension LASSO, we propose interval LASSO based ELM (iLASSO-ELM), which is generated by incorporating interval selection of hidden layer output matrix into original LASSO-ELM. The proposed model combines the coarse screening of interval selection and fine screening of LASSO. Thus, it can identify the relevant hidden nodes quickly and prevent over-fitting. A comparison of the proposed iLASSO-ELM with six other models, namely, ELM, partial least square based ELM (PLS-ELM), ridge regression based ELM (RR-ELM), elastic net based ELM (EN-ELM), LASSO-ELM and Least-Squares Support Vector Machines method (LS-SVM), was evaluated on four benchmark-near infrared (NIR) spectroscopic datasets. Additionally, the Wilcoxon signed rank test was used to statistically compare the predictive performance of the two competing calibration models. Experimental results show that iLASSO-ELM has the minimum root mean square errors of predictions and performs, at least statistically, not worse than other models.",2018,Analytical Methods
"A Comparison of Pretest, Stein-Type and Penalty Estimators in Logistic Regression Model","Various estimators are proposed based on the preliminary test and Stein-type strategies to estimate the parameters in a logistic regression model when it is priori suspected that some parameters may be restricted to a subspace. Two different penalty estimators as LASSO and ridge regression are also considered. A Monte Carlo simulation experiment was conducted for different combinations, and the performance of each estimator was evaluated in terms of simulated relative efficiency. The positive-part Stein-type shrinkage estimator is recommended for use since its performance is robust regardless of the reliability of the subspace information. The proposed estimators are applied to a real dataset to appraise their performance.",2017,
Interdependent effects of habitat quality and climate on population growth of an endangered plant,"Summary 1. To predict the viability of populations, it is essential to clarify how performance depends both on large-scale environmental changes, such as climate warming, and on the local habitat. However, in spite of their potential importance, effects of interactions between large-scale environmental changes and the local environment on population viability have rarely been examined. 2. We investigated how population dynamics of the endangered alpine plant Dracocephalum austriacum depend on local habitat quality and climatic variation, as well as how effects of climate depend on local habitat. We used lasso regression shrinkage and integral projection models to identify effects on vital rates and population growth rates in seven populations over seven annual transitions. 3. Populations on steeper slopes had lower survival and stochastic population growth rate than populations on more gentle slopes. In years with low spring temperatures and high summer temperatures, survival and population growth rate were lower. In addition, the negative effects of high summer temperatures did depend on local habitat quality, being more negative in populations on steeper slopes. 4. Combining the net positive effects of high spring temperature and the net negative effects of high summer temperature on plant vital rates with predicted climate change over the next 30 years suggested that effects on D. austriacum would be relatively small. 5. Synthesis. Our results show that different aspects of a warmer climate may have opposing effects on populations, and that climatic effects may depend on local habitat quality. Such interactive effects should be accounted for when determining effects of large-scale environmental changes on population and community dynamics.",2011,Journal of Ecology
Network-based investigation of genetic modules associated with functional brain networks in schizophrenia,"We developed a new sparse multivariate regression method, collaborative sparse reduced rank regression(C-sRRR) for detecting genetic networks associated with brain functional networks in schizophrenia (SZ). Our study: 1) introduced both genetic and brain network structure to group single nucleotide polymorphism (SNP) and voxels simultaneously for utilizing the interacting effects implied in both features; 2) used collaborative sparse group lasso to perform genetic variants selection and nuclear norm penalty to address the interrelationship among voxels; 3) developed an efficient algorithm for solving the non-smooth optimization. In real data analysis, we constructed 8605 genetic sub-networks (modules) from 722177 SNPs with a median module size of 9. A functional brain network was extracted which also showed significant discriminative characteristics between SZ and healthy controls. A sub sampling strategy was applied to identify 57 highly ranked genes from 14 high-ranking modules. 14 of them are SZ susceptibility genes and 6 genes were consistent with the findings in previous study.",2013,2013 IEEE International Conference on Bioinformatics and Biomedicine
Utility of radiomics based on contrast-enhanced CT and clinical data in the differentiation of benign and malignant gallbladder polypoid lesions,"To develop and validate a novel method based on radiomics for the preoperative differentiation of benign and malignant gallbladder polypoid lesions (PLG). A total of 145 patients with pathological proven gallbladder polypoid lesionsâ€‰â‰¥â€‰1 cm were included in this retrospective study. All the patients underwent abdominal contrast-enhanced computed tomography (CT) examinations 3 weeks before cholecystectomy from January 2013 to January 2019. Seventy percent of the cases were randomly selected for the training dataset, and 30% of the cases were independently used for testing. Radiomics features extracted from portal venous-phase CT of the PLG and clinical features were analyzed, and the LASSO regression algorithm was used for data dimension reduction. Multivariable logistic regression was used to generate radiomics signatures, clinical signatures, and combination signatures. The receiver operating characteristic (ROC) curve and decision curve were plotted to assess the differentiating performance of the three signatures. The area under the ROC curve (AUC) of the radiomics signature and clinical signature was 0.924 and 0.861 in the testing dataset, respectively. For the radiomics signature, the accuracy was 88.6%, with 88.0% specificity and 89.5% sensitivity. When combined, the AUC was 0.931, the specificity was 84.0%, and the sensitivity was 89.5%. The differences between the AUC values of the two sole models and the combination model were statistically nonsignificant. Radiomics based on CT images can be helpful to differentiate benign and malignant gallbladder polypsâ€‰â‰¥â€‰1 cm in size.",2020,Abdominal Radiology
Contribution Ã  la sÃ©lection de modÃ¨le via pÃ©nalisation Lasso en Ã‰pidÃ©miologie. (Contribution to model selection via Lasso penalization in Epidemiology),"Mes travaux portent principalement sur le developpement, lâ€™adaptation, lâ€™implementation et lâ€™application de methodes statistiques de selection de modele. Ma principale contribution consiste a adapter des methodes de l'apprentissage statistique supervise qui sont devenues tres populaires lors de la derniere decennie, les regressions penalisees de type Lasso, a l'analyse de donnees issues d'etudes epidemiologiques. L'enjeu est de s'attaquer aux problemes des donnees volumineuses (\textit{Big Data}) tout en respectant les objectifs et specificites de la discipline. Le volume important se refere ici au fait que le nombre d'observations et/ou le nombre de variables est bien plus important que celui qui etait classique dans le domaine, sans exclure le cas ou le nombre de variables est superieur au nombre d'observations (donnees de grande dimension). 
 
Le contexte de la pratique epidemiologique est en plein changement avec les evolutions technologiques et la consequente disponibilite croissante des Big Data. Le Systeme National des Donnees de Sante (SNDS), regroupant les principales bases de donnees de sante publique existantes en France, constitue un exemple de Big Data en sante. Le donnees ``omiques'' (genomiques, transcriptomiques, proteomiques, metabolomiques, microbiomiques, mycobiomiques, viromiques,$\ldots$) issues des avancees des techniques de sequencage a haut debit constituent un autre exemple de Big Data en sante. Enfin, les mesures de l'\textit{exposome} (par opposition aux facteurs genetiques), qui designe en epidemiologie lâ€™ensemble des expositions environnementales que subit un individu au long de sa vie peut egalement constituer une source de Big Data. 
 
Ce document s'articule autour de trois chapitres. Il resume mon activite de recherche depuis 2005, soit depuis mon recrutement a lâ€™Universite de Bordeaux apres ma these. 
Le premier chapitre est une introduction generale dans laquelle je contextualise, motive et enonce la problematique abordee tout au long de mes recherches. Le deuxieme chapitre est consacre a mes travaux en lien avec les etudes sur les traumatismes accidentels et expositions medicamenteuses a partir des donnees du SNDS. Le troisieme chapitre est consacre a mes travaux en lien avec des etudes biomedicales: la prediction de la charge virale censuree par un seuil de detection a partir des mutations du VIH, d'une part, et l'automatisation de la detection des seuils d'anomalie des hemogrammes en population generale, d'autre part.",2018,
Variable selection for models with missing data,"RAMON ISRAEL GARCIA: Variable Selection for Models with Missing Data. (Under the direction of Joseph G. Ibrahim and Hongtu Zhu.) This dissertation is composed of three papers which address the problem of variable selection for models with missing data. In the first paper, we consider variable selection for generalized linear models with missing data, including missing covariate and/or response data. The second paper deals with variable selection in the Cox regression model with covariates missing at random. For the third paper, we consider jointly selecting fixed and random effects in mixed effect models. In all three papers, we calculate the maximum penalized likelihood estimates using the smoothly clipped absolute deviation (SCAD) and adaptive LASSO (ALASSO) penalty functions and propose a unified model selection and estimation procedure for use in the presence of missing data. The maximum penalized likelihood estimates are shown to posses consistency and sparsity properties and are asymptotically normal. A computationally attractive algorithm is developed which simultaneously optimizes the penalized likelihood function and penalty parameters. Particularly, we propose to use a model selection criterion, called the ICQ criterion, for selecting the penalty parameters. We show that the variable selection procedure based on ICQ consistently selects important covariates and/or fixed and random effects. The methodology is very general and can be applied to numerous situations involving missing data, from covariates missing at random in arbitrary regression models to nonignorably missing longitudinal responses and/or covariates to mixed effects models.",2009,
Development Of Proxy Models For Reservoir Simulation By Sparsity Promoting Methods And Machine Learning Techniques,"Learning from data has been a rich topic of research in many engineering disciplines. In particular, in reservoir engineering, data-driven methodologies have been applied successfully to infer interwell connections and flow patterns in the subsurface and in assisting field development plans, including, history matching and performance prediction phases, of conventional and unconventional reservoirs. Although real-time data acquisition and analysis are becoming routine in many workflows, there is still a disconnect with the traditional theoretical first laws principles, whereby conservation laws and phenomenological behavior are used to derive the underlying spatio-temporal evolution equations. 
In this work, we propose to combine sparsity promoting methods and machine learning techniques to find the governing equation from the spatio-temporal data series from a reservoir simulator. The idea is to connect data with the physical interpretation of the dynamical system. We achieve this by identifying the nonlinear ODE system equations of our discretized reservoir system. The solution is assumed sparse because we know there is only few terms are relevant for each governing equation. The sparse structure is invoked by two methods: sparse regression with hard threshold (SINDy) and sparse regression with soft threshold (LASSO). For each method to work properly without overfitting, unique ways have been developed for seeking a balance between accuracy and complexity of the model with either l1 or l2 norm penalty. In addition, the sparsity structure can be further fixed with the physical fact that flow term is only related with its adjacent cells. 
We apply the method to a two-dimensional single phase flow system. First, the time series data is generated from the simulator with recording points equally spread in space. Then a large library is built containing possible linear, nonlinear terms of the governing ODE equation and finally the combination of the terms is identified through a coefficient vector for each equation. Difference in each technique and detailed modification to the threshold tolerance and penalty factor will be discussed and compared. Extensions to the two-phase flow case is also underway and promising initial results will also be shown in this paper. The validation process is achieved by comparing the original single/two phase simulator results and the results solved from the identified ODE system by Newton iteration.",2018,
On LARS/Homotopy Equivalence Conditions for Over-Determined LASSO,"We revisit the positive cone condition given by Efron for the over-determined least absolute shrinkage and selection operator (LASSO). It is a sufficient condition ensuring that the number of nonzero entries in the solution vector keeps increasing when the penalty parameter decreases, based on which the least angle regression (LARS) and homotopy algorithms yield the same iterates. We show that the positive cone condition is equivalent to the diagonal dominance of the Gram matrix inverse, leading to a simpler way to check the positive cone condition in practice. Moreover, we elaborate on a connection between the positive cone condition and the mutual coherence condition given by Donoho and Tsaig , ensuring the exact recovery of any k -sparse representation using both LARS and homotopy.",2012,IEEE Signal Processing Letters
Double-Selection based High-Dimensional Factor Model with Application in Asset Pricing,"This paper proposes a principal component analysis (PCA) approach after a double-selection Lasso and applies it to both Chinese and US stock market data. Similar to the idea of Post-Lasso, we perform least squares regression on the principal component factors. To accommodate the nonlinear nature of the data, this paper compares the support vector regression (SVR) model with least squares regression model. Empirical results show that the SVR method can improve the prediction ability, as evidenced by the superior accumulated rate of return using the test set sample of both markets.",2019,2019 IEEE Global Conference on Signal and Information Processing (GlobalSIP)
Bootstrapping promotes the RSFC-behavior associations: an application of individual cognitive traits prediction,"Resting state functional connectivity records enormous functional interaction information between any pair of brain nodes, which enriches the prediction of individual phenotypes. To reduce the high dimensional features in prediction, correlation analysis is a common way for feature selection. However, rs-fMRI signal exhibits typically low signal-to-noise ratio and correlation analysis is sensitive to outliers and data distribution, which may bring unstable and uninformative features to subsequent prediction. To alleviate this problem, a bootstrapping-based feature selection framework was proposed and applied on three widely used regression models: connectome-based predictive model (CPM), support vector regression (SVR) and least absolute shrinkage and selection operator (LASSO). A large open-source dataset from Human Connectome Project (HCP) was adopted in the study and a series of cognitive traits were acted as the prediction targets. To systematically investigate the influences of different parameter settings on the bootstrapping-based framework, a total of 216 parameter combinations were evaluated through the R value between the predicted and real cognitive traits, and the best identified performance among them was chosen out as the final prediction accuracy for each cognitive trait. By using bootstrapping without replacement, the best performances of CPM with positive and negative feature sets, SVR and LASSO averagely increased by 28.0%, 33.2%, 11.6% and 24.3% in R values in contrast to the baseline method without bootstrapping. By using bootstrapping with replacement, these best performances increased by 22.1%, 22.9%, 9.4% and 19.6%. Furthermore, the bootstrapping-based feature selection methods could effectively refine the original feature sets obtained from correlation analysis, which thus retained the more stable and informative feature sets. The results demonstrate that bootstrapping-based feature selection is an easy-to-use and effective method to improve RSFC prediction of cognitive traits and is highly recommended in future RSFC prediction studies.",2019,bioRxiv
The Construction of Risk Prediction Models Using GWAS Data and Its Application to a Type 2 Diabetes Prospective Cohort,"Recent genome-wide association studies (GWAS) have identified several novel single nucleotide polymorphisms (SNPs) associated with type 2 diabetes (T2D). Various models using clinical and/or genetic risk factors have been developed for T2D risk prediction. However, analysis considering algorithms for genetic risk factor detection and regression methods for model construction in combination with interactions of risk factors has not been investigated. Here, using genotype data of 7,360 Japanese individuals, we investigated risk prediction models, considering the algorithms, regression methods and interactions. The best model identified was based on a Bayes factor approach and the lasso method. Using nine SNPs and clinical factors, this method achieved an area under a receiver operating characteristic curve (AUC) of 0.8057 on an independent test set. With the addition of a pair of interaction factors, the model was further improved (p-value 0.0011, AUC 0.8085). Application of our model to prospective cohort data showed significantly better outcome in disease-free survival, according to the log-rank trend test comparing Kaplan-Meier survival curves (p--value 2:09 x 10(-11)). While the major contribution was from clinical factors rather than the genetic factors, consideration of genetic risk factors contributed to an observable, though small, increase in predictive ability. This is the first report to apply risk prediction models constructed from GWAS data to a T2D prospective cohort. Our study shows our model to be effective in prospective prediction and has the potential to contribute to practical clinical use in T2D.",2014,PLoS ONE
Clinical and Hematologic Impact of Fetal and Perinatal Variables on Mutant GATA1 Clone Size in Neonates with Down Syndrome,"Children with Down syndrome (DS; trisomy 21) have an increased risk of acute myeloid leukemia (ML-DS) in the first 5 years of life. In most cases ML-DS is preceded by Transient Abnormal Myelopoiesis (TAM), a fetal/neonatal pre-leukemic disorder unique to DS which regresses after birth. Both TAM and ML-DS harbor acquired N-terminal mutations in the hematopoietic transcription factor gene GATA1 . In a prospective study of 200 DS neonates, we recently showed that 29% had acquired GATA1 mutations including 17/200 (8.5%) with clinical or hematologic evidence of TAM; the remaining 20.5% were clinically and hematologically 9silent9, with smaller mutant GATA1 clones and lower blast frequency compared to overt TAM. The reasons why some DS neonates develop overt TAM and the factors which determine mutant GATA1 clone size are unknown. To address this, we analysed data from neonates in the prospective Oxford-Imperial DS Cohort Study and investigated the impact of 30 clinical and hematologic factors on clone size using statistical and mathematical modelling. Mutant GATA1 clones were determined in 54 neonates by targeted next generation sequencing of GATA1 exon 2 (mutation detection limit 0.3%). Clone size was determined by analysing original unprocessed reads using less stringent filtering parameters and counting reads containing mutated v total sequence. Correlation analysis identified 4 hematologic variables correlated with mutant GATA1 clone size: circulating nucleated red cells (r=+0.5003; p=0.0001), platelets (r=+0.436; p=0.001), total leukocytes (r=+0.7094; p 150x10 9 /L (p=0.019). Numbers of neutrophils, monocytes, basophils, eosinophils and lymphocytes did not correlate with GATA1 clone size. Clinical variables significantly correlated with clone size were hepatomegaly (p=0.0016), splenomegaly (p=0.0001) and rash (0.0174). The only pregnancy-related variables affecting mutant GATA1 clone size were intrauterine growth restriction and maternal diabetes (p=0.0156). Linear regression to determine the joint impact of all 30 variables on clone size (r2=0.88) followed by Lasso penalization identified the same 4 hematologic variables (nucleated red cells, platelets, total leukocytes and % blasts); Lasso penalized regression with these 4 variables gave a coefficient of determination of 0.63. Together these data suggest that chronic intrauterine hypoxia may affect expansion/differentiation of mutant GATA1 clones in DS. Consistent with this, nucleated red cells from 3 neonates with TAM all harbored GATA1 mutations identical to those in total circulating nucleated cells. Since neither perinatal infection nor gestational age at birth correlated with mutant GATA1 clone size, infection-related cytokines and the timing of acquisition of a mutant GATA1 clone during fetal development may not play a major role in determining clone size. Finally, a hierarchical model to investigate the impact of GATA1 mutation on hematopoietic stem and progenitor (HSPC) differentiation in DS neonates using a Bayesian approach also predicted increased erythroid cell output from GATA1 mutated HSPC v HSPC without a GATA1 mutation. In conclusion, in neonates with DS the size of the mutant GATA1 clone correlates with the presence of clinical signs of hepatomegaly, splenomegaly and skin rash; mutant GATA1 clone size correlates with the numbers of circulating nucleated red cells, platelets and blast cells suggesting that GATA1 mutant HSPC retain the ability to differentiate down the erythroid and megakaryocyte lineage; intrauterine hypoxia may be one of the factors driving expansion and/or maturation of the GATA1 mutant clone during fetal life in DS. Disclosures No relevant conflicts of interest to declare.",2014,Blood
Novel body fat estimation using machine learning and 3-dimensional optical imaging,"Estimates of body composition have been derived using 3-dimensional optical imaging (3DO), but no equations to date have been calibrated using a 4-component (4C) model criterion. This investigation reports the development of a novel body fat prediction formula using anthropometric data from 3DO imaging and a 4C model. Anthropometric characteristics and body composition of 179 participants were measured via 3DO (Size StreamÂ® SS20) and a 4C model. Machine learning was used to identify significant anthropometric predictors of body fat (BF%), and stepwise/lasso regression analyses were employed to develop new 3DO-derived BF% prediction equations. The combined equation was externally cross-validated using paired 3DO and DXA assessments (nâ€‰=â€‰158), producing a R2 value of 0.78 and a constant error of (Xâ€‰Â±â€‰SD) 0.8â€‰Â±â€‰4.5%. 3DO BF% estimates demonstrated equivalence with DXA based on equivalence testing with no proportional bias in the Blandâ€“Altman analysis. Machine learning methods may hold potential for enhancing 3DO-derived BF% estimates.",2020,European Journal of Clinical Nutrition
Linear models in genomic studies,"Abstract With the help of molecular markers, genome-wide association studies (GWAS) are conducted to identify genes associated with diseases. Association mapping uses unrelated individuals from the same population that has undergone recombination in many generations since the inception of the mutant gene and is the basis for detection of causal genes. The data that forms the basis for computational detection of causal genes are of three kinds, phenotypic values (single trait or several traits), genotypes of hundreds of thousands of SNP markers, and data on gene expression, a sort of intermediate phenotypes that are used to associate genes with disease phenotypes. Most of the studies except a few, however, consider single trait at a time and take either phenotypes and marker genotypes only or considers phenotypes, genotypes and gene expression all together. In actual situations, on the other hand, the problem is multivariate since many complex disease syndromes consist of a large number of highly related clinical or molecular phenotypes. For instance, asthma is influenced by as many as 53 clinical traits that can be represented as a quantitative trait network (QTN). The methodological issue is then to conduct association analysis that takes into account jointly all the relevant traits instead of a single trait only. Linear models in which a dependent variable (expression of a disease trait) is related to a set of independent variables (for instance, SNPs) provide with a very versatile tool that can be used for the association analysis both for a single as well as multiple traits. To this end, we systematically discuss the sparse regression methodology of Ridge Regression, Lasso, and GFLasso with illustrations from published literature.",2014,Current Medicine Research and Practice
Development and Validation of an Empiric Tool to Predict Favorable Neurologic Outcomes Among PICU Patients*,"Objectives: To create a novel tool to predict favorable neurologic outcomes during ICU stay among children with critical illness. Design: Logistic regression models using adaptive lasso methodology were used to identify independent factors associated with favorable neurologic outcomes. A mixed effects logistic regression model was used to create the final prediction model including all predictors selected from the lasso model. Model validation was performed using a 10-fold internal cross-validation approach. Setting: Virtual Pediatric Systems (VPS, LLC, Los Angeles, CA) database. Patients: Patients less than 18 years old admitted to one of the participating ICUs in the Virtual Pediatric Systems database were included (2009â€“2015). Interventions: None. Measurements and Main Results: A total of 160,570 patients from 90 hospitals qualified for inclusion. Of these, 1,675 patients (1.04%) were associated with a decline in Pediatric Cerebral Performance Category scale by at least 2 between ICU admission and ICU discharge (unfavorable neurologic outcome). The independent factors associated with unfavorable neurologic outcome included higher weight at ICU admission, higher Pediatric Index of Morality-2 score at ICU admission, cardiac arrest, stroke, seizures, head/nonhead trauma, use of conventional mechanical ventilation and high-frequency oscillatory ventilation, prolonged hospital length of ICU stay, and prolonged use of mechanical ventilation. The presence of chromosomal anomaly, cardiac surgery, and utilization of nitric oxide were associated with favorable neurologic outcome. The final online prediction tool can be accessed at https://soipredictiontool.shinyapps.io/GNOScore/. Our model predicted 139,688 patients with favorable neurologic outcomes in an internal validation sample when the observed number of patients with favorable neurologic outcomes was among 139,591 patients. The area under the receiver operating curve for the validation model was 0.90. Conclusions: This proposed prediction tool encompasses 20 risk factors into one probability to predict favorable neurologic outcome during ICU stay among children with critical illness. Future studies should seek external validation and improved discrimination of this prediction tool.",2018,Critical Care Medicine
Deep learning applied to glacier evolution modelling,"We present a parameterized glacier evolution model, with a :::: novel :::::::: approach ::: to ::::::: simulate :::: and :::::::::: reconstruct :::::: annual :::::::::: glacier-wide : surface mass balance (SMB) component :::: series : based on a deep artificial neural network (i.e. deep learning). :::: This :::::: method :::: has :::: been :::::::: included ::: as ::: the ::::: SMB ::::::::: component ::: of ::: an :::::::::: open-source :::::::: regional :::::: glacier :::::::: evolution :::::: model. : While most glacier models tend to incorporate more and more physical processes, here we take an alternative approach by creating a parameterized model based on data science. Annual glacier-wide SMBs can be simulated :::: from ::::::::::: topo-climatic ::::::::: predictors using 5 either deep learning or Lasso (regularized multilinear regression), whereas the glacier geometry is updated using a glacierspecific parameterization. We compare and cross-validate our nonlinear deep learning SMB model against other standard linear statistical methods on a dataset of 32 French alpine glaciers. Deep learning is found to outperform linear methods, with improved explained variance (up to +64% in space and +108% in time) and accuracy (up to +47% in space and +58% in time), resulting in an estimated r of 0.77 and RMSE of 0.51 m.w.e. Substantial nonlinear structures are captured by deep learning, 10 with around 35% of nonlinear behaviour in the temporal dimension. For the glacier geometry evolution, the main uncertainties come from the ice thickness data used to initialize the model. These results should encourage the use of deep learning in glacier modelling as a powerful nonlinear tool, capable of capturing the nonlinearities of the climate and glacier systems, that can serve to reconstruct or simulate SMB time series for individual glaciers at regional scale :: in : a :::::: whole ::::: region : for past and future climates. 15",2019,
Stigma as a barrier to health care utilization among female sex workers and men who have sex with men in Burkina Faso.,"PURPOSE
The aim of this study is to examine the prevalence and correlates of perceived health care stigma among female sex workers (FSWs) and men who have sex with men (MSM), including other stigma types, suicidal ideation, and participation in social activities.


METHODS
FSWs (NÂ = 350) and MSM (NÂ = 330) aged â‰¥18 were recruited in Bobo-Dioulasso, Burkina Faso. Perceived health care stigma was defined as either ever being afraid of or avoiding health care services because someone might find out the participant has sex with men (for MSM) or sells sex (for FSW). Correlates of perceived health care stigma were examined using multivariable logistic regression.


RESULTS
The prevalence of perceived health care stigma was 14.9% (52/350) and 24.5% (81/330) in FSWs and MSM, respectively. Among FSWs, experienced or social stigma, including verbal harassment (adjusted odds ratio [aOR]Â = 3.59, 95% confidence interval [CI] 1.48-8.71), feeling rejected by friends (aORÂ = 2.30, 95% CI 1.14-4.64), and feeling police refused to protect them (aORÂ = 2.58, 95% CI 1.27-5.25), was associated with perceived health care stigma. Among MSM, experiencing verbal harassment (aORÂ =Â 1.95, 95% CI 1.09-3.50) and feeling scared to walk in public (aORÂ = 2.93, 95% CI 1.47-5.86) were associated with perceived health care stigma.


CONCLUSIONS
In these key populations, perceived health care stigma was prevalent and associated with experienced and social stigmas. To increase coverage of effective HIV services, interventions should incorporate approaches to comprehensively mitigate stigma.",2018,Annals of epidemiology
Comparison of different variable selection methods for partial least squares soft sensor development,"Data-driven soft sensors have been widely used in both academic research and industrial applications for predicting hard-to-measure variables or replacing physical sensors to reduce cost. It has been shown that the performance of these data-driven soft sensors can be greatly improved by selecting only the vital variables that strongly affect the primary variables, rather than using all the available process variables. In this work, a comprehensive evaluation of different variable selection methods for soft sensor development is presented. The following seven variable selection methods are considered: stepwise regression (SR), partial least squares with regression coefficients (PLS-BETA), PLS with variable importance in projection (PLS-VIP), uninformative variable elimination with PLS (UVE-PLS), genetic algorithm with PLS (GA-PLS), least absolute shrinkage and selection operator (Lasso), and competitive adaptive reweighted sampling with PLS (CARS-PLS). Their strengths and limitations for soft sensor development are examined using a simulated case study and an industrial case study. Independent tuning datasets are used to optimize each method and to analyze the sensitivity of each method to its tuning parameters. Then independent test datasets are used to compare the prediction performances of PLS soft sensors developed based on different variable selection methods.",2014,2014 American Control Conference
Methylation Biomarker Panel Performance in EsophaCap Cytology Samples for Diagnosing Barrett's Esophagus: A Prospective Validation Study.,"PURPOSE
Barrett's esophagus is the only known precursor of esophageal adenocarcinoma (EAC). Although endoscopy and biopsy are standard methods for Barrett's esophagus diagnosis, their high cost and risk limit their use as a screening modality. Here, we sought to develop a Barrett's esophagus detection method based on methylation status in cytology samples captured by EsophaCap using a streamlined sensitive technique, methylation on beads (MOB).


EXPERIMENTAL DESIGN
We conducted a prospective cohort study on 80 patients (52 in the training set; 28 in the test set). We used MOB to extract and bisulfite-convert DNA, followed by quantitative methylation-specific PCR to assess methylation levels of 8 previously selected candidate markers. Lasso regression was applied to establish a prediction model in the training set, which was then tested on the independent test set.


RESULTS
In the training set, five of eight candidate methylation biomarkers (p16, HPP1, NELL1, TAC1, and AKAP12) were significantly higher in Barrett's esophagus patients than in controls. We built a four-biomarker-plus-age lasso regression model for Barrett's esophagus diagnosis. The AUC was 0.894, with sensitivity 94.4% [95% confidence interval (CI), 71%-99%] and specificity 62.2% (95% CI, 44.6%-77.3%) in the training set. This model also performed with high accuracy for Barrett's esophagus diagnosis in an independent test set: AUC = 0.929 (P < 0.001; 95% CI, 0.810%-1%), with sensitivity=78.6% (95% CI, 48.8%-94.3%) and specificity = 92.8% (95% CI, 64.1%-99.6%).


CONCLUSIONS
EsophaCap, in combination with an epigenetic biomarker panel and the MOB method, is a promising, well-tolerated, low-cost esophageal sampling strategy for Barrett's esophagus diagnosis. This approach merits further prospective studies in larger populations.",2019,Clinical cancer research : an official journal of the American Association for Cancer Research
College of Arts and Sciences Theories on Group Variable Selection in Multivariate Regression Models Table of Contents,"We study group variable selection on multivariate regression model. Group variable selection is selecting the non-zero rows of coefficient matrix, since there are multiple response variables and thus if one predictor is irrelevant to estimation then the corresponding row must be zero. In a high dimensional setup, shrinkage estimation methods are applicable and guarantee smaller MSE than OLS according to James-Stein phenomenon (1961). As one of shrinkage methods, we study penalized least square estimation for a group variable selection. Among them, we study L0 regularization and L0 + L2 regularization with the purpose of obtaining accurate prediction and consistent feature selection, and use the corresponding computational procedure Hard TISP and Hard-Ridge TISP (She, 2009) to solve the numerical difficulties. These regularization methods show better performance both on prediction and selection than Lasso (L1 regularization), which is one of popular penalized least square method. L0 acheives the same optimal rate of prediction loss and estimation loss as Lasso, but it requires no restriction on design matrix or sparsity for controlling the prediction error and a relaxed condition than Lasso for controlling the estimation error. Also, for selection consistency, it requires much relaxed incoherence condition, which is correlation between the relevant subset and irrelevant subset of predictors. Therefore L0 can work better than Lasso both on prediction and sparsity recovery, in practical cases such that collinearity is high or sparsity is not low. We study another method, L0 + L2 regularization which uses the combined penalty of L0 and L2. For the corresponding procedure Hard-Ridge TISP, two parameter work independently for selection and shrinkage (to enhance prediction) respectively, and therefore it gives better performance on some cases (such as low signal strength) than L0 regularization. For L0 regularization, Î» works for selection but it is tuned in terms of prediction accuracy. L0 + L2 regularization gives the optimal rate of prediction and estimation errors without any restriction, when the coefficient of l2 penalty is appropriately assigned. Furthermore, it can achieve a better rate of estimation error with an ideal choice of block-wise weight to l2 penalty.",2013,
Sparse conditional logistic regression for analyzing large-scale matched data from epidemiological studies: a simple algorithm,"This paper considers the problem of estimation and variable selection for large high-dimensional data (high number of predictors p and large sample size N, without excluding the possibility that N < p) resulting from an individually matched case-control study. We develop a simple algorithm for the adaptation of the Lasso and related methods to the conditional logistic regression model. Our proposal relies on the simplification of the calculations involved in the likelihood function. Then, the proposed algorithm iteratively solves reweighted Lasso problems using cyclical coordinate descent, computed along a regularization path. This method can handle large problems and deal with sparse features efficiently. We discuss benefits and drawbacks with respect to the existing available implementations. We also illustrate the interest and use of these techniques on a pharmacoepidemiological study of medication use and traffic safety.",2015,BMC Bioinformatics
Fast Bayesian Lasso for High-Dimensional Regression,"The lasso (Tibshirani, 1996) is an essential tool in modern high-dimensional regression and variable selection. The Bayesian lasso of Park and Casella (2008) interprets the lasso objective function as a posterior under a Laplace prior and proposes a three-step Gibbs sampler to sample from this posterior. The Bayesian lasso yields a natural approach to quantifying the uncertainty of lasso estimates. Furthermore, the Gibbs sampler for the Bayesian lasso has been shown to be geometrically ergodic (Khare and Hobert, 2013). The geometric rate constant of this Markov chain, however, tends to 1 if the number of regression coefficients grows faster than the sample size (Rajaratnam and Sparks, 2015). Thus, convergence of the Bayesian lasso Gibbs sampler can still be quite slow in modern high-dimensional settings despite the apparent theoretical safeguard of geometric ergodicity. In order to address this challenge, we propose a new method to draw from the same posterior via a tractable two-step blocked Gibbs sampler, which we call the fast Bayesian lasso. We provide a theoretical underpinning to the new method by proving rigorously that the fast Bayesian lasso is geometrically ergodic. We then demonstrate numerically that this blocked sampler exhibits vastly superior convergence behavior in high-dimensional regimes.",2015,arXiv: Methodology
Bayesian Randomized Response Technique,"When sensitive attributes are investigated, Randomized Response Technique (RRT) is a popular approach to reduce the bias arisen from untruthful response. Nonetheless, traditional RRT has weakness that it mainly focuses on estimating the moments of univariate random variables but not dependence among multiple random variables. This paper is to introduce a new method to estimate the covariance matrix of random vectors under the framework of RRT. Modified Cholesky decomposition is applied to reparameterize the covariance matrix so that the parameters of the covariance matrix can be expressed as regression on a row-by-row basis. This simplifies the structure of the matrix and ensures the positive definiteness of the estimator. To keep inference nonparametric, moment equations of the randomized realizations are adopted as the quasi-likelihood. Moreover, Bayesian lasso is applied to impose shrinkage effect in estimation. This helps reduce estimation error when the covariance matrix is sparse. An easy-to-implement Gibbs sampling scheme is proposed for the inference. A simulation study is conducted to evaluate the accuracy of estimation. An empirical study related to software piracy behavior is conducted to compare the difference of the estimates under randomized and non-randomized settings.",2012,
Bayesian variable selection and estimation in maximum entropy quantile regression,"ABSTRACT Quantile regression has gained increasing popularity as it provides richer information than the regular mean regression, and variable selection plays an important role in the quantile regression model building process, as it improves the prediction accuracy by choosing an appropriate subset of regression predictors. Unlike the traditional quantile regression, we consider the quantile as an unknown parameter and estimate it jointly with other regression coefficients. In particular, we adopt the Bayesian adaptive Lasso for the maximum entropy quantile regression. A flat prior is chosen for the quantile parameter due to the lack of information on it. The proposed method not only addresses the problem about which quantile would be the most probable one among all the candidates, but also reflects the inner relationship of the data through the estimated quantile. We develop an efficient Gibbs sampler algorithm and show that the performance of our proposed method is superior than the Bayesian adaptive Lasso and Bayesian Lasso through simulation studies and a real data analysis.",2017,Journal of Applied Statistics
Robust Lasso With Missing and Grossly Corrupted Observations,"This paper studies the problem of accurately recovering a <formula formulatype=""inline""><tex Notation=""TeX"">$k$</tex> </formula>-sparse vector <formula formulatype=""inline""><tex Notation=""TeX"">$\beta^{\star}\in\BBR^{p}$</tex> </formula> from highly corrupted linear measurements <formula formulatype=""inline""><tex Notation=""TeX"">$y=X\beta^{\star}+e^{\star}+w$</tex> </formula>, where <formula formulatype=""inline""> <tex Notation=""TeX"">$e^{\star}\in\BBR^{n}$</tex></formula> is a sparse error vector whose nonzero entries may be unbounded and <formula formulatype=""inline""><tex Notation=""TeX"">$w$</tex></formula> is a stochastic noise term. We propose a so-called extended Lasso optimization which takes into consideration sparse prior information of both <formula formulatype=""inline""><tex Notation=""TeX"">$\beta^{\star}$</tex> </formula> and <formula formulatype=""inline""> <tex Notation=""TeX"">$e^{\star}$</tex></formula>. Our first result shows that the extended Lasso can faithfully recover both the regression as well as the corruption vector. Our analysis relies on the notion of extended restricted eigenvalue for the design matrix <formula formulatype=""inline""><tex Notation=""TeX"">$X$</tex></formula>. Our second set of results applies to a general class of Gaussian design matrix <formula formulatype=""inline""><tex Notation=""TeX"">$X$</tex> </formula> with i.i.d. rows <formula formulatype=""inline""><tex Notation=""TeX"">${\cal N}(0,\Sigma)$</tex></formula>, for which we can establish a surprising result: the extended Lasso can recover exact signed supports of both <formula formulatype=""inline""> <tex Notation=""TeX"">$\beta^{\star}$</tex></formula> and <formula formulatype=""inline""><tex Notation=""TeX"">$e^{\star}$</tex> </formula> from only <formula formulatype=""inline""> <tex Notation=""TeX"">$\Omega(k\log p\log n)$</tex></formula> observations, even when a linear fraction of observations is grossly corrupted. Our analysis also shows that this amount of observations required to achieve exact signed support is indeed optimal.",2013,IEEE Transactions on Information Theory
