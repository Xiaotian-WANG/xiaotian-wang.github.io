title,abstract,year,journal
A Machine Learning-based Recommendation System for Swaptions Strategies,"Derivative traders are usually required to scan through hundreds, even thousands of possible trades on a daily basis. Up to now, not a single solution is available to aid in their job. Hence, this work aims to develop a trading recommendation system, and apply this system to the so-called Mid-Curve Calendar Spread (MCCS), an exotic swaption-based derivatives package. In summary, our trading recommendation system follows this pipeline: (i) on a certain trade date, we compute metrics and sensitivities related to an MCCS; (ii) these metrics are feed in a model that can predict its expected return for a given holding period; and after repeating (i) and (ii) for all trades we (iii) rank the trades using some dominance criteria. To suggest that such approach is feasible, we used a list of 35 different types of MCCS; a total of 11 predictive models; and 4 benchmark models. Our results suggest that in general linear regression with lasso regularisation compared favourably to other approaches from a predictive and interpretability perspective.",2018,ArXiv
An algorithm for the multivariate group lasso with covariance estimation,"ABSTRACT We study a group lasso estimator for the multivariate linear regression model that accounts for correlated error terms. A block coordinate descent algorithm is used to compute this estimator. We perform a simulation study with categorical data and multivariate time series data, typical settings with a natural grouping among the predictor variables. Our simulation studies show the good performance of the proposed group lasso estimator compared to alternative estimators. We illustrate the method on a time series data set of gene expressions.",2015,Journal of Applied Statistics
Algorithmes d'ensemble actif pour le LASSO,"Le LASSO est une methode de regression ajoutant a la methode des moindres-carres une contrainte ou une penalisation sur la norme l1 du coefficient lineaire. Cette contrainte a un effet de selection de variable et de regularisation sur l'estimateur. Un estimateur LASSO est defini comme etant la solution d'un probleme pouvant etre vu comme un programme quadratique. Cette these se base sur deux algorithmes dedies a la resolution du LASSO publies en 2000 par M. Osbourne et alii. L'un, une methode par homotopie, a ete reformule en 2004 par J. Friedman et alii sous le nom de LAR-LASSO (ou LARS), s'imposant alors comme la methode standard. Le second, presente comme une methode d'ensemble actif, fut largement ignore, semble-t-il pour deux raisons: une application apparemment limitee a la formulation ""contrainte"", et une comprehension plus difficile de l'algorithme. Nous reformulons donc le principe general du second, que nous baptisons ""Descente sur Ensemble Actif"" (DEA) et sa derivation sur le LASSO, ainsi que la methode par homotopie que nous mettons en evidence comme directement derivee de la DEA. La formulation simplifiee des deux methodes permet d'en ameliorer la comprehension, mais aussi l'efficacite en temps de calcul. Elle met en outre en evidence l'applicabilite de la DEA sur la formulation ""penalisee"" du LASSO, donnant un algorithme plus simple encore. Enfin, elle conduit a une analyse et un traitement de cas limites dans lesquels ces algorithmes peuvent echouer. Nous proposons ensuite une application directe du LASSO sur un nombre infini de variables formant un espace multidimensionel, et etudions l'adaptation des algorithmes d'ensemble actifs dans ce cadre.",2011,
Characterization of a five-microRNA signature as a prognostic biomarker for esophageal squamous cell carcinoma,"This study aims to identify a miRNAs signature for predicting overall survival (OS) in esophageal squamous cell carcinoma (ESCC) patients. MiRNA expression profiles and corresponding clinical information of 119 ESCC patients were obtained from NCBI GEO and used as the training set. Differentially expressed miRNAs (DEmiRNAs) were screened between early-stage and late-stage samples. Cox regression analysis, recursive feature elimination (RFE)-support vector machine (SVM) algorithm, and LASSO Cox regression model were used to identify prognostic miRNAs and consequently build a prognostic scoring model. Moreover, promising target genes of these prognostic miRNAs were predicted followed by construction of miRNA-target gene networks. Functional relevance of predicted target genes of these prognostic miRNAs in ESCC was analyzed by performing function enrichment analyses. There were 46 DEmiRNAs between early-stage and late-stage samples in the training set. A risk score model based on five miRNAs was built. The five-miRNA risk score could classify the training set into a high-risk group and a low-risk group with significantly different OS time. Risk stratification ability of the five-miRNA risk score was successfully validated on an independent set from the Cancer Genome Atlas (TCGA). Various biological processes and pathways were identified to be related to these miRNAs, such as Wnt signaling pathway, inflammatory mediator regulation of TRP channels pathway, and estrogen signaling pathway. The present study suggests a pathological stage-related five-miRNA signature that may have clinical implications in predicting prognosis of ESCC patients.",2019,Scientific Reports
EDLRT: Entropy-based dummy variables logistic regression tree,"An algorithm named EDLRT (entropy-based dummy variable logistic regression tree) has been developed to handle decision tree processes. The main feature of EDLRT is constructing an entropy-based non-linear regression tree in the form of logistic formula. EDLRT comprises two key steps: the first step is to establish a decision tree by selecting the splitting variables with maximum mutual information; the second step is to convert the splitting points into dummy variables and fit them into a logistic regression model, and use genetic or Lasso algorithm to estimate the coefficients of parameters. The mathematical treatment of various types of variables for entropy evaluation and splitting point determination is illustrated. The advantage in using mutual information as a key criterion in splitting variable selection is elucidated. Step-by-step procedure of decision tree construction and dummy variable manipulation are illustrated by case study. EDLRT is very tolerant to missing values and it is also very effective for outlier detection. These advantages are demonstrated with case studies.",2010,Intell. Data Anal.
Finding the balance between model complexity and performance: Using ventral striatal oscillations to classify feeding behavior in rats,"The ventral striatum (VS) is a central node within a distributed network that controls appetitive behavior, and neuromodulation of the VS has demonstrated therapeutic potential for appetitive disorders. Local field potential (LFP) oscillations recorded from deep brain stimulation electrodes within the VS are a pragmatic source of neural systems-level information about appetitive behavior that could be used in responsive neuromodulation systems. Here, we recorded LFPs from the bilateral nucleus accumbens core and shell (subregions of the VS) during limited access to palatable food across varying conditions of hunger and food palatability in male rats. We used standard statistical methods (logistic regression) as well as the machine learning algorithm lasso to predict aspects of feeding behavior using VS LFPs. These models were able to predict the amount of food eaten, the increase in consumption following food deprivation, and the type of food eaten. Further, we were able to predict whether the initiation of feeding was imminent up to 42.5 seconds before feeding began and classify current behavior as either feeding or not-feeding. In classifying this behavior, we found an optimal balance between model complexity and performance with models using 3 LFP features primarily from the alpha and high gamma frequencies. As shown here, unbiased methods can identify systems-level neural activity linked to symptoms of mental illness with potential application to the development and personalization of novel treatments. Author Summary As neuropsychiatry begins to leverage the power of computational methods to understand disease states and to develop better therapies, it is vital that we acknowledge the trade-offs between model complexity and performance. We show that computational methods can elucidate a neural signature of feeding behavior and show how these methods could be used to discover neural patterns related to other behaviors and used as therapeutic targets. Further, our results helps to contextualize both the limitations and potential of applying computational methods to neuropsychiatry by showing how changing the data being used to train predictive models (e.g., population vs. individual data) can have a large impact on how model performance generalizes across time, internal states and individuals.",2018,bioRxiv
Application of radiomics signature captured from pretreatment thoracic CT to predict brain metastases in stage III/IV ALK-positive non-small cell lung cancer patients.,"Background
The purpose of this study is to develop a radiomics approach to predict brain metastasis (BM) for stage III/IV ALK-positive non-small cell lung cancer (NSCLC) patients.


Methods
Patients with ALK-positive III/IV NSCLC from 2014 to 2017 were enrolled retrospectively. Their pretreatment thoracic CT images were collected, and the gross tumor volume (GTV) was defined by two experienced radiation oncologists. An in-house feature extraction code-set was performed based on MATLAB 2015b (Mathworks, Natick, MA, USA) in patients' CT images to extract features. Patients were randomly divided into training set and test set (4:1) by using createDataPartition function in caret package. A test-retest in RIDER NSCLC dataset was performed to identify stable radiomics features. LASSO Cox regression and a leave-one-out cross-validation were conducted to identify optimal features for the logistic regression model to evaluate the predictive value of radiomics feature(s) for BM. Furthermore, extended validation for the radiomics feature(s) and Cox regression analyses which combined radiomics feature(s) and treatment elements were implemented to predict the risk of BM during follow-up.


Results
In total, 132 patients were included, among which 27 patients had pretreatment BM. The median follow-up time was 11.8 (range, 0.1-65.2) months. In the training set, one radiomics feature (W_GLCM_LH_Correlation) showed discrimination ability of BM (P value =0.014, AUC =0.687, 95% CI: 0.551-0.824, specificity =83.5%, sensitivity =57.1%). It also exhibited reposeful performance in the test set (AUC =0.642, 95% CI: 0.501-0.783, specificity =60.0%, sensitivity =83.3%). Those 105 patients without pretreatment BM were divided into stage III (n=57) and stage IV (n=48) groups. The radiomics feature (W_GLCM_LH_Correlation) had moderate performance to predict BM during/after treatment in separate groups (stage III: AUC =0.682, 95% CI: 0.537-0.826, specificity =64.4%, sensitivity =75.0%; stage IV: AUC =0.653, 95% CI: 0.503-0.804, specificity =70.4%, sensitivity =75.0%). Meanwhile, stage III patients could be divided into low risk and high risk groups for BM during surveillance according to Cox regression analysis (log-rank P value =0.021).


Conclusions
We identified one wavelet texture feature derived from pretreatment thoracic CT that presented potential in predicting BM in stage III/IV ALK-positive NSCLC patients. This could be beneficial to risk stratification for such patients. Further investigation is necessary to include expanded sample size investigation and external multicenter validation.",2019,Journal of thoracic disease
The Bayesian Elastic Net: Classifying Multi-Task Gene-Expression Data,"Highly correlated relevant features are frequently encountered in variable-selection problems, with gene-expression analysis an important example. It is desirable to select all of these highly correlated features simultaneously as a group, for better model interpretation and robustness. Further, irrelevant features should be excluded, resulting in a sparse solution (of importance for avoiding over-fitting with limited data). We address the problem of sparse and grouped variable selection by introducing a new Bayesian Elastic Net model. One advantage of the proposed model is that by imposing priors on individual parameters in the Laplace distribution, we reduce the number of tuning parameters to one, as compared with two such parameters in the original Elastic Net. In addition, we extend the new Bayesian Elastic Net model to the problem of probit regression, in order to deal with classification problems with a sparse but correlated set of covariates (features). Extension to multi-task learning is also considered, with inference performed using variational Bayesian analysis. The model is validated by first performing experiments on simulated data and on previously published gene-expression data; in these experiments we also perform comparisons to the original Elastic Net and to Bayesian Lasso. Finally, we present and analyze a new gene-expression data set for the time-evolving properties of influenza, measured using blood samples from human subjects in a recent challenge study.",2009,
A Modified Adaptive Lasso for Identifying Interactions in the Cox Model with the Heredity Constraint.,"In many biomedical studies, identifying effects of covariate interactions on survival is a major goal. Important examples are treatment-subgroup interactions in clinical trials, and gene-gene or gene-environment interactions in genomic studies. A common problem when implementing a variable selection algorithm in such settings is the requirement that the model must satisfy the strong heredity constraint, wherein an interaction may be included in the model only if the interaction's component variables are included as main effects. We propose a modified Lasso method for the Cox regression model that adaptively selects important single covariates and pairwise interactions while enforcing the strong heredity constraint. The proposed method is based on a modified log partial likelihood including two adaptively weighted penalties, one for main effects and one for interactions. A two-dimensional tuning parameter for the penalties is determined by generalized cross-validation. Asymptotic properties are established, including consistency and rate of convergence, and it is shown that the proposed selection procedure has oracle properties, given proper choice of regularization parameters. Simulations illustrate that the proposed method performs reliably across a range of different scenarios.",2014,Statistics & probability letters
Empirical extensions of the lasso penalty to reduce the false discovery rate in high-dimensional Cox regression models.,"Correct selection of prognostic biomarkers among multiple candidates is becoming increasingly challenging as the dimensionality of biological data becomes higher. Therefore, minimizing the false discovery rate (FDR) is of primary importance, while a low false negative rate (FNR) is a complementary measure. The lasso is a popular selection method in Cox regression, but its results depend heavily on the penalty parameter Î». Usually, Î» is chosen using maximum cross-validated log-likelihood (max-cvl). However, this method has often a very high FDR. We review methods for a more conservative choice of Î». We propose an empirical extension of the cvl by adding a penalization term, which trades off between the goodness-of-fit and the parsimony of the model, leading to the selection of fewer biomarkers and, as we show, to the reduction of the FDR without large increase in FNR. We conducted a simulation study considering null and moderately sparse alternative scenarios and compared our approach with the standard lasso and 10 other competitors: Akaike information criterion (AIC), corrected AIC, Bayesian information criterion (BIC), extended BIC, Hannan and Quinn information criterion (HQIC), risk information criterion (RIC), one-standard-error rule, adaptive lasso, stability selection, and percentile lasso. Our extension achieved the best compromise across all the scenarios between a reduction of the FDR and a limited raise of the FNR, followed by the AIC, the RIC, and the adaptive lasso, which performed well in some settings. We illustrate the methods using gene expression data of 523 breast cancer patients. In conclusion, we propose to apply our extension to the lasso whenever a stringent FDR with a limited FNR is targeted. Copyright Â© 2016 John Wiley & Sons, Ltd.",2016,Statistics in medicine
Statistical and machine learning methods evaluated for incorporating soil and weather into corn nitrogen recommendations,"Abstract Nitrogen (N) fertilizer recommendation tools could be improved for estimating corn (Zea mays L.) N needs by incorporating site-specific soil and weather information. However, an evaluation of analytical methods is needed to determine the success of incorporating this information. The objectives of this research were to evaluate statistical and machine learning (ML) algorithms for utilizing soil and weather information for improving corn N recommendation tools. Eight algorithms [stepwise, ridge regression, least absolute shrinkage and selection operator (Lasso), elastic net regression, principal component regression (PCR), partial least squares regression (PLSR), decision tree, and random forest] were evaluated using a dataset containing measured soil and weather variables from a regional database. The performance was evaluated based on how well these algorithms predicted corn economically optimal N rates (EONR) from 49 sites in the U.S. Midwest. Multiple algorithm modeling scenarios were examined with and without adjustment for multicollinearity and inclusion of two-way interaction terms to identify the soil and weather variables that could improve three dissimilar N recommendation tools. Results showed the out-of-sample root-mean-square error (RMSE) for the decision tree and some random forest modeling scenarios were better than the stepwise or ridge regression, but not significantly different than any other algorithm. The best ML algorithm for adjusting N recommendation tools was the random forest approach (r2 increased between 0.72 and 0.84 and the RMSE decreased between 41 and 94â€¯kgâ€¯Nâ€¯haâˆ’1). However, the ML algorithm that best adjusted tools while using a minimal amount of variables was the decision tree. This method was simple, needing only one or two variables (regardless of modeling scenario) and provided moderate improvement as r2 values increased between 0.15 and 0.51 and RMSE decreased between 16 and 66â€¯kgâ€¯Nâ€¯haâˆ’1. Using ML algorithms to adjust N recommendation tools with soil and weather information shows promising results for better N management in the U.S. Midwest.",2019,Comput. Electron. Agric.
Double/Debiased Machine Learning for Treatment and Causal Parameters,"Most modern supervised statistical/machine learning (ML) methods are explicitly designed to solve prediction problems very well. Achieving this goal does not imply that these methods automatically deliver good estimators of causal parameters. Examples of such parameters include individual regression coefficients, average treatment effects, average lifts, and demand or supply elasticities. In fact, estimates of such causal parameters obtained via naively plugging ML estimators into estimating equations for such parameters can behave very poorly due to the regularization bias. Fortunately, this regularization bias can be removed by solving auxiliary prediction problems via ML tools. Specifically, we can form an orthogonal score for the target low-dimensional parameter by combining auxiliary and main ML predictions. The score is then used to build a de-biased estimator of the target parameter which typically will converge at the fastest possible 1/root(n) rate and be approximately unbiased and normal, and from which valid confidence intervals for these parameters of interest may be constructed. The resulting method thus could be called a ""double ML"" method because it relies on estimating primary and auxiliary predictive models. In order to avoid overfitting, our construction also makes use of the K-fold sample splitting, which we call cross-fitting. This allows us to use a very broad set of ML predictive methods in solving the auxiliary and main prediction problems, such as random forest, lasso, ridge, deep neural nets, boosted trees, as well as various hybrids and aggregators of these methods.",2017,arXiv: Machine Learning
"Forecasting Time Charter Equivalent Oil Tanker Freight Rates - determinant driven, route-specific Markov regime-switching models","In this thesis, we address the issue of explaining and forecasting oil tanker freight rates for specific tanker routes. These freight rates are known to exhibit periods of extreme volatility. To predict the freight rates, we therefore utilise a Markov regime-switching multiple regression model with two states a normal state, and a volatile state. This thesis hence postulates that predictions of short-term freight rates can be improved through a framework that can capture the distinctive nature of freight rates by switching between two regimes, while combining this with hypothetically superior route-specific and global determinants. We make a substantial attempt to combine market domain knowledge with statistical methods. Our approach to doing so is twofold. Firstly, motivated by the findings in the existing literature, the observations of structural breaks, and the plethora of attempts at modelling the freight rate, we characterise the market. Six tanker routes, TD1, TD3, TD7, TD12, TC1, and TC2, are investigated. An extensive assessment of the key determining factors of the freight rates is given. A candidate predictor analysis is done, based on subsampling in combination with a selection algorithm. By using the novel approach of stability selection and LASSO penalization with a random tuning parameter, we are able to rank the factors based on their potential modelling importance. Secondly, we develop a Markov regime-switching regression model for one-month ahead forecasting of the freight rates on the selected routes. The data is tested for structural breaks using a Chow test, and indications of multiple regimes are found. With a subset of variables for each route, we formulate two-regime regression models with switching coefficients. Seasonal changes and varying lags are accounted for, and the result is six regime models which are tailored specifically to each route. Three objectives are evaluated on out-of-sample data for each route: i ) An evaluation of whether similar parsimonious models outperform variable rich models. Six additional parsimonious regime models are therefore created, one for each route. The parsimonious models are found to provide better predictions based on performance metrics and the Diebold-Mariano test for forecasting accuracy. Top performing variables in the parsimonious models include, amongst others, secondhand prices, import and export factors, Chinese crude imports, vessel fixtures, and the ClarkSea index. ii ) An assessment of the forecasting capabilities of the regime model on never-before-seen data. These models yield promising results, and consistently rank in the top positions in regards to forecasting when compared to a set of benchmark models. iii ) An evaluation of the benefit of incorporating route-specific variables. The route-specific regime models are compared to a generic benchmark regime model with globally universal variables. Route-specific regime models are found to provide valuable outcomes and they improve the forecast in most cases, as opposed to the generic factor-driven models.",2018,
Analysis of Electrochemical Impedance Spectroscopy Data Using the Distribution of Relaxation Times: A Bayesian and Hierarchical Bayesian Approach,"Abstract Electrochemical impedance spectroscopy (EIS) is one of the most important experimental techniques employed in electrochemistry because it can be used to deconvolve physico-chemical phenomena occurring at disparate timescales. Unfortunately, the analysis of EIS data is frequently challenging since it can require the selection of ad hoc equivalent circuits. The distribution of relaxation times (DRT) method is complementary to the approach of fitting equivalent circuits because the DRT maps the EIS data onto a function containing the timescale characteristics of the system under study. While conceptually simple, the DRT cannot be obtained by simple minimization of the least squares because the corresponding optimization problem is ill posed. Regularization methods, such as ridge/Tikhonov or Lasso regression, add a penalty term to the least squares minimization problem enabling the DRT deconvolution. In this work, we show that such regularization methods may be understood in a Bayesian context. For example, ridge/Tikhonov regression implicitly encapsulates the prior insight that the derivatives of the DRT are regular. We use this Bayesian approach as a starting point to extend the DRT regularization by considering frequency dependent oscillation levels. This approach is shown to be more robust with respect to both discontinuities and over smoothing than typical regularized DRT methods. Furthermore, the Bayesian approach is versatile and may be extended to include more informative priors.",2015,Electrochimica Acta
Identifying the Prognosis Factors and Predicting the Survival Probability in Patients with Nonâ€Metastatic Chondrosarcoma from the SEER Database,"OBJECTIVE
To identify prognostic factors and establish nomograms for predicting overall survival (OS) and cause specific survival (CSS) of patients with non-metastatic chondrosarcoma.


METHODS
We collected information on patients with non-metastatic chondrosarcoma from the Surveillance, Epidemiology, and End Results (SEER) database between 2005 and 2014, together with data from the First Affiliated Hospital of Zhengzhou University from 2011 to 2016. Variables including patients' baseline demographics (age, race, and gender), tumor characteristics (tumor size and extension, histology subtype, primary site, and American Joint Committee on Cancer [AJCC] stage), therapy (surgery, chemotherapy, and radiotherapy), and socioeconomic status (SES) were extracted for further analysis. OS and CSS were retrieved as our researching endpoints. Patients from the database were regarded as the training set, and univariate analysis, Lasso regression and multivariate analysis as well as the random forest were used to explore the predictors and establish nomograms. To validate nomograms internally and externally, we applied bootstrapped validation internally with the training dataset, while the dataset for external validation was obtained from the First Affiliated Hospital of Zhengzhou University. We estimated the discriminative ability of nomograms based on Cox proportional hazard regression models by means of calibration curves and the concordance index (C-index) of internal and external validation.


RESULTS
After the implementation of exclusion criteria, there were 1267 patients in the training set and 72 patients in the testing set with non-metastatic chondrosarcomas. Age, gender, grade, histological subtype, primary site, surgery, radiation, chemotherapy, being employed/unemployed, tumor size, and tumor extension were significantly associated with prognosis in the univariate analysis. Age, gender, tumor size and extension, primary site, surgery, radiotherapy, chemotherapy, histological grade, and subtype were independent prognostic factors in the Cox models. The C-index of nomograms (internal: OS, 0.787; CSS, 0.821; external: OS, 0.777; CSS, 0.821) were higher than following conventional systems: AJCC sixth (OS, 0.640; CSS, 0.673) and seventh edition (OS, 0.675; CSS, 0.711).


CONCLUSIONS
Age, gender, tumor size and extension, surgery, histological grade, and subtype were independent prognostic factors for both OS and CSS. In addition, we revealed that chondrosarcomas in the trunk, radiotherapy, and chemotherapy were correlated with poor prognosis. Our nomograms based on significant clinicopathologic features can well predict the 3-year and 5-year survival probability of patients with non-metastatic chondrosarcoma and assist oncologists in making accurate survival evaluation.",2019,Orthopaedic Surgery
A study on tuning parameter selection for the high-dimensional lasso,"ABSTRACT High-dimensional predictive models, those with more measurements than observations, require regularization to be well defined, perform well empirically, and possess theoretical guarantees. The amount of regularization, often determined by tuning parameters, is integral to achieving good performance. One can choose the tuning parameter in a variety of ways, such as through resampling methods or generalized information criteria. However, the theory supporting many regularized procedures relies on an estimate for the variance parameter, which is complicated in high dimensions. We develop a suite of information criteria for choosing the tuning parameter in lasso regression by leveraging the literature on high-dimensional variance estimation. We derive intuition showing that existing information-theoretic approaches work poorly in this setting. We compare our risk estimators to existing methods with an extensive simulation and derive some theoretical justification. We find that our new estimators perform well across a wide range of simulation conditions and evaluation criteria.",2018,Journal of Statistical Computation and Simulation
Some theoretical results on the Grouped Variables Lasso,"We consider the linear regression model with Gaussian error. We estimate the unknown parameters by a procedure inspired by the Group Lasso estimator introduced in [22]. We show that this estimator satisfies a sparsity inequality, i.e., a bound in terms of the number of non-zero components of the oracle regression vector. We prove that this bound is better, in some cases, than the one achieved by the Lasso and the Dantzig selector.",2008,Mathematical Methods of Statistics
Adaptive Piecewise Polynomial Estimation via Trend Filtering 1,"We study trend filtering, a recently proposed tool of Kim et al. [SIAM Rev. 51 (2009) 339â€“360] for nonparametric regression. The trend filtering estimate is defined as the minimizer of a penalized least squares criterion, in which the penalty term sums the absolute kth order discrete derivatives over the input points. Perhaps not surprisingly, trend filtering estimates appear to have the structure of kth degree spline functions, with adaptively chosen knot points (we say â€œappearâ€ here as trend filtering estimates are not really functions over continuous domains, and are only defined over the discrete set of inputs). This brings to mind comparisons to other nonparametric regression tools that also produce adaptive splines; in particular, we compare trend filtering to smoothing splines, which penalize the sum of squared derivatives across input points, and to locally adaptive regression splines [Ann. Statist. 25 (1997) 387â€“413], which penalize the total variation of the kth derivative. Empirically, we discover that trend filtering estimates adapt to the local level of smoothness much better than smoothing splines, and further, they exhibit a remarkable similarity to locally adaptive regression splines. We also provide theoretical support for these empirical findings; most notably, we prove that (with the right choice of tuning parameter) the trend filtering estimate converges to the true underlying function at the minimax rate for functions whose kth derivative is of bounded variation. This is done via an asymptotic pairing of trend filtering and locally adaptive regression splines, which have already been shown to converge at the minimax rate [Ann. Statist. 25 (1997) 387â€“413]. At the core of this argument is a new result tying together the fitted values of two lasso problems that share the same outcome vector, but have different predictor matrices.",2014,
A prediction scheme of tropical cyclone frequency based on lasso and random forest,"This study aims to propose a novel prediction scheme of tropical cyclone frequency (TCF) over the Western North Pacific (WNP). We concerned the large-scale meteorological factors inclusive of the sea surface temperature, sea level pressure, the NiÃ±o-3.4 index, the wind shear, the vorticity, the subtropical high, and the sea ice cover, since the chronic change of these factors in the context of climate change would cause a gradual variation of the annual TCF. Specifically, we focus on the correlation between the year-to-year increment of these factors and TCF. The least absolute shrinkage and selection operator (Lasso) method was used for variable selection and dimension reduction from 11 initial predictors. Then, a prediction model based on random forest (RF) was established by using the training samples (1978â€“2011) for calibration and the testing samples (2012â€“2016) for validation. The RF model presents a major variation and trend of TCF in the period of calibration, and also fitted well with the observed TCF in the period of validation though there were some deviations. The leave-one-out cross validation of the model exhibited most of the predicted TCF are in consistence with the observed TCF with a high correlation coefficient. A comparison between results of the RF model and the multiple linear regression (MLR) model suggested the RF is more practical and capable of giving reliable results of TCF prediction over the WNP.",2017,Theoretical and Applied Climatology
Optimal disease outbreak decisions using stochastic simulation,Management policies for disease outbreaks balance the expected morbidity and mortality costs versus the cost of intervention policies. We present a methodology for dynamic determination of optimal policies in a stochastic compartmental model with parameter uncertainty. Our approach is to first carry out sequential Bayesian estimation of outbreak parameters and then solve the dynamic programming equations. The latter step is simulation-based and relies on regression Monte Carlo techniques. To improve performance we investigate lasso regression and global policy iteration. Comparisons demonstrate the realized cost savings of choosing interventions based on the computed dynamic policy over simpler decision rules.,2011,Proceedings of the 2011 Winter Simulation Conference (WSC)
Radiomics prognostication model in glioblastoma using diffusion- and perfusion-weighted MRI,"We aimed to develop and validate a multiparametric MR radiomics model using conventional, diffusion-, and perfusion-weighted MR imaging for better prognostication in patients with newly diagnosed glioblastoma. A total of 216 patients with newly diagnosed glioblastoma were enrolled from two tertiary medical centers and divided into training (nâ€‰=â€‰158) and external validation sets (nâ€‰=â€‰58). Radiomic features were extracted from contrast-enhanced T1-weighted imaging, fluid-attenuated inversion recovery, diffusion-weighted imaging, and dynamic susceptibility contrast imaging. After radiomic feature selection using LASSO regression, an individualized radiomic score was calculated. A multiparametric MR prognostic model was built using the radiomic score and clinical predictors. The results showed that the multiparametric MR prognostic model (radiomics score + clinical predictors) exhibited good discrimination (C-index, 0.74) and performed better than a conventional MR radiomics model (C-index, 0.65, Pâ€‰<â€‰0.0001) or clinical predictors (C-index, 0.66; Pâ€‰<â€‰0.0001). The multiparametric MR prognostic model also showed robustness in external validation (C-index, 0.70). Our results indicate that the incorporation of diffusion- and perfusion-weighted MR imaging into an MR radiomics model to improve prognostication in glioblastoma patients improved its performance over that achievable using clinical predictors alone.",2020,Scientific Reports
"Neural function, injury, and stroke subtype predict treatment gains after stroke.","OBJECTIVE
This study was undertaken to better understand the high variability in response seen when treating human subjects with restorative therapies poststroke. Preclinical studies suggest that neural function, neural injury, and clinical status each influence treatment gains; therefore, the current study hypothesized that a multivariate approach incorporating these 3 measures would have the greatest predictive value.


METHODS
Patients 3 to 6 months poststroke underwent a battery of assessments before receiving 3 weeks of standardized upper extremity robotic therapy. Candidate predictors included measures of brain injury (including to gray and white matter), neural function (cortical function and cortical connectivity), and clinical status (demographics/medical history, cognitive/mood, and impairment).


RESULTS
Among all 29 patients, predictors of treatment gains identified measures of brain injury (smaller corticospinal tract [CST] injury), cortical function (greater ipsilesional motor cortex [M1] activation), and cortical connectivity (greater interhemispheric M1-M1 connectivity). Multivariate modeling found that best prediction was achieved using both CST injury and M1-M1 connectivity (r(2) = 0.44, p = 0.002), a result confirmed using Lasso regression. A threshold was defined whereby no subject with >63% CST injury achieved clinically significant gains. Results differed according to stroke subtype; gains in patients with lacunar stroke were best predicted by a measure of intrahemispheric connectivity.


INTERPRETATION
Response to a restorative therapy after stroke is best predicted by a model that includes measures of both neural injury and function. Neuroimaging measures were the best predictors and may have an ascendant role in clinical decision making for poststroke rehabilitation, which remains largely reliant on behavioral assessments. Results differed across stroke subtypes, suggesting the utility of lesion-specific strategies.",2015,Annals of neurology
An \ell_1-oracle inequality for the Lasso in finite mixture of multivariate Gaussian regression models,"We consider a multivariate finite mixture of Gaussian regression models for high-dimensional data, where the number of covariates and the size of the response may be much larger than the sample size. We provide an $\ell_1$-oracle inequality satisfied by the Lasso estimator according to the Kullback-Leibler loss. This result is an extension of the $\ell_1$-oracle inequality established by Meynet in \cite{Meynet} in the multivariate case. We focus on the Lasso for its $\ell_1$-regularization properties rather than for the variable selection procedure, as it was done in Stadler in \cite{Stadler}.",2014,arXiv: Statistics Theory
Linear Regression with a Large Number of Weak Instruments using a Post-l 1-Penalized Estimator,"This paper proposes a new two stage least squares (2SLS) estimator which is consistent and asymptotically normal in the presence of many weak instruments and heteroskedasticity. The first stage consists of two components: first, an adaptive absolute shrinkage and selection operator (LASSO) that selects the instruments and second, an OLS regression with the selected regressors. This procedure is a post-l1-penalized estimator as proposed by Belloni and Chernozhukov (2010). The second stage uses an OLS regression with the fitted values from the post-l1-penalized regression of the first stage. The methodology exploits the model selection benefits of the adaptive LASSO and reduces the post-IV selection bias. More importantly, it provides a consistent and asymptotically normal estimator in a 2SLS framework in the presence of many weak instruments and heteroskedasticity, which is infeasible for the conventional 2SLS in this context. These results are driven by the fact that after the instrument selection stage the growth rate of the concentration parameter is higher than the growth rate of the number of instruments.",2011,
Natural Language Processing for Automated Quantification of Brain Metastases Reported in Free-Text Radiology Reports.,"PURPOSE
Although the bulk of patient-generated health data are increasing exponentially, their use is impeded because most data come in unstructured format, namely as free-text clinical reports. A variety of natural language processing (NLP) methods have emerged to automate the processing of free text ranging from statistical to deep learning-based models; however, the optimal approach for medical text analysis remains to be determined. The aim of this study was to provide a head-to-head comparison of novel NLP techniques and inform future studies about their utility for automated medical text analysis.


PATIENTS AND METHODS
Magnetic resonance imaging reports of patients with brain metastases treated in two tertiary centers were retrieved and manually annotated using a binary classification (single metastasis v two or more metastases). Multiple bag-of-words and sequence-based NLP models were developed and compared after randomly splitting the annotated reports into training and test sets in an 80:20 ratio.


RESULTS
A total of 1,479 radiology reports of patients diagnosed with brain metastases were retrieved. The least absolute shrinkage and selection operator (LASSO) regression model demonstrated the best overall performance on the hold-out test set with an area under the receiver operating characteristic curve of 0.92 (95% CI, 0.89 to 0.94), accuracy of 83% (95% CI, 80% to 87%), calibration intercept of -0.06 (95% CI, -0.14 to 0.01), and calibration slope of 1.06 (95% CI, 0.95 to 1.17).


CONCLUSION
Among various NLP techniques, the bag-of-words approach combined with a LASSO regression model demonstrated the best overall performance in extracting binary outcomes from free-text clinical reports. This study provides a framework for the development of machine learning-based NLP models as well as a clinical vignette of patients diagnosed with brain metastases.",2019,JCO clinical cancer informatics
Model selection strategies for identifying most relevant covariates in homoscedastic linear models,"A new method in two variations for the identification of most relevant covariates in linear models with homoscedastic errors is proposed. In contrast to many known selection criteria, the method is based on an interpretable scaled quantity. This quantity measures a maximal relative error one makes by selecting covariates from a given set of all available covariates. The proposed model selection procedures rely on asymptotic normality of test statistics, and therefore normality of the errors in the regression model is not required. In a simulation study the performance of the suggested methods along with the performance of the standard model selection criteria AIC, BIC, Lasso and relaxed Lasso is examined. The simulation study illustrates the favorable performance of the proposed method as compared to the above reference criteria, especially when regression effects possess influence of several orders in magnitude. The accuracy of the normal approximation to the test statistics is also investigated; it has been already satisfactory for sample sizes 50 and 100. As an illustration the US college spending data from 1994 is analyzed.",2010,Comput. Stat. Data Anal.
Developing a dengue forecast model using machine learning: A case study in China,"BACKGROUND
In China, dengue remains an important public health issue with expanded areas and increased incidence recently. Accurate and timely forecasts of dengue incidence in China are still lacking. We aimed to use the state-of-the-art machine learning algorithms to develop an accurate predictive model of dengue.


METHODOLOGY/PRINCIPAL FINDINGS
Weekly dengue cases, Baidu search queries and climate factors (mean temperature, relative humidity and rainfall) during 2011-2014 in Guangdong were gathered. A dengue search index was constructed for developing the predictive models in combination with climate factors. The observed year and week were also included in the models to control for the long-term trend and seasonality. Several machine learning algorithms, including the support vector regression (SVR) algorithm, step-down linear regression model, gradient boosted regression tree algorithm (GBM), negative binomial regression model (NBM), least absolute shrinkage and selection operator (LASSO) linear regression model and generalized additive model (GAM), were used as candidate models to predict dengue incidence. Performance and goodness of fit of the models were assessed using the root-mean-square error (RMSE) and R-squared measures. The residuals of the models were examined using the autocorrelation and partial autocorrelation function analyses to check the validity of the models. The models were further validated using dengue surveillance data from five other provinces. The epidemics during the last 12 weeks and the peak of the 2014 large outbreak were accurately forecasted by the SVR model selected by a cross-validation technique. Moreover, the SVR model had the consistently smallest prediction error rates for tracking the dynamics of dengue and forecasting the outbreaks in other areas in China.


CONCLUSION AND SIGNIFICANCE
The proposed SVR model achieved a superior performance in comparison with other forecasting techniques assessed in this study. The findings can help the government and community respond early to dengue epidemics.",2017,PLoS Neglected Tropical Diseases
IsoLasso: A LASSO Regression Approach to RNA-Seq Based Transcriptome Assembly - (Extended Abstract),"The new second generation sequencing technology revolu- tionizes many biology related research fields, and posts various compu- tational biology challenges. One of them is transcriptome assembly based on RNA-Seq data, which aims at reconstructing all full-length mRNA transcripts simultaneously from millions of short reads. In this paper, we consider three objectives in transcriptome assembly: the maximization of prediction accuracy, minimization of interpretation, and maximization of completeness. The first objective, the maximization of prediction ac- curacy, requires that the estimated expression levels based on assembled transcripts should be as close as possible to the observed ones for ev- ery expressed region of the genome. The minimization of interpretation follows the parsimony principle to seek as few transcripts in the pre- diction as possible. The third objective, the maximization of complete- ness, requires that the maximum number of mapped reads (or ""expressed segments"" in gene models) be explained by (i.e., contained in) the pre- dicted transcripts in the solution. Based on the above three objectives, we present IsoLasso, a new RNA-Seq based transcriptome assembly tool. IsoLasso is based on the well-known LASSO algorithm, a multivariate regression method designated to seek a balance between the maximiza- tion of prediction accuracy and the minimization of interpretation. By including some additional constraints in the quadratic program involved in LASSO, IsoLasso is able to make the set of assembled transcripts as complete as possible. Experiments on simulated and real RNA-Seq datasets show that IsoLasso achieves higher sensitivity and precision si- multaneously than the state-of-art transcript assembly tools.",2011,
Model Selection in Validation Sampling: an Asymptotic Likelihood-based Lasso Approach,"We propose an asymptotic likelihood-based LASSO approach for model selection in regression analysis when data are subject to validation sampling. The method makes use of an initial estimator of the regression coefficients and their asymptotic covariance matrix to form an asymptotic likelihood. This ``working'' objective function facilitates the formulation of the LASSO and the implementation of a fast algorithm. Our method circumvents the need to use a likelihood set-up that requires full distributional assumptions about the data. We show that the resulting estimator is consistent in model selection and that the method has lower prediction errors than a model that uses only the validation sample. Furthermore, we show that this formulation gives an optimal estimator in a certain sense. Extensive simulation studies are conducted for the linear regression model, the generalized linear regression model, and the Cox model. Our simulation results support our claims. The method is further applied to a dataset to illustrate its practical use.",2011,Statistica Sinica
Theoretical Properties of the Overlapping Groups Lasso,"We present two sets of theoretical results on the grouped lasso with overlap of Jacob, Obozinski and Vert (2009) in the linear regression setting. This method allows for joint selection of predictors in sparse regression, allowing for complex structured sparsity over the predictors encoded as a set of groups. This flexible framework suggests that arbitrarily complex structures can be encoded with an intricate set of groups. Our results show that this strategy results in unexpected theoretical consequences for the procedure. In particular, we give two sets of results: (1) finite sample bounds on prediction and estimation, and (2) asymptotic distribution and selection. Both sets of results give insight into the consequences of choosing an increasingly complex set of groups for the procedure, as well as what happens when the set of groups cannot recover the true sparsity pattern. Additionally, these results demonstrate the differences and similarities between the the grouped lasso procedure with and without overlapping groups. Our analysis shows the set of groups must be chosen with caution - an overly complex set of groups will damage the analysis.",2011,arXiv: Machine Learning
Design and Evaluation of Personalized Targeting Policies: Application to Free Trials,"Effectively targeting at scale is one of the most important problems that todayâ€™s firms face. We provide a three-pronged framework to design and evaluate personalized targeting policies that is compatible with a high-dimensional covariate space. First, we define the optimal policy design problem and describe two solution approaches â€“ (a) policy design using outcome estimates, and (b) policy design using CATE estimates. Second, we consider five outcome estimators (linear regression, lasso, CART, random forest, and XGBoost) and two CATE estimators (causal tree and causal forest) for this task. Third, we use the Inverse Propensity Score (IPS) estimator to evaluate the reward from any targeting policy offline. We apply our framework to data from a large-scale field experiment on free trials conducted by a leading SaaS firm, where new users were randomly assigned to 7, 14, or 30 days of free trial. Among the uniform targeting policies, the 7-days-for-all policy maximizes the subscription rate. Next, we design personalized targeting policies to optimize subscriptions. We find that policies based on the outcome estimators â€“ lasso and XGBoost â€“ offer the best performance. In contrast, policies based on the two CATE estimators â€“ causal tree and causal forest â€“ perform poorly because they are unable to personalize the policy sufficiently. We then link a methodâ€™s effectiveness in designing a policy with its ability to personalize the treatment sufficiently without overfitting (i.e., capture spurious heterogeneity). Finally, we show that policies designed to maximize short-run conversions also perform well on long-run outcomes such as consumer loyalty and profitability.",2019,
Solar power forecasting with sparse vector autoregression structures,"The strong growth that is felt at the level of photovoltaic (PV) power generation craves for more sophisticated and accurate forecasting methods that could be able to support its proper integration into the energy distribution network. Through the combination of the vector autoregression model (VAR) with the least absolute shrinkage and selection operator (LASSO) framework, a set of sparse VAR structures can be obtained in order to capture the dynamic of the underlying system. The robust and efficient alternating direction method of multipliers (ADMM), well known for its great ability dealing with high-dimensional data (scalability and fast convergence), is applied to fit the resulting LASSO-VAR variants. This spatial-temporal forecasting methodology has been tested, using 1-hour and 15-minutes resolution, for 44 microgeneration units time-series located in a city in Portugal. A comparison with the conventional autoregressive (AR) model is performed leading to an improvement up to 11%.",2017,2017 IEEE Manchester PowerTech
Variable selection for sparse logistic regression,"We consider the variable selection problem in a sparse logistical regression model. Inspired by the square-root Lasso, we develop a weighted score Lasso for logistical regression. The new method yields the estimation $${\ell }_1$$ error bound under similar assumptions as introduced in Bach et al. (Electron J Stat 4:384â€“414, 2010). Compared to standard Lasso, the weighted score Lasso provides a direct choice for the tuning parameter. Both theoretical and simulation results confirm the satisfactory performance of the proposed method. We illustrate our methodology with a real microarray data set.",2020,Metrika
Prognostic modelling with logistic regression analysis: a comparison of selection and estimation methods in small data sets.,"Logistic regression analysis may well be used to develop a prognostic model for a dichotomous outcome. Especially when limited data are available, it is difficult to determine an appropriate selection of covariables for inclusion in such models. Also, predictions may be improved by applying some sort of shrinkage in the estimation of regression coefficients. In this study we compare the performance of several selection and shrinkage methods in small data sets of patients with acute myocardial infarction, where we aim to predict 30-day mortality. Selection methods included backward stepwise selection with significance levels alpha of 0.01, 0.05, 0. 157 (the AIC criterion) or 0.50, and the use of qualitative external information on the sign of regression coefficients in the model. Estimation methods included standard maximum likelihood, the use of a linear shrinkage factor, penalized maximum likelihood, the Lasso, or quantitative external information on univariable regression coefficients. We found that stepwise selection with a low alpha (for example, 0.05) led to a relatively poor model performance, when evaluated on independent data. Substantially better performance was obtained with full models with a limited number of important predictors, where regression coefficients were reduced with any of the shrinkage methods. Incorporation of external information for selection and estimation improved the stability and quality of the prognostic models. We therefore recommend shrinkage methods in full models including prespecified predictors and incorporation of external information, when prognostic models are constructed in small data sets.",2000,Statistics in medicine
Earth Pressure Multipoint Prediction for EPS Shield Based on Multi-Model Ensemble,"The tunnel face stability is greatly influenced by the earth pressure in the chamber. It is of great practical significance to improve the performance of the earth pressure multipoint predictive model. In this study, a multi-model ensemble approach based on Lasso, Support Vector Regression, Random Forest and Gradient Boosting Decision Tree is presented for earth pressure multipoint prediction in EPB shield. The Leave-One-Out is adopted to validate the predictive performance. The feature importance is provided by the Lasso, Random Forest and Gradient Boosting Decision Tree model. The experimental results show that the performance of the multi-model ensemble is better than all single model.",2018,2018 Chinese Automation Congress (CAC)
Probabilistic Inference on Multiple Normalized Genome-Wide Signal Profiles With Model Regularization,"Understanding genome-wide protein-DNA interaction signals forms the basis for further focused studies in gene regulation. In particular, the chromatin immunoprecipitation with massively parallel DNA sequencing technology (ChIP-Seq) can enable us to measure the in vivo genome-wide occupancy of the DNA-binding protein of interest in a single run. Multiple ChIP-Seq runs thus inherent the potential for us to decipher the combinatorial occupancies of multiple DNA-binding proteins. To handle the genome-wide signal profiles from those multiple runs, we propose to integrate regularized regression functions (i.e., LASSO, Elastic Net, and Ridge Regression) into the well-established SignalRanker and FullSignalRanker frameworks, resulting in six additional probabilistic models for inference on multiple normalized genome-wide signal profiles. The corresponding model training algorithms are devised with computational complexity analysis. Comprehensive benchmarking is conducted to demonstrate and compare the performance of nine related probabilistic models on the ENCODE ChIP-Seq datasets. The results indicate that the regularized SignalRanker models, in contrast to the original SignalRanker models, can demonstrate excellent inference performance comparable to the FullSignalRanker models with low model complexities and time complexities. Such a feature is especially valuable in the context of the rapidly growing genome-wide signal profile data in the recent years.",2017,IEEE Transactions on NanoBioscience
A New Variable Selection Approach Inspired by Supersaturated Designs Given a Large-Dimensional Dataset,"The problem of variable selection is fundamental to statistical modelling in diverse fields of sciences. In this paper, we study in particular the problem of selecting important variables in regression problems in the case where observations and labels of a real-world dataset are available. At first, we examine the performance of several existing statistical methods for analyzing a real large trauma dataset which consists of 7000 observations and 70 factors, that include demographic, transport and intrahospital data. The statistical methods employed in this work are the nonconcave penalized likelihood methods (SCAD, LASSO, and Hard), the generalized linear logistic regression, and the best subset variable selection (with AIC and BIC), used to detect possible risk factors of death. Supersaturated designs (SSDs) are a large class of factorial designs which can be used for screening out the important factors from a large set of potentially active variables. This paper presents a new variable selection approach inspired by supersaturated designs given a dataset of observations. The merits and the effectiveness of this approach for identifying important variables in observational studies are evaluated by considering several two-levels supersaturated designs, and a variety of different statistical models with respect to the combinations of factors and the number of observations. The derived results are encouraging since the alternative approach using supersaturated designs provided specific information that are logical and consistent with the medical experience, which may also assist as guidelines for trauma management.",2014,Journal of data science
