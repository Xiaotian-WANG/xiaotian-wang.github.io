title,abstract,year,journal
Object Definition for Aboveground Biomass and Leaf Area Index Estimation,"Objects perform better than pixels when estimating aboveground biomass and Leaf Area Index (LAI) from hyperspectral data. An essential question though, is how to define the objects. With per-pixel classification all observation units are square pixels of fixed dimensions. With objectoriented the observation units (objects) are groups of contiguous pixels. The objects can be defined either by stratification, i.e. based on an external variable (e.g. soil type), or by segmentation, i.e. based on spectral similarity. In the latter case, the maximal spectral heterogeneity defined by the user, determines the size and shape of the resulting objects. It is therefore crucial to find the optimal heterogeneity. Furthermore, the bands included in the segmentation process will have a significant effect as well. This paper aims at finding the band combination that results in optimal predictions. Optimal being defined as lowest prediction errors. A field campaign was hold to collect biomass and LAI data for over 200 plots in the Peyne catchment in southern France. These data were linked to a HyMap image. The optimal band combination will be found with Lasso regression, which is a shrinkage method. These methods put constraints not only on the prediction error, but also on the size of the regression coefficients. Particularly with intercorrelated variables this is a valuable addition since it prohibits extreme coefficient values as one obtains with regular multiple regression. Lasso regression is a special type of shrinkage method since the coefficients of less significant variables are put to zero, so band selection is performed during the analysis. The bands that are found to be significant can be used in the image segmentation, for which band selection is a prerequisite, since no selection takes place during the process. Results indicate that different object sizes do influence prediction accuracy, that those sizes are different for aboveground biomass and LAI and that the bands used in prediction are different as well.",2007,
PET/CT radiomics in breast cancer: promising tool for prediction of pathological response to neoadjuvant chemotherapy,"PurposeTo assess the role of radiomics parameters in predicting pathological complete response (pCR) to neoadjuvant chemotherapy (NAC) in patients with locally advanced breast cancer.MethodsSeventy-nine patients who had undergone pretreatment staging 18F-FDG PET/CT and treatment with NAC between January 2010 and January 2018 were included in the study. Primary lesions on PET images were delineated, and extraction of first-, second-, and higher-order imaging features was performed using LIFEx software. The relationship between these parameters and pCR to NAC was analyzed by multiple logistic regression models.ResultsNineteen patients (24%) had pCR to NAC. Different models were generated on complete information and imputed datasets, using univariable and multivariable logistic regression and least absolute shrinkage and selection operator (lasso) regression. All models could predict pCR to NAC, with area under the curve values ranging from 0.70 to 0.73. All models agreed that tumor molecular subtype is the primary predictor of the primary endpoint.ConclusionsOur models predicted that patients with subtype 2 and subtype 3 (HER2+ and triple negative, respectively) are more likely to have a pCR to NAC than those with subtype 1 (luminal). The association between PET imaging features and pCR suggested that PET imaging features could be considered as potential predictors of pCR in locally advanced breast cancer patients.",2019,European Journal of Nuclear Medicine and Molecular Imaging
Self-paced Mixture of Regressions,"Mixture of regressions (MoR) is the wellestablished and effective approach to model discontinuous and heterogeneous data in regression problems. Existing MoR approaches assume smooth joint distribution for its good anlaytic properties. However, such assumption makes existing MoR very sensitive to intra-component outliers (the noisy training data residing in certain components) and the inter-component imbalance (the different amounts of training data in different components). In this paper, we make the earliest effort on Self-paced Learning (SPL) in MoR, i.e., Self-paced mixture of regressions (SPMoR) model. We propose a novel selfpaced regularizer based on the Exclusive LASSO, which improves inter-component balance of training data. As a robust learning regime, SPL pursues confidence sample reasoning. To demonstrate the effectiveness of SPMoR, we conducted experiments on both the sythetic examples and real-world applications to age estimation and glucose estimation. The results show that SPMoR outperforms the stateof-the-arts methods.",2017,
New Robust Variable Selection Methods for Linear Regression Models,"type=""main"" xml:id=""sjos12057-abs-0001""> Motivated by an entropy inequality, we propose for the first time a penalized profile likelihood method for simultaneously selecting significant variables and estimating unknown coefficients in multiple linear regression models in this article. The new method is robust to outliers or errors with heavy tails and works well even for error with infinite variance. Our proposed approach outperforms the adaptive lasso in both theory and practice. It is observed from the simulation studies that (i) the new approach possesses higher probability of correctly selecting the exact model than the least absolute deviation lasso and the adaptively penalized composite quantile regression approach and (ii) exact model selection via our proposed approach is robust regardless of the error distribution. An application to a real dataset is also provided.",2014,Scandinavian Journal of Statistics
"Multi-modality neuroimaging brain-age in UK Biobank: relationship to biomedical, lifestyle and cognitive factors","The brain-age paradigm is proving increasingly useful for exploring aging-related disease and can predict important future health outcomes. Most brain-age research uses structural neuroimaging to index brain volume. However, aging affects multiple aspects of brain structure and function, which can be examined using multimodality neuroimaging. Using UK Biobank, brain-age was modeled in nÂ = 2205 healthy people with T1-weighted MRI, T2-FLAIR, T2âˆ—, diffusion-MRI, task fMRI, and resting-state fMRI. In a held-out healthy validation set (nÂ = 520), chronological age was accurately predicted (rÂ = 0.78, mean absolute errorÂ = 3.55Â years) using LASSO regression, higher than using any modality separately. Thirty-four neuroimaging phenotypes were deemed informative by the regression (after bootstrapping); predominantly gray-matter volume and white-matter microstructure measures. When applied to new individuals from UK Biobank (nÂ = 14,701), significant associations with multimodality brain-predicted age difference (brain-PAD) were found for stroke history, diabetes diagnosis, smoking, alcohol intake and some, but not all, cognitive measures (corrected p < 0.05). Multimodality neuroimaging can improve brain-age prediction, and derived brain-PAD values are sensitive to biomedical and lifestyle factors that negatively impact brain and cognitive health.",2020,Neurobiology of aging
Identification and Validation of an Individualized Prognostic Signature of Bladder Cancer Based on Seven Immune Related Genes,"Background There has been no report of prognostic signature based on immune-related genes (IRGs). This study aimed to develop an IRG-based prognostic signature that could stratify patients with bladder cancer (BLCA). Methods RNA-seq data along with clinical information on BLCA were retrieved from the Cancer Genome Atlas (TCGA) and gene expression omnibus (GEO). Based on TCGA dataset, differentially expressed IRGs were identified via Wilcoxon test. Among these genes, prognostic IRGs were identified using univariate Cox regression analysis. Subsequently, we split TCGA dataset into the training (n = 284) and test datasets (n = 119). Based on the training dataset, we built a least absolute shrinkage and selection operator (LASSO) penalized Cox proportional hazards regression model with multiple prognostic IRGs. It was validated in the training dataset, test dataset, and external dataset GSE13507 (n = 165). Additionally, we accessed the six types of tumor-infiltrating immune cells from Tumor Immune Estimation Resource (TIMER) website and analyzed the difference between risk groups. Further, we constructed and validated a nomogram to tailor treatment for patients with BLCA. Results A set of 47 prognostic IRGs was identified. LASSO regression and identified seven BLCA-specific prognostic IRGs, i.e., RBP7, PDGFRA, AHNAK, OAS1, RAC3, EDNRA, and SH3BP2. We developed an IRG-based prognostic signature that stratify BLCA patients into two subgroups with statistically different survival outcomes [hazard ratio (HR) = 10, 95% confidence interval (CI) = 5.6â€“19, P < 0.001]. The ROC curve analysis showed acceptable discrimination with AUCs of 0.711, 0.754, and 0.772 at 1-, 3-, and 5-year follow-up respectively. The predictive performance was validated in the train set, test set, and external dataset GSE13507. Besides, the increased infiltration of CD4+ T cells, CD8+ T cells, macrophage, neutrophil, and dendritic cells in the high-risk group (as defined by the signature) indicated chronic inflammation may reduce the survival chances of BLCA patients. The nomogram demonstrated to be clinically-relevant and effective with accurate prediction and positive net benefit. Conclusion The present immune-related signature can effectively classify BLCA patients into high-risk and low-risk groups in terms of survival rate, which may help select high-risk BLCA patients for more intensive treatment.",2020,Frontiers in Genetics
Sparse Regression with Multi-type Regularized Feature Modeling.,"Within the statistical and machine learning literature, regularization techniques are often used to construct sparse (predictive) models. Most regularization strategies only work for data where all predictors are of the same type, such as Lasso regression for continuous predictors. However, many predictive problems involve different predictor types. We propose a multi-type Lasso penalty that acts on the objective function as a sum of subpenalties, one for each predictor type. As such, we perform predictor selection and level fusion within a predictor in a data-driven way, simultaneous with the parameter estimation process. We develop a new estimation strategy for convex predictive models with this multi-type penalty. Using the theory of proximal operators, our estimation procedure is computationally efficient, partitioning the overall optimization problem into easier to solve subproblems, specific for each predictor type and its associated penalty. The proposed SMuRF algorithm improves on existing solvers in both accuracy and computational efficiency. This is demonstrated with an extensive simulation study and the analysis of a case-study on insurance pricing analytics.",2018,arXiv: Computation
Identification of miRNA profiling in prediction of tumor recurrence and progress and bioinformatics analysis for patients with primary esophageal cancer: Study based on TCGA database.,"OBJECT
This study focused on the identification of prognostic miRNAs for the prediction of tumor recurrence and progress in esophageal cancer.


METHODS
MiRNA profiling and clinical characteristics of esophageal cancer patients was downloaded from the TCGA database. Univariate analysis was performed to select potential prognostic miRNAs and covariates. LASSO based logistic regression was conducted to identify the prognostic miRNAs given covariates. Bioinformatics analysis including gene ontology, disease ontology and pathway enrichment analysis were performed. A nomogram was generated based on multivariate logistic regression to illustrate the association between the identified miRNAs and the risk of tumor recurrence and progress.


RESULTS
A total of 1881 miRNAs and 10 clinical characteristics were obtained from TCGA database. 18 miRNAs were finally identified in which 6 miRNAs were identified for the first time to be associated with the tumor recurrence and progress of esophageal cancer given covariates. Bioinformatics analysis suggested that the identified miRNAs were associated with the tumor recurrence and progress of esophageal cancer. The association between identified miRNAs and risk of tumor recurrence and progress were presented in a nomogram.


CONCLUSION
The 6 newly identified miRNAs may be potential biomarkers for the prediction of tumor recurrence and progress of esophageal cancer.",2018,"Pathology, research and practice"
A 7-lncRNA signature associated with the prognosis of colon adenocarcinoma,"Background
Colon adenocarcinoma (COAD) is the most common colon cancer exhibiting high mortality. Due to their association with cancer progression, long noncoding RNAs (lncRNAs) are now being used as prognostic biomarkers. In the present study, we used relevant clinical information and expression profiles of lncRNAs originating from The Cancer Genome Atlas database, aiming to construct a prognostic lncRNA signature to estimate the prognosis of patients.


Methods
The samples were randomly spilt into training and validation cohorts. In the training cohort, prognosis-related lncRNAs were selected from differentially expressed lncRNAs using the univariate Cox analysis. Furthermore, the least absolute shrinkage and selection operator (LASSO) regression and multivariate Cox analysis were employed for identifying prognostic lncRNAs. The prognostic signature was constructed by these lncRNAs.


Results
The prognostic model was able to calculate each COAD patient's risk score and split the patients into groups of low and high risks. Compared to the low-risk group, the high-risk group had significant poor prognosis. Next, the prognostic signature was validated in the validation, as well as all cohorts. The receiver operating characteristic (ROC) curve and c-index were determined in all cohorts. Moreover, these prognostic lncRNA signatures were combined with clinicopathological risk factors to construct a nomogram for predicting the prognosis of COAD in the clinic. Finally, seven lncRNAs (CTC-273B12.10, AC009404.2, AC073283.7, RP11-167H9.4, AC007879.7, RP4-816N1.7, and RP11-400N13.2) were identified and validated by different cohorts. The Kyoto Encyclopedia of Genes and Genomes analysis of the mRNAs co-expressed with the seven prognostic lncRNAs suggested four significantly upregulated pathways, which were AGE-RAGE, focal adhesion, ECM-receptor interaction, and PI3K/Akt signaling pathways.


Conclusion
Thus, our study verified that the seven lncRNAs mentioned can be used as biomarkers to predict the prognosis of COAD patients and design personalized treatments.",2020,PeerJ
Validation of prediction models based on lasso regression with multiply imputed data,"BackgroundIn prognostic studies, the lasso technique is attractive since it improves the quality of predictions by shrinking regression coefficients, compared to predictions based on a model fitted via unpenalized maximum likelihood. Since some coefficients are set to zero, parsimony is achieved as well. It is unclear whether the performance of a model fitted using the lasso still shows some optimism. Bootstrap methods have been advocated to quantify optimism and generalize model performance to new subjects. It is unclear how resampling should be performed in the presence of multiply imputed data.MethodThe data were based on a cohort of Chronic Obstructive Pulmonary Disease patients. We constructed models to predict Chronic Respiratory Questionnaire dyspnea 6 months ahead. Optimism of the lasso model was investigated by comparing 4 approaches of handling multiply imputed data in the bootstrap procedure, using the study data and simulated data sets. In the first 3 approaches, data sets that had been completed via multiple imputation (MI) were resampled, while the fourth approach resampled the incomplete data set and then performed MI.ResultsThe discriminative model performance of the lasso was optimistic. There was suboptimal calibration due to over-shrinkage. The estimate of optimism was sensitive to the choice of handling imputed data in the bootstrap resampling procedure. Resampling the completed data sets underestimates optimism, especially if, within a bootstrap step, selected individuals differ over the imputed data sets. Incorporating the MI procedure in the validation yields estimates of optimism that are closer to the true value, albeit slightly too larger.ConclusionPerformance of prognostic models constructed using the lasso technique can be optimistic as well. Results of the internal validation are sensitive to how bootstrap resampling is performed.",2014,BMC Medical Research Methodology
A Comparison of the Lasso and Marginal Regression,"The lasso is an important method for sparse, high-dimensional regression problems, with efficient algorithms available, a long history of practical success, and a large body of theoretical results supporting and explaining its performance. But even with the best available algorithms, finding the lasso solutions remains a computationally challenging task in cases where the number of covariates vastly exceeds the number of data points. 
 
Marginal regression, where each dependent variable is regressed separately on each covariate, offers a promising alternative in this case because the estimates can be computed roughly two orders faster than the lasso solutions. The question that remains is how the statistical performance of the method compares to that of the lasso in these cases. 
 
In this paper, we study the relative statistical performance of the lasso and marginal regression for sparse, high-dimensional regression problems. We consider the problem of learning which coefficients are non-zero. Our main results are as follows: (i) we compare the conditions under which the lasso and marginal regression guarantee exact recovery in the fixed design, noise free case; (ii) we establish conditions under which marginal regression provides exact recovery with high probability in the fixed design, noise free, random coefficients case; and (iii) we derive rates of convergence for both procedures, where performance is measured by the number of coefficients with incorrect sign, and characterize the regions in the parameter space recovery is and is not possible under this metric. 
 
In light of the computational advantages of marginal regression in very high dimensional problems, our theoretical and simulations results suggest that the procedure merits further study.",2012,J. Mach. Learn. Res.
A prognostic 4-gene expression signature for squamous cell lung carcinoma.,"Squamous cell lung carcinoma (SQCLC), a common and fatal subtype of lung cancer, caused lots of mortalities and showed different outcomes in prognosis. This study was to screen key genes and to figure a prognostic signature to cluster the patients with SQCLC. RNA-Seq data from 550 patients with SQCLC were downloaded from The Cancer Genome Atlas (TCGA). Genetically changed genes were identified and analyzed in univariate survival analysis. Genes significantly influencing prognosis were selected with frequency higher than 100 in lasso regression. Meanwhile, area under the curve (AUC) values and hazard ratios (HR) for seed genes were obtained with R Language. Functional enrichment analysis was performed and clustering effectiveness of the selected common gene set was analyzed with Kaplan-Meier. Finally, the stability and validity of the optimal clustering model were verified. A total of 7,222 genetically changed genes were screened, including 1,045 ones with pâ€‰<â€‰0.05, 1,746, pâ€‰<â€‰0.1, and 2,758, pâ€‰<â€‰0.2. The common gene sets with more than 100 frequencies were 14-Genes, 10-Genes and 6-Genes. Genes with pâ€‰<â€‰0.05 participated in positive regulation of ERK1 and ERK2 cascade, angiogenesis, platelet degranulation, cell-matrix adhesion, extracellular matrix organization, macrophage activation, and so on. A four-gene clustering model in 14-Genes (DPPA, TTTY16, TRIM58, HKDC1, ZNF589, ALDH7A1, LINC01426, IL19, LOC101928358, TMEM92, HRASLS, JPH1, LOC100288778, GCGR) was verified as the optimal. The discovery of four-gene clustering model in 14-Genes can cluster the patient samples effectively. This model would help predict the outcomes of patients with SQCLC then improve the treatment strategies.",2017,Journal of cellular physiology
Linear Models with R,Introduction Before You Start Initial Data Analysis When to Use Linear Modeling History Estimation Linear Model Matrix Representation Estimating b Least Squares Estimation Examples of Calculating b Example QR Decomposition Gauss-Markov Theorem Goodness of Fit Identifiability Orthogonality Inference Hypothesis Tests to Compare Models Testing Examples Permutation Tests Sampling Confidence Intervals for b Bootstrap Confidence Intervals Prediction Confidence Intervals for Predictions Predicting Body Fat Autoregression What Can Go Wrong with Predictions? Explanation Simple Meaning Causality Designed Experiments Observational Data Matching Covariate Adjustment Qualitative Support for Causation Diagnostics Checking Error Assumptions Finding Unusual Observations Checking the Structure of the Model Discussion Problems with the Predictors Errors in the Predictors Changes of Scale Collinearity Problems with the Error Generalized Least Squares Weighted Least Squares Testing for Lack of Fit Robust Regression Transformation Transforming the Response Transforming the Predictors Broken Stick Regression Polynomials Splines Additive Models More Complex Models Model Selection Hierarchical Models Testing-Based Procedures Criterion-Based Procedures Summary Shrinkage Methods Principal Components Partial Least Squares Ridge Regression Lasso Insurance Redlining-A Complete Example Ecological Correlation Initial Data Analysis Full Model and Diagnostics Sensitivity Analysis Discussion Missing Data Types of Missing Data Deletion Single Imputation Multiple Imputation Categorical Predictors A Two-Level Factor Factors and Quantitative Predictors Interpretation with Interaction Terms Factors with More than Two Levels Alternative Codings of Qualitative Predictors One Factor Models The Model An Example Diagnostics Pairwise Comparisons False Discovery Rate Models with Several Factors Two Factors with No Replication Two Factors with Replication Two Factors with an Interaction Larger Factorial Experiments Experiments with Blocks Randomized Block Design Latin Squares Balanced Incomplete Block Design Appendix: About R Bibliography Index,2014,
Penalized regression combining the L1 norm and a correlation based penalty,"We consider the problem of feature selection in linear regression model with p covariates and n observations. We propose a new method to simultaneously select variables and favor a grouping effect, where strongly correlated predictors tend to be in or out of the model together. The method is based on penalized least squares with a penalty function that combines the L1 and a Correlation based Penalty (CP) norms. We call it L1CP method. Like the Lasso penalty, L1CP shrinks some coefficients to exactly zero and additionally, the CP term explicitly links strength of penalization to the correlation among predictors. A detailed simulation study in small and high dimensional settings is performed. It illustrates the advantages of our approach compared to several alternatives. Finally, we apply the methodology to two real data sets: US Crime Data and GC-Retention PAC data. In terms of prediction accuracy and estimation error, our empirical study suggests that the L1CP is more adapted than the Elastic-Net to situations where p â‰¤ n (the number of variables is less or equal to the sample size). If p â‰« n, our method remains competitive and also allows the selection of more than n variables.",2014,Sankhya
Prediction of Critical Clearing Time for Transient Stability Based on Ensemble Extreme Learning Machine Regression Model,"To improve the accuracy of critical clearing time (CCT) prediction model, an ensemble regression learning method is employed, and a CCT prediction method based on the ensemble extreme learning machine regression model is proposed. First, the static power flow values are utilized as the original feature set, and the least absolute shrinkage selection operator (Lasso) is used for determining the input features. The extreme learning machine, regarded as the base learner, is used to establish the mapping relationship between the input features and CCT value. On this basis, the AdaBoost.RT algorithm is employed as the ensemble regression learning framework to construct multiple base learners. When performing CCT prediction, the predicted values of each base learner are weight integration to obtain the final prediction result. Simulation results on New England 39-bus system demonstrate the effectiveness of the proposed method.",2019,2019 IEEE Innovative Smart Grid Technologies - Asia (ISGT Asia)
Variable selection. Editorial.,"In the past 10 years, the development of statistical methods has been greatly shaped by the demand of analyzing high-dimensional data generated in the fields of biological and medical sciences. Variable selection is a fundamental task in analyzing modern high-dimensional data. Many papers have been devoted to this important topic. Methods such as LASSO and SCAD are now household names in the literature. This special issue comprises five invited contributions from some leading experts in this area, with emphasis on solving interesting application problems. BÃ¼hlmann, RÃ¼timann and Kalisch provided a very nice review article on two widely applicable variable selection techniques: stability selection and aggregated multiple p-values. Both techniques are based on sub-sampling and aim to control false positive selections in observational data analysis. Lu, Zhang and Zeng presented a new variable selection method that is designed to identify important variables involved in optimal treatment decision-making. Their method is based on a penalized regression framework and can be easily implemented by existing software packages. The new method is illustrated with an application to an AIDS clinical trial. Jiang, Huang and Zhang studied a new cross-validated area under the ROC curve (CV-AUC) criterion for tuning parameter selection for sparely penalized logistic regression. The CV-AUC criterion is specifically designed for optimizing the classification performance for binary outcome data. It is shown that CV-AUC outperforms other popular competitors such as AIC, BIC or E-BIC. CV-AUC is used to select genes related to cancer based on microarray data. The survey paper by Zhang and Lin focuses on six important properties for high-dimension-lowsample-size classification problems: predictability, consistency, generality, stochastic stability, robustness and interpretability/sparsity. The authors reviewed several popular classifiers and compared their performance on simulated and real data. Li and Tibshirani proposed a new method for the identification of features that are associated with an outcome variable in RNA-Seq data. Their method is non-parametric and hence more robust than those parametric competitors that are based on Poisson or negative-binomial models. The new method is general enough to be applied to data with quantitative, survival, two-class or multipleclass outcomes. We hope that the readers find this special issue interesting and inspiring. Finally, we want to thank all authors and referees for their enthusiastic support.",2013,Statistical methods in medical research
The Clinical Picture of Psychosis in Manifest Huntington's Disease: A Comprehensive Analysis of the Enroll-HD Database,"Background: Psychotic symptoms have been under-investigated in Huntington's disease (HD) and research is needed in order to elucidate the characteristics linked to the unique phenotype of HD patients presenting with psychosis. Objective: To evaluate the frequency and factors associated with psychosis in HD. Methods: Cross-sectional study including manifest individuals with HD from the Enroll-HD database. Both conventional statistical analysis (Stepwise Binary Logistic Regression) and five machine learning algorithms [Least Absolute Shrinkage and Selection Operator (LASSO); Elastic Net; Support Vector Machines (SVM); Random Forest; and class-weighted SVM] were used to describe factors associated with psychosis in manifest HD patients. Results: Approximately 11% of patients with HD presented history of psychosis. Logistic regression analysis indicated that younger age at HD clinical diagnosis, lower number of CAG repeats, history of [alcohol use disorders, depression, violent/aggressive behavior and perseverative/obsessive behavior], lower total functional capacity score, and longer time to complete trail making test-B were associated with psychosis. All machine learning algorithms were significant (chi-square p < 0.05) and capable of distinguishing individual HD patients with history of psychosis from those without a history of psychosis with prediction accuracy around 71-73%. The most relevant variables were similar to those found in the conventional analyses. Conclusions: Psychiatric and behavioral symptoms as well as poorer cognitive performance were related to psychosis in HD. In addition, psychosis was associated with lower number of CAG repeats and younger age at clinical diagnosis of HD, suggesting that these patients may represent a unique phenotype in the HD spectrum.",2018,Frontiers in Neurology
A Family of Penalty Functions for Structured Sparsity,"We study the problem of learning a sparse linear regression vector under additional conditions on the structure of its sparsity pattern. We present a family of convex penalty functions, which encode this prior knowledge by means of a set of constraints on the absolute values of the regression coefficients. This family subsumes the li norm and is flexible enough to include different models of sparsity patterns, which are of practical and theoretical importance. We establish some important properties of these functions and discuss some examples where they can be computed explicitly. Moreover, we present a convergent optimization algorithm for solving regularized least squares with these penalty functions. Numerical simulations highlight the benefit of structured sparsity and the advantage offered by our approach over the Lasso and other related methods.",2010,
A CONSTRUCTIVE APPROACH TO ` 0-PENALIZED REGRESSION By,"We propose a constructive approach to estimating sparse, highdimensional linear regression models. The approach is a computational algorithm motivated from the KKT conditions for the `0-penalized least squares solutions. It generates a sequence of solutions iteratively, based on support detection using primal and dual information and root finding. We refer to the algorithm as SDAR for brevity. Under a sparse Riesz condition on the design matrix and certain other conditions, we show that with high probability, the `2 estimation error of the solution sequence decays exponentially to the minimax error bound in O(log(R âˆš J)) iterations, where J is the number of important predictors and R is the relative magnitude of the nonzero target coefficients; and under a mutual coherence condition and certain other conditions, the `âˆž estimation error decays to the optimal error bound in O(log(R)) iterations. Moreover the SDAR solution recovers the oracle least squares estimator within a finite number of iterations with high probability if the sparsity level is known. Computational complexity analysis shows that the cost of SDAR is O(np) per iteration. We also consider an adaptive version of SDAR for use in practical applications where the true sparsity level is unknown. Simulation studies demonstrate that SDAR outperforms Lasso, MCP and two greedy methods in accuracy and efficiency.",2018,
Applications of L1 regularisation,"The lasso algorithm for variable selection in linear models, introduced by Tibshirani, works by imposing an $l_1$~norm bound constraint on the variables in a least squares model and then tuning the model estimation calculation using this bound. This introduction of the bound is interpreted as a form of regularisation step. It leads to a form of quadratic program which is solved by a straight-forward modification of a standard active set algorithm for each value of this bound. Considerable interest was generated by the discovery that the complete solution trajectory parametrised by this bound is piecewise linear and can be calculated very efficiently. Essentially it takes no more work than the solution of either the unconstrained least squares problem or the quadratic program at a single bound value. This has resulted in the study both of the selection problem for different objective and constraint choices and of applications to such areas as data compression and the generation of sparse solutions of very under-determined systems. One important class of generalisation is to quantile regression estimation problems. The original continuation idea extends to these polyhedral objectives in an interesting two phase procedure which involves both the constrained and Lagrangian forms of the problem at each step. However, it is significantly less computationally effective than is the original algorithm for least squares objectives. In contrast, the piecewise linear estimation problem can be solved for each value of the $l_1$~bound by a relatively efficient simplicial descent algorithm, and that this can be used to explore trajectory information in a manner which is at least competitive with the homotopy algorithm in this context. The form of line search used in the descent steps has an important bearing on the effectiveness of the algorithm. A comparison is given between the relative performance of the simplicial descent algorithm used and an interior point method on the piecewise linear estimation problem. 
 
 References I. Barrodale and F. D. K. Roberts. An improved algorithm for $l_1$ linear approximation. SIAM J. Numer. Anal. , 10:839--848, 1973. P. Bloomfield and W. L. Steiger. Least Absolute Deviations . Birkhauser, Boston, 1983. H. D. Bondell and B. J. Reich. Simultaneous regression shrinkage, variable selection, and supervising clustering of predictors with {OSCAR}. Biometrics , 64:115--123, 2008. doi:10.1111/j.1541-0420.2007.00843.x E. Candes and T. Tao. The Dantzig selector: statistical estimation when $p$ is much larger than $n$. Ann. Statist. , 35(6):2313--2351, 2007. doi:10.1214/009053606000001523 D. L. Donoho and Y. Tsaig. Fast solution of $l_1$-norm minimization problems when the solution may be sparse. IEEE Trans. Inf. Theory , 54:4789--4812, 2008. doi:10.1109/TIT.2008.929958 R. Koenker. Quantile Regression . Cambridge University Press, 2005. Y. Li and J. Zhu. {$L_1$}-norm quantile regression. J. Comp. Graph. Stat. , 17(1):163--185, 2008. doi:10.1198/106186008X289155 M. R. Osborne. Simplicial algorithms for minimizing polyhedral functions . Cambridge University Press, 2001. M. R. Osborne, Brett Presnell, and B. A. Turlach. On the Lasso and its dual. J. Comp. Graph. Stat. , 9(2):319--337, 2000. http://www.jstor.org/stable/1390657 M. R. Osborne and B. A. Turlach. A homotopy algorithm for the quantile regression lasso and related piecewise linear problems. J. Comp. Graph. Stat. , 2010. Accepted for publication. doi:10.1198/jcgs.2011.09184 S. Portnoy and R. Koenker. The Gaussian hare and the Laplacian tortoise: computability of squared error vs absolute error estimates. Stat. Sci. , 12:279--300, 1997. http://www.jstor.org/stable/2246216 S. Rosset and Ji Zhu. Piecewise linear regularised solution paths. Ann. Stat. , 35(3):1012--1030, 2007. doi:10.1214/009053606000001370 R. Tibshirani. Regression shrinkage and selection via the lasso. J. R. Stat. Soc., Ser. B , 58(1):267--288, 1996. http://www.jstor.org/stable/2346178 B. A. Turlach, W. N. Venables, and S. J. Wright. Simultaneous variable selection. Technometrics , 47(3):349--363, 2005. doi:10.1198/004017005000000139 Y. Yao and Y. Lee. Another look at linear programming for feature selection via methods of regularization. Technical Report 800, Department of Statistics, Ohio State University, 2007. M. Yuan and H. Zou. Efficient global approximation of generalised nonlinear {$\ell _1$} regularised solution paths and its applications. J. Am. Stat. Assoc. , 104:1562--1573, 2009. doi:10.1198/jasa.2009.tm08287 J. Zhu, T. Hastie, S. Rosset, and R. Tibshirani. $l_1$-norm support vector machines. Adv. Neural Inf. Process. Syst. , 16:49--56, 2004. http://books.nips.cc/papers/files/nips16/NIPS2003_AA07.pdf H. Zou and T. Hastie. Regularization and variable selection via the elastic net. J. R. Stat. Soc., Ser. B , 67:301--320, 2005. http://www.jstor.org/stable/3647580 H. Zou and M. Yuan. Regularised simultaneous model selection in multiple quantiles regression. Comp. Stat. Data Anal. , 52:5296--5304, 2008. doi:10.1016/j.csda.2008.05.013",2011,Anziam Journal
From Mineralogy to Petrography: Study on the Applicability of Machine Learning,"This thesis aims to understand if it is possible to apply machine learning techniques to predict the petrographical composition of a sample of soil given its mineralogical structure. The problem formalization requires predicting, from a set of continuous attributes (mineralogical constitution), a set of continuous features (petrographical constitution). The soil samples we have at disposal, for which both mineralogical and petrographical composition are known, present two challenges: their limited quantity and the non-uniform distribution of values of their features. These distributions are gaussian or exponential. We have addressed the prediction task with both regression and classification techniques. In the second case, we have discretized petrographical attributes using a custom methodology. These procedure maps the continuous domain of the feature into a discrete one represented by intervals of values. We have used these intervals as class labels. This allows classifying a sample by indicating its possible range of values. We applied Linear, Lasso, Ridge and Support Vector Machine (SVR) as regression techniques. The classification method used is the Decision Tree. Each attribute prediction is treated as a disjointed problem: all mineralogical attributes are used to predict one petrographical feature. Furthermore, we tested a custom third approach exploiting Non-Negative Matrix Factorization (NMF) technique, which jointly forecasts all petrographical features using all mineralogical attributes. This custom methodology is considered as a regression for what concerns performances evaluation. We have designed a way to evaluate the ability of regression models to predict rare values: it consists of two visual metrics called regression-precision and regression-recall inspired by precision and recall classification metrics. In order to compare regression and classifications predictions, we discretized continuous outcomes of regression techniques using the same intervals adopted for classification labels. After this operation, we confront predictions through metrics proper of classification: precision, recall and F-measure. Results show that models perform better on exponentially distributed petrographical attributes. Linear, Lasso and Ridge regressions are more promising than Decision Tree. NMF and SVR behave the worst. In the future we will focus on the possibility to exploit some relations that partially link petrographical composition with mineralogical one, with the help of a domain expert, trying in such a way to fulfill with domain knowledge the lack of data. However, although we still have not achieved final results, our preliminary analysis reveal that there is the possibility to successfully apply regression techniques for this kind of soil analysis.",2018,
A Partial Least Squares based algorithm for parsimonious variable selection,"BackgroundIn genomics, a commonly encountered problem is to extract a subset of variables out of a large set of explanatory variables associated with one or several quantitative or qualitative response variables. An example is to identify associations between codon-usage and phylogeny based definitions of taxonomic groups at different taxonomic levels. Maximum understandability with the smallest number of selected variables, consistency of the selected variables, as well as variation of model performance on test data, are issues to be addressed for such problems.ResultsWe present an algorithm balancing the parsimony and the predictive performance of a model. The algorithm is based on variable selection using reduced-rank Partial Least Squares with a regularized elimination. Allowing a marginal decrease in model performance results in a substantial decrease in the number of selected variables. This significantly improves the understandability of the model. Within the approach we have tested and compared three different criteria commonly used in the Partial Least Square modeling paradigm for variable selection; loading weights, regression coefficients and variable importance on projections. The algorithm is applied to a problem of identifying codon variations discriminating different bacterial taxa, which is of particular interest in classifying metagenomics samples. The results are compared with a classical forward selection algorithm, the much used Lasso algorithm as well as Soft-threshold Partial Least Squares variable selection.ConclusionsA regularized elimination algorithm based on Partial Least Squares produces results that increase understandability and consistency and reduces the classification error on test data compared to standard approaches.",2011,Algorithms for Molecular Biology : AMB
Combinatorial selection and least absolute shrinkage via the Clash algorithm,"The least absolute shrinkage and selection operator (LASSO) for linear regression exploits the geometric interplay of the â„“2-data error objective and the â„“1-norm constraint to arbitrarily select sparse models. Guiding this uninformed selection process with sparsity models has been precisely the center of attention over the last decade in order to improve learning performance. To this end, we alter the selection process of LASSO to explicitly leverage combinatorial sparsity models (CSMs) via the combinatorial selection and least absolute shrinkage (Clash) operator. We provide concrete guidelines how to leverage combinatorial constraints within Clash, and characterize CLASH's guarantees as a function of the set restricted isometry constants of the sensing matrix. Finally, our experimental results show that Clash can outperform both LASSO and model-based compressive sensing in sparse estimation.",2012,2012 IEEE International Symposium on Information Theory Proceedings
Selection of components and degrees of smoothing via lasso in high dimensional nonparametric additive models,"This paper proposes a procedure for selecting components and degrees of smoothing in high dimensional nonparametric additive models. In the procedure, different components have different penalties, and all the smoothing parameters in one component have the same penalties. The idea is similar to, but in fact different from, Wang et al.'s [Wang, H., Li, G.D., Tsai, C.L., 2007. Regression coefficient and autoregressive order shrinkage and selection via the lasso. Journal of the Royal Statistical Society, Series B 69, 63-78] modified lasso, which requires different penalties for different parameters. The procedure obtains the sequence of components according to the importance of these components by Efron et al.'s [Efron, B., Hastie, T., Johnstone, I., Tibshirani, R., 2004. Least angle regression. Annals of Statistics 32, 407-489] LARS. CV or BIC selector can be used to select the tuning parameters in the procedure, where some asymptotic properties are proved. Some simulation results and two examples are used to illustrate the procedure.",2008,Comput. Stat. Data Anal.
Landscape changes caused by high altitude ski-pistes affect bird species richness and distribution in the Alps,"Abstract There is a paucity of research on the wider landscape-level effects of ski-piste construction on alpine fauna. In this study, the response of alpine bird communities to the landscape changes induced by the construction of ski-pistes was investigated in the western Italian Alps. The aims were: (i) to test the hypothesis that ski-pistes have a detrimental effect on alpine grassland bird communities at a landscape-scale; and, (ii) to model local probability distributions of bird species according to different scenarios of ski-piste restoration and ski-piste proliferation above the treeline. Species richness and presence/absence data from point counts were analyzed in relation to GIS-derived landscape variables based on a 16Â ha hexagon grid. Predictive variables were selected through the LASSO model selection procedure, and logistic regression was used to estimate the probability of occurrence of each species in each hexagon. Grassland species richness, and probability of occurrence of water pipit, wheatear and black redstart, significantly decreased with increasing extent of ski-piste edge. Length of ski-piste edge was in the set of best models considering a large range of habitat and landscape predictors, and are therefore clearly features that exert a strong negative effect on high alpine grassland bird communities. Predictions of species occurrence were made by applying the models to different scenarios of habitat change. These showed predicted detrimental impacts of a relatively small 10% increase in ski-piste extent, but also that grassland restoration on existing ski-pistes could result in significantly increased occurrence rates of alpine grassland species. This study suggests that ski-pistes are perceived by birds as detrimental features of the alpine landscape. To minimize their impact, new, environmentallyâ€“friendly ways of constructing pistes should be developed, which could include habitat restoration and management to obtain a level of grass cover such that edges of ski-pistes are no longer perceived by birds.",2011,Biological Conservation
Negative Binomial Modelling and Applications for Microbiome Count Data,"The human microbiome plays an important role in human health and disease. Identification of factors that affect the microbiome composition will eventually allow modulation of the microbiome for therapeutic purposes. The aim of this study is to find a suitable statistics distribution model for the set of microbial operational taxonomic units (OTUs), which are used to categorize bacteria based on sequence similarity, and to use these models to analyze the supra-gingival and sub-gingival plaque microbiome. We model the OTU data with a Negative Binomial (NB) distribution and fit the maximum-likelihood estimates for the NB model parameters. We then develop a gamma-prior distribution to model the underlying composition of each OTU. We use the mean of the calculated posterior distribution as an estimator of the underlying composition of each OTU, analyzing oral cavity microbiome communities based on the posterior means. Likelihood ratio tests identified NB models for some OTUs that differed significantly between sub-gingival plaques and supra-gingival plaques. We also developed a NÃ¤Ä±ve Bayes Discriminant Analysis (NBDA) approach based on the calculated NB distributions, and performed LASSO regression on the simple proportions and the estimated underlying compositions. The NBDA and LASSO approaches identified OTUs that play a critical role in classification. By replacing simple proportions with distribution models, we explore the underlying composition of OTUs better without losing too much discriminant information.",2016,
Person's discriminating visual features for recognising gender: LASSO regression model and feature analysis,"Gender is one of the demographic attributes of a person, which is considered as a soft trait in the area of biometric. Several studies have been conducted to extract gender information based on a person's face image, gait pattern, fingerprint, iris, speech and hand geometry. In this paper, we concentrate on predicting gender using a person's image aesthetic, which has never been studied before. We propose a visual preference model for discriminating males from females using LASSO regression. The preference model uses 57 dimensional feature vector containing 14 different perceptual image features. The model is evaluated on a database of 34,000 images from 170 Flickr users (110 males and 60 females). Results show that maximum and average accuracy of predicting gender are around 91.67% and 84.38%, respectively, on 100 random sampling of training and testing datasets. The proposed method outperforms all existing state-of-the-art methods. In this paper, we also address two important research questions: which f...",2017,IJBM
Comprehensive analysis of a ceRNA network reveals potential prognostic cytoplasmic lncRNAs involved in HCC progression,"The aberrant expression of long noncoding RNAs (lncRNAs) has drawn increasing attention in the field of hepatocellular carcinoma (HCC) biology. In the present study, we obtained the expression profiles of lncRNAs, microRNAs (miRNAs), and messenger RNAs (mRNAs) in 371 HCC tissues and 50 normal tissues from The Cancer Genome Atlas (TCGA) and identified hepatocarcinogenesis-specific differentially expressed genes (DEGs, log fold changeâ€‰â‰¥â€‰2, FDR < 0.01), including 753 lncRNAs, 97 miRNAs, and 1,535 mRNAs. Because the specific functions of lncRNAs are closely related to their intracellular localizations and because the cytoplasm is the main location for competitive endogenous RNA (ceRNA) action, we analyzed not only the interactions among these DEGs but also the distributions of lncRNAs (cytoplasmic, nuclear or both). Then, an HCC-associated deregulated ceRNA network consisting of 37 lncRNAs, 10 miRNAs, and 26 mRNAs was constructed after excluding those lncRNAs located only in the nucleus. Survival analysis of this network demonstrated that 15 lncRNAs, 3 miRNAs, and 16 mRNAs were significantly correlated with the overall survival of HCC patients (pâ€‰<â€‰0.01). Through multivariate Cox regression and lasso analysis, a risk score system based on 13 lncRNAs was constructed, which showed good discrimination and predictive ability for HCC patient survival time. This ceRNA network-construction approach, based on lncRNA distribution, not only narrowed the scope of target lncRNAs but also provided specific candidate molecular biomarkers for evaluating the prognosis of HCC, which will help expand our understanding of the ceRNA mechanisms involved in the early development of HCC.",2019,Journal of Cellular Physiology
Sparse identification for nonlinear optical communication systems: SINO method.,"We introduce low complexity machine learning method method (based on lasso regression, which promotes sparsity, to identify the interaction between symbols in different time slots and to select the minimum number relevant perturbation terms that are employed) for nonlinearity mitigation. The immense intricacy of the problem calls for the development of ""smart"" methodology, simplifying the analysis without losing the key features that are important for recovery of transmitted data. The proposed sparse identification method for optical systems (SINO) allows to determine the minimal (optimal) number of degrees of freedom required for adaptive mitigation of detrimental nonlinear effects. We demonstrate successful application of the SINO method both for standard fiber communication links (over 3 dB gain) and for few-mode spatial-division-multiplexing systems.",2016,Optics express
Multiple Regression Methods Show Great Potential for Rare Variant Association Tests,"The investigation of associations between rare genetic variants and diseases or phenotypes has two goals. Firstly, the identification of which genes or genomic regions are associated, and secondly, discrimination of associated variants from background noise within each region. Over the last few years, many new methods have been developed which associate genomic regions with phenotypes. However, classical methods for high-dimensional data have received little attention. Here we investigate whether several classical statistical methods for high-dimensional data: ridge regression (RR), principal components regression (PCR), partial least squares regression (PLS), a sparse version of PLS (SPLS), and the LASSO are able to detect associations with rare genetic variants. These approaches have been extensively used in statistics to identify the true associations in data sets containing many predictor variables. Using genetic variants identified in three genes that were Sanger sequenced in 1998 individuals, we simulated continuous phenotypes under several different models, and we show that these feature selection and feature extraction methods can substantially outperform several popular methods for rare variant analysis. Furthermore, these approaches can identify which variants are contributing most to the model fit, and therefore both goals of rare variant analysis can be achieved simultaneously with the use of regression regularization methods. These methods are briefly illustrated with an analysis of adiponectin levels and variants in the ADIPOQ gene.",2012,PLoS ONE
Naive Feature Selection: Sparsity in Naive Bayes,"Due to its linear complexity, naive Bayes classification remains an attractive supervised learning method, especially in very large-scale settings. We propose a sparse version of naive Bayes, which can be used for feature selection. This leads to a combinatorial maximum-likelihood problem, for which we provide an exact solution in the case of binary data, or a bound in the multinomial case. We prove that our bound becomes tight as the marginal contribution of additional features decreases. Both binary and multinomial sparse models are solvable in time almost linear in problem size, representing a very small extra relative cost compared to the classical naive Bayes. Numerical experiments on text data show that the naive Bayes feature selection method is as statistically effective as state-of-the-art feature selection methods such as recursive feature elimination, $l_1$-penalized logistic regression and LASSO, while being orders of magnitude faster. For a large data set, having more than with $1.6$ million training points and about $12$ million features, and with a non-optimized CPU implementation, our sparse naive Bayes model can be trained in less than 15 seconds.",2019,ArXiv
