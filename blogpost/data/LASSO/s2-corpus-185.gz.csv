title,abstract,year,journal
Evaluation and Validation of Plasma Proteins Using Two Different Protein Detection Methods for Early Detection of Colorectal Cancer,"OBJECTIVE
Plasma protein biomarkers could be an efficient alternative for population-based screening for early detection of colorectal cancer (CRC). The objective of this study was to evaluate and validate plasma proteins individually and as a signature for early detection of CRC.


METHODS
In a three-stage design, proteins were measured firstly by liquid chromatography/multiple reaction monitoring-mass spectrometry (LC/MRM-MS) and later by proximity extension assay (PEA) in a discovery set consisting of 96 newly diagnosed CRC cases and 94 controls free of neoplasms at screening colonoscopy. Two algorithms (one for each measurement method) were derived by Lasso regression and .632+ bootstrap based on 11 proteins that were included in both the LC/MRM-MS and PEA measurements. Additionally, another algorithm was constructed from the same eleven biomarkers plus amphireglin, the most promising protein marker in the PEA measurements that had not been available from the LC/MRM-MS measurements. Lastly the three prediction signatures were validated with PEA in independent samples of participants of screening colonoscopy (CRC (n = 56), advanced adenoma (n = 101), and participants free of neoplasm (n = 102)).


RESULTS
The same four proteins were included in all three prediction signatures; mannan binding lectin serine protease 1, osteopontin, serum paraoxonase lactonase 3 and transferrin receptor protein 1, and the third prediction signature additionally included amphiregulin. In the independent validation set from a true screening setting, the five-marker blood-based signature including AREG presented areas under the curves of 0.82 (95% CI, 0.74-0.89), 0.86 (95% CI, 0.77-0.92) and 0.76 (95% CI, 0.64-0.86) for all, early and late stages CRC, respectively.


CONCLUSION
Two different measurement methods consistently identified four protein markers and an algorithm additionally including amphiregulin, a marker measured by PEA only, showed promising performance for detecting early stage CRC in an independent validation in a true screening setting. These proteins may be potential candidates for blood-based tests for early detection of CRC.",2019,Cancers
SCHMIDTMACâ€”a program to display and analyze directional data,"Abstract SCHMIDTMAC is a program to display orientation data on an equal-area projection. It has been developed in Pascal for use on a Apple Macintosh microcomputer; to make it user-friendly, extensive use has been made of the Macintosh graphic interface. It provides the main following facilities: (a) easy modification of a stereogram displayed on the screen, (b) possible identification (by clicking on a point, or by enclosing a group of points with a â€œlassoâ€) of displayed orientations, (c) calculation of orientation densities with a new grid, characterized by a variable number of nodes and apical angles of the counting cones, a net that allows variable smoothing of densities and â€œeconomicâ€ counting and contouring algorithms, and (d) statistical estimations of, for example, the best axis or best plane for simple distributions, or automatic set-partitioning and the best axis of each set of more complex distributions.",1989,Computers & Geosciences
Distribution Network Topology Reconstruction Method Based on Lasso and Its Supplementary Criterions,"In order to solve the problem of topology reconstruction in distribution network, a new data driven algorithm is proposed, which uses only the timing voltage to reconstruct the un-loopy and loopy distribution network topology without the prior knowledge. Firstly, the topology reconstruction problem is transformed into a convex optimization problem, and the Lasso regularization method is utilized to obtain a sparse correlation coefficient matrix (CCM), which represents the connectivity of the topology. Secondly, the â€œAndâ€ rule is employed to reduce the redundancy of CCM. And then the criterion of the voltage correlation analysis model is adopted as a supplemental criterion to reduce the error rate of CCM. Finally, the topology reconstruction of the distribution network is realized based on the accurate CCM. Simulation results show that the algorithm has high accuracy, universality and low computational complexity.",2017,
"Factory, Family and Neighbourhood: the Political Economy of Informal Labour in Sheffield","This article explores the experience of formal and informal steel labour in the contexts of the factory, the family, and the neighbourhood in â€˜Endcliffeâ€™, an ex-industrial district of Sheffield, UK. The article reiterates Claude Meillassouxâ€™s claim, in his book Maidens, meal and money, that the informal economy is an ideological space for the cheap reproduction of labour in the interests of capital. Nevertheless, it also examines subjective and ethnographic understandings of the meanings of â€˜capitalâ€™ and â€˜labourâ€™ and of the political nature of their shifting boundaries. In Endcliffe, capitalist subcontracting, state welfare, and economic policies of local regeneration have increased the informalization and casualization of steel labour and blurred the social spaces of the factory, the family, and the neighbourhood. The increased permeability between formal and informal economic processes and the re-embeddedness of production in the social and political texture of the neighbourhood tangles idioms of kinship and capitalist ideologies of production and turns the structural conflict between â€˜capitalâ€™ and â€˜labourâ€™ into a generational and gender conflict within the working class.The article shows that the â€˜New Labourâ€™ governmentâ€™s attempt to transform Britain into a post-industrial and classless society has paradoxically fostered the re-emergence of ancient modes of production and forms of bonded labour.",2005,
Forecasting and nowcasting emerging market GDP growth rates: The role of latent global economic policy uncertainty and macroeconomic data surprise factors,"In this paper, we assess the predictive content of latent economic policy uncertainty and data surprises factors for forecasting and nowcasting GDP using factor-type econometric models. Our analysis focuses on five emerging market economies, including Brazil, Indonesia, Mexico, South Africa, and Turkey; and we carry out a forecasting horse-race in which predictions from various different models are compared. These models may (or may not) contain latent uncertainty and surprise factors constructed using both local and global economic datasets. The set of models that we examine in our experiments includes both simple benchmark linear econometric models as well as dynamic factor models (DFMs) that are estimated using a variety of frequentist and Bayesian data shrinkage methods based on the least absolute shrinkage operator (LASSO). We find that the inclusion of our new uncertainty and surprise factors leads to superior predictions of GDP growth, particularly when these latent factors are constructed using Bayesian variants of the LASSO. Overall, our findings point to the importance of spillover effects from global uncertainty and data surprises, when predicting GDP growth in emerging market economies.",2020,Journal of Forecasting
Sparse Learning over Infinite Subgraph Features,"We present a supervised-learning algorithm from graph data (a set of graphs) for arbitrary twice-differentiable loss functions and sparse linear models over all possible subgraph features. To date, it has been shown that under all possible subgraph features, several types of sparse learning, such as Adaboost, LPBoost, LARS/LASSO, and sparse PLS regression, can be performed. Particularly emphasis is placed on simultaneous learning of relevant features from an infinite set of candidates. We first generalize techniques used in all these preceding studies to derive an unifying bounding technique for arbitrary separable functions. We then carefully use this bounding to make block coordinate gradient descent feasible over infinite subgraph features, resulting in a fast converging algorithm that can solve a wider class of sparse learning problems over graph data. We also empirically study the differences from the existing approaches in convergence property, selected subgraph features, and search-space sizes. We further discuss several unnoticed issues in sparse learning over all possible subgraph features.",2014,arXiv: Machine Learning
Diagnostic et prÃ©valence du syndrome mÃ©tabolique chez les diabÃ©tiques suivis dans un contexte de ressources limitÃ©es: cas du Burkina-Faso,"INTRODUCTION: les consequences du syndrome metabolique impliquent son diagnostic effectif pour une prise en charge globale des comorbidites depistees. Objectif : Determiner la capacite a diagnostiquer le syndrome metabolique en routine, sa prevalence chez les diabetiques, leurs connaissances et pratiques vis-a-vis du risque cardio-metabolique. METHODES: il s'est agi d'une etude transversale aupres de 388 diabetiques au CHU de Bobo-Dioulasso. Les criteres de la federation internationale du diabete (2009) ont ete utilises. RESULTATS: l'Ã¢ge moyen etait de 53,5Â±13,5 ans, le sex ratio de 0,7. L'obesite abdominale etait presente dans 61,9% des cas; L'HTA l'etait dans 56,4% des cas. La prescription du bilan lipidique a ete documentee dans 55,4% des cas pour le HDL et 56,2% pour les triglycerides pour un taux de realisation de 49,3% et 62,9%. Le taux de depistage des criteres lipidiques etait de 26,8%. Un taux de HDL bas a ete note dans 46 cas (43,4%) et une hypertriglyceridemie dans 24 cas (17,6%). In fine, la prevalence du syndrome metabolique etait de 48,9% (n=190). Seuls 27,4% savaient que d'autres facteurs de risque cardiovasculaire pouvaient etre associes au diabete et seulement 6,7% pratiquaient une activite physique reguliere. CONCLUSION: malgre la faible contribution du laboratoire, le syndrome metabolique est frequent parmi nos diabetiques. Les patients sont peu sensibilises sur le risque vasculaire et la pratique d'une activite physique reguliere reste faible. Un programme d'education adaptee contribuerait a un meilleur depistage et a une prise en charge optimale des cas.",2014,The Pan African Medical Journal
Beating the Simple Average: Egalitarian LASSO for Combining Economic Forecasts,"Despite the clear success of forecast combination in many economic environments, several important issues remain incompletely resolved. The issues relate to selection of the set of forecasts to combine, and whether some form of additional regularization (e.g., shrinkage) is desirable. Against this background, and also considering the frequently-found superiority of simple-average combinations, we propose LASSO-based procedures that select and shrink toward equal combining weights. We then provide an empirical assessment of the performance of our ""egalitarian LASSO"" procedures. The results indicate that simple averages are highly competitive, and that although out-of-sample RMSE improvements on simple averages are possible in principle using our methods, they are hard to achieve in real time, due to the intrinsic difficulty of small-sample real-time cross validation of the LASSO tuning parameter. We therefore propose alternative direct combination procedures, most notably ""best average"" combination, motivated by the structure of egalitarian LASSO and the lessons learned, which do not require choice of a tuning parameter yet outperform simple averages.",2017,Social Science Research Network
Detecting genetic interactions in pathway-based genome-wide association studies.,"Pathway-based genome-wide association studies (GWAS) can exploit collective effects of causal variants in a pathway to increase power of detection. However, current methods for pathway-based GWAS do not consider epistatic effects of genetic variants, although interactions between genetic variants may play an important role in influencing complex traits. In this paper, we employed a Bayesian Lasso logistic regression model for pathway-based GWAS to include all possible main effects and a large number of pairwise interactions of single nucleotide polymorphisms (SNPs) in a pathway, and then inferred the model with an efficient group empirical Bayesian Lasso (EBLasso) method. Using the inferred model, the statistical significance of a pathway was tested with the Wald statistics. Reliable effects in a significant pathway were selected using the stability selection technique. Extensive computer simulations demonstrated that our group EBlasso method significantly outperformed two competitive methods in most simulation setups and offered similar performance in other simulation setups. When applying to a GWAS dataset for Parkinson disease, EBLasso identified three significant pathways including the primary bile acid biosynthesis pathway, the neuroactive ligand-receptor interaction, and the MAPK signaling pathway. All effects identified in the primary bile acid biosynthesis pathway and many of effects in the other two pathways were epistatic effects. The group EBLasso method provides a valuable tool for pathway-based GWAS to identify main and epistatic effects of genetic variants.",2014,Genetic epidemiology
Wavenumber selection based analysis in Raman spectroscopy improves skin cancer diagnostic specificity.,"Real-time Raman spectroscopy can be used to assist in assessing skin lesions suspicious for cancer. Most of the diagnostic algorithms are based on full band of the Raman spectra, either in the fingerprint region or the high wavenumber region. In this paper we explored wavenumber selection based analysis in Raman spectroscopy for skin cancer diagnosis. Wavenumber selection was implemented using windows of wavenumber and leave-one-out cross-validated stepwise regression or least and shrinkage selection operator (LASSO). The diagnostic algorithms were then generated from the selected windows of wavenumber using multivariate statistical analyses, including principal component and general discriminate analysis (PC-GDA) and partial least squares (PLS). In total a combined cohort of 645 confirmed lesions from 573 patients encompassing skin cancers, precancers and benign skin lesions were included, which were divided into training cohort (n = 518) and testing cohort (n = 127) according to the measurement time. It was found that the area under the receiver operating characteristic curve (ROC) was improved from 0.861-0.891 to 0.891-0.911 and the diagnostic specificity for fixed sensitivity 0.99-0.90 was improved from 0.17-0.65 to 0.20-0.75 with wavenumber selection based analysis.",2016,The Analyst
OSCAR-based reconstruction for compressed sensing and parallel MR imaging,"Compressed sensing combined with parallel imaging has allowed significant reduction in MRI scan time. However, image reconstruction remains challenging and common methods rely on a coil calibration step. In this work, we focus on calibrationless reconstruction methods that promote group sparsity. The latter have allowed theoretical improvements in CS recovery guarantees. Here, we compare the performances of several regularization terms (group-LASSO, sparse group-LASSO and OSCAR) thatdefine with the data consistency term the convex but non-smooth objective function to be minimized. The same primal-dual algorithm is used to perform this minimization. Our results demonstrate that OSCAR-based reconstruction is competitive with state-of-the-art `1-ESPIRiT. Introduction Compressed sensing (CS) theory has made a breakthrough in Magnetic Resonance Imaging (MRI) since it has unlocked one of the major issues in MRI, namely the slow data acquisition. In the high resolution setting, CS must be combined with parallel imaging (PI) to preserve high signalto-noise ratio, leading to harder reconstruction problems. In the existing CS-PI literature, most algorithms reconstruct a single full field-of-view MR image using a (self-) calibration step that",2019,
A robust sparse-modeling framework for estimating schizophrenia biomarkers from fMRI,"BACKGROUND
Our goal is to identify the brain regions most relevant to mental illness using neuroimaging. State of the art machine learning methods commonly suffer from repeatability difficulties in this application, particularly when using large and heterogeneous populations for samples.


NEW METHOD
We revisit both dimensionality reduction and sparse modeling, and recast them in a common optimization-based framework. This allows us to combine the benefits of both types of methods in an approach which we call unambiguous components. We use this to estimate the image component with a constrained variability, which is best correlated with the unknown disease mechanism.


RESULTS
We apply the method to the estimation of neuroimaging biomarkers for schizophrenia, using task fMRI data from a large multi-site study. The proposed approach yields an improvement in both robustness of the estimate and classification accuracy.


COMPARISON WITH EXISTING METHODS
We find that unambiguous components incorporate roughly two thirds of the same brain regions as sparsity-based methods LASSO and elastic net, while roughly one third of the selected regions differ. Further, unambiguous components achieve superior classification accuracy in differentiating cases from controls.


CONCLUSIONS
Unambiguous components provide a robust way to estimate important regions of imaging data.",2017,Journal of Neuroscience Methods
Product â€™ s Quality Prediction with respect to equipments data,The semiconductor manufacturing process is a complex process that consists in a big number of equipments and enormous data. This paper presents a Least Absolute Shrinkage and Selection Operator (LASSO) based method for predicting the productâ€™s quality with respect to data of many equipments. The ability of the prediction model allows the productâ€™s quality to be estimated in real-time instead of a sampling inspection. An application to data provided by semiconductor manufacturing is presented and the results show the ability of the proposed method to predict the product quality efficiently and effectively with an improvement of more than 90% compared to the multivariate linear regression.,2015,
The Mangri shear zone in central Himalayas: the role of tectonometamorphic discontinuities in the exhumation of crystalline units,"La zona di taglio di Mangri nell'Himalaya centrale: il ruolo delle discontinuita tettono-metamorfiche nell'esumazione delle unita cristalline.Vengono presentati e discussi i risultati dell'analisi strutturale, petrografica e geocronologica di due sezioni del Cristallino dell'Alto Himalaya (HHC) in Nepal centrale. In entrambe le sezioni, all'interno dell'HHC, ben al di sopra del Main Central Thrust, sono state individuate due importanti zone di taglio duttili con cinematica compressiva e senso di spostamento del tetto verso SW. L'eta di queste zone di taglio, determinata mediante analisi U-Pb-Th in situ su monazite, e di 25 e 26 Ma. L'attivita di queste zone di taglio si riflette anche sul percorso PTt in quanto le rocce del footwall hanno registrato mediamente pressioni piu alte di almeno 2 Kbar rispetto alle rocce del tetto.Le rocce del tetto hanno iniziato quindi il loro percorso di esumazione alcuni milioni di anni prima dell'inizio della attivita del Main Central Thrust, struttura fino ad ora ritenuta la principale responsabile della esumazione dell' HHC assieme al contemporaneo South Tibetan Detachment System (STDS).La scoperta di un leucogranito indeformato, con eta di 23-24 Ma che taglia il STDS, pone ulteriori limiti all'importanza della esumazione dell'HHC per mezzo del MCT e del STDS, che possono avere avuto solo 1-2 milioni di anni di tempo per muoversi contemporaneamente. Questi risultati evidenziano una importanza limitata dei meccansimi di esumazione con l'estrusione o il channel flow che richiedono un grande lasso di tempo per la deformazione contemporanea lungo il MCT e il STDS.",2012,
A novel immune-related genes prognosis biomarker for melanoma: associated with tumor microenvironment.,"BACKGROUND
Melanoma is a cancer of the skin with potential to spread to other organs and is responsible for most deaths due to skin cancer. It is imperative to identify immune biomarkers for early melanoma diagnosis and treatment.


RESULTS
63 immune-related genes of the total 1039 unique IRGs retrieved were associated with overall survival of melanoma. A multi-IRGs classifier constructed using eight IRGs showed a powerful predictive ability. The classifier had better predictive power compared with the current clinical data. GSEA analysis showed multiple signaling differences between high and low risk score group. Furthermore, biomarker was associated with multiple immune cells and immune infiltration in tumor microenvironment.


CONCLUSIONS
The immune-related genes prognosis biomarker is an effective potential prognostic classifier in the immunotherapies and surveillance of melanoma.


METHODS
Melanoma samples of genes were retrieved from TCGA and GEO databases while the immune-related genes (IRGs) were retrieved from the ImmPort database. WGCNA, Cox regression analysis and LASSO analysis were used to classify melanoma prognosis. ESTIMATE and CIBERSORT algorithms were used to explore the relationship between risk score and tumor immune microenvironment. GSEA analysis was performed to explore the biological signaling pathway.",2020,Aging
"La transmission du virus amaril en Afrique occidentale : Ã©cologie, rÃ©partition, frÃ©quence et contrÃ´le des vecteurs, et observations concernant l'Ã©pidÃ©miologie de la fiÃ¨vre jaune","Aufhors prespnf, in a summarizcd form, most of the published clata perfaining io fhe fransmission of yellow fever in TBest Africa, as rue11 as some basic information clealing wifh fhe situation in East and Central Africa. They discuss also the observations carriecl ouf drrring fhe recent yellow fever epidemics of 1969 in Nigeriu, Togoland, Uppert Entomologiste m6dica1, hIission 0.R.S.T.O.K auprts de lâ€™O.C.C.G.E., B.P. 171, Bobo-Dioulasso, HautcVolta. ** Entomologiste mÃ©dical, Mission O.R.S.T.O.M. auprÃ¨s de lâ€™O.C.C.G.E. (adresse actuelle : O.R.S.T.O.hI., B.P. 529, Papeete., Tahiti, PolynÃ©sie FranÃ§aise). ** * hlÃ©decin du Serwce de SantÃ© des ArmÃ©es. Entomologiste mÃ©di,cal O.R.S.T.O.M., Centre 0.R.S.T.O.M: dei Dakar-Hann, B.P. 1386, Dakar, SÃ©nÃ©gal. Cah. ORSTOM, sÃ©r. Enf. mÃ©d. ParasifoT. (197.1), IX, 1, 3-60 9 J. HiiMON, G. PICHON ET Ã i. CORNET Volta, Ghana and Mali. They stress that, despite years of research in the past as well during the recent years, the maintenance of the yellow fever virus in West Africa stays unexplained, and that the transmission cycles giving rise to human yellow fever outbursts are mostly of un hypothetical nature. They underline that, during the jafrican epidemics of the last thirty years, the interhuman transmission of yellow fever bas been very unorthodox because selvafic vectors seem often to have played a more important part thnn Aecles aegypti. Authors present in la condensed form, with ten maps, the avaitable data dealing with the ecology, distribution and relative densities of the major potential vectors occurring in West Africa. A large part of these informations is coming from unpublished documents and from mimeographied reports with a restricted distribution. A short section deals with the prevention and the control of the yellow fever vectors, as routine measures and during epidemics. In conclusion authors underline the necessity to increase research on yellow fever epidemiology in West Africa and suggest what could be the major lines of investigations within the frame of a collective regional program involving the WHO regional reference laboratory and the field research units as rue11 as national health authorities.",1971,
Shrinkage Estimation of Dynamic Panel Data Models with Interactive Fixed Effects,"We consider the problem of determining the number of factors and selecting the proper regressors in linear dynamic panel data models with interactive fixed effects. Based on the preliminary estimates of the slope parameters and factors a la Bai and Ng (2009) and Moon and Weidner (2014a), we propose a method for simultaneous selection of regressors and factors and estimation through the method of adaptive group Lasso (least absolute shrinkage and selection operator). We show that with probability approaching one, our method can correctly select all relevant regressors and factors and shrink the coefficients of irrelevant regressors and redundant factors to zero. Further, we demonstrate that our shrinkage estimators of the nonzero slope parameters exhibit some oracle property. We conduct Monte Carlo simulations to demonstrate the superb finite-sample performance of the proposed method. We apply our method to study the determinants of economic growth and find that in addition to three common unobserved factors selected by our method, government consumption share has negative effects, whereas investment share and lagged economic growth have positive effects on economic growth.",2016,Journal of Econometrics
"Palynology of the Cenomanian Raha Formation, Gulf of Suez, Egypt: Biostratigraphical, palaeoenvironmental and palaeobiogeographical implications","Abstract The current study presents a fully qualitative palynological investigation carried out on the Raha Formation encountered from three wells in the Bakr Oil Field of the Gulf of Suez, Egypt. Around 30 species of pteridophytic spores, 26 species of angiosperm pollen, 24 species of gymnosperm pollen and 27 species of dinoflagellate cysts have been recorded. However, achritarchs, microforaminiferal test linings and freshwater algae are impoverished and sparsely documented throughout the Raha Formation. Two palynozones have been identified based on some stratigraphically significant pollen and spores, arranged from youngest to oldest: (1) Palynozone I (Classopollis brasiliensisâ€“Tricolpites sagax Assemblage Zone) of late Cenomanian age; (2) Palynozone II (Afropollis jardinusâ€“Crybelosporites pannuceus Assemblage Zone) of early-middle Cenomanian age. The distribution and ecological affiliation of specific palynomorph species, as well as various palynofacies parameters, are interpreted. A shallow marine environment from supratidal to distal inner neritic under proximal suboxicâ€“anoxic to dysoxicâ€“anoxic shelf conditions is reconstructed. Palaeobiogeographically, the absence of elaters from the recovered taxa is interpreted in terms of minor floral variation. This may be attributed to climatic and/or an environment-controlled niche establishment, which possibly was shaped by the existence of a physical barrier hindering the distribution of such type of elaterate parent plants.",2018,Austrian Journal of Earth Sciences
PredSym: estimating software testing budget for a bug-free release,"Symbolic execution tools are widely used during a software testing phase for finding hidden bugs and software vulnerabilities. Accurately predicting the time required by a symbolic execution tool to explore a chosen code coverage helps in planning the budget required in the testing phase. In this work, we present an automatic tool, PredSym, that uses static program features to predict the coverage explored by a symbolic execution tool Ã¢Â€Â“ KLEE, for a given time budget and to predict the time required to explore a given coverage. PredSym uses LASSO regression to build a model that does not suffer from overfitting and can predict both the coverage and the time with a worst error of 10% on unseen datapoints. PredSym also gives code improvement suggestions based on a heuristic for improving the coverage generated by KLEE.",2016,"Proceedings of the 7th International Workshop on Automating Test Case Design, Selection, and Evaluation"
Machine Learning Approach for Prediction of Reaction Yield with Simulated Catalyst Parameters,"Prediction of reaction yields by machine learning approach is demonstrated in tungsten-catalyzed epoxidation of alkenes. The various electronic and vibrational parameters of the phosphonic acids are collected by DFT simulation, and chosen by LASSO as the essential parameters for prediction of the reaction yields. With the trained model, we can predict yields of the reaction with unverified phosphonic acids with an error of 26%.",2018,Chemistry Letters
Eigenvalue Condition and model selection consistency of lasso,"Lasso is a popular method for sparse linear regression, especially for problems in which $p > n$. But when $p \gg n$, existing results from literature about the model selection consistency of lasso always require special and strong conditions, including the information from unknown true coefficients. An important question is: If the lasso solution can select the true variables without such strict condition? 
In this paper, we investigate a new train of thought to lead the model selection consistency of lasso. One important but more standard and much weaker condition, Eigenvalue Condition, is proposed. We can prove that the probability of lasso selecting wrong variables can decays at an exponential rate in ultra-high dimensional settings without other restrains except Eigenvalue Condition. Since penalized least squares have similar framework of solution. This technical tool can be extended to other methods which have similar structure. In the different dimensional settings, we show the different performance of lasso under different assumptions of noise terms. Results from simulations are carried out to demonstrate our results.",2015,arXiv: Statistics Theory
Utilizing Sparse-Aware Volterra for Power Amplifier Behavioral Modeling,"This paper presents a method for reducing the number of weights in a time series behavioral model for a power amplifier. The least-absolute shrinkage and selection operator (Lasso) algorithm is used to reduce the kernel size, preserving the important kernels, while eliminating the less important kernels. The algorithm is evaluated on a behavioral model for a class AB amplifier, the algorithm reduces the number of weights by greater than 70% without degrading model performance by a significant amount.",2014,
Rapid Detecion of N-Acetylneuraminic Acid from False Clownfish using HPLC-FLD for Symbiosis to Host Sea Anemone,"The symbiosis of false clownfish to its host sea anemone is unique as sea anemone is known to discharge toxin to approaching prey. This study aims to investigate by which biochemical property of the false clownfish mucus that enables the fish to adapt to the stinging tentacles of the sea anemone. N-acetylneuraminic acid (Neu5Ac) is a member of the sialic acid family that is secreted in the mucus of many marine and terrestrial organisms. It is important for cell recognition in endocrine regulation and cell immune system.Â  Mucus samples were collected from sea anemone Heteractis magnifica and three fish species, Amphiprion ocellaris, Abudefduf sexfasciatus, and Thalassoma lunare.Â  Samples were prepared byderivatization with thiobarbituric acid prior to rapid detection by HPLC-FLD.Â  The principal result showed that false clownfish, A. ocellaris, significantly lacks Neu5Ac (1.636 mg/ml) as compared to other reef fish tested (50.433 mg/ml and 71.893 mg/ml respectively).Â  As Neu5Ac is detected by the tentacles of sea anemone to trigger toxin discharge, it is concluded that the lack of Neu5Ac by false clownfish protects it from being stung.",2015,Asian Journal of Applied Sciences
Iterative Null-space Projection Method with Adaptive Thresholding in Sparse Signal Recovery and Matrix Completion,"Adaptive thresholding methods have proved to yield high SNRs and fast convergence in finding the solution to the Compressed Sensing (CS) problems. Recently, it was observed that the robustness of a class of iterative sparse recovery algorithms such as Iterative Method with Adaptive Thresholding (IMAT) has outperformed the well-known LASSO algorithm in terms of reconstruction quality, convergence speed, and the sensitivity to the noise. In this paper, we introduce a new method towards solving the CS problem. The logic of this method is based on iterative projections of the thresholded signal onto the null-space of the sensing matrix. The thresholding is carried out by recovering the support of the desired signal by projection on thresholding subspaces. The simulations reveal that the proposed method has the capability of yielding noticeable output SNR values with about as many samples as twice the sparsity number, while other methods fail to recover the signals when approaching the algebraic bound for the number of samples required. The computational complexity of our method is also comparable to other methods as observed in the simulations. We have also extended our Algorithm to Matrix Completion (MC) scenarios and compared its efficiency to other well-reputed approaches for MC in the literature.",2018,ArXiv
Profils en acides gras des tissus et mÃ©tabolisme concertÃ© des lipides dans lâ€™organisme du porc en croissance,"Les differents types dâ€™acides gras sont des elements importants pour la qualite
technologique et dietetique des produits animaux. Cette etude avait pour objectifs de decrire
et predire la variation du profil en acides gras dans differents tissus chez le porc. Des
analyses statistiques descriptives (ACP) et integratives entre tissus (AFM) ont permis de
montrer une large similitude dans la variation des acides gras a chaine moyenne (inferieurs
a 14 atomes de carbone) et des acides palmitique et stearique (C16 :0, C18 :0) dâ€™une part,
et les acides gras polyinsatures de la famille omega-6 dâ€™autre part entre tous les tissus en
reponse au regime de lâ€™animal. A lâ€™inverse, il existe une specificite tissulaire dans la
variabilite des acides gras de la famille omega-3. Des analyses predictives de type PLS2 ont
montre que lâ€™expression de nombreux genes impliques dans le metabolisme glucidique et la
synthese des acides gras, mais aussi lâ€™oxydation des acides gras expliquent une part de la
variabilite des proportions dâ€™acides gras au sein de chaque tissu. Cependant, le profil global
en acides gras tissulaire nâ€™est quâ€™imparfaitement predit. Une analyse de type LASSO a
permis dâ€™etablir un petit nombre de predicteurs moleculaires pertinents pour la variation de
lâ€™acide gras palmitique C16 :0 et des acides gras de la famille omega-6.",2015,
Efficient Methods for Overlapping Group Lasso,"The group Lasso is an extension of the Lasso for feature selection on (predefined) nonoverlapping groups of features. The nonoverlapping group structure limits its applicability in practice. There have been several recent attempts to study a more general formulation where groups of features are given, potentially with overlaps between the groups. The resulting optimization is, however, much more challenging to solve due to the group overlaps. In this paper, we consider the efficient optimization of the overlapping group Lasso penalized problem. We reveal several key properties of the proximal operator associated with the overlapping group Lasso, and compute the proximal operator by solving the smooth and convex dual problem, which allows the use of the gradient descent type of algorithms for the optimization. Our methods and theoretical results are then generalized to tackle the general overlapping group Lasso formulation based on the eq norm. We further extend our algorithm to solve a nonconvex overlapping group Lasso formulation based on the capped norm regularization, which reduces the estimation bias introduced by the convex penalty. We have performed empirical evaluations using both a synthetic and the breast cancer gene expression dataset, which consists of 8,141 genes organized into (overlapping) gene sets. Experimental results show that the proposed algorithm is more efficient than existing state-of-the-art algorithms. Results also demonstrate the effectiveness of the nonconvex formulation for overlapping group Lasso.",2013,IEEE Transactions on Pattern Analysis and Machine Intelligence
Experimental evaluation of regression model-based walking speed estimation using lower body-mounted IMU,"This study provides a concurrent comparison of regression model-based walking speed estimation accuracy using lower body mounted inertial sensors. The comparison is based on different sets of variables, features, mounting locations and regression methods. An experimental evaluation was performed on 15 healthy subjects during free walking trials. Our results show better accuracy of Gaussian process regression compared to least square regression using Lasso. Among the variables, external acceleration tends to provide improved accuracy. By using both time-domain and frequency-domain features, waist and ankle-mounted sensors result in similar accuracies: 4.5% for the waist and 4.9% for the ankle. When using only frequency-domain features, estimation accuracy based on a waist-mounted sensor suffers more compared to the one from ankle.",2016,2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)
Factors Associated With Increased Serum Alanine Aminotransferase Levels During the French Guiana Dengue Epidemic of 2005Y2006,"Mild elevation of aminotransferases is a common feature of dengue virus infection, and severe acute liver injury has been described. The aim of this retrospective study was to assess relationships between antipyretic drugs and chronic alcohol use and the increase in serum alanine aminotransferase (ALAT) level in patients hospitalized with dengue fever during the 2005Y2006 dengue epidemics in French Guiana. In the 162 included patients in this study (99 adults [62%] and 63 children [38%]), 2 analysis were performed comparing (1) 64 adults (65%) with an ALAT value greater than twice the upper limit of the normal value and 35 controls (35%) and (2) 24 children (39%) with an ALAT value greater than twice the upper limit of the normal value and 38 controls (61%). The factors associated with ALATelevation were: (1) acetaminophen exposure and length of intake in adults and (2) acetaminophen overdose during the hospitalization of children. Another analysis suggested a role for alcohol consumption. Acetaminophen and alcohol consumption should be searched and taken into account in dengue fever. (Infect Dis Clin Pract 2010;18: 41Y45) Dengue fever is a public health problem worldwide. It is the most geographically widespread arboviral infection, with 50 to 100 million infections per year, including 500,000 cases of dengue hemorrhagic fever (DHF), especially in Southeast Asia, the Pacific Islands, and the Americas. Although the majority of infections are asymptomatic, dengue fever often presents as a febrile illness that, in a minority of cases, progresses to the lifethreatening DHF or dengue shock syndrome (DSS). There is no curative treatment for dengue virus infection, although the WHO recommends acetaminophen for symptomatic relief. Nonsteroidal anti-inflammatory drugs (NSAIDs) should be avoided. Whereas mild elevation of aminotransferases is a common feature of dengue fever, severe acute liver injury and acute liver failure are uncommon. The aim of this study was to assess the relationship between increases in serum alanine aminotransferase (ALAT) and various clinical features, biological data, and environmental factors, including antipyretics and chronic alcohol use, in patients hospitalized with dengue fever during the 2005Y2006 dengue serotype 2 virus epidemic in French Guiana. METHODS This retrospective study took place in French Guiana. We evaluated patients who were admitted with suspected dengue fever to hospitals in Cayenne, Saint-Laurent-du-Maroni, and Koru between November 2005 and August 2006. Cases and controls fit the following inclusion criteria: (1) the diagnosis of dengue had to be confirmed by reverse transcriptase polymerase chain reaction (PCR), NS1 antigen positivity or increased antibody and (2) at least one serum ALAT result was available. Medical history and demographic data were collected from medical charts available at the Cayenne Hospital. In this paper, patients older or younger than 15 years are referred to as adults and children, respectively, and are analyzed separately. Biological data included routine liver function tests and, when available, viral serological tests, blood cell and platelets counts, and serum creatinine, C-reactive protein, and phosphokinase creatine levels. Dengue infection was classified as classical dengue fever, DHF, or DSS according to WHO guidelines. When possible, the medical history was obtained via a direct patient interview by one of the authors (C.D.) between September 5 and October 25, 2006. During this study period, patients were asked to answer a standardized questionnaire, including details on intake of both alcohol and medications (1) routinely and (2) during the week preceding their hospitalization for dengue fever (A and D questionnaire). Chronic alcohol abuse was defined as weekly consumption of more than 210 g in men and 140 g in women. Information regarding the use of medications was obtained from physicianâ€™s orders and nursing notes. Acetaminophen intake (recorded in grams) was assessed in detail, including daily intake, time between doses, and total intake. The daily dose of acetaminophen was defined as being supratherapeutic when greater than 4 g and 60 mg/kg in adults and children, respectively. The total duration of exposure to acetaminophen included intake both before hospitalization and during the hospital stay. All variables were selected for investigation before the study was conducted. Two Case-Control Analyses Were Performed The first analysis was restricted to adults, and the second was restricted to children. In both analyses, Bcases[ was defined as patients with at least one ALAT value greater than or equal to twice (2N) the upper limit of the normal value (ULNV) of the local laboratory reference value (35 UI/L). BControls[ was defined as patients with ALAT values lower than 2N. The 2N ALAT limit level was chosen according to the Lee hepatitis definition and because of its frequent use in other studies. In the adult case-control study, the role of alcohol was also studied, comparing patients with at least one result of ALAT ORIGINAL ARTICLE Infectious Diseases in Clinical Practice & Volume 18, Number 1, January 2010 www.infectdis.com 41 From the Departments of Infectious Diseases, *Xavier Bichat Hospital, Paris 7 University, Paris; â€ AndrÃ©e Rosemon Hospital, Cayenne, French Guyana; â€¡PÃ´le des Maladies de lâ€™Appareil Digestif, Department of Hepatology, Beaujon Hospital, 92118 Clichy; Â§Institut National de la SantÃ© et de la Recherche MÃ©dicale (Inserm), WHO Collaborating Centre for Electronic Disease Surveillance, UniversitÃ© Pierre et Marie Curie, 27 rue Chaligny, F-75571 Paris Cedex 12; ||French School of Public Health, Rennes and Paris; Â¶Department of Public Health, AP-HP, HÃ´pital Tenon, Paris; and LAssociation for Research in Infectious Diseases (Association de recherche en pathologie infectieuse (ARPI)), Paris 7 University, Paris, France. Reprints: Caroline Dumortier, MD, SecrÃ©tariat du Pr Catherine Leport Service de Maladies Infectieuses et Tropicales, HÃ´pital Xavier Bichat, 46 rue Henri Huchard, 75018 Paris, France. E-mail: carolined01@ hotmail.com. The authors have no funding or conflicts of interest to disclose. Copyright * 2010 by Lippincott Williams & Wilkins ISSN: 1056-9103 Copyright @ 20 Lippincott Williams & Wilkins. Unauthorized reproduction of this article is prohibited. 10 greater than or equal to 10N to those with least one result of ALAT of less than 10N. Data were recorded and analyzed using statistical software (Epi Info; Centers for Disease Control and Prevention Division of Integrated Surveillance Systems and Services, Atlanta, Ga). Descriptive statistics were used to evaluate the distribution of the demographic, clinical, and biological data. Categorical variables were compared by W and Fisher exact test. Continuous normally distributed data were compared by Student t test. A univariate analysis was performed with Epi Info analysis software.",2009,
Model-aware Quantile Regression for Discrete Data,"Quantile regression is a class of methods voted to the modelling of conditional quantiles. In a Bayesian framework quantile regression has typically been carried out exploiting the Asymmetric Laplace Distribution as a working likelihood. Despite the fact that this leads to a proper posterior for the regression coefficients, the resulting posterior variance is however affected by an unidentifiable parameter, hence any inferential procedure beside point estimation is unreliable. We propose a model-based approach for quantile regression that considers quantiles of the generating distribution directly, and thus allows for a proper uncertainty quantification. We then create a link between quantile regression and Generalized Linear Models by mapping the quantiles to the parameter of the response variable, and we exploit it to fit the model with R-INLA. We extend it also to the case of discrete responses, where there is no 1-to-1 relationship between quantiles and distributionâ€™s parameter, by introducing continuous generalizations of the most common discrete variables (Poisson, Binomial and Negative Binomial) to be exploited in the fitting. Introduction Quantile regression is a supervised technique aimed at modeling the quantiles of the conditional distribution of some response variable. With respect to â€œstandardâ€ regression, which is concerned with modeling the conditional mean, quantile regression is especially useful when the tails of the distribution are of interest, as for example when the focus is on extreme behavior rather than average, or when it is important to assess whether or not covariates affect uniformly different levels of the population. Even though the idea dates back to Galton (1883) (as noted in Gilchrist (2008)), quantile regression was formally introduced only relatively recently by Koenker and Bassett (1978). Since then, the use of quantiles in regression problems has seen an impressive growth and has been thoroughly explored in both the parametric (see Yue and Rue (2011), Wang et al. (2017)), and non-parametric framework (see Yu and Jones (1998), Takeuchi et al. (2005), Li et al. (2007)) with applications ranging from the Random Forest quantile regression of Meinshausen (2006) to D-vine copulas for quantiles in Kraus and Czado (2017), through quantile regression in graphical models as in Ali et al. (2016). Despite these many different flavors of quantile regression, as noted by Lum and Gelfand (2012), all quantile-based models can be grouped into just two categories: Conditional Quantile Models, where the estimation procedure is carried out separately for each quantile of interest and Joint Quantile Models, where multiple quantiles of interest are estimated simultaneously. Modelling quantiles jointly requires stronger assumptions on both 1 ar X iv :1 80 4. 03 71 4v 1 [ st at .M E ] 1 0 A pr 2 01 8 covariates and responses, does not allow for linear modeling of the quantiles and it is computationally intensive even when using rough approximations. Its advocates, such as Reich et al. (2011) and Tokdar and Kadaney (2012) stress the fact that joint modeling results in ordered quantile curves, hence it is noticeable immune to quantile crossing, which is a paradoxical phenomenon occurring when the quantile curves are not an increasing function of the quantile level Î±. As we believe quantile crossing should be interpreted as a flag that the model is not correct or that data is insufficient rather than an issue to be solved, in the following we will refer exclusively to Conditional Quantile Models. One of the most significant developments in the quantile regression literature has been the introduction of the Asymmetric Laplace Distribution (ALD) as a working likelihood Yu and Moyeed (2001). From a frequentist point of view, the use of the ALD gave rise to a class of likelihood based method for fitting quantile models and has been instrumental in introducing random effects in linear and non linear quantile regression models; see for example Geraci and Bottai (2007), Geraci and Bottai (2014), Geraci (2017) or Marino and Farcomeni (2015) for a more comprehensive review. The introduction of the ALD has been even more critical in the Bayesian framework, where the likelihood is a required in inferential procedure Yu and Moyeed (2001). As a result fully bayesian versions of quantile regression, such as Yue and Rue (2011), as well as Bayesian versions of regularized methods, such as Quantile Bayesian Lasso and Quantile Bayesian Elastic Net, have been developed in the last couple of years, exples being Alhamzawi et al. (2011) or Li et al. (2010). Extensions of the Asymmetric Laplace Distribution such as the Asymmetric Laplace Process Lum and Gelfand (2012), broadened quantile regression to spatially dependent data. Despite their popularity however, ALD based methods are not always satisfactory, especially in terms uncertainty quantification. The use of the ALD introduces an identifiable parameter in the posterior variance, hence any inference beside point estimation is precluded. In this work we propose a new approach to quantile regression, based on the direct modelling of the quantiles of the generating distribution. Our proposal allows to extend the GLMM framework to quantile modeling by reformulating quantile regression in terms of link functions. This formulation not only recast quantile regression in a much more cohesive setting and overcomes the natural fragmentation that derives from the vastness of the quantile regression literature, but it is also key to an efficient and ready to use fitting procedute, as the connection allows to estimate the model using R-INLA Rue et al. (2009) Rue et al. (2017). We focus on the delicate case of discrete responses, where the quantiles cannot be direct modeled. The literature on quantiles for counts (or discrete data in general) is largely based on the works of Machado and Santos Silva (2005), which proposes a jittering procedure to make discrete observations continuous by adding noise. In the Bayesian framework count responses have also been modeled through the ALD. For example Lee and Neocleous (2010) couples the jittering with the ALD; alternatively, Congdon (2017) proposes a twostage regression that uses ALD based quantile regression to model the continuous rate parameter of a Poisson distribution. This work is structured as follows. In Section 1, after giving a brief refresher of what quantile regression is and what are the issues with the ALD, we introduce our model based approach for quantile regression. In Section 2 we analyze the case where the response variable is discrete, by introducing a continuous version of the most popular distributions for counts. Finally Section 3 shows an application of our proposed method in disease mapping, using the Scottish Lip Cancer data.",2018,
"La urbanizaciÃ³n en cuenca y su relaciÃ³n con el paisaje en las mÃ¡rgenes de los rÃ­os, el caso del rÃ­o Tomebamba","En las ultimas decadas el acelerado proceso de urbanizacion ha sido un problema general a nivel mundial, que ha afectado al paisaje particularmente a las orillas de los rios y quebradas que han perdido grandes extensiones de suelo de proteccion. Este proyecto busca construir y ayudar a definir criterios de diseno urbano en las orillas del rio Tomebamba; cuyo espacio de estudio se ubica al noreste de la ciudad de Cuenca, a lo largo de la avenida Ordonez Lasso via a la costa entre las calles Camino al Tejar y Los Cedros. 
 
Para ello, como punto de partida se define un marco conceptual y referencial, en el que se analiza las distintas interpretaciones del hombre con relacion al proceso de urbanizacion y como esta ha afectado al paisaje a traves de conceptos y criterios asociados al desarrollo urbano del area a estudiar en los ultimos 25 anos. 
 
Estos antecedentes seran de ayuda para el desarrollo de analisis abordados en el capitulo dos y tres, y consecuentemente en la definicion de los criterios a intervenir en las orillas del rio, que mejoren la calidad espacial.",2017,
Transcription-factor centric genome mining strategy for discovery of diverse unprecedented RiPP gene clusters,"Ribosomally synthesized and post-translationally modified peptides (RiPPs) are a rapidly emerging group of natural products with diverse biological activity. Most of their biosynthetic mechanisms are well studied and the â€œgenome miningâ€ strategy based on homology has led to the unearthing of many new ribosomal natural products, including lantipeptides, lasso peptides, cyanobactins. These precursor-centric or biosynthetic protein-centric genome mining strategies have encouraged the discovery of RiPPs natural products. However, a limitation of these strategies is that the newly identified natural products are similar to the known products and novel families of RiPP pathways were overlooked by these strategies. In this work, we applied a transcription-factor centric genome mining strategy and diverse unique crosslinked RiPP gene clusters were predicted in several sequenced microorganisms. Our research could significantly expand the category of biosynthetic pathways of RiPP natural products and predict new resources for novel RiPPs.",2018,bioRxiv
Using machine learning algorithms to map the groundwater recharge potential zones.,"Groundwater recharge is indispensable for the sustainable management of freshwater resources, especially in the arid regions. Here we address some of the important aspects of groundwater recharge through machine learning algorithms (MLAs). Three MLAs including, SVM, MARS, and RF were validated for higher prediction accuracies in generating groundwater recharge potential maps (GRPMs). Accordingly, soil permeability samples were prepared and are arbitrarily grouped into training (70%) and validation (30%) samples. The GRPMs are generated using sixteen effective factors, such as elevation (denoted using a digital elevation model; DEM), aspect, slope angle, TWI (topographic wetness index), fault density, MRVBF (multiresolution index of valley bottom flatness), rainfall, lithology, land use, drainage density, distance from rivers, distance from faults, annual ETP (evapo-transpiration), minimum temperature, maximum temperature, and rainfall 24-hr. Subsequently, the VI (variables importance) is assessed based on the LASSO algorithm. The GRPMs of three MLAs were validated using the ROC-AUC (receiver operating characteristic-area under curve) and various techniques including true positive rate (TPR), false positive rate (FPR), F-measures, fallout, sensitivity, specificity, true skill statistics (TSS), and corrected classified instances (CCI). Based on the validation, the RF algorithm performed better (AUCÂ =Â 0.987) than the SVM (AUCÂ =Â 0.963) and the MARS algorithm (AUCÂ =Â 0.962). Furthermore, the accuracy of these MLAs are included in excellent class, based on the ROC curve threshold. Our case study shows that the GRPMs are potential guidelines for decision-makers in drafting policies related to the sustainable management of the groundwater resources.",2020,Journal of environmental management
Neighborhood selection with application to social networks,"The topic of this paper is modeling and analyzing dependence in stochastic social networks. Using a latent variable block model allows the analysis of dependence between blocks via the analysis of a latent graphical model. Our approach to the analysis of the graphical model then is based on the idea underlying the neighborhood selection scheme put forward by Meinshausen and B\""{u}hlmann (2006). However, because of the latent nature of our model, estimates have to be used in lieu of the unobserved variables. This leads to a novel analysis of graphical models under uncertainty, in the spirit of Rosenbaum et al. (2010), or Belloni et al. (2017). Lasso-based selectors, and a class of Dantzig-type selectors are studied.",2017,arXiv: Statistics Theory
Ajustada victoria en Ecuador,"El candidato oficialista, Lenin Moreno, ha vencido a Guillermo Lasso en las presidenciales tras una campana crispada y cargada de acusaciones cruzadas. Conciliador, su objetivo es liderar una transicion moderada y restanar heridas.",2017,
Nonclassical Pseudospectral Method for a Class of Singular Boundary Value Problems Arising in Physiology,"In this paper, nonclassical pseudospectral method is presented for solution of a classof nonlinear singular boundary value problems arising in physiology. Properties of non-classical pseudospectral method are presented. These properties are utilizeto reduce the computation of singular boundary value problems to system of equations. Numerical method is tested for its efficiency by considering two examples from physiology",2012,Journal of Applied Mathematics
Temporally Dynamic Resting-State Functional Connectivity Networks for Early MCI Identification,"Resting-state functional Magnetic Resonance Imaging (R-fMRI) scan provides a rich characterization of the dynamic changes or temporal variabilities caused by neural interactions that may happen within the scan duration. Multiple functional connectivity networks can be estimated from R-fMRI time series to effectively capture subtle yet short neural connectivity changes induced by disease pathologies. To effectively extract the temporally dynamic information, we utilize a sliding window approach to generate multiple shorter, yet overlapping sub-series from a full R-fMRI time series. Whole-brain sliding window correlations are computed based on these sub-series to generate a series of temporal networks, characterize the neural interactions between brain regions at different time scales. Individual estimation of these temporal networks overlooks the intrinsic temporal smoothness between successive overlapping R-fMRI sub-series. To handle this problem, we suggest to jointly estimate temporal networks by maximizing a penalized log likelihood via a fused lasso regularization: 1) l 1-norm penalty ensures a sparse solution; 2) fused regularization preserves the temporal smoothness while allows correlation variability. The estimated temporal networks were applied for early Mild Cognitive Impairment (eMCI) identification, and our results demonstrate the importance of including temporally dynamic R-fMRI scan information for accurate diagnosis of eMCI.",2013,
Costruzioni in zona sismica,"in zona 2 (alta sismicitÃ ): obbligo dell'autorizzazione preventiva all'avvio dei lavori â€œrilevantiâ€ nei riguardi della pubblica incolumitÃ  e obbligo del deposito della documentazione relativa al progetto prima dellâ€™avvio dei lavori di â€œminore rilevanzaâ€ e â€œprivi di rilevanzaâ€ e della certificazione per interventi di sopraelevazione in zona 3 (sismicitÃ  bassa): obbligo del deposito della documentazione relativa al progetto prima dellâ€™avvio dei lavori e della certificazione per interventi di sopraelevazione, obbligo dell'autorizzazione preventiva all'avvio dei lavori â€œrilevantiâ€ nei riguardi della pubblica incolumitÃ  quali le nuove costruzioni che si discostino dalle usuali tipologie o che per la loro particolare complessitÃ  strutturale richiedano piÃ¹ articolate calcolazioni e verifiche nonchÃ© gli interventi relativi ad edifici di interesse strategico e alle opere infrastrutturali la cui funzionalitÃ  durante gli eventi sismici assume rilievo fondamentale per le finalitÃ  di protezione civile, nonchÃ© relativi agli edifici e alle opere infrastrutturali che possono assumere rilevanza in relazione alle conseguenze di un loro eventuale collasso in zona 4 (sismicitÃ  molto bassa): obbligo del deposito della documentazione relativa al",2019,
Fan for a motor vehicle,"Geblase (5) fur die Frischluftzufuhr in das Innere eines Kraftfahrzeugs, mit einem Gehause aus mindestens zwei Gehauseteilen (9, 10), die uber ihre Gehausetrennungen (11, 12, 13) nach ausen hin abdichtbar sind, bei dem dem ersten Gehauseteil (9) eine erste Luftansaug- oder Luftauslassoffnung (2, 8) und dem zweiten Gehauseteil (10) eine zweite Luftansaug- oder Luftausblasoffnung (2, 8) zugeordnet ist, bei der die Gehausetrennungen eine abdichtende Verbindung der beiden Gehauseteile in einer ersten Relativposition (A) und in mindestens einer weiteren Relativposition (B, C) zueinander erlauben, mit Befestigungsmitteln (14, 15, 16, 17) zum Verbinden der zwei Gehauseteile in der ersten Relativposition (A) der Gehausetrennung und mit zusatzlichen Befestigungsmitteln (18, 19) zu deren Verbinden in der weiteren Relativposition (B, C), wobei die zusatzlichen Befestigungsmittel bei einer Verbindung in der ersten Relativposition ungenutzt bleiben.",2007,
Group defamation and freedom of speech : the relationship between language and violence,"Preface Group Defamation and Racist Oppression Group Defamation and the Oppression of Black Americans by Kenneth Clark Group Defamation and the Genocide of American Indians by Lawrence Hauptman Race, Language, and War in Two Cultures: World War II in Asia by John Dower Group Defamation and the Holocaust by Michael Blain The Relationship Between Language and Violence Group Defamation: From Language to Thought to Action by Laraine R. Fergenson Outsider Jurisprudence: Toward a Victim's Analysis of Racial Hate Messages by Mari J. Matsuda Group Defamation in International and Comparative Law Group Defamation and International Law by Louis Henkin From Red Lion Square to Skokie to the Fatal Shore: Racial Defamation and Freedom of Speech in Australia by David Partlett Racial Incitement in Israel by David Kretzmer Group Defamation in Canada by Robert Martin Group Defamation and the First Amendment The Supreme Court Speaks: R.A.V. v. St. Paul and Wisconsin v. Mitchell Rethinking Group Libel by Lee C. Bollinger Pornography as Defamation and Discrimination by Catharine MacKinnon To Stimulate, Provoke, or Incite?: Hate Speech and the First Amendment by Kenneth Lasson Freedom of Speech: Should It Be Available to Pornographers, Nazis, and the Klan? by Leon Friedman Legal Approaches to the Control of Group Defamation The People of the State of New York v. Jesse A. Stump: Majority Opinion by Monroe H. Freedman The People of the State of New York v. Jesse A. Stump: Dissenting Opinion by Eric M. Freedman Model Group Defamation Statute--First Prize by Joseph Ribacoff Model Group Defamation Statute--First Honorable Mention by Heidi Bachana Model Group Defamation Statute--Second Honorable Mention by Devin House Index",1995,
The Social Cost of Electricity,"The Social Cost of Electricity. Scenarios and Policy Implications / Anil Markandya, Andrea Bigano and Roberto Porchia, Edward Elgar/FEEM, Dec. 2010, Â£75 sommaire : http://www.e-elgar.com/Bookentry_contents.lasso?id=13446 ""This book reports and rationalizes the state of the art concerning the social costs of electricity generation. Social costs are assessed by addingÂ  to the private generation costs, the external costs relatedÂ  to damages to human health, the environment, crops, materials, an...",2011,
A Machine Learning Approach to Predicting Need for Hospitalization for Pediatric Asthma Exacerbation at the Time of Emergency Department Triage,"OBJECTIVES
Pediatric asthma is a leading cause of emergency department (ED) utilization and hospitalization. Earlier identification of need for hospital-level care could triage patients more efficiently to high- or low-resource ED tracks. Existing tools to predict disposition for pediatric asthma use only clinical data, perform best several hours into the ED stay, and are static or score-based. Machine learning offers a population-specific, dynamic option that allows real-time integration of available nonclinical data at triage. Our objective was to compare the performance of four common machine learning approaches, incorporating clinical data available at the time of triage with information about weather, neighborhood characteristics, and community viral load for early prediction of the need for hospital-level care in pediatric asthma.


METHODS
Retrospective analysis of patients ages 2 to 18 years seen at two urban pediatric EDs with asthma exacerbation over 4Â years. Asthma exacerbation was defined as receiving both albuterol and systemic corticosteroids. We included patient features, measures of illness severity available in triage, weather features, and Centers for Disease Control and Prevention influenza patterns. We tested four models: decision trees, LASSO logistic regression, random forests, and gradient boosting machines. For each model, 80% of the data set was used for training and 20% was used to validate the models. The area under the receiver operating characteristic (AUC) curve was calculated for each model.


RESULTS
There were 29,392 patients included in the analyses: mean (Â±SD) age of 7.0 (Â±4.2) years, 42% female, 77% non-Hispanic black, and 76% public insurance. The AUCs for each model were: decision tree 0.72 (95% confidence interval [CI]Â = 0.66-0.77), logistic regression 0.83 (95% CIÂ = 0.82-0.83), random forests 0.82 (95% CIÂ = 0.81-0.83), and gradient boosting machines 0.84 (95% CIÂ = 0.83-0.85). In the lowest decile of risk, only 3% of patients required hospitalization; in the highest decile this rate was 100%. After patient vital signs and acuity, age and weight, followed by socioeconomic status (SES) and weather-related features, were the most important for predicting hospitalization.


CONCLUSIONS
Three of the four machine learning models performed well with decision trees preforming the worst. The gradient boosting machines model demonstrated a slight advantage over other approaches at predicting need for hospital-level care at the time of triage in pediatric patients presenting with asthma exacerbation. The addition of weight, SES, and weather data improved the performance of this model.",2018,Academic Emergency Medicine
Stabilizing l1-norm prediction models by supervised feature grouping,"Emerging Electronic Medical Records (EMRs) have reformed the modern healthcare. These records have great potential to be used for building clinical prediction models. However, a problem in using them is their high dimensionality. Since a lot of information may not be relevant for prediction, the underlying complexity of the prediction models may not be high. A popular way to deal with this problem is to employ feature selection. Lasso and l1-norm based feature selection methods have shown promising results. But, in presence of correlated features, these methods select features that change considerably with small changes in data. This prevents clinicians to obtain a stable feature set, which is crucial for clinical decision making. Grouping correlated variables together can improve the stability of feature selection, however, such grouping is usually not known and needs to be estimated for optimal performance. Addressing this problem, we propose a new model that can simultaneously learn the grouping of correlated features and perform stable feature selection. We formulate the model as a constrained optimization problem and provide an efficient solution with guaranteed convergence. Our experiments with both synthetic and real-world datasets show that the proposed model is significantly more stable than Lasso and many existing state-of-the-art shrinkage and classification methods. We further show that in terms of prediction performance, the proposed method consistently outperforms Lasso and other baselines. Our model can be used for selecting stable risk factors for a variety of healthcare problems, so it can assist clinicians toward accurate decision making.",2016,Journal of biomedical informatics
"The Southernmost Occurrence of the Aquatic Sloth Thalassocnus (Mammalia, Tardigrada) in Two New Pliocene Localities in Chile","Abstract. Thalassocnus is a sloth (Mammalia, Tardigrada) adapted to an aquatic lifestyle. It was first described from the Neogene deposits of the Pisco Formation of Peru, from where most of the specimens come. The genus is represented by five species ranging from the late Miocene to the late Pliocene, occupying successive stratigraphic levels. Morpho-functional studies of the cranial and postcranial skeleton of Thalassocnus have demonstrated the progressive adaptation of these sloths to a marine environment, establishing gradual differences from from the geologically oldest to the youngest species of the genus. The first records of Thalassocnus outside the Pisco Formation have been referred to the Neogene BahÃ­a Inglesa Formation, in northern Chile, where older species were recovered. In this paper, we describe materials from two new Pliocene localities in Chile: the Coquimbo and the HorcÃ³n formations, in northern and central Chile, respectively. The Coquimbo Formation material was collected from the Lomas del Sauce locality and consists of a partial skeleton of a single individual. Detailed comparisons of the elements with diagnostic features enabled the referral of this specimen to T. carolomartini. The material from the HorcÃ³n Formation was collected from the Playa La Luna locality and consists of an isolated phalanx, which is attributed to one of the species of Thalassocnus younger than T. natans. Thus, we present the first record of younger species of Thalassocnus in Chile and the southernmost occurrence of the genus.",2017,Ameghiniana
EMLasso: logistic lasso with missing data.,"In clinical settings, missing data in the covariates occur frequently. For example, some markers are expensive or hard to measure. When this sort of data is used for model selection, the missingness is often resolved through a complete case analysis or a form of single imputation. An alternative sometimes comes in the form of leaving the most damaged covariates out. All these strategies jeopardise the goal of model selection. In earlier work, we have applied the logistic Lasso in combination with multiple imputation to obtain results in such settings, but we only provided heuristic arguments to advocate the method. In this paper, we propose an improved method that builds on firm statistical arguments and that is developed along the lines of the stochastic expectation-maximisation algorithm. We show that our method can be used to handle missing data in both categorical and continuous predictors, as well as in a nonpenalised regression. We demonstrate the method by applying it to data of 273 lung cancer patients. The objective is to select a model for the prediction of acute dysphagia, starting from a large set of potential predictors, including clinical and treatment covariates as well as a set of single-nucleotide polymorphisms.",2013,Statistics in medicine
Targeting mycobacterial proteolytic complexes with natural products.,"Controlled proteolysis is key to bacterial viability. In this issue of Chemistry & Biology, Gavrish and colleagues characterize a natural product, lassomycin, targeting the Mycobacterium tuberculosis caseinolytic (Clp) protease. Unusually, lassomycin activates ClpC1, inducing ATPase activity and decoupling it from proteolysis.",2014,Chemistry & biology
"McGlasson, H. C., 1862- : Confederate Service Record, 1915.","This service record is an account of military actions during the American Civil War by veteran H. C. McGlasson (1862- ), dated from 1915.",1915,
RiPPMiner: a bioinformatics resource for deciphering chemical structures of RiPPs based on prediction of cleavage and cross-links,"Ribosomally synthesized and post-translationally modified peptides (RiPPs) constitute a rapidly growing class of natural products with diverse structures and bioactivities. We have developed RiPPMiner, a novel bioinformatics resource for deciphering chemical structures of RiPPs by genome mining. RiPPMiner derives its predictive power from machine learning based classifiers, trained using a well curated database of more than 500 experimentally characterized RiPPs. RiPPMiner uses Support Vector Machine to distinguish RiPP precursors from other small proteins and classify the precursors into 12 sub-classes of RiPPs. For classes like lanthipeptide, cyanobactin, lasso peptide and thiopeptide, RiPPMiner can predict leader cleavage site and complex cross-links between post-translationally modified residues starting from genome sequences. RiPPMiner can identify correct cross-link pattern in a core peptide from among a very large number of combinatorial possibilities. Benchmarking of prediction accuracy of RiPPMiner on a large lanthipeptide dataset indicated high sensitivity, specificity, accuracy and precision. RiPPMiner also provides interfaces for visualization of the chemical structure, downloading of simplified molecular-input line-entry system and searching for RiPPs having similar sequences or chemical structures. The backend database of RiPPMiner provides information about modification system, precursor sequence, leader and core sequence, modified residues, cross-links and gene cluster for more than 500 experimentally characterized RiPPs. RiPPMiner is available at http://www.nii.ac.in/rippminer.html.",2017,Nucleic Acids Research
"á¼€ÏÏ‡á½´ Ï„á¿†Ï‚ Î¸Î±Î»Î¬Ï„Ï„Î·Ï‚ - á¼€ÏÏ‡á½´ Ï„á¿¶Î½ ÎºÎ±Îºá¿¶Î½? Kompetitive Motivationen bei Thukydides, Ps.-Xenophon und Isokrates","After observing that the thalassocracy was a key point of political consideration in the 5th and 4th centuries B.C., this article focuses on the moral interplay between Thucydides, Ps.-Xenophon and Isocrates. A proper analysis of the semantics of greed, ambition and power in an intertextual dialogue and from the sea-hegemony perspective attempts to show how competitive values of time , deos and ophelia have influenced the political thinking in the 5th and 4th centuries B.C. and how Isocrates applies a moral approach in pursuing the success and in combining competitive and cooperative values, which he defines as â€žjust greedâ€. The special weight is laid by Isocrates instead of deos on the combination of virtue and eunoia as far as success in foreign politics is concerned, but he does not condemn entirely the political realism of Thucydides.",2016,Historika : Studi di Storia Greca e Romana
Elastic Net for Cox's Proportional Hazards Model with a Solution Path Algorithm.,"For least squares regression, Efron et al. (2004) proposed an efficient solution path algorithm, the least angle regression (LAR). They showed that a slight modification of the LAR leads to the whole LASSO solution path. Both the LAR and LASSO solution paths are piecewise linear. Recently Wu (2011) extended the LAR to generalized linear models and the quasi-likelihood method. In this work we extend the LAR further to handle Cox's proportional hazards model. The goal is to develop a solution path algorithm for the elastic net penalty (Zou and Hastie (2005)) in Cox's proportional hazards model. This goal is achieved in two steps. First we extend the LAR to optimizing the log partial likelihood plus a fixed small ridge term. Then we define a path modification, which leads to the solution path of the elastic net regularized log partial likelihood. Our solution path is exact and piecewise determined by ordinary differential equation systems.",2012,Statistica Sinica
BARD1 serum autoantibodies for the detection of lung cancer,"PURPOSE
Currently the screening for lung cancer for risk groups is based on Computed Tomography (CT) or low dose CT (LDCT); however, the lung cancer death rate has not decreased significantly with people undergoing LDCT. We aimed to develop a simple reliable blood test for early detection of all types of lung cancer based on the immunogenicity of aberrant forms of BARD1 that are specifically upregulated in lung cancer.


METHODS
ELISA assays were performed with a panel of BARD1 epitopes to detect serum levels of antibodies against BARD1 epitopes. We tested 194 blood samples from healthy donors and lung cancer patients with a panel of 40 BARD1 antigens. Using fitted Lasso logistic regression we determined the optimal combination of BARD1 antigens to be used in ELISA for discriminating lung cancer from healthy controls. Random selection of samples for training sets or validations sets was applied to validate the accuracy of our test.


RESULTS
Fitted Lasso logistic regression models predict high accuracy of the BARD1 autoimmune antibody test with an AUC = 0.96. Validation in independent samples provided and AUC = 0.86 and identical AUCs were obtained for combined stages 1-3 and late stage 4 lung cancers. The BARD1 antibody test is highly specific for lung cancer and not breast or ovarian cancer.


CONCLUSION
The BARD1 lung cancer test shows higher sensitivity and specificity than previously published blood tests for lung cancer detection and/or diagnosis or CT scans, and it could detect all types and all stages of lung cancer. This BARD1 lung cancer test could therefore be further developed as i) screening test for early detection of lung cancers in high-risk groups, and ii) diagnostic aid in complementing CT scan.",2017,PLoS ONE
"Research of Features in the Pulse Waves of HBP Patients based on Principal Component Analysis and LS,Lasso","Objective: To study the features in the pulse waves of HBP patients.Methods: To collect the pulse waves of HBP patients and compare them with normal volunteers.We choose continuous cycles waves with different period length and then do the LSQ regression by 12 harmonics fitting which are correspond with every cycle lengths to build mathematical model and extract 193 parameters after pretreatment,then do principal component analysis and regression for classification and identification to find the features of HBP patients pulse waves by EFBLS.Result: There are significant differences between HBP patients pulse waves and the normal volunteers'and the accurate rate is 81% to identify by principal component analysis,the accurate rate of LSQ regression by 7 features is 93%,of Lasso is 82%.And the features distribute at different places in radial artery called ""zuoguan"" and ""youchi"" in pulse-diagnosis of TCM.Conclusion: Pathological changes of HBP patients can embody in pulse waves we collected from the radial artery and the characters of the waves change regularly in special places.This research can offer some scientific basis for taking pulse in radial artery in TCM,also for the method of feeling different places when pulse-taking.",2013,Chinese Journal of Basic Medicine in Traditional Chinese Medicine
A Joint Graphical Model for Inferring Gene Networks Across Multiple Subpopulations and Data Types.,"Reconstructing gene networks from gene expression data is a long-standing challenge. In most applications, the observations can be divided into several distinct but related subpopulations and the gene expression measurements can be collected from multiple data types. Most existing methods are designed to estimate a single gene network from a single dataset. These methods may be suboptimal since they do not exploit the similarities and differences among different subpopulations and data types. In this article, we propose a joint graphical model to estimate the multiple gene networks simultaneously. Our model decomposes each subpopulation-specific gene network as a sum of common and unique components and imposes a group lasso penalty on gene networks corresponding to different data types. The gene network variations across subpopulations can be learned automatically by the decompositions of networks, and the similarities and differences among data types can be captured by the group lasso penalty. The simulation studies demonstrate that our method outperforms the state-of-the-art methods. We also apply our method to the cancer genome atlas breast cancer datasets to reconstruct subtype-specific gene networks. Hub nodes in the estimated subnetworks unique to individual cancer subtypes rediscover well-known genes associated with breast cancer subtypes and provide interesting predictions.",2019,IEEE transactions on cybernetics
Etude de la diversitÃ© des microorganismes thermophiles des sources thermales de Tunisie,"Les sources thermales de Tunisie sont localisees dans tout le pays avec une concentration dans le sud et le nord est (region de Korbous). La majorite de ces sources sont exploitees essentiellement en thalassotherapie et pour alimenter des bains thermaux (hammam). Si des collections d'isolat ont ete obtenues a partir de quelques unes de ces sources, les travaux d'ecologie microbiennes sur ces environnements sont tres peu nombreux et la diversite microbienne d'un grand nombre de ces sources demeure inconnue. En consequence, une approche moleculaire a ete realisee afin d'analyser la diversite microbienne globale de ces sources. GrÃ¢ce a l'analyse de l'ADNR 16S extraits des sediments et de l'eau, les groupes microbiens les plus dominants ont ete selectionnes et des banques de clones archaea et bacteria ont ete construits pour chaque source. Cette etude a donc permisÂ : I) de decrire la diversite microbienne des sources thermales les plus chaudes de la Tunisie. II) de determiner les parametres influencant cette diversite. En complement, des cultures dâ€™enrichissements ont ete effectues et ont permis deÂ : III) construire une collection d'isolats dont quatre nouvelles especes appartenant aux genres anoxybacillus, thermosipho, fervidobacterium et thermoanaerovibrio qui ont ete decrites.",2011,
Risk factors for postoperative urinary tract infection and urinary retention in patients undergoing surgery for colorectal cancer.,"The aim of this study was to analyze risk factors for postoperative urinary tract infection (UTI) and urinary retention (UR) in patients with colorectal cancer. Using Nationwide Inpatient Sample 2006-2009, a retrospective analysis of surgical patients with colorectal cancer was conducted. Patients were stratified into groups, with or without UTI/UR. The LASSO algorithm for logistic regression identified independent risk factors. A total of 93,931 surgical patients with colorectal cancer were identified. The incidences of UTI and UR were 5.91 and 2.52 per cent, respectively. Overall in-hospital mortality was 2.68 per cent. The UTI group demonstrated significantly higher in-hospital mortality rates compared with those without. Both UTI and UR groups were associated with prolonged hospital stay and increased hospital charge. Multivariate logistic regression analysis revealed age older than 60 years, females, anemia, congestive heart failure, coagulopathy, diabetes with chronic complications, fluid and electrolyte, paralysis, pulmonary circulation disorders, renal failure, and weight loss were independent risk factors of UTI. Age older than 60 years, male gender, rectal and rectosigmoid cancers, and postoperative anastomotic leakage and ileus were independent risk factors for UR. Postoperative UTI increases in-house mortality. Postoperative UTI/UR in patients with colorectal cancer increases length of stay and hospital charges. Knowledge of these specific risk factors for UTI and UR is needed to counsel patients and prevent these complications in this high-risk population.",2012,The American surgeon
"L'effimero quotidiano. Uno studio sulla moda e i suoi spazi nella societÃ  ""liquida""","Obiettivo della presente tesi e comprendere come anche lo moda ed i suoi oggetti incorporino i caratteri della precarieta presente in tutti gli aspetti della societa contemporanea. In un mondo in cui tutto scappa e nulla resta, lo modalita quali l'usa e getta, l'affitto dei capi o degli oggetti, illeasing ed altre soluzioni transitorie sembrano prendere il soprawento. 
 La moda, intesa come ""costume"", ha perso quasi immediatamente, nella storia dell'umanita, i connotati iniziali di necessita umana, correlata alla soprawivenza, di coprirsi ed e diventata elemento di distinzione sociale o caratterizzazione legata ad un ruolo sociale. 
 La moda, quindi, come status. 
 La societa tradizionale, anche quando era caratterizzata da condizioni economiche e sociali differenti, era intrisa da una mentalita, diffusa a tutti i livelli di casta, stato o classe, fortemente basata sul ""possesso permanente"" che, a partire dall'uso del bene, portava al successivo riutilizzo dello stesso con il medesimo fine: inizialmente attraverso modifiche e riparazioni, poi con il riciclo per usi differenti, fino al termine della sua efficienza, non dopo averne utilizzato ogni parte e sfruttato ogni funzione. Si e quindi passati da una societa che dava un peso enorme alla durata di un bene ad una che agisce elevando a valori lo novita e lo transitorieta, e che ha ridotto drasticamente il lasso di tempo tra il provare un desiderio e lo sua soddisfazione. Annullando il 
 divario tra utilita e desiderabilita, ne conseguente lo cessazione di questa ultima, quasi nell'immediato: cio da luogo all'inutilita e, subito dopo, al rifiuto. 
 La moda (dal latino modus, maniera, norma, regola) diventa, allora, simbolo e lente d'ingrandimento di questa societa contemporanea effimera (dal greco ephemeros, composto da epi e emero, che dura solo un giorno), in cui il desiderio di possesso diventa 
 solo la ricerca del soddisfacimento di un desiderio temporaneo, legato ad una rapida appropriazione e al suo immediato conseguente smaltimento. L' analisi del possesso, della sua trasformazione e delle sue forme attuali, unite all'osservazione dei fenomeni legati alla moda e all'abbigliamento sia nei termini di oggetti sia nella loro collocazione spaziale secondo precise regole di progettazione architettonica costituiscono il corpo di questa dissertazione. 
 La tesi si sviluppera evidenziando alcuni argomenti principali: 
 L'EFFIMERO QUOTIDIANO 
 La prima parte evidenziera le grandi trQsformazioni, specialmente sociali, che hanno investito il nostro tempo; massificazione, sviluppo in ambito tecnologico, dei trasporti e delle comunicazioni, processi di affermazione dell'individualizzazione e intensificazione delle specializzazioni in ambito lavorativo. Questi aspetti sono analizzati al fine di comprendere meglio cosa e accaduto nei tempi piu recenti e quali meccanismi stanno alla base della societa odierna. Per una migliore comprensione di questi mutamenti mi awarro delle teorie di grandi pensatori, tra i quali Baruch Spinoza, ErikFromm e Zigmund Bauman, che forniranno alcune chiavi di lettura di grande interesse ed utilita. In seconda analisi mi occupero del concetto di possesso: del suo significato, delle mutazioni di questo nel tempo, del rapporto con altri concetti ad esso legati (ad esempio durata, utilita) ed alle forme piu attuali (Ieasing, multiproprieta, noleggio, credito al consumo); per completare l'analisi concettuale faro un tuffo nel passato, giungendo fino al Medioevo, per verificare quanti degli elementi della societa contemporanea fossero gia presenti ed in che modo, e quali differenze siano emerse col passare dei secoli. 
 MODA E STILE 
 A questa parte spettera il compito di indagare sui molteplici significati della ""moda"" attraverso l'analisi delle diverse definizioni che ne son state date nel tempo. I molteplici ambiti (antropologico, storico, economico, sociale etc.) che si sono interessati allinguaggio della moda ed alla sua evoluzione, consentiranno di evidenziare sfaccettature e significati differenti ma profondamente legati, di estremo interesse. Seguira un'analisi delle trasformazioni della moda in termini di ""modalita di produzione"": l'esame di questa evoluzione e volta alla volonta di verificare quanto le trasformazioni legate alla creazione dell'abito costituiscano una sorta di ""specchio"" dei cambiamenti piu profondi che caratterizzeranno le societa corrispondenti. Verranno indagati i significati di ""omologazione"" e di ""distinzione"" che connotano fortemente il concetto di ""moda"" e le letture che di essi fanno pensatori come Simmel, Veblen, Bourdieu, Acquaviva, Bauman e Mary Douglas. Si analizzeranno le forme piu attuali della moda, emerse proprio dall'evoluzione di quei concetti contemporanei legati all' ""a-temporalita"" ed all'""a-spazialita"". 
 MODA E CONSUMO 
 In questa parte mi occupero degli aspetti di natura piu filosofica della moda. Innanzitutto ci si interessera al ""consumo"": si analizzeranno le motivazioni che hanno portato al sorgere della ""cultura del consumo"" al posto della ""cultura del possesso"", le modificazioni che si sono verificate nella condotta dell'uomo in tal senso e si valuteranno i significati ed i processi indotti dal consumo, soffermandosi sulla piu attuale mentalita dell ""usa e getta"". Si valuteranno, infine, le componenti intrinseche dell""'oggetto"": il suo essere segno, mezzo di comunicazione, motore sociale, elemento evocativo. Si definiranno inoltre i concetti di marca e trends. 
 ESASPERAZIONI ATTUALI 
 Quanto e importante lo shopping? Quale funzione ricopre nella societa attuale? Queste sono alcune delle domande a cui cerchero di dare una risposta in questo paragrafo. Nuove figure professionali e la frenetica attivita di specialisti nel settore concorrono alla creazione dell' ""immagine perfetta"" degli individui. Si vive in un'era in cui ogni cosa deve apparire ""scintillante"", le merci quanto le persone,e si ricorre cosi ad ogni possibile artificio pur di fornire questa sembianza impeccabile. Sara importante, cost occuparsi anche del palcoscenico su cui quotidianamente si mette in atto lo spettacolo del consumo: la ""vetrina"", intesa come luogo di incontro tra merci e consumo e delle implicazioni commerciali ed urbane che essa ha comportato, non tralasciando anche le conseguenze visibili della ""vetrinizzazione"", una sorta di ""resa"" pubblica, dell'individuo e della societa. 
 MODA E PROGETTO 
 L'ultimo argomento su cui mi soffermaro, ponendo attenzione sia alle implicazioni commerciali sia a quelle progettuali, e il punto vendita. Il negozio si e modificato nel tempo come luogo e come concezione: non e solo piu contenitore ma anche luogo che eroga servizi, attirando, comunicando una filosofia di vivere, spingendo il consumatore a tornare. E' un ""luogo pubblico"", in cui si stabiliscono contatti e si percepiscono emozioni. Sivaluteranno cosi sia le componenti legate alla distribuzione, alla trasmissione di linguaggi specifici sia gli elementi che consentono la strutturazione fisica ed emotiva del punto vendita.",2008,
Studi Perbandingan Implementasi Hasil Belajar Siswa Pada Mata Pelajaran Fisika Dengan Menggunakan Model Cooperative Learning Tipe Jigsaw Dan Tipe Stad Di Smp Negeri 6 Gorontalo,"This study aims to determine the comparative learning outcomes between students taught with cooperative learning model (C ooperative Learning) type STAD and type Jigsaw. The research was conducted in SMPN 6 Gorontalo. Sampling using random cluster sampling, in this study amounted to 58 people from SMPN 6 Gorontalo on class IX 1and class IX3 as a subject. Data capture learning outcomes using test instruments learning outcomes (6 items). Data analysis using t-test and the values obtained thitung significant level of 4.88 at Î± = 0.05 and obtained ttable at 2.00, then thitung> ttable and the average value for the class of Jigsaw is 77.38% and the average v value for the classof STAD is 74.31%, so it can be concluded there is a difference between learning outcomes of students who are taught physics by type Jigsaw cooperative learning model and type STAD, it can be said that the learning outcomes being taught the class with the type of Jigsaw cooperative learning model is better compared to classes taught by model type STAD cooperative learning.",2013,
La certificazione di sostenibilitÃ  tra strumento di progetto e verifica : applicazione ragionata del protocollo LEED NC al Campus Tiscali,"Il presente lavoro ha come oggetto di studio il Campus Tiscali, situato in localita Sa Illetta, a Cagliari, dove nel 2001 lâ€™omonima azienda italiana e sarda del settore delle telecomunicazioni, a seguito della propria affermazione sul mercato internazionale, ha deciso di realizzare la sua sede centrale. A dieci anni dallâ€™inaugurazione, avvenuta nel 2003, il complesso di uffici e strutture di servizio ospita funzioni che hanno saputo adeguarsi ai cambiamenti del mercato, attraverso una parziale integrazione della destinazione dâ€™uso principale con lâ€™emergente realta del co-working e lâ€™apertura alla collaborazione sinergica con altri professionisti del settore. Attivita flessibili sono racchiuse entro un contenitore statico, concepito originariamente come innovativo, ma soggetto ad una rapida obsolescenza delle soluzioni impiegate, devoluzione tecnologica, alla luce di una crescente sensibilita verso le tematiche ambientali, e stata parallelamente accompagnata da un processo di adeguamento normativo, che in edilizia stabilisce da un lato limiti prescrittivi, mentre dallâ€™altro fornisce strumenti per incrementare, misurare e certificare le prestazioni degli edifici. 
La valutazione della sostenibilita di un edificio e un problema complesso, che richiede approfondite considerazioni relative alla scala del progetto e allâ€™intervallo di tempo entro il quale si estende lâ€™analisi. Le conseguenze generate da un intervento antropico non riguardano la sola impronta lasciata sul sito, ma si estendono alla scelta dei componenti di dettaglio, dei processi produttivi che hanno permesso di ottenerli e delle modalita per la loro messa in opera, che condizioneranno gli scenari successivi alla realizzazione. In questo modo si richiama un approccio onnicomprensivo che contempla lâ€™intero ciclo di vita dellâ€™edificio, passando per il progetto, la costruzione, la gestione e la dismissione, fasi nelle quali la quantificazione degli impatti puo subire notevoli variazioni. Secondo i dati forniti dalla Commissione Europea, gli edifici in Europa comportano: 
â€¢ lâ€™impiego del 50% dei materiali estratti in fase di costruzione; 
â€¢ un consumo di acqua pari al 30% in fase di costruzione e gestione; 
â€¢ il consumo del 42% dellâ€™energia e il 35% delle emissioni di gas serra in fase di gestione 
â€¢ una produzione di rifiuti pari al 30% in fase di costruzione e dismissione. 
Per poter assistere ad un miglioramento degli scenari rappresentati da questi dati occorre agire non solo sulla limitazione degli effetti negativi di un edificio, ma incentivare anche la capacita di influenzare positivamente la sostenibilita nelle sue componenti fondamentali, di tipo ambientale, economico e sociale, secondo la definizione data dalla Norma ISO 15392:2008, Sustainabiiity in building construction. 
Nel 2000 il Premio Nobel per la Chimica Paul J. Crutzen utilizzo il termine â€œantropoceneâ€ per indicare lâ€™era geologica attuale, il cui inizio simbolico fu individuato dallo stesso scienziato con lâ€™invenzione del motore a vapore da parte di James Watt, sul finire del XVIII secolo. La specificita di questa definizione risiede nellâ€™aver riconosciuto per la prima volta ad una sola specie, quella umana, la capacita di alterare sensibilmente i cicli dellâ€™acqua e del carbonio, attraverso un inarrestabile consumo di risorse. 
La seconda meta del XX secolo e segnata da alcune tappe fondamentaliodello di sviluppo seguito fino a quel m Carson pubblica â€œSileni Springâ€, titolo evonitense colloe ormai estinte, a seguito degli effetti dnamics Group del Massachusetts Institute of Techndal Club dnaturali, il prodotto industriale procarebbero portato ad un collasso entro i cento anni orret, e da sottolineare come sia stato messo s basato sullâ€™analogia tra il concetto di sviluppo tere istituzionale in due momenti fondamentali. Nello stesso anno, in occasione della Conferenza delle Nazioni Unite, viene approvata la Dichtituito il Programma ambientale delle Nazment pubblica il Rapporto â€œOur Commommissione cui si deve la definizione di sviluppo sostenibile, inteso come lo sviluppo capace di soddisfare i bisogni delle generazioni attuali senza compromettere gli stessi bisogni da parte delle generazioni future. Il documento raccoglie in tre parti le preoccupazioni, le sfide e gli sforzi comuni dei Paesi partecipanti. 
Il valore delle azioni intraprese a scala locale su un problema di portata globale viene sancito dallâ€™Earth Summit di Rio de Janeiro del 1992, che tra i suoi risultati comprende lâ€™iniziativa Agenda 21, strumento da declinare ai diversi livelli del territorio per dare luogo, entro il XXI secolo, ad unaseriedi azioni capaci di incidere gli equilibri esistenti, a partire dalle citta. Successivamente alla Carta delle citta europee per uno sviluppo durevole e sostenibile, siglata ad Aalborg nel 1994, la necessita di ridimensionare gli impatti del settore edilizio emerge dalla Conferenza Habitat II, a Istanbul, nel 1996. 
Nel 2006 lâ€™American Society of Heating, Refrigerating and Air Conditioning Engineers (ASHRAE) ha introdotto il concetto di Green Building, basato sui criteri del consumo netto di energia, dato dal bilancio tra fonti non rinnovabili e rinnovabili, pari a zero, minimizzazione delle emissioni, di rifiuti solidi e liquidi, nonche contenimento dellâ€™impatto sugli ecosistemi. Al contempo, le strategie messe in atto per ottenere questi risultati non avrebbero dovuto sacrificare, bensi incentivare, la qualita della vita, reale e percepita, da parte degli utenti.3 Tuttavia, dire quale effettivamente sia lâ€™architettura sostenibile, davanti ad approcci anche distanti gli uni dagli altri, risulta unâ€™operazione complessa. A partire dalla necessita di dare chiara lettura dei risultati raggiunti, a prescindere dalla capacita del singolo progetto di darne comunicazione dietro ad unâ€™immagine accattivante, nascono gli strumenti di valutazione. 
Per stabilire in che misura un progetto si impegni a perseguire questi obiettivi, possiamo individuare quali strumenti di valutazione sono disponibili, distinguendoli secondo due approcci: 
â€¢ qualitativo e a punteggio, con il quale il livello di sostenibilita ambientale e determinato dal soddisfacimento di specifici requisiti e dal totale dei punti acquisiti per ciascuno, in base allâ€™attribuzione di pesi differenti; 
â€¢ quantitativo, con il quale e possibile determinare in maniera rigorosa lâ€™energia incorporata dallâ€™edificio nel suo intero ciclo di vita, fino alla dismissione, fornendo un indicatore sintetico. 
Mentre in questâ€™ultima categoria ricade la Life Cycle As-sessment (LCA), possiamo includere nella prima le certificazioni di sostenibilita LEED4 (Stati Uniti), BREEAM5 (Inghilterra) e CASBEE6 (Giappone). Tali protocolli rappresentano uno dei possibili strumenti in grado di individuare le prestazioni di un edificio, sulla base di categorie relative a tematiche differenti ma integrate allâ€™interno del progetto, e di classificarlo secondo un sistema a livelli. Tra i fattori presi in considerazione troviamo anche il consumo di energia, il quale non costituisce tuttavia un parametro esclusivo. Questo dettaglio distingue la certificazione ambientale dallâ€™attestato di prestazione energetica attualmente utilizzato in Italia, che viene espresso in funzione di un intervallo di classi, distinte secondo lâ€™indicatore dei kWh/m2anno o dei kWh/m3anno, in relazione alla destinazione dâ€™uso. Mentre in questo secondo caso si ha una prospettiva limitata ai dati relativi al fabbisogno energetico, il primo strumento ha alla base una visione piu ampia. Dal punto di vista della cogenza, la redazione dellâ€™attestato di prestazione energetica e imprescindibile per le operazioni immobiliari di vendita o locazione di immobili pubblici e privati, oltre che un elemento influente nella scelta da parte degli acquirenti. Solo la Regione Friuli Venezia Giulia ha imposto che la certificazione ambientale, denominata nello specifico contesto VEA, sia necessaria nei medesimi casi, mentre nella maggior parte del lâ€™obbligo solo per i nuovi edici pubblici, i nuovi immobili destinati a Edilizia Residenziale Pubblica e nel caso si voglia accedere a particolari bonus e incentivi. 
In Italia sono impiegati prevalentemente due protocolli. Il protocollo ITACA e il piu utilizzato a livello pubblico, sebbene con modifiche implementate per un miglior adattamento alle specificita regionali, cui si e tentato di porre ordine attraverso una pubblicazione nazionale nel 2011. Sul piano privato prevale il LEED, uno strumento di respiro internazionale che puo rappresentare un investimento in termini di costi e di immagine.",2014,
GRAM: A GeneRAlized Model to predict the molecular effect of a non-coding variant in a cell-type specific manner,"There has been much effort to prioritize genomic variants with respect to their impact on ""function"". However, function is often not precisely defined: sometimes it is the disease association of a variant; on other occasions, it reflects a molecular effect on transcription or epigenetics. Here, we coupled multiple genomic predictors to build GRAM, a GeneRAlized Model, to predict a well-defined experimental target: the expression-modulating effect of a non-coding variant on its associated gene, in a transferable, cell-specific manner. Firstly, we performed feature engineering: using LASSO, a regularized linear model, we found transcription factor (TF) binding most predictive, especially for TFs that are hubs in the regulatory network; in contrast, evolutionary conservation, a popular feature in many other variant-impact predictors, has almost no contribution. Moreover, TF binding inferred from in vitro SELEX is as effective as that from in vivo ChIP-Seq. Second, we implemented GRAM integrating only SELEX features and expression profiles; thus, the program combines a universal regulatory score with an easily obtainable modifier reflecting the particular cell type. We benchmarked GRAM on large-scale MPRA datasets, achieving AUROC scores of 0.72 in GM12878 and 0.66 in a multi-cell line dataset. We then evaluated the performance of GRAM on targeted regions using luciferase assays in the MCF7 and K562 cell lines. We noted that changing the insertion position of the construct relative to the reporter gene gave very different results, highlighting the importance of carefully defining the exact prediction target of the model. Finally, we illustrated the utility of GRAM in fine-mapping causal variants and developed a practical software pipeline to carry this out. In particular, we demonstrated in specific examples how the pipeline could pinpoint variants that directly modulate gene expression within a larger linkage-disequilibrium block associated with a phenotype of interest (e.g., for an eQTL).",2019,PLoS Genetics
Redefining the Protein-Protein Interface: Coarse Graining and Combinatorics for an Improved Understanding of Amino Acid Contributions to the Protein-Protein Binding Affinity.,"The ability to intervene in biological pathways has for decades been limited by the lack of a quantitative description of protein-protein interactions (PPIs). Herein we generate and compare millions of simple PPI models for insight into the mechanisms of specific recognition and binding. We use a coarse-grained approach whereby amino acids are counted in the interface, and these counts are used as binding affinity predictors. We perform lasso regression, a modern regression technique aimed at interpretability, with every possible amino acid combination (over 106 unique feature sets) to select only those amino acid predictors that provide more information than noise. This approach circumvents arbitrary binning and assumptions about the binding environment that obscure other binding affinity models. Aggregated analysis of these models trained at various interfacial cutoff distances informs the roles of specific amino acids in different binding contexts. We find that a simple amino acid count model outperforms detailed intermolecular contact and binned residue type models. We identify the prevalence of serine, glycine, and tryptophan in the interface as particularly important for predicting binding affinity across a range of distance cutoffs. Although current sample size limitations prevent a robust consensus model for binding affinity prediction, our approach underscores the relevance of a residue-based description of the protein-protein interface to increase our understanding of specific interactions.",2017,Langmuir : the ACS journal of surfaces and colloids
"Policy evaluation, high-dimension and machine learning","This dissertation is comprised of three essays that apply machine learning and high-dimensional statistics to causal inference. The first essay proposes a parametric alternative to the synthetic control method (Abadie and Gardeazabal, 2003; Abadie et al., 2010) that relies on a Lasso-type first-step. We show that the resulting estimator is doubly robust, asymptotically Gaussian and ``immunized'' against first-step selection mistakes. The second essay studies a penalized version of the synthetic control method especially useful in the presence of micro-economic data. The penalization parameter trades off pairwise matching discrepancies with respect to the characteristics of each unit in the synthetic control against matching discrepancies with respect to the characteristics of the synthetic control unit as a whole. We study the properties of the resulting estimator, propose data-driven choices of the penalization parameter and discuss randomization-based inference procedures. The last essay applies the Generic Machine Learning framework (Chernozhukov et al., 2018) to study heterogeneity of the treatment in a randomized experiment designed to compare public and private provision of job counselling. From a methodological perspective, we discuss the extension of the Generic Machine Learning framework to experiments with imperfect compliance.",2019,
Identification of genes associated with survival of breast cancer patients,"BackgroundWe aimed to investigate the potential of microRNA expression profiles to predict survival in breast cancer.MethodsMicroRNA and mRNA expression data of breast cancer were downloaded from The Cancer Genome Atlas. LASSO regression was used to identify microRNAs signature predicting survival of breast cancer patients. Transfection experiment was conducted to explore the influence of microRNAs on their potential targets.ResultsWe identified 56 differentially expressed microRNAs in breast cancer tissues compared to adjacent normal tissues. 10 microRNAs with non-zero coefficient were selected from the 56 microRNAs using LASSO Cox regression. After predicting the targets for the 10 microRNAs, we further obtained 155 targets that were associated with overall survival of breast cancer patients. Spearmanâ€™s correlation analysis found that the expression of SCUBE2, SCRN3, YTHDF3, ITFG1, ITPRIPL2, and JAK1 was an inversely correlated with their microRNAs. Transfection experiment showed that YTHDF3 was down-regulated in cells transfected with miR-106b-5p mimics compared with those transfected with negative control of mimics (fold change 4.21; Pâ€‰<â€‰0.01).ConclusionsIn conclusion, we identified a 10-miRNA signature associated with prognosis of breast cancer patients. The expression of YTHDF3 was down-regulated by miR-106b-5p.",2018,Breast Cancer
[Variation of the parasite density of Plasmodium falciparum in asymptomatic carriers: consequences for malaria chemoresistance studies].,"During the studies on malaria chemoresistance, we noted great variations in parasite density of Plasmodium falciparum between screening in the morning and final selection in the afternoon, in asymptomatic people. To better understand this phenomenon, we conducted a study in october 1987 on primary school children in a village near the city of Bobo-Dioulasso, at the peak malaria prevalence. We performed 3 blood-smears at 8 a.m., 2 p.m. and 8 p.m., on Day 0 and Day 4, to an initial number of 86 children, aged from 6 to 9 years. By the end of the study 44 children remained who fulfilled the inclusion criteria Among them 35 showed a parasitaemia on Day 0 and 9 remained negative. On Day 4, 28 were positive and 16 remained negative. Of the 35 children positive at entry to the study 16 remained continuously positive, the others were negative on at least one occasion. Of the 28 children positive on Day 4, 14 remained continuously positive. For the 16 people with a parasitaemia continuously positive on Day 0.7 (43.7 p. cent) became spontaneously negative on Day 4. But considering the small size of our sample, the analysis of the nycthemeral variation and of the variation between the two days did not show a significant difference. Further studies involving a greater number of blood-smears during a longer period and concerning more people, should be conducted. The possibility of spontaneous negativation of the parasitaemia without drug absorption shows that there are some cases of false malaria chemosensitivity that are declared when the in vivo tests are not coupled with in vitro tests.",1992,Medecine tropicale : revue du Corps de sante colonial
Development and validation of a survival model for lung adenocarcinoma based on autophagy-associated genes,"Given that abnormal autophagy is involved in the pathogenesis of cancers, we sought to explore the potential value of autophagy-associated genes in lung adenocarcinoma (LUAD). RNA sequencing and clinical data on tumour and normal samples were acquired from The Cancer Genome Atlas (TCGA) database and randomly assigned to training and testing groups. Differentially expressed autophagy-associated genes (AAGs) were screened. Within the training group, Cox regression and Lasso regression analyses were conducted to screen five prognostic AAGs, which were used to develop a model. Kaplanâ€“Meier (KM) and receiver operating characteristic (ROC) curves were plotted to determine the performance of the model in both groups. Immunohistochemistry was used to demonstrate the differential expression of AAGs in tumour and normal tissues at the protein level. Gene Ontology (GO) functional annotation and Kyoto Encyclopedia of Genes and Genomes (KEGG) pathway enrichment analyses were utilized to further elucidate the roles of AAGs in LUAD. The data from the TCGA database included 497 tumour and 54 normal samples, within which 30 differentially expressed AAGs were screened. Using Cox regression and Lasso regression analyses for the training group, 5 prognostic AAGs were identified and the prognostic model was constructed. Patients with low risk had better overall survival (OS) in the training group (3-year OS, 73.0% vs 48.0%; 5-year OS, 45.0% vs 33.8%; Pâ€‰=â€‰1.305Eâˆ’04) and in the testing group (3-year OS, 66.8% vs 41.2%; 5-year OS, 31.7% vs 25.8%; Pâ€‰=â€‰1.027Eâˆ’03). The areas under the ROC curves (AUC) were significant for both the training and testing groups (3-year AUC, 0.810 vs 0.894; 5-year AUC, 0.792 vs 0.749). We developed a survival model for LUAD and validated the performance of the model, which may provide superior outcomes for the patients.",2020,Journal of Translational Medicine
A Comparative Study of Feature Selection Methods on Genomic Datasets,"Feature selection plays an important role in reducing the size of datasets by choosing the most informative features and discarding the rest. The use of feature selection in microarray datasets for detecting cancer is widely investigated. In this paper we provide a series of comparisons between perturbation-based feature selection (PFS) and traditional methods, such as principal component analysis (PCA), correlation based feature selection (CFS), and least-angle regression (LARS), and more recent methods, such as Hilbert-Schmidt independence criterion Lasso (HSIC-Lasso), minimum redundancy maximum relevance (mRMR), and a feature selection using support vector machines (FS-SVM). The performance of each method is demonstrated by conducting a series of comparisons on genomic cancer datasets, as well as, inflammatory bowel disease datasets. The experiments show that PFS and HSIC-Lasso are both scalable to large datasets.",2019,2019 IEEE 32nd International Symposium on Computer-Based Medical Systems (CBMS)
Scaled sparse linear regression,"Scaled sparse linear regression jointly estimates the regression coefficients and noise level in a linear model. It chooses an equilibrium with a sparse regression method by iteratively estimating the noise level via the mean residual square and scaling the penalty in proportion to the estimated noise level. The iterative algorithm costs little beyond the computation of a path or grid of the sparse regression estimator for penalty levels above a proper threshold. For the scaled lasso, the algorithm is a gradient descent in a convex minimization of a penalized joint loss function for the regression coefficients and noise level. Under mild regularity conditions, we prove that the scaled lasso simultaneously yields an estimator for the noise level and an estimated coefficient vector satisfying certain oracle inequalities for prediction, the estimation of the noise level and the regression coefficients. These inequalities provide sufficient conditions for the consistency and asymptotic normality of the noise-level estimator, including certain cases where the number of variables is of greater order than the sample size. Parallel results are provided for least-squares estimation after model selection by the scaled lasso. Numerical results demonstrate the superior performance of the proposed methods over an earlier proposal of joint convex minimization. Copyright 2012, Oxford University Press.",2011,Biometrika
Are Higher-Order Factors Useful in Pricing the Cross-Section of Hedge Fund Returns?,"This paper investigates hedge fundsâ€™ exposures to various risk factors across different investment strategies through models with both linear and second-order factors. We extend the analysis from an augmented linear model based on FamaÂ & French (1993) and Fung & Hsieh (2001) to second-order models that include all quadratic and interaction terms by adopting a novel multistep strategy that combines the variable selection capabilities of the LASSO regression with the Fama & MacBeth (1973) two-step method. We find that, for some strategies, several quadratic and interaction terms are statistically significant. Nonetheless, there is no evidence that the second-order models have more overall explanatory or predictive power than the linear model. Moreover, while both linear and second-order models perform well for directional funds (like emerging markets, event driven and managed futures), missing factors may still remain for semi-directional funds, such as fund of funds, long/short equity hedge and multi-strategy.",2019,Brazilian Review of Finance
"Continuous coil assembly, test fixture with continuous coil assembly and test methods","Eine Durchlaufspulenanordnung zur Verwendung in einer Prufvorrichtung zum Prufen von Langprodukten im Durchlaufverfahren hat eine Erregerspulenanordnung mit einer Erregerspule (122), die eine Durchlassoffnung (112) zum Hindurchfuhren eines Langprodukts (190) entlang einer Durchlaufrichtung umschliest. Die Erregerspulenanordnung hat eine Anschlusseinrichtung zum Anschluss der Erregerspule an eine Wechselspannungsquelle (130). Weiterhin ist eine um die Durchlassoffnung herum angeordnete Empfangerelementanordnung vorgesehen, die eine Anschlusseinrichtung (148) zum Anschluss der Empfangerspulenanordnung an eine Auswerteeinrichtung (150) der Prufvorrichtung aufweist. Die vorzugsweise als Empfangerspulenanordnung ausgelegte Empfangerelementanordnung weist zwei oder mehr uber den Umfang der Durchlassoffnung (112) verteilte Segmentelementanordnungen (142-1 bis 142-8) auf, wobei jede Segmentelementanordnung einen Erfassungsbereich hat, der nur einen Umfangsabschnitt des Umfangs der Oberflache des Gegenstandes abdeckt.",2012,
Predicting bacterial resistance from whole-genome sequences using k-mers and stability selection,"BackgroundSeveral studies demonstrated the feasibility of predicting bacterial antibiotic resistance phenotypes from whole-genome sequences, the prediction process usually amounting to detecting the presence of genes involved in antibiotic resistance mechanisms, or of specific mutations, previously identified from a training panel of strains, within these genes. We address the problem from the supervised statistical learning perspective, not relying on prior information about such resistance factors. We rely on a k-mer based genotyping scheme and a logistic regression model, thereby combining several k-mers into a probabilistic model. To identify a small yet predictive set of k-mers, we rely on the stability selection approach (Meinshausen et al., J R Stat Soc Ser B 72:417â€“73, 2010), that consists in penalizing logistic regression models with a Lasso penalty, coupled with extensive resampling procedures.ResultsUsing public datasets, we applied the resulting classifiers to two bacterial species and achieved predictive performance equivalent to state of the art. The models are extremely sparse, involving 1 to 8 k-mers per antibiotic, hence are remarkably easy and fast to evaluate on new genomes (from raw reads to assemblies).ConclusionOur proof of concept therefore demonstrates that stability selection is a powerful approach to investigate bacterial genotype-phenotype relationships.",2018,BMC Bioinformatics
Afrique : Afrique de l'Ouest : Burkina Faso : RÃ©gion des Hauts-Bassins : Province du Houet : Bobo-Dioulasso : Cultures maraÃ®chÃ¨res : Prise de vue 1/2,"Legende manuscrite sur le document original : ''Bobo-Dioulasso. Cultures maraicheres. '' DESCRIPTION COMPLEMENTAIRE : Le maraichage comme phenomene urbain: au cÅ“ur de la ville de Bobo-Dioulasso, sur les rives du marigot le Houet, mise en culture de parcelles. La presence de planches de legumes montre le soin apporte a cette production dont le debouche de proximite est le marche urbain. -- Geolocalisation : approximative centree sur Bobo-Dioulasso.",1983,
Fast Adaptive Algorithm for Robust Evaluation of Quality of Experience,"Outlier detection is an integral part of robust evaluation for crowdsourceable Quality of Experience (QoE) and has attracted much attention in recent years. In QoE for multimedia, outliers happen because of different test conditions, human errors, abnormal variations in context, etc. In this paper, we propose a simple yet effective algorithm for outlier detection and robust QoE evaluation named iterative Least Trimmed Squares (iLTS). The algorithm assigns binary weights to samples, i.e., 0 or 1 indicating if a sample is an outlier, then the outlier-trimmed subset least squares solutions give robust ranking scores. An iterative optimization is carried alternatively between updating weights and ranking scores which converges to a local optimizer in finite steps. In our test setting, iLTS is up to 190 times faster than LASSO-based methods with a comparable performance. Moreover, a varied version of this method shows adaptation in outlier detection, which provides an automatic detection to determine whether a data sample is an outlier without a priori knowledge about the amount of the outliers. The effectiveness and efficiency of iLTS are demonstrated on both simulated examples and real-world applications. A Matlab package is provided to researchers exploiting crowdsourcing paired comparison data for robust ranking.",2014,arXiv: Multimedia
Discovery of Sporo-pollen Fossil of Upper Cretaceous Epoch on the Jiacuo District in North Tibet and Its Implication,"With geologic survey at Dinggu and Jiacuo on a scale of 1:250,000, plentiful sporo-pollen fossils of Schixzaeoisporites, Cyathidtes, Hammenia, Classopllis annulatus and Exespollenites, etc have been found at first within the strata on the Jiacuo district, in combination with year-measuring data of andesite of upper and lower layers, Abushan Formation of the Upper Cretaceous is plotted out from this set of strata which provide the evidence of the red bed ascribing over this area.",2006,Guizhou Geology
Balanced estimation for high-dimensional measurement error models,"Noisy and missing data are often encountered in real applications such that the observed covariates contain measurement errors. Despite the rapid progress of model selection with contaminated covariates in high dimensions, methodology that enjoys virtues in all aspects of prediction, variable selection, and computation remains largely unexplored. In this paper, we propose a new method called as the balanced estimation for high-dimensional error-in-variables regression to achieve an ideal balance between prediction and variable selection under both additive and multiplicative measurement errors. It combines the strengths of the nearest positive semi-definite projection and the combined L1 and concave regularization, and thus can be efficiently solved through the coordinate optimization algorithm. We also provide theoretical guarantees for the proposed methodology by establishing the oracle prediction and estimation error bounds equivalent to those for Lasso with the clean data set, as well as an explicit and asymptotically vanishing bound on the false sign rate that controls overfitting, a serious problem under measurement errors. Our numerical studies show that the amelioration of variable selection will in turn improve the prediction and estimation performance under measurement errors.",2018,Comput. Stat. Data Anal.
Morphometric characterization of thalassohaline Artemia franciscana populations from the Colombian Caribbean,"Aim To establish possible interpopulation relationships among Colombian Artemia franciscana (Crustacea, Anostraca) populations. 
 
 
 
Location Colombian Caribbean coast (Manaure, Galerazamba, Salina Cero and Tayrona) and a similar thalassohaline reference population from San Francisco Bay (SFB-USA). 
 
 
 
Methods Morphometric characters of male and female cultured individuals of A. franciscana were measured. The populations were grouped according to: (1) population type (populations grouped according to two broad regions of origin: North America and the Caribbean coast), and (2) specific geographical origin (populations selected according to five specific local origins: Manaure, Galerazamba, Salina Cero, Tayrona and SFB) and evaluated using forward stepwise discriminant analysis (SPSS, Ver. 10). 
 
 
 
Results Optimal discriminant variables for males grouped by the type of population were left setae and antenna length, and for females they were abdominal length and antenna length. However, for males grouped by their specific geographical origin, the optimal variables were furca length, left setae, antenna length, eye separation, abdominal width and abdominal length, and for the females, they were furca length, abdominal length, left setae and eye separation. Male and female Colombian Caribbean populations were separated from the North American populations. However, our results show that the classification based on male characters provides better group membership than females. 
 
 
 
Main conclusions Male morphometric characters separated the type of population groups more clearly than the female characters, because all Colombian populations were correctly positioned in the Caribbean coast region and the SFB population in the North American region, with no overlapping between the two types, as was the case for the female individuals. Likewise, male individuals correctly position the Salina Cero population to its neighbouring Galerazamba population and to the other Colombian populations. In contrast, female individuals from Salina Cero did not cluster with the other Colombian coast populations (Galerazamba, Tayrona and Manaure) or with the SFB population.",2003,Journal of Biogeography
The Criminal and Labor Market Impacts of Incarceration,"This paper investigates the preand post-release impacts of incarceration on criminal behavior, economic wellbeing and family formation using new data from Harris County, Texas. The research design identifies exogenous variation in the extensive and intensive margins of incarceration by leveraging the random assignment of defendants to courtrooms. I develop a new data-driven estimation procedure to address multidimensional and non-monotonic sentencing patterns observed in courtrooms. My findings indicate that incarceration generates modest incapacitation effects, which are offset in the long-run by an increased likelihood of defendants reoffending after being released. Additional evidence finds that incarceration reduces post-release employment and wages, increases take-up of food stamps, decreases the likelihood of marriage and increases the likelihood of divorce. Based on changes in defendant behavior alone, I estimate that a one-year prison term for marginal defendants conservatively generates $56,200 to $66,800 in social costs, which would require substantial general deterrence in the population to at least be welfare neutral. Keywords: incarceration, recidivism, labor market outcomes, family formation, monotonicity JEL: J24, K42, J62 âˆ—Department of Economics, University of Michigan (email: mgms@umich.edu). I would like to thank Cristian Pop-Eleches, Bernard SalaniÃ© and Miguel Urquiola for their advice and support. I also benefited from conversations with Doug Almond, Sandra Black, Scott Cunningham, Keshav Dogra, Keith Finlay, Colin Hottman, Ju Hyun Kim, Christopher King, Wojciech Kopczuk, Ilyana Kuzmienko, Steve Levitt, John List, Jens Ludwig, Maya Rossin-Slater, Aurelie Ouss, Emily Owens, Kevin Schnepel, Hyelim Son, Patrick Sun, and Lesley Turner. I would also like to thank the participants in the NBER Summer Institute and Columbia Applied Microeconomics Workshop for their comments. I am particularly indebted to the staff at the Ray Marshall Center who have generously hosted my research in Texas. This project would not have been possible without the approval of the Harris County District Clerk, the Harris County Sheriffâ€™s Office the Texas Department of Criminal Justice, the Texas Department of Public Safety, the Texas Health and Human Services Commission, and the Texas Workforce Commission. Funding for this project was provided by the National Science Foundation (SES-1260892). 1 2 MICHAEL MUELLER-SMITH The United States currently has the highest incarceration rate in the world (Walmsley [2009]), a consequence of three decades of dramatic growth in the prison population since the late 1970s (Carson [2013]). Over this same time period governmental expenditures on police protection, judicial and legal systems, and corrections also surged (Bureau of Justice Statistics [1980] and Kyckelhahn [2013]). Recent estimates indicate that the annual U.S. correctional population included over 7 million adults (Glaze and Herberman [2013]), and combined federal, state and local expenditures on justice-related programs topped $260 billion per year. Despite the reach and cost associated with these changes to criminal justice policy, causal evidence on how this use of incarceration has impacted the population remains scarce (see Donohue III [2009]). To help address this gap in the literature, I investigate the impacts of incarceration using original data from Harris County, Texas. The new data is comprised of over 2.6 million criminal court records accounting for 1.1 million unique defendants. It captures the universe of misdemeanor and felony criminal charges between 1980 and 2009 regardless of final conviction status. It has also been linked to state prison and county jail administrative data, unemployment insurance wage records, public assistance benefits, marriage and divorce records as well as future criminal behavior. Taken together, the combined data permits a broad range of policy-relevant analysis, promoting a better understanding of the potential mechanisms underpinning the treatment effects and providing for a more comprehensive cost benefit analysis. The research design leverages the random assignment of criminal defendants to courtrooms as a source of exogenous variation in both the extensive and intensive margins of incarceration. The courts are staffed by judges and prosecutors who differ in their propensity to incarcerate. As a result, which courtroom a defendant is randomly assigned to strongly predicts whether he will be incarcerated and for how long. This increasingly popular identification strategy has been used in a number of applications where judges, case workers, or other types of programs administrators are given discretion on how to respond to a randomly assigned caseload. 1Even though parole boards may adjust some sentences ex-post, my evidence indicates that the courts exert influence over actual time served. 2For studies specifically related to incarceration, see Kling [2006], Di Tella and Schargrodsky [2004], or Aizer and Doyle [2013]. For research in other fields, see Doyle [2007, 2008], Autor and Houseman [2010], Belloni et al. [2012], Munroe and Wilse-Samson [2012], Doyle et al. [2012], French and Song [2012] Maestas et al. [2013], Autor et al. [2013], Dahl et al. [2013], andDobbie and Song [2015]. THE CRIMINAL AND LABOR MARKET IMPACTS OF INCARCERATION 3 The application considered in this paper is moderately more complex than other potential uses of this research design. Sentencing takes on multiple dimensions (e.g. incarceration, fines, drug treatment, etc.) and judges display non-monotonic tendencies (e.g. a judge may incarcerate drug offenders at a relatively higher rate but property offenders at a relatively lower rate). Since failure to account for these features of the data can lead to violations of the exclusion restriction and monotonicity assumption, a new estimation procedure is developed. In this new approach, I relax the first stage equation to allow the impact of court assignment on sentencing outcomes to flexibly respond to observed defendant characteristics. Because this can generate many instruments due to the curse of dimensionality, the least absolute selection and shrinkage operator (LASSO) is used in conjunction with cross validation as a data-driven tool to achieve disciplined dimension reduction without skewing statistical testing. I then use this approach to construct instruments for each observed aspect of sentencing, not just incarceration, in order to control for court tendencies on non-focal sentencing dimensions. My empirical findings indicate that incarceration for marginal defendants is less attractive from a policy perspective than has been shown in prior work. I measure modest incapacitation effects while defendants are in jail or prison: felony defendants are 6 percentage points less likely to be charged with a new criminal offense while incarcerated. This benefit, however, is offset by increases in post-release criminal behavior: each additional year that a felony defendant was incarcerated increases the probability of facing new charges post-release by 5.6 percentage points per quarter. What is particularly concerning about these results is that the incapacitation effect is disproportionately driven by misdemeanor charges, while the post-release criminal behavior shows mainly increases in felony offenses. Partially driving this result is a pattern of former inmates being charged with new crime types. In particular, I find that former inmates are especially likely to commit more property (e.g. theft or burglary) and drug-related crimes after being released, even if these crimes were not their original offenses. In contrast with prior work, I find strong evidence that incarceration has lasting negative effects on labor market outcomes after defendants have been released. I find that each additional year of 3Prior researchers have acknowledged the potential for these features to also affect their findings, but data limitations have generally limited their ability to address these concerns in any formal way. 4 MICHAEL MUELLER-SMITH incarceration reduces post-release employment by 3.6 percent points. Among felony defendants with stable pre-charge earnings incarcerated for one or more years, reemployment drops by at least 24 percent in the 5 years after being released. Misdemeanor defendants show a small increase in take-up of cash welfare payments, and felony defendants show increases in Food Stamps benefits, which provide further evidence of lasting economic hardship post-release. The impacts of incarceration extend beyond recidivism and labor market outcomes. Incarceration appears to negatively impact family formation and stability as measured through marriage and divorce activity. While incarcerated, young felony defendants exhibit significantly lower rates of marriage that are not compensated post-release indicating a net decline in marriage rather than a temporal shift. Further supporting this conclusion, I find that divorce rates among older felons increase while in prison and post-release. Using these new estimates, I reevaluate the welfare impacts of incarceration. Because I cannot measure general deterrence effects in my research design, the cost benefit exercise is partial in nature and only accounts for the administrative expenses, criminal behavior effects and economic impacts associated with the defendantâ€™s own outcomes. Using the most conservative estimates, I find that a one-year prison term for marginal defendants decreases social welfare by $56,200 to $66,800 of which negative impacts to economic activity account for 41 to 48 percent of overall costs. In order for this sentence to be neutral in social welfare terms, a one-year prison term for a marginal (low-risk) offender would need to deter at least 0.4 rapes, 2.2 assaults, 2.5 robberies, 62 larcenies or 4.8 habitual drug users in the general population. The remainder of this paper organized i",2014,
Zupan's Markets Features Verlasso Salmon At The Taste Of Zupan's | Verlasso,"Verlasso joined 30 premium food vendors at the Taste of Zupanâ€™s, a gourmet tasting event with proceeds providing holiday meals for families in need.",2012,
The Political Feasibility of Postponing Retirement,"Conventional economic wisdom suggests that to face the aging process, social security systems will have to be retrenched. In particular, retirement age will have to be largely increased. Yet, is this policy measure feasible in OECD countries? Since the answer belongs mainly to the realm of politics, I evaluate the political feasibility of postponing retirement under aging. After calibrating the model to France, Germany, Italy, Spain, the UK, and the US, I simulate the effects for the 2050 demographic, economic and political scenario. Galasso and Profeta (2004) suggested that a higher effective retirement age always decreases the size of the system chosen by the voters, often increases its generosity, and may be the only viable solution to pension system problems in the face of population aging. This paper shows that this policy measure is also politically feasible, as a majority of the voters will be willing to support a rise in the retirement age, due to the negative income effect induced by aging â€“ via the large social security system.",2012,CESifo DICE report
Romanization in the East: a case study: Sagalassos and Pisidia (SW Turkey),"The Romanization of the provinces under the Empire has become one of the hottest topics in current archaeological and historical research. In the past, Romanization was thought of as a 'deliberate' promulgation of 'Roman' art, technology, religion and customs by Roman authorities at the expense of the conquered civilizations and nations. Meanwhile, it has become clear that this picture was much more complex, that it was not a unilateral but a bilateral transfer of culture, and even multi-directional. In the following pages, I attempt to reconstruct the effect on a middle-sized provincial town in the east, that is Pisidian Sagalassos, of incorporation into the Roman empire. A comparison of the urban developments at Sagalassos between the Hellenistic period and the three centuries that followed the installation of the Principate, shows a complete transformation of the urban settlement, both in character and morphology.",2002,
Modal properties for a small ship - A comparison of Vlassov-Timoshenko beam theory and two dimensional FEM modelling with full scale measurements,"When analyzing the dynamic properties of a ship performing bending or torsional vibrations, the determination of the ship's modal properties is an important intermediate step. In this paper the hull is modelled with a two-dimensional finite element method for vertical modes and with a Vlassov-Timoshenko beam model for vertical and coupled torsional-horizontal modes. The entrained water is calculated with a finite volume method and represented so as to be independent of mode shape. The resulting natural frequencies and mode shapes are then compared with extensive full scale measurements. The correlation between measurements and calculations is essentially the same for the two models, with a mean discrepancy of about 6%.",1984,
Standard-M odelBundles on N on-Sim ply Connected Calabi{Yau Threefolds,"A bstract:W egiveaproofoftheexistenceofG = SU (5),stableholom orphicvectorbundles on elliptically bered Calabi{Yau threefolds with fundam entalgroup Z 2. The bundles we construct have Euler characteristic 3 and an anom aly that can be absorbed by M -theory ve-branes.Such bundlesprovide the basisforconstructing the standard m odelin heterotic M -theory. They are also applicable to vacua ofthe weakly coupled heterotic string. W e explicitly presenta classofthreefam ily m odelswith gaugegroup SU (3)C SU (2)L U (1)Y .",2002,
Phenotypic plasticity in life-history traits of femaleThalassoma bifasciatum (Pisces: Labridae): 2. Correlation of fecundity and growth rate in comparative studies,"SynopsisThe cost of reproduction is a central concept in theories of life-history evolution. One way to empirically examine the tradeoff between current reproduction and future reproductive prospects is to use natural intraspecific variation in life-history traits. However, this approach is complicated by the sensitivity of life-history traits to variation in the level of resources. We report here an attempt to measure the cost of increasing reproductive activity in populations of female bluehead wrasse,Thalassoma bifasciatum, a coral-reef fish. All of the significant correlations of fecundity and growth rate were positive, in contradiction to the tradeoff predicted by the cost concept. In one of two regions studied, the populations with relatively high mean growth rate had a relatively large mean fecundity. The trait means were also positively associated over time: in months of rapid growth, female reproductive activity was high. Even after removing the effects of habitat and time period in a comparison of individual traits, no growth cost to reproduction appears. Variation in the abundance of resources over space and time is likely to interfere with the measurement of the cost of reproduction in many natural systems.",2005,Environmental Biology of Fishes
Variable Selection for Partially Linear Varying Coefficient Quantile Regression Model,"In this paper, we propose a variable selection procedure for partially linear varying coefficient model under quantile loss function with adaptive Lasso penalty. The functional coefficients are estimated by B-spline approximations. The proposed procedure simultaneously selects significant variables and estimates unknown parameters. The major advantage of the proposed procedures over the existing ones is easy to implement using existing software, and it requires no specification of the error distributions. Under the regularity conditions, we show that the proposed procedure can be as efficient as the Oracle estimator, and derive the optimal convergence rate of the functional coefficients. A simulation study and a real data application are undertaken to assess the finite sample performance of the proposed variable selection procedure.",2013,International Journal of Biomathematics
Regularization and clustering of neuron spike data with the Group Lasso,"We model neuron spiking data by a set of logistic regressions, one for each neuron. We regularize them with a Group Lasso penalty on the pairwise differences between coefficient vectors.",2006,
FASt global convergence of gradient methods for solving regularized M-estimation,"We analyze the convergence rates of composite gradient methods for solving problems based on regularized M-estimators, working within a high-dimensional framework that allows the data dimension d to grow with (and possibly exceed) the sample size n. This high-dimensional structure precludes the usual global assumptions-namely, strong convexity and smoothness conditions-that underlie much of classical optimization analysis. We define appropriately restricted versions of these conditions, and show that they are satisfied with high probability for various statistical models. Under these conditions, our theory guarantees that composite gradient descent has a globally geometric rate of convergence up to the statistical precision of the model, meaning the typical distance between the true unknown parameter Î¸* and an optimal solution Î¸Ì‚. This result is substantially sharper than previous results, which yielded sublinear convergence or linear convergence up to the noise level, and builds on our earlier work for constrained estimation problems. Our analysis applies to a wide range of M-estimators and statistical models, including sparse linear regression using Lasso (â„“1-regularized regression); group Lasso for block sparsity; log-linear models with regularization; low-rank matrix recovery using nuclear norm regularization; and matrix decomposition. Overall, our analysis reveals interesting connections between statistical precision and computational efficiency in high-dimensional estimation.",2012,2012 IEEE Statistical Signal Processing Workshop (SSP)
PME familiales quÃ©bÃ©coises,"La litterature dediee a la succession des entreprises familiales s'interesse essentiellement a la relation entre un predecesseur et un successeur unique. Or dans la pratique, il est frequent qu'une fratrie prenne la suite. En s'appuyant sur quatre cas de successions familiales quebecoises, cet article s'interesse a l'impact des parties prenantes externes a la famille (PPEF) dans la creation et le fonctionnement d'une co-succession en fratrie. Nous classons ces PPEF en deux categories : les permanentes et les occasionnelles. Nous montrons comment elles influencent la naissance de la succession en fratrie et en quoi elles repre- sentent un facteur cle de succes dans le fonctionnement de la co-succession.",2014,
"Draft Genome Sequences of Thalassobacter Strains 1CONIMAR09 and 16PALIMAR09, Two Members of the Roseobacter Lineage Isolated from Coastal Areas of the Mediterranean Sea around Mallorca Island","We report the draft genome sequence of two new members of the Roseobacter lineage, Thalassobacter strains 1CONIMAR09 and 16PALIMAR09, which were isolated from the seawater coast of Mallorca Island. Each genome harbored putative genes for obtaining energy by chemolithotrophy and making aerobic anoxygenic photosynthesis.",2015,Genome Announcements
La mortalitÃ© des enfants du Sahel en ville et au village,"Lalou (Richard), LeGrand (Thomas K.).-La mortalidad infantil de Sahel en medio urbano y rural En poco mas de cuarenta aÅˆos, las ciudades del Tercer Mundo han pasado de tener los mejores a los peores niveles de salud. Durante los aÅˆos cincuenta y sesenta, la ciudad era un espacio privilegiado para la salud. Desde entonces, su rapido crecimiento no ha ido acompaÅˆado, en general, de una expansion suficiente de las estructuras sanitarias y medicas y de los bienes colectivos ; ciudad se ha convertido en un sinonimo de insalubridad y preca- riedad, especialmente en las zonas que han crecido sin planificacion. Este articulo propone en primer lugar un marco conceptual para el analisis de la salud infantil segun lugar de resi- dencia. A continuacion pretende verificar si la ciudad sigue siendo, en el area africana del Sahel, un espacio privilegiado para la salud; para ello utiliza datos procedentes de las En- cuestas sobre Mortalidad Infantil en el Sahel, llevadas a cabo en las ciudades de Bamako y Bobo-Dioulasso y en una region rural de Senegal. Estos datos muestran una fuerte sobre- mortalidad infantil en las zonas rurales, especialmente a los dos aÅˆos. Estas diferencias per- sisten incluso en comparacion con los barrios urbanos mas pobres. En conjunto, sin embargo, el efecto de las caracteristicas individuales y del hogar sobre la mortalidad infantil son similares en medio urbano y rural.",1996,Population
Evaluation of multiple variate selection methods from a biological perspective: a nutrigenomics case study,"Genomics-based technologies produce large amounts of data. To interpret the results and identify the most important variates related to phenotypes of interest, various multivariate regression and variate selection methods are used. Although inspected for statistical performance, the relevance of multivariate models in interpreting biological data sets often remains elusive. We compare various multivariate regression and variate selection methods applied to a nutrigenomics data set in terms of performance, utility and biological interpretability. The studied data set comprised hepatic transcriptome (10,072 predictor variates) and plasma protein concentrations [2 dependent variates: Leptin (LEP) and Tissue inhibitor of metalloproteinase 1 (TIMP-1)] collected during a high-fat diet study in ApoE3Leiden mice. The multivariate regression methods used were: partial least squares â€œPLSâ€; a genetic algorithm-based multiple linear regression, â€œGA-MLRâ€; two least-angle shrinkage methods, â€œLASSOâ€ and â€œELASTIC NETâ€; and a variant of PLS that uses covariance-based variate selection, â€œCovProc.â€ Two methods of ranking the genes for Gene Set Enrichment Analysis (GSEA) were also investigated: either by their correlation with the protein data or by the stability of the PLS regression coefficients. The regression methods performed similarly, with CovProc and GA performing the best and worst, respectively (R-squared values based on â€œdouble cross-validationâ€ predictions of 0.762 and 0.451 for LEP; and 0.701 and 0.482 for TIMP-1). CovProc, LASSO and ELASTIC NET all produced parsimonious regression models and consistently identified small subsets of variates, with high commonality between the methods. Comparison of the gene ranking approaches found a high degree of agreement, with PLS-based ranking finding fewer significant gene sets. We recommend the use of CovProc for variate selection, in tandem with univariate methods, and the use of correlation-based ranking for GSEA-like pathway analysis methods.",2012,Genes & Nutrition
A comparison of genomic selection methods for breeding value prediction,"Recent advances in molecular genetics techniques have made dense marker maps available, and the prediction of breeding value at the genome level has been employed in genetics research. However, an increasingly large number of markers raise both statistical and computational issues in genomic selection (GS), and many methods have been developed for genomic prediction to address these problems, including ridge regression-best linear unbiased prediction (RR-BLUP), genomic best linear unbiased prediction, BayesA, BayesB, BayesCÏ€, and Bayesian LASSO. In this paper, these methods were compared regarding inference under different conditions, using real data from a wheat data set and simulated scenarios with a small number of quantitative trait loci (QTL) (20), a moderate number of QTL (60, 180) and an extreme number of QTL (540). This study showed that the genetic architecture of a trait should be fully considered when a GS method is chosen. If a small amount of loci had a large effect on a trait, great differences were found between the predictive ability of various methods and BayesCÏ€ was recommended. Although there was almost no significant difference between the predictive ability of BayesCÏ€ and BayesB, BayesCÏ€ is more feasible than BayesB for real data analysis. If a trait was controlled by a moderate number of genes, the absolute differences between the various methods were small, but BayesA was also found to be the most accurate method. Furthermore, BayesA was widely adaptable and could perform well with different numbers of QTL. If a trait was controlled by an extreme number of minor genes, almost no significant differences were detected between the predictive ability of various methods, but RR-BLUP slightly outperformed the others in both simulated scenarios and real data analysis, thus demonstrating its robustness and indicating that it was quite effective in this case.",2015,Science Bulletin
A Machine Learning Approach to Forecast Bitcoin Prices,"Bitcoin is an established cryptographic digital currency whose value lays in the computational complexity rather than a physical commodity. Bitcoin is an open source software program with three aspects. (i) Peer-to-Peer network â€“ low barrier entry; (ii) Mining â€“ inevitable concentration of power; (iii) Software upgrades. The nodes on the network follow a decentralized consensus for establishing the value of ledger and updating the blockchain which serves as a single source of truth for all transactions. As cryptocurrencies are developing more compelling utilities, creating ever faster and safer payment systems they are shifting the â€œmoney paradigmâ€. Bitcoins are an evolution in money and provide a unique opportunity to forecast their price unlike the existing fiat currencies. The goal of this paper is to implement, train and evaluate several machine learning models in order to predict the price of the most popular cryptocurrency â€“ Bitcoins. The various machine learning algorithms employed are â€“ Linear Regression, K-Nearest Neighbors, Ridge Regression, Lasso Regression, Polynomial Regression, Linear Support Vector Machine, and Kernel Support Vector Machine. General Terms Bitcoins, Cryptocurrency, Blockchain, Machine Learning, Regression, Forecasting.",2018,International Journal of Computer Applications
Study on Operation Duration of Electrolytic Chlorination System,"The vital signs of acorn barnacle as the main kind of thalassophyte are studyed under relating temperatures duration,so is the flow path of the circulating water system in Binhai Power Plant could be adhered,polluted and blocked. The determination basis for duration of anti-fouling device putting-into operation is proposed. It is clear that there is no need of killing the thalassophyte in the circulating water system when the sea water temperature below five degrees centigrade and thus saving one-third of electrical energy per year. The theoreticle gaps are filled in in our country and the economical benefits are achieved.",2014,Northeast Electric Power Technology
The key actors maintaining elders in functional autonomy in Bobo-Dioulasso (Burkina Faso),"BackgroundGlobally, a significant increase in functional disability among the elderly is expected in the near future. It is therefore vital to begin considering how Sub-Saharan Africa countries can best start building or strengthening the care and support system for that target population. Study objectives are: 1) identify the key actors of the social system who maintain elders in functional autonomy at home in Bobo-Dioulasso (Burkina Faso) and 2) to describe the functional status of older people living at home.MethodsWe conducted a longitudinal descriptive study among the elderly aged 60 and above (351). Their functional status was evaluated using the Functional Autonomy Measurement System (SMAF). Data analysis was done using the statistical software package STATA (SE11).ResultsIn Bobo-Dioulasso, 68% of seniors have good functional capacity or a slight incapacity and 32% have moderate to severe incapacities. Older people die before (3%) or during (14%) moderate to severe disabilities. This would mean that the quality of medical and/or social care is not good for maintaining functional autonomy of older people with moderate to severe disabilities. Two main groups of people contribute to maintain elders in functional autonomy: the elderly themselves and their family. Community, private or public structures for maintaining elders in functional autonomy are non-existent. The social system for maintaining elders in functional autonomy is incomplete and failing. In case of functional handicap at home, the elders die. But stakeholders are not conscious of this situation; they believe that this system is good for maintaining elders in functional autonomy.ConclusionIt is likely that the absence of formal care and support structure likely shortens the lifespan of severely disabled older people. Stakeholders have not yet looked at this possibility. The stakeholders should seriously think about: 1) how to establish the third level of actors who can fulfill the needs to maintain elders in functional autonomy that are not satisfied by others (family members or the older individuals themselves), and 2) how to reinforce the role of each actor and the collaboration between the different groups of people of this system.",2014,BMC Public Health
ë°ì´í„° ë§ˆì´ë‹ ê¸°ë²•ì„ í†µí•œ êµìœ¡ íŒ¨ë„ë°ì´í„° ë¶„ì„,"ëª…í™•í•œ ê¸°ì¡´ ì´ë¡ ì´ ì—†ì–´ë„ ì¶•ì ëœ ë°ì´í„° ë¶„ì„ì„ í†µí•˜ì—¬ ê²°ê³¼ë¥¼ ë„ì¶œí•  ìˆ˜ ìžˆëŠ” ë°ì´í„° ë§ˆì´ë‹ ê¸°ë²•ì´ ë¹…ë°ì´í„° ì‹œëŒ€ì— ê°ê´‘ì„ ë°›ê³  ìžˆë‹¤. ìˆ˜ë ´ ë˜ëŠ” ê³¼ì í•© ë“±ì˜ ë¬¸ì œë¡œ ì¸í•´ ì†Œìˆ˜ì˜ ë³€ìˆ˜ë§Œì„ ëª¨í˜•í™”í•´ ì˜¨ ê¸°ì¡´ ì—°êµ¬ë°©ë²•ê³¼ ë‹¬ë¦¬, ë°ì´í„° ë§ˆì´ë‹ ê¸°ë²•ìœ¼ë¡œëŠ” ìˆ˜ë°± ê°œì˜ ë³€ìˆ˜ë¥¼ í•œ ëª¨í˜•ì— íˆ¬ìž…í•  ìˆ˜ ìžˆìœ¼ë©°, ë”°ë¼ì„œ ì—°êµ¬ë°©ë²•ì  ì¸¡ë©´ì—ì„œ ì—¬ëŸ¬ ìž¥ì ì„ ê°€ì§„ë‹¤. êµ­ê°€ê¸°ê´€ì—ì„œ ì‹­ìˆ˜ë…„ ê°„ ìˆ˜ì§‘í•´ ì˜¨ êµìœ¡ íŒ¨ë„ìžë£ŒëŠ” ì–‘ì  Â· ì§ˆì ì¸ ì¸¡ë©´ì—ì„œ ë°ì´í„° ë§ˆì´ë‹ê¸°ë²• ì ìš©ì— ì ì ˆí•˜ë‹¤. ë³¸ ì—°êµ¬ëŠ” ë¹…ë°ì´í„° ë¶„ì„ì—ì„œ ìžì£¼ ì´ìš©ë˜ëŠ” ë²Œì íšŒê·€ëª¨í˜•ì¸ LASSOë¥¼ KYPS 5ì°¨ ìžë£Œë¶„ì„ì— ì´ìš©í•¨ìœ¼ë¡œì¨ ë°ì´í„° ë§ˆì´ë‹ ê¸°ë²•ì˜ êµìœ¡ íŒ¨ë„ìžë£Œ ì ìš© ì‚¬ë¡€ë¥¼ ì œì‹œí•˜ì˜€ë‹¤. ìˆ˜ì‹­ ê°œì˜ ë³€ìˆ˜ë§Œì„ ì´ìš©í•˜ì˜€ë˜ ê¸°ì¡´ ì—°êµ¬ì™€ ë‹¬ë¦¬, ë³¸ ì—°êµ¬ëŠ” ì´ 315ê°œì˜ ì„¤ëª…ë³€ìˆ˜ë¥¼ í•œ ëª¨í˜•ì— íˆ¬ìž…í•˜ì—¬ 15ê°œì˜ ë³€ìˆ˜ë¥¼ ì„ íƒí•˜ì˜€ë‹¤. ê¸°ì¡´ ì—°êµ¬ì—ì„œ ëª¨í˜•í™”ëœ ë³€ìˆ˜ë¿ë§Œ ì•„ë‹ˆë¼ ìƒˆë¡œìš´ ë³€ìˆ˜ë¥¼ ë°œêµ´í•  ìˆ˜ ìžˆì—ˆë‹¤. ë³¸ ì—°êµ¬ì˜ í•¨ì˜ ë° í›„ì† ì—°êµ¬ ì£¼ì œ ë˜í•œ ë…¼ì˜ë˜ì—ˆë‹¤.",2016,
WasserzÃ¤hler mit nach aussen gedichtetem Messeinsatz,"Wasserzahler mit
einem in ein in der Wasser-Rohrleitung angeordnetes wasserfuhrendes
Anschlussgehause
(1) einschraubbaren Metallgehause
(2), das einen aus Kunststoff bestehenden Messeinsatz (4) umfassend
ein Kanalring-Oberteil (5), ein Kanalring-Unterteil (6) und einen
zwischen diesen gelagerten Flugel
dichtend auf die Auslassoffnung
(10) des Anschlussgehauses
(1) druckt,
wobei der aus Kunststoff bestehende Messeinsatz (4) mit seinem unteren
Rand unmittelbar dichtend auf eine die Einlassoffnung (18) des Anschlussgehauses umgebende
Dichtung (9) angedruckt
wird, derart, dass kein Wasser in Kontakt mit dem Metallgehause (2)
gelangen kann.",2003,
Optimal beam pattern design for very large sensor arrays with sparse sampling,"The main goal of this work is to adaptively employ a large set of microphone sensors distributed in multiple dimensions to scan an acoustic field. Processing data from a large set of sensors will necessarily involve intelligent definition of suitable subsets of sensors active at various times. This paper presents a novel method for optimal beam pattern design for large scale sensor arrays using convex and non-convex optimization techniques to define optimal subsets of sensors capable to select a target location while suppressing a large number of interferences. The first of two optimization techniques we present, uses a LASSO-type approach to convexify the corresponding combinatorial optimization problem. The second approach employs simulated annealing to search for optimal solutions with a fixed size subset of active sensors. Our numerical simulations show that for scenarios of practical interest, the convex optimization solution is almost optimal.",2013,"2013 Asilomar Conference on Signals, Systems and Computers"
Parallel Selective Algorithms for Nonconvex Big Data Optimization,"We propose a decomposition framework for the parallel optimization of the sum of a differentiable (possibly nonconvex) function and a (block) separable nonsmooth, convex one. The latter term is usually employed to enforce structure in the solution, typically sparsity. Our framework is very flexible and includes both fully parallel Jacobi schemes and Gauss-Seidel (i.e., sequential) ones, as well as virtually all possibilities â€œin betweenâ€ with only a subset of variables updated at each iteration. Our theoretical convergence results improve on existing ones, and numerical results on LASSO, logistic regression, and some nonconvex quadratic problems show that the new method consistently outperforms existing algorithms.",2015,IEEE Transactions on Signal Processing
Android Development for Beginners - Volume 1,"It's not just a book-- It's a full training course with master instructor Mark Lassoff. Your purchase of the book includes the book itself and access to over 7 hours of video instruction! In these videos you can code along with the instrutor as the different facets of Android Development are explained and demonstrated. Unless you've been sleeping for the last couple of years, you know that Mobile is H-O-T! And the most popular mobile platform in the world? That's Android. Do you have a great idea for an App that you'd love to get on the market? Does your company need an app to grow it's online audience? Perhaps you've always wanted to learn Android Development for fun? Whatever your reason-- Android Development for Beginners is for you! This course covers the skills needed to produce competent, quality, error-free Android applications. More importantly, this course builds the foundation you need to learn more advanced skills and create professional, quality applications as you learn more. While this is a course for beginners, to be successful you need to know the basics of Java. The course will review the more complex Java used in the Android ecosystem, but you should understand Java Basics-- Variables, Loops, Functions, Conditionals should be enough. Everything you need to participate in the course is 100% free and downloadable. Good luck-- and see you in class!",2012,
13e congrÃ¨s de lâ€™International Society for Antiviral Research (ISAR),"Ce congres, organise du 16 au 21 avril 2000 a Baltimore sous la direction de George J. Galasso, avait un comite scientifique dirige par Richard J. Whitley. Comme a l'accoutumee, ce fut une succession de presentations consacrees aux differentes familles de virus et organisees, pour le confort de tous, en sessions orales plenieres et en sessions d'affiches laissees en place toute la duree du congres.",2000,Virologie
Automated MRI prediction of Alzheimer's disease development by machine learning methods,"Alzheimerâ€™s is an irreversible brain disease that impairs memory, thinking and behavior and leads ultimately to death. Research has shown that individuals with MCI (mild cognitive impairment), the pre-stage of Alzheimerâ€™s, have an increased risk of developing Alzheimerâ€™s over the next few years [1]. It is useful and important to diagnose and predict MCIâ€™s conversion to Alzheimerâ€™s as early as possible for appropriate treatment. In our study, we use numerous machine learning, feature selection as well as clustering methods for this prediction purpose. High precision of prediction is observed for both 10-fold and 2-fold cross-validation. We also use L1 and L2-norm shrinkage terms to control the model complexity. As a result, the prediction error is reduced. These findings illustrate that machine learning methods accurately and reliably predict MCIâ€™s conversion, and potentially provide a great assistance to medical diagnosis. Index words: Alzheimerâ€™s disease, MRI, Mild cognitive impairment, SVM, Logistic, Lasso, Loss function, Regularization Automated MRI Prediction of Alzheimerâ€™s Disease Development by Machine Learning Methods",2011,
"Thalassobacter arenae sp. nov., isolated from sea sand in Korea.","A Gram-negative, short rod-shaped bacterium, strain GA2-M15(T), was isolated from a sea-sand sample at Homi Cape, Pohang city, Republic of Korea. 16S rRNA gene sequence analysis demonstrated that this isolate was unique, showing 95.9 % sequence similarity to the type strain of Thalassobacter stenotrophicus and similarities of 94.0-95.2 % to the type strains of species of the genera Octadecabacter (94.4-95.2 %), Jannaschia (94.0-94.4 %) and Thalassobius (94.0-94.7 %). Chemotaxonomic characteristics (diphosphatidylglycerol, phosphatidylglycerol, phosphatidylethanolamine and phosphatidylcholine as the major polar lipids and C(18 : 1)omega7c as the predominant fatty acid) and DNA G+C content (56 mol%) were also similar to those of Thalassobacter stenotrophicus. 16S rRNA gene sequence similarity, physiological properties and some fatty acid components showed that strain GA2-M15(T) could be differentiated from Thalassobacter stenotrophicus. On the basis of these results, strain GA2-M15(T) is considered to represent a novel species of the genus Thalassobacter, for which the name Thalassobacter arenae sp. nov. is proposed. The type strain is GA2-M15(T) (=KACC 12675(T) =DSM 19593(T)).",2009,International journal of systematic and evolutionary microbiology
Prozesskammermodul zum gleichzeitigen Abscheiden von Schichten auf mehreren Substraten,"Die
Erfindung betrifft eine Vorrichtung zum gleichzeitigen Abscheiden
jeweils mindestens einer Schicht auf einer Vielzahl von Substraten
(8), wobei die schichtbildenden Komponenten in Form von Prozessgasen zusammen
mit einem Tragergas
mittelst eines von einer Zuleitung (6) gespeisten Gaseinlassorgans
(5) in eine Prozesskammer (4) eines Reaktorgehauses (1) eingebracht werden,
aus der zumindest das Tragergas
durch einen Gasauslass (9) entfernt wird, wobei die Substrate (8)
auf mindestens einen Substrathalter (7) aufliegen. Um eine herstellungstechnisch
und betriebstechnisch einfache Vorrichtung zum gleichzeitigen Abschneiden
jeweils einer Schicht auf mehreren Substraten anzugeben, wird vorgeschlagen,
dass im Reaktorgehause
eine Vielzahl von Prozesskammern (4) modulartig angeordnet sind,
die jeweils ein Gaseinlassorgan (5) aufweisen, mit dem die jeweilige Prozesskammer
(4) mit dem Prozessgas und dem Tragergas versorgt werden und
aus welchen Prozesskammern (4) zumindest das Tragergas durch einen jeweiligen
Gasauslasskanal (9) entfernt wird.",2005,
On algorithms for solving least squares problems under an L1 penalty or an L1 constraint,"Tibshirani (1996) proposed the least absolute shrinkage and selection operator (LASSO) which estimates a vector of regression coefficients by minimising the residual sum of squares subject to a constraint (penalty) on the sum of the absolute values of the coefficient estimates. In this paper, we describe several algorithms that can be used to calculate the LASSO solution.",2005,
Randomised and L1-penalty approaches to segmentation in time series and regression models,"It is a common approach in statistics to assume that the parameters of a stochastic model change. The simplest model involves parameters than can be exactly or approximately piecewise constant. In such a model, the aim is the posteriori detection of the number and location in time of the changes in the parameters. This thesis develops segmentation methods for non-stationary time series and regression models using randomised methods or methods that involve L1 penalties which force the coefficients in a regression model to be exactly zero. Randomised techniques are not commonly found in nonparametric statistics, whereas L1 methods draw heavily from the variable selection literature. Considering these two categories together, apart from other contributions, enables a comparison between them by pointing out strengths and weaknesses. This is achieved by organising the thesis into three main parts. 
First, we propose a new technique for detecting the number and locations of the change-points in the second-order structure of a time series. The core of the segmentation procedure is the Wild Binary Segmentation method (WBS) of Fryzlewicz (2014), a technique which involves a certain randomised mechanism. The advantage of WBS over the standard Binary Segmentation lies in its localisation feature, thanks to which it works in cases where the spacings between change-points are short. Our main change-point detection statistic is the wavelet periodogram which allows a rigorous estimation of the local autocovariance of a piecewise-stationary process. We provide a proof of consistency and examine the performance of the method on simulated and real data sets. 
Second, we study the fused lasso estimator which, in its simplest form, deals with the estimation of a piecewise constant function contaminated with Gaussian noise (Friedman et al. (2007)). We show a fast way of implementing the solution path algorithm of Tibshirani and Taylor (2011) and we make a connection between their algorithm and the taut-string method of Davies and Kovac (2001). In addition, a theoretical result and a simulation study indicate that the fused lasso estimator is suboptimal in detecting the location of a change-point. 
Finally, we propose a method to estimate regression models in which the coefficients vary with respect to some covariate such as time. In particular, we present a path algorithm based on Tibshirani and Taylor (2011) and the fused lasso method of Tibshirani et al. (2005). Thanks to the adaptability of the fused lasso penalty, our proposed method goes beyond the estimation of piecewise constant models to models where the underlying coefficient function can be piecewise linear, quadratic or cubic. Our simulation studies show that in most cases the method outperforms smoothing splines, a common approach in estimating this class of models.",2014,
Register to Win a VerlassoÂ® Gourmet Gift Package | Verlasso,"We have hit 1,000 fans on the Verlasso Salmon Facebook page! To reward Verlasso fans, we are hosting a Verlasso Gourmet Gift Package Sweepstakes.",2013,
High-dimensional dependence modelling using Bayesian networks for the degradation of civil infrastructures and other applications,"This thesis explores high-dimensional deterioration-related problems using Bayesian networks (BN). Asset managers become more and more familiar on how to reason with uncertainty as traditional physics-based models fail to fully encompass the dynamics of large-scale degradation issues. Probabilistic dependence is able to achieve this while the ability to incorporate randomness is enticing.In fact, dependence in BN is mainly expressed in two ways. On the one hand, classic conditional probabilities that lean on thewell-known Bayes rule and, on the other hand, a more recent classof BN featuring copulae and rank correlation as dependence metrics. Both theoretical and practical contributions are presented for the two classes of BN referred to as discrete dynamic andnon-parametric BN, respectively. Issues related to the parametrization for each class of BN are addressed. For the discrete dynamic class, we extend the current framework by incorporating an additional dimension. We observed that this dimension allows to have more control on the deterioration mechanism through the main endogenous governing variables impacting it. For the non-parametric class, we demonstrate its remarkable capacity to handle a high-dimension crack growth issue for a steel bridge. We further show that this type of BN can characterize any Markov process.",2017,
DNA Copy Number Reconstruction via Regularization,"Author(s): Zhang, Zhongyang | Advisor(s): Sabatti, Chiara; Zhou, Qing | Abstract: Recent advances in genomics have underscored the surprising ubiquity of DNA copy number variants (CNVs). They carry information on the modalities of genome evolution and about the deregulation of DNA replication in cancer cells; their study can be helpful to localize tumor suppressor genes, distinguish different populations of cancerous cell, as well identify genomic variations responsible for disease phenotypes. A number of different high-throughput technologies can be used to identify copy number variable sites, and the literature documents multiple effective algorithms. We augment this literature with a focus on computational speed and simultaneous analysis of multiple sequences.One the one hand, we explore CNV reconstruction for single sample via estimation with a fused-lasso penalty. We mount a fresh attack on this difficult optimization problem by a majorization-minimization (MM) framework. We also reframe the reconstruction problem in terms of imputation via discrete optimization. This approach is easier and more accurate than parameter estimation. The accuracy of our imputations is comparable to that of hidden Markov models at a substantially lower computational cost.On the other hand, we investigate the specific problem of detecting regions where variation in copy number is relatively common in the sample at hand: this encompasses the cases of copy number polymorphisms (CNPs), related samples, technical replicates, and cancerous sub-populations from the same individual. We present a segmentation method to reconstruct CNV regions, that is based on penalized estimation and is capable of processing multiple signals jointly. Our approach is computationally very attractive and leads to sensitivity and specificity levels comparable to those of state-of-the-art specialized methodologies. Its versatility and speed make the method applicable to data obtained with a wide range of technologies and particularly useful in the initial screening stages of large data sets.Finally, we perform CNV detection and analysis in a set of pedigrees from two Central American isolate and admixed populations. We characterize CNPs in this sample in terms of their frequencies and prevalence on different genetic backgrounds.",2012,
Sequential adaptive elastic net approach for single-snapshot source localization,"This paper proposes efficient algorithms for accurate recovery of direction-of-arrivals (DoAs) of sources from single-snapshot measurements using compressed beamforming (CBF). In CBF, the conventional sensor array signal model is cast as an underdetermined complex-valued linear regression model and sparse signal recovery methods are used for solving the DoA finding problem. A complex-valued pathwise weighted elastic net (c-PW-WEN) algorithm is developed that finds solutions at the knots of penalty parameter values over a path (or grid) of elastic net (EN) tuning parameter values. c-PW-WEN also computes least absolute shrinkage and selection operator (LASSO) or weighted LASSO in its path. A sequential adaptive EN (SAEN) method is then proposed that is based on c-PW-WEN algorithm with adaptive weights that depend on previous solution. Extensive simulation studies illustrate that SAEN improves the probability of exact recovery of true support compared to conventional sparse signal recovery approaches such as LASSO, EN, or orthogonal matching pursuit in several challenging multiple target scenarios. The effectiveness of SAEN is more pronounced in the presence of high mutual coherence.",2018,The Journal of the Acoustical Society of America
Convergence analysis of a variable metric forwardâ€“backward splitting algorithm with applications,"The forwardâ€“backward splitting algorithm is a popular operator-splitting method for solving monotone inclusion of the sum of a maximal monotone operator and an inverse strongly monotone operator. In this paper, we present a new convergence analysis of a variable metric forwardâ€“backward splitting algorithm with extended relaxation parameters in real Hilbert spaces. We prove that this algorithm is weakly convergent when certain weak conditions are imposed upon the relaxation parameters. Consequently, we recover the forwardâ€“backward splitting algorithm with variable step sizes. As an application, we obtain a variable metric forwardâ€“backward splitting algorithm for solving the minimization problem of the sum of two convex functions, where one of them is differentiable with a Lipschitz continuous gradient. Furthermore, we discuss the applications of this algorithm to the fundamental of the variational inequalities problem, constrained convex minimization problem, and split feasibility problem. Numerical experimental results on LASSO problem in statistical learning demonstrate the effectiveness of the proposed iterative algorithm.",2018,Journal of Inequalities and Applications
Site Application: The Archaeological Site of Sagalassos (Turkey),"During the summer of 2015 (4â€“11 July), a passive-and-active, electrical-resistivity tomography survey of two areas at the archaeological site of Sagalassos (Turkey) were undertaken. The first one, labelled Area 1, was the excavated structures of the Roman Bath, and the second one, labelled Area 2, was the stadium area. Data were collected along non-conventional profiles using a dipoleâ€“dipole array and variable electrode spacing. To obtain their distribution within a three-dimensional volume, two physical parameters were measured: the electrical resistivity and the self-potential. For Area 1, the aim of geophysical survey was to obtain information about the structural stability. For Area 2, the aim was to investigate about the existence of tombs. A two-dimensional least-squares algorithm based on the smoothness-constrained technique, implemented in Res2Dinv software, was used in order to invert the 2D apparent resistivity data, while ErtLab software was used for 3D total-volume data distribution in the subsurface.",2019,
Development and Validation of a Prognostic Nomogram for Gastric Cancer Based on DNA Methylation-Driven Differentially Expressed Genes,"Background/Aims: The incidence of gastric cancer (GC) ranks fifth among common tumors and GC is the third leading cause of cancer-related death worldwide. The aim of this study was to develop and validate a nomogram for predicting the overall survival (OS) of patients with GC. Methods: DNA methylation (DNAm)-driven genes were identified by integrating DNAm and gene expression profiling analyses from The Cancer Genome Atlas (TCGA) GC cohort. Then, a risk score model was built based on Kaplan-Meier (K-M), least absolute shrinkage and selector operation (LASSO), and multivariate Cox regression analyses. After analyzing the clinical parameters, a nomogram was constructed and assessed. Another cohort (GSE62254) was used for external validation. Results: Thirteen differentially expressed DNAm-driven genes were narrowed down to a six-gene signature (PODN, NPY, MICU3, TUBB6 and RHOJ were hypermethylated, and MYO1A was hypomethylated), which was associated with OS (P < 0.05) after survival and LASSO regression analyses. These differentially expressed genes (DEGs) with altered DNAm statuses were included in the prognostic risk score model. The univariate Cox regression analysis indicated that risk score, age, and number of positive lymph nodes were significantly associated with survival time in GC patients. The multivariate Cox regression analysis also indicated that these variables were significant prognostic factors for GC. A nomogram including these variables was constructed, and its performance in predicting the 1-, 3- and 5-year survival outcomes of GC patients was estimated through time-dependent receiver operating characteristic (ROC) curves. In addition, the clinical benefit of this model was revealed by decision curve analysis (DCA). Pathway enrichment analysis suggested that these DNAm-driven genes might impact tumor progression by affecting signaling pathways such as the ""ECM RECEPTOR INTERACTION"" and ""DNA REPLICATION"" pathways. Conclusions: The altered status of the DNAm-driven gene signature (PODN, MYO1A, NPY, MICU3, TUBB6 and RHOJ) was significantly associated with the OS of GC patients. A nomogram incorporating risk score, age and number of positive lymph nodes can be conveniently used to facilitate the individualized prediction of OS in patients with GC.",2020,International Journal of Biological Sciences
Identification of gene biomarkers in patients with postmenopausal osteoporosis,"Postmenopausal osteoporosis (PMOP) is a major public health concern worldwide. The present study aimed to provide evidence to assist in the development of specific novel biomarkers for PMOP. Differentially expressed genesÂ (DEGs) were identified between PMOP and normal controls by integrated microarray analyses of the Gene Expression Omnibus (GEO) database, and the optimal diagnostic gene biomarkers for PMOP were identified with LASSO and Boruta algorithms. Classification models, including support vector machineÂ (SVM), decision tree and random forests models, were established to test the diagnostic value of identified gene biomarkers for PMOP. Functional annotations and proteinâ€‘protein interaction (PPI) network constructions were also conducted. Integrated microarray analyses (GSE56815, GSE13850 and GSE7429) of the GEO database were employed, and 1,320Â DEGs were identified between PMOP and normal controls. An 11â€‘gene combination was also identified as an optimal biomarker for PMOP by feature selection and classification methods using SVM, decision tree and random forest models. This combination was comprised of the following genes: DehydrogenaseÂ E1 and transketolase domain containingÂ 1 (DHTKD1), osteoclast stimulating factorÂ 1 (OSTF1), GÂ proteinâ€‘coupled receptorÂ 116 (GPR116), BCL2 interacting killer, adrenoceptor Î²1 (ADRB1), neogeninÂ 1 (NEO1), RB binding proteinÂ 4 (RBBP4), GPR87, cylicinÂ 2, EFâ€‘hand calcium binding domainÂ 1 and DEAHâ€‘box helicaseÂ 35. RBBP4 (degree=12) was revealed to be the hub gene of this PMOPâ€‘specific PPI network. Among these 11Â genes, three genes (OSTF1, ADRB1 and NEO1) were speculated to serve roles in PMOP by regulating the balance between bone formation and bone resorption, while two genes (GPR87 and GPR116) may be involved in PMOP by regulating the nuclear factorâ€‘ÎºB signaling pathway. Furthermore, DHTKD1 and RBBP4 may be involved in PMOP by regulating mitochondrial dysfunction and interacting with ESR1, respectively. In conclusion, the findings of the current study provided an insight for exploring the mechanism and developing novel biomarkers for PMOP. Further studies are required to test the diagnostic value for PMOP prior to use in a clinical setting.",2019,Molecular Medicine Reports
Can Good Politicians Compensate for Bad Institutions? Evidence from an Original Survey of Italian Mayors,"Can competent political leaders bring significant policy changes to communities otherwise doomed by â€œbadâ€ informal institutions? This question has remained unanswered because of the lack of a convincing measure of politiciansâ€™ competence. I develop a novel survey technique to overcome this challenge and apply it in interviews to 306 Italian mayors. I study the impact of mayorsâ€™ competence on the policies they enact using a difference-in-differences approach. Results show that more competent mayors are associated with better policies but the association is only present in cases where the quality of informal institutions is low. In these municipalities, the election of more competent mayors translates into a more effective use of funds, an increase in long-term investments, and better service provision without an increase in taxes. Results hold across different measures of institutional quality. âˆ—I am indebted to David Stasavage, Shanker Satyanath and Oeindrila Dube. I would also like to thank Alberto Alesina, Anthony Bertelli, Rachel BrulÃ©, Emine Deniz, Livio Di Lonardo, Vincenzo Galasso, Paola Giuliano, Miriam Golden, Dorothy Kronick, Horacio Larreguy, John Marshall, Tommaso Nannicini, Pablo Querubin, Cyrus Samii, Jacob Shapiro, Edoardo Teso, Emily West and participants to the NYU Political Economy Workshop, the 2017 Midwest Political Science Association meeting, the 2017 Petralia Workshop, the 2017 Northeast Workshop in Empirical Political Science at Princeton, the 2018 European Political Science Association meeting, and the 2018 American Political Science Association meeting for many helpful comments. I am grateful to Anci for their institutional support, to Francesco Porcelli at SOSE for sharing important data on the quality of service provision in Italian municipalities, and to Dr. Giancarlo Verde and Pasquale Recano at the Italian Ministry of Interior. Beatrice Carella, Guido Deiana, Francesca Doniselli, Gloria Gennaro, Melissa Giorgio, Davide Laporta, Carlotta Piantieri and Alessandro Rossi provided excellent research assistance. All mistakes are my own. â€ Postdoctoral Fellow, Kellogg School of Management, Northwestern University, maria.carreri@kellogg.northwestern.edu.",2019,
Empirical Study on Risk Factors for Long-Only Equity Hedge Funds in China,"This paper utilizes the multi-factor model to identify risk factors of long-only equity hedge funds in China. These factors cover asset based style factors, liquidity factor, sentiment factors and macro factors. In addition to the conventional OLS model, this paper also employs Stepwise and LASSO regression models to select the subset of effective factors for each hedge fund sample. The results show that these factors can explain over 50% of the return variations of most long-only equity hedge funds in terms of adjust R2s with different sets of effective factors. Meanwhile, from the perspective of statistical significance (|t| > 2), about 88%, 64% and 41% of long-only equity hedge funds are characterized mainly by the asset-based style factors, sentiment factors and macro factors respectively, whereas about 13% of hedge funds are explained significantly by the liquidity factor.",2019,
The Adaptive Gril Estimator with a Diverging Number of Parameters,"We consider the problem of variables selection and estimation in linear regression model in situations where the number of parameters diverges with the sample size. We propose the adaptive Generalized Ridge-Lasso (\mboxAdaGril) which is an extension of the the adaptive Elastic Net. AdaGril incorporates information redundancy among correlated variables for model selection and estimation. It combines the strengths of the quadratic regularization and the adaptively weighted Lasso shrinkage. In this article, we highlight the grouped selection property for AdaCnet method (one type of AdaGril) in the equal correlation case. Under weak conditions, we establish the oracle property of AdaGril which ensures the optimal large performance when the dimension is high. Consequently, it achieves both goals of handling the problem of collinearity in high dimension and enjoys the oracle property. Moreover, we show that AdaGril estimator achieves a Sparsity Inequality, i.e., a bound in terms of the number of non-zero components of the â€œtrueâ€ regression coefficient. This bound is obtained under a similar weak Restricted Eigenvalue (RE) condition used for Lasso. Simulations studies show that some particular cases of AdaGril outperform its competitors.",2013,Communications in Statistics - Theory and Methods
The MURALE project : image-based 3D modeling for archaeology,"Murale is a European IST project that will develop 3D capture and visualisation technology for archaeology. The project will put special emphasis on the usability on the site, by the archaeologists themselves. The paper describes techniques that are being developed by three of the Murale partners in particular. These comprise two methods to generate 3D models of objects, and approaches to deal with the textures of materials and terrain. Put together with the database and visualisation expertise brought in by the other partners, Murale will not only contribute to the enhanced visualisation of archaeological sites and finds, but also to a faster and more complete documentation of the progress of excavations. The ancient city of Sagalassos, one of the major excavation sites in the eastern part of the Mediterranean, will be used as the primary test site.",2002,
Francisco Serra Freire Of Verlasso Ensures High Quality Farmed Salmon | Verlasso,"Meet Verlasso team member Francisco Serra Freire. Freire is responsible for developing the unique salmon diets to reduce our fish in, fish out ratios.",2012,
"Discussion of ""least Angle Regression"" by Efron Et Al.","DISCUSSION OF â€œLEAST ANGLE REGRESSIONâ€ BY EFRONET AL.By Berwin A. TurlachUniversity of Western AustraliaI would like to begin by congratulating the authors (referred to belowas EHJT) for their interesting paper in which they propose a new variableselection method (LARS) for building linear models and show how their newmethod relates to other methods that have been proposed recently. I foundthe paper to be very stimulating and found the additional insight that itprovides about the Lasso technique to be of particular interest.My comments center around the question of how we can select linearmodels that conform with the marginality principle [Nelder (1977, 1994)and McCullagh and Nelder (1989)]; that is, the response surface is invariantunder scaling and translation of the explanatory variables in the model.Recently one of my interests was to explore whether the Lasso techniqueor the nonnegative garrote [Breiman (1995)] could be modiï¬ed such that itincorporates the marginality principle. However, it does not seem to be atrivial matter to change the criteria that these techniques minimize in such away that the marginality principle is incorporated in a satisfactory manner.On the other hand, it seems to be straightforward to modify the LARStechnique to incorporate this principle. In their paper, EHJT address thisissue somewhat in passing when they suggest toward the end of Section 3that one ï¬rst ï¬t main eï¬€ects only and interactions in a second step to controlthe order in which variables are allowed to enter the model. However, sucha two-step procedure may have a somewhat less than optimal behavior asthe following, admittedly artiï¬cial, example shows.Assume we have a vector of explanatory variables X =(X",2004,arXiv: Statistics Theory
Mmond-Weir Duality Theorems for Generalized Invexity Fractional Programming,"In this paper,a class of new generalized invexity concept is defined on basis of[1],and then Mond-Weir duality theorems with the weakduality theorems,strong duality theorem and converse duality theorem are proved under this new generalized invexity condition for a classof nonconvex nonlinear fractional programming.",2008,Jilin Normal University Journal
A multi-task learning formulation for predicting disease progression,"Alzheimer's Disease (AD), the most common type of dementia, is a severe neurodegenerative disorder. Identifying markers that can track the progress of the disease has recently received increasing attentions in AD research. A definitive diagnosis of AD requires autopsy confirmation, thus many clinical/cognitive measures including Mini Mental State Examination (MMSE) and Alzheimer's Disease Assessment Scale cognitive subscale (ADAS-Cog) have been designed to evaluate the cognitive status of the patients and used as important criteria for clinical diagnosis of probable AD. In this paper, we propose a multi-task learning formulation for predicting the disease progression measured by the cognitive scores and selecting markers predictive of the progression. Specifically, we formulate the prediction problem as a multi-task regression problem by considering the prediction at each time point as a task. We capture the intrinsic relatedness among different tasks by a temporal group Lasso regularizer. The regularizer consists of two components including an L2,1-norm penalty on the regression weight vectors, which ensures that a small subset of features will be selected for the regression models at all time points, and a temporal smoothness term which ensures a small deviation between two regression models at successive time points. We have performed extensive evaluations using various types of data at the baseline from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database for predicting the future MMSE and ADAS-Cog scores. Our experimental studies demonstrate the effectiveness of the proposed algorithm for capturing the progression trend and the cross-sectional group differences of AD severity. Results also show that most markers selected by the proposed algorithm are consistent with findings from existing cross-sectional studies.",2011,
Consistent Change Point Detection for Piecewise Constant Signals With Normalized Fused LASSO,"We consider the problem of offline change point detection from noisy piecewise constant signals. We propose normalized fused LASSO (FL), an extension of the FL, obtained by normalizing the columns of the sensing matrix of the LASSO equivalent. We analyze the performance of the proposed method, and in particular, we show that it is consistent in detecting change points as the noise variance tends to zero. Numerical experiments support our theoretical findings.",2017,IEEE Signal Processing Letters
Comparison of classification methods that combine clinical data and high-dimensional mass spectrometry data,"BackgroundThe identification of new diagnostic or prognostic biomarkers is one of the main aims of clinical cancer research. Technologies like mass spectrometry are commonly being used in proteomic research. Mass spectrometry signals show the proteomic profiles of the individuals under study at a given time. These profiles correspond to the recording of a large number of proteins, much larger than the number of individuals. These variables come in addition to or to complete classical clinical variables. The objective of this study is to evaluate and compare the predictive ability of new and existing models combining mass spectrometry data and classical clinical variables. This study was conducted in the context of binary prediction.ResultsTo achieve this goal, simulated data as well as a real dataset dedicated to the selection of proteomic markers of steatosis were used to evaluate the methods. The proposed methods meet the challenge of high-dimensional data and the selection of predictive markers by using penalization methods (Ridge, Lasso) and dimension reduction techniques (PLS), as well as a combination of both strategies through sparse PLS in the context of a binary class prediction. The methods were compared in terms of mean classification rate and their ability to select the true predictive values. These comparisons were done on clinical-only models, mass-spectrometry-only models and combined models.ConclusionsIt was shown that models which combine both types of data can be more efficient than models that use only clinical or mass spectrometry data when the sample size of the dataset is large enough.",2014,BMC Bioinformatics
Thalassospira sp. isolated from the oligotrophic eastern Mediterranean Sea exhibits chemotaxis toward inorganic phosphate during starvation.,"The eastern Mediterranean Sea represents an ultraoligotrophic environment where soluble phosphate limits the growth of bacterioplankton. Correspondingly, genes coding for high-affinity phosphate uptake systems and for organophosphonate utilization are highly prevalent in the plankton metagenome. Chemotaxis toward inorganic phosphate constitutes an alternative strategy to cope with phosphate limitation, but so far has only been demonstrated for two bacterial pathogens and an archaeon, and not in any free-living planktonic bacterium. In the present study, bacteria affiliated with the genus Thalassospira were found to constitute a regular, low-abundance member of the bacterioplankton that can be detected throughout the water column of the eastern Mediterranean Sea. A representative (strain EM) was isolated in pure culture and exhibited a strong positive chemotaxis toward inorganic phosphate that was induced exclusively in phosphate-starved cultures. Phosphate-depleted cells were 2-fold larger than in exponentially growing cultures, and 43% of the cells retained their motility even during prolonged starvation over 10 days. In addition, Thalassospira sp. strain EM was chemotactically attracted by complex substrates (yeast extract and peptone), amino acids, and 2-aminoethylphosphonate but not by sugar monomers. Similarly to the isolate from the eastern Mediterranean, chemotaxis toward phosphate was observed in starved cultures of the other two available isolates of the genus, T. lucentensis DSM 14000T and T. profundimaris WP0211T. Although Thalassospira sp. represents only up to 1.2% of the total bacterioplankton community in the water column of the eastern Mediterranean Sea, its chemotactic behavior potentially leads to an acceleration of nutrient cycling and may also explain the persistence of marine copiotrophs in this extremely nutrient-limited environment.",2011,Applied and environmental microbiology
Technologietransfer von der Luftfahrt in die Verkehrstechnik: Adaptive Blattverwindung fÃ¼r HubschrauberrotorblÃ¤tter - Experimentelle Ergebnisse und Ausblicke.,"Applying adaptronics to helicopters has a high potential to significantly suppress noise, reduce vibration and increase the overall aerodynamic efficiency. This paper presents recent investiga-tions on a very promising specific concept named Adaptive Blade Twist (ABT). This concept allows to directly control the twist of the helicopter blades by smart adaptive elements and thus to posi-tively influence the main rotor area which is the primary source for helicopter noise and vibration. Since the interaction of non-stationary helicopter aerodynamics and elastomechanical structural characteristics of the helicopter blades causes flight envelope limitations, vibration and noise, a good comprehension of the aerodynamics is essential for the development of structural solutions to effectively influence the local airflow conditions and finally develop the structural concept. Therefore the ABT concept will be presented with respect to these considerations. This concept is based upon the actively controlled tension-torsion-coupling of the structure. For this, an actuator is integrated within a helicopter blade that is made of anisotropic fibre composites material. Driving the actuator results in a local twist of the blade tip, in such a way that the blade can be considered as a torsional actuator. Influencing the blade twist distribution finally results in a higher aerodynamic efficiency. The paper starts with giving a review on conventional concepts and potential adaptive solutions for shape control. Hereafter, some calculations of the adaptive twist control concept are presented. These are based on a representative model in which the active part of the rotor blade is simplified with a thin-walled rectangular beam, that is structurally equivalent to a model rotor blade of the Bo105 with a scaling factor 2.54. The calculations are performed using an expanded Wlassow Theory. The results are valid for static and dynamic conditions. For the dynamic condition exces-sive deformations near the blade resonance frequency shall be utilised. Therefore, the actuated blade section has to be properly designed for this preconditions. This has been demonstrated and verified in experiments which will not be discussed in this paper. For experimental investigations on the ABT concept the skin of the outer part of the model rotor blade was manufactured of fibre composite material using the above mentioned tension-torsion-coupling effect with an additional uncoupling layer between skin and spar. In order to assess the influence of the aerodynamic effects, small scale wind tunnel tests at the RWTH Aachen were made. The experimental results have shown that near to the resonance frequency dynamic forces generate a torsional deformation of Â± 1,9 degrees at the blade tip",2001,
